# Your interest papers
---
## cs.CV
---
### DeepMend: Learning Occupancy Functions to Represent Shape for Repair. (arXiv:2210.05728v1 [cs.CV])
- Authors : Nikolas Lamb, Sean Banerjee, Natasha Kholgade
- Link : [http://arxiv.org/abs/2210.05728](http://arxiv.org/abs/2210.05728)
> ABSTRACT  :  We present DeepMend, a novel approach to reconstruct **restoration**s to fractured shapes using learned occupancy functions. Existing shape repair approaches predict low-resolution voxelized **restoration**s, or require symmetries or access to a pre-existing complete oracle. We represent the occupancy of a fractured shape as the conjunction of the occupancy of an underlying complete shape and the fracture surface, which we model as functions of latent codes using neural networks. Given occupancy samples from an input fractured shape, we estimate latent codes using an inference loss augmented with novel penalty terms that avoid empty or voluminous **restoration**s. We use inferred codes to reconstruct the **restoration** shape. We show results with simulated fractures on synthetic and real-world scanned objects, and with scanned real fractured mugs. Compared to the existing voxel approach and two baseline methods, our work shows state-of-the-art results in accuracy and avoiding **restoration** artifacts over non-fracture regions of the fractured shape.  
### Curved Representation Space of Vision Transformers. (arXiv:2210.05742v1 [cs.CV])
- Authors : Juyeop Kim, Junha Park, Songkuk Kim, Seok Lee
- Link : [http://arxiv.org/abs/2210.05742](http://arxiv.org/abs/2210.05742)
> ABSTRACT  :  Neural networks with self-attention (a.k.a. Transformers) like ViT and **Swin** have emerged as a better alternative to traditional convolutional neural networks (CNNs) for computer vision tasks. However, our understanding of how the new architecture works is still limited. In this paper, we focus on the phenomenon that Transformers show higher robustness against corruptions than CNNs, while not being overconfident (in fact, we find Transformers are actually underconfident). This is contrary to the intuition that robustness increases with confidence. We resolve this contradiction by investigating how the output of the penultimate layer moves in the representation space as the input data moves within a small area. In particular, we show the following. (1) While CNNs exhibit fairly linear relationship between the input and output movements, Transformers show nonlinear relationship for some data. For those data, the output of Transformers moves in a curved trajectory as the input moves linearly. (2) When a data is located in a curved region, it is hard to move it out of the decision region since the output moves along a curved trajectory instead of a straight line to the decision boundary, resulting in high robustness of Transformers. (3) If a data is slightly modified to jump out of the curved region, the movements afterwards become linear and the output goes to the decision boundary directly. Thus, Transformers can be attacked easily after a small random jump and the perturbation in the final attacked data remains imperceptible, i.e., there does exist a decision boundary near the data. This also explains the underconfident prediction of Transformers. (4) The curved regions in the representation space start to form at an early training stage and grow throughout the training course. Some data are trapped in the regions, obstructing Transformers from reducing the training loss.  
### Common Corruption Robustness of Point Cloud Detectors: Benchmark and **Enhancement**. (arXiv:2210.05896v1 [cs.CV])
- Authors : Shuangzhi Li, Zhijie Wang, Felix Juefei, Qing Guo, Xingyu Li, Lei Ma
- Link : [http://arxiv.org/abs/2210.05896](http://arxiv.org/abs/2210.05896)
> ABSTRACT  :  Object detection through LiDAR-based point cloud has recently been important in autonomous driving. Although achieving high accuracy on public benchmarks, the state-of-the-art detectors may still go wrong and cause a heavy loss due to the widespread corruptions in the real world like rain, snow, sensor noise, etc. Nevertheless, there is a lack of a large-scale dataset covering diverse scenes and realistic corruption types with different severities to develop practical and robust point cloud detectors, which is challenging due to the heavy collection costs. To alleviate the challenge and start the first step for robust point cloud detection, we propose the physical-aware simulation methods to generate degraded point clouds under different real-world common corruptions. Then, for the first attempt, we construct a benchmark based on the physical-aware common corruptions for point cloud detectors, which contains a total of 1,122,150 examples covering 7,481 scenes, 25 common corruption types, and 6 severities. With such a novel benchmark, we conduct extensive empirical studies on 8 state-of-the-art detectors that contain 6 different detection frameworks. Thus we get several insight observations revealing the vulnerabilities of the detectors and indicating the **enhancement** directions. Moreover, we further study the effectiveness of existing robustness **enhancement** methods based on data augmentation and data denoising. The benchmark can potentially be a new platform for evaluating point cloud detectors, opening a door for developing novel robustness **enhancement** methods.  
### ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors. (arXiv:2210.05950v1 [cs.CV])
- Authors : Chenjie Cao, Qiaole Dong, Yanwei Fu
- Link : [http://arxiv.org/abs/2210.05950](http://arxiv.org/abs/2210.05950)
> ABSTRACT  :  The image inpainting task fills missing areas of a corrupted image. Despite impressive results have been achieved recently, it is still challenging to restore corrupted images with both vivid textures and reasonable structures. Some previous methods only tackle regular textures while losing holistic structures limited by receptive fields of Convolution Neural Networks (CNNs). To this end, we study learning a Zero-initialized residual addition based Incremental Transformer on Structural priors (ZITS++), an improved model over our conference ZITS model. Specifically, given one corrupt image, we present the Transformer Structure Restorer (TSR) module to restore holistic structural priors at low image resolution, which are further upsampled by Simple Structure Upsampler (SSU) module to higher image resolution. Further, to well recover image texture details, we take the Fourier CNN Texture **Restoration** (FTR) module, which has both the Fourier and large-kernel attention convolutions. Typically, FTR can be independently pre-trained without image structural priors. Furthermore, to enhance the FTR, the upsampled structural priors from TSR are further processed by Structure Feature Encoder (SFE), and updating the FTR by a novel incremental training strategy of Zero-initialized Residual Addition (ZeroRA). Essentially, a new masking positional encoding is proposed to encode the large irregular masks. Extensive experiments on various datasets validate the efficacy of our model compared with other competitors. We also conduct extensive ablation to compare and verify various priors for image inpainting tasks.  
### Face Super-Resolution with Progressive Embedding of Multi-scale Face Priors. (arXiv:2210.06002v1 [cs.CV])
- Authors : Chenggong Zhang, Zhilei Liu
- Link : [http://arxiv.org/abs/2210.06002](http://arxiv.org/abs/2210.06002)
> ABSTRACT  :  The face super-resolution (FSR) task is to reconstruct high-resolution face images from low-resolution inputs. Recent works have achieved success on this task by utilizing facial priors such as facial landmarks. Most existing methods pay more attention to global shape and structure information, but less to local texture information, which makes them cannot recover local details well. In this paper, we propose a novel recurrent convolutional network based framework for face super-resolution, which progressively introduces both global shape and local texture information. We take full advantage of the intermediate outputs of the recurrent network, and landmarks information and facial action units (AUs) information are extracted in the output of the first and second steps respectively, rather than low-resolution input. Moreover, we introduced AU classification results as a novel quantitative metric for facial details **restoration**. Extensive experiments show that our proposed method significantly outperforms state-of-the-art FSR methods in terms of image quality and facial details **restoration**.  
### Can Artificial Intelligence Reconstruct Ancient Mosaics?. (arXiv:2210.06145v1 [cs.CV])
- Authors : Fernando Moral, Elena Merino, Pedro Reviriego, Fabrizio Lombardi
- Link : [http://arxiv.org/abs/2210.06145](http://arxiv.org/abs/2210.06145)
> ABSTRACT  :  A large number of ancient mosaics have not reached us because they have been destroyed by erosion, earthquakes, looting or even used as materials in newer construction. To make things worse, among the small fraction of mosaics that we have been able to recover, many are damaged or incomplete. Therefore, **restoration** and reconstruction of mosaics play a fundamental role to preserve cultural heritage and to understand the role of mosaics in ancient cultures. This reconstruction has traditionally been done manually and more recently using computer graphics programs but always by humans. In the last years, Artificial Intelligence (AI) has made impressive progress in the generation of images from text descriptions and reference images. State of the art AI tools such as DALL-E2 can generate high quality images from text prompts and can take a reference image to guide the process. In august 2022, DALL-E2 launched a new feature called outpainting that takes as input an incomplete image and a text prompt and then generates a complete image filling the missing parts. In this paper, we explore whether this innovative technology can be used to reconstruct mosaics with missing parts. Hence a set of ancient mosaics have been used and reconstructed using DALL-E2; results are promising showing that AI is able to interpret the key features of the mosaics and is able to produce reconstructions that capture the essence of the scene. However, in some cases AI fails to reproduce some details, geometric forms or introduces elements that are not consistent with the rest of the mosaic. This suggests that as AI image generation technology matures in the next few years, it could be a valuable tool for mosaic reconstruction going forward.  
## eess.IV
---
### Revisiting neutron propagation-based phase contrast imaging and tomography: use of phase retrieval to amplify the effective degree of brilliance. (arXiv:1909.11186v3 [eess.IV] UPDATED)
- Authors : Morten Sales, Winfried Kockelmann, ren Schmidt
- Link : [http://arxiv.org/abs/1909.11186](http://arxiv.org/abs/1909.11186)
> ABSTRACT  :  Propagation-based neutron phase-contrast tomography was demonstrated using the ISIS pulsed spallation source. The proof-of-concept tomogram with Paganin-type phase-retrieval filter applied exhibited an effective net boost of $23\pm 1$ in the signal-to-noise ratio as compared to an attenuation-based tomogram, implying a boost in the effective degree of neutron brilliance of over two orders of magnitude. This comparison is for phase retrieval versus conventional absorption with no additional collimation in place. Expressions are provided for the optimal phase-contrast geometry as well as conditions for the validity of the method. The underpinning theory is derived under the assumption of the sample being composed of a single material. The effective boost in brilliance may be employed to give reduced acquisition time, or may instead be used to keep **exposure** times fixed while improving the measured contrast.  
## cs.LG
---
### Short-term prediction of stream turbidity using surrogate data and a meta-model approach. (arXiv:2210.05821v1 [stat.ML])
- Authors : Bhargav Rele, Caleb Hogan, Sevvandi Kandanaarachchi, Catherine Leigh
- Link : [http://arxiv.org/abs/2210.05821](http://arxiv.org/abs/2210.05821)
> ABSTRACT  :  Many water-quality monitoring programs aim to measure turbidity to help guide effective management of waterways and catchments, yet distributing turbidity sensors throughout networks is typically cost prohibitive. To this end, we built and compared the ability of dynamic regression (ARIMA), long short-term memory neural nets (LSTM), and generalized additive models (GAM) to forecast stream turbidity one step ahead, using surrogate data from relatively low-cost in-situ sensors and publicly available databases. We iteratively trialled combinations of four surrogate covariates (rainfall, water level, air temperature and total global solar **exposure**) selecting a final model for each type that minimised the corrected Akaike Information Criterion. Cross-validation using a rolling time-window indicated that ARIMA, which included the rainfall and water-level covariates only, produced the most accurate predictions, followed closely by GAM, which included all four covariates. We constructed a meta-model, trained on time-series features of turbidity, to take advantage of the strengths of each model over different time points and predict the best model (that with the lowest forecast error one-step prior) for each time step. The meta-model outperformed all other models, indicating that this methodology can yield high accuracy and may be a viable alternative to using measurements sourced directly from turbidity-sensors where costs prohibit their deployment and maintenance, and when predicting turbidity across the short term. Our findings also indicated that temperature and light-associated variables, for example underwater illuminance, may hold promise as cost-effective, high-frequency surrogates of turbidity, especially when combined with other covariates, like rainfall, that are typically measured at coarse levels of spatial resolution.  
### Common Corruption Robustness of Point Cloud Detectors: Benchmark and **Enhancement**. (arXiv:2210.05896v1 [cs.CV])
- Authors : Shuangzhi Li, Zhijie Wang, Felix Juefei, Qing Guo, Xingyu Li, Lei Ma
- Link : [http://arxiv.org/abs/2210.05896](http://arxiv.org/abs/2210.05896)
> ABSTRACT  :  Object detection through LiDAR-based point cloud has recently been important in autonomous driving. Although achieving high accuracy on public benchmarks, the state-of-the-art detectors may still go wrong and cause a heavy loss due to the widespread corruptions in the real world like rain, snow, sensor noise, etc. Nevertheless, there is a lack of a large-scale dataset covering diverse scenes and realistic corruption types with different severities to develop practical and robust point cloud detectors, which is challenging due to the heavy collection costs. To alleviate the challenge and start the first step for robust point cloud detection, we propose the physical-aware simulation methods to generate degraded point clouds under different real-world common corruptions. Then, for the first attempt, we construct a benchmark based on the physical-aware common corruptions for point cloud detectors, which contains a total of 1,122,150 examples covering 7,481 scenes, 25 common corruption types, and 6 severities. With such a novel benchmark, we conduct extensive empirical studies on 8 state-of-the-art detectors that contain 6 different detection frameworks. Thus we get several insight observations revealing the vulnerabilities of the detectors and indicating the **enhancement** directions. Moreover, we further study the effectiveness of existing robustness **enhancement** methods based on data augmentation and data denoising. The benchmark can potentially be a new platform for evaluating point cloud detectors, opening a door for developing novel robustness **enhancement** methods.  
### One Positive Label is Sufficient: Single-Positive Multi-Label Learning with Label **Enhancement**. (arXiv:2206.00517v4 [cs.LG] UPDATED)
- Authors : Ning Xu, Congyu Qiao, Jiaqi Lv, Xin Geng, Ling Zhang
- Link : [http://arxiv.org/abs/2206.00517](http://arxiv.org/abs/2206.00517)
> ABSTRACT  :  Multi-label learning (MLL) learns from the examples each associated with multiple labels simultaneously, where the high cost of annotating all relevant labels for each training example is challenging for real-world applications. To cope with the challenge, we investigate single-positive multi-label learning (SPMLL) where each example is annotated with only one relevant label, and show that one can successfully learn a theoretically grounded multi-label classifier for the problem. In this paper, a novel SPMLL method named SMILE, i.e., Single-positive MultI-label learning with Label **Enhancement**, is proposed. Specifically, an unbiased risk estimator is derived, which could be guaranteed to approximately converge to the optimal risk minimizer of fully supervised learning and shows that one positive label of each instance is sufficient to train the predictive model. Then, the corresponding empirical risk estimator is established via recovering the latent soft label as a label **enhancement** process, where the posterior density of the latent soft labels is approximate to the variational Beta density parameterized by an inference model. Experiments on benchmark datasets validate the effectiveness of the proposed method.  
### Dilated FCN: Listening Longer to Hear Better. (arXiv:1907.11956v1 [cs.SD] CROSS LISTED)
- Authors : Shuyu Gong, Zhewei Wang, Tao Sun, Yuanhang Zhang, Li Xu, Jundong Liu
- Link : [http://arxiv.org/abs/1907.11956](http://arxiv.org/abs/1907.11956)
> ABSTRACT  :  Deep neural network solutions have emerged as a new and powerful paradigm for speech **enhancement** (SE). The capabilities to capture long context and extract multi-scale patterns are crucial to design effective SE networks. Such capabilities, however, are often in conflict with the goal of maintaining compact networks to ensure good system generalization. In this paper, we explore dilation operations and apply them to fully convolutional networks (FCNs) to address this issue. Dilations equip the networks with greatly expanded receptive fields, without increasing the number of parameters. Different strategies to fuse multi-scale dilations, as well as to install the dilation modules are explored in this work. Using Noisy VCTK and AzBio sentences datasets, we demonstrate that the proposed dilation models significantly improve over the baseline FCN and outperform the state-of-the-art SE solutions.  
## cs.AI
---
### Can Artificial Intelligence Reconstruct Ancient Mosaics?. (arXiv:2210.06145v1 [cs.CV])
- Authors : Fernando Moral, Elena Merino, Pedro Reviriego, Fabrizio Lombardi
- Link : [http://arxiv.org/abs/2210.06145](http://arxiv.org/abs/2210.06145)
> ABSTRACT  :  A large number of ancient mosaics have not reached us because they have been destroyed by erosion, earthquakes, looting or even used as materials in newer construction. To make things worse, among the small fraction of mosaics that we have been able to recover, many are damaged or incomplete. Therefore, **restoration** and reconstruction of mosaics play a fundamental role to preserve cultural heritage and to understand the role of mosaics in ancient cultures. This reconstruction has traditionally been done manually and more recently using computer graphics programs but always by humans. In the last years, Artificial Intelligence (AI) has made impressive progress in the generation of images from text descriptions and reference images. State of the art AI tools such as DALL-E2 can generate high quality images from text prompts and can take a reference image to guide the process. In august 2022, DALL-E2 launched a new feature called outpainting that takes as input an incomplete image and a text prompt and then generates a complete image filling the missing parts. In this paper, we explore whether this innovative technology can be used to reconstruct mosaics with missing parts. Hence a set of ancient mosaics have been used and reconstructed using DALL-E2; results are promising showing that AI is able to interpret the key features of the mosaics and is able to produce reconstructions that capture the essence of the scene. However, in some cases AI fails to reproduce some details, geometric forms or introduces elements that are not consistent with the rest of the mosaic. This suggests that as AI image generation technology matures in the next few years, it could be a valuable tool for mosaic reconstruction going forward.  
### ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding. (arXiv:2210.06155v1 [cs.CL])
- Authors : Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo, Zhenyu Zhang, Zhengjie Huang, Teng Hu, Weichong Yin, Yongfeng Chen, Yin Zhang, Shikun Feng, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang
- Link : [http://arxiv.org/abs/2210.06155](http://arxiv.org/abs/2210.06155)
> ABSTRACT  :  Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel document pre-training solution with layout knowledge **enhancement** in the whole workflow, to learn better representations that combine the features from text, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at <a href="http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout.">this http URL</a>  
# Paper List
---
## cs.CV
---
**143** new papers in cs.CV:-) 
1. Performance Deterioration of Deep Learning Models after Clinical Deployment: A Case Study with Auto-segmentation for Definitive Prostate Cancer Radiotherapy. (arXiv:2210.05673v1 [eess.IV])
2. Visual Language Maps for Robot Navigation. (arXiv:2210.05714v1 [cs.RO])
3. DeepMend: Learning Occupancy Functions to Represent Shape for Repair. (arXiv:2210.05728v1 [cs.CV])
4. TetGAN: A Convolutional Neural Network for Tetrahedral Mesh Generation. (arXiv:2210.05735v1 [cs.CV])
5. Distance Map Supervised Landmark Localization for MR-TRUS Registration. (arXiv:2210.05738v1 [cs.CV])
6. Curved Representation Space of Vision Transformers. (arXiv:2210.05742v1 [cs.CV])
7. Toward Sustainable Continual Learning: Detection and Knowledge Repurposing of Similar Tasks. (arXiv:2210.05751v1 [cs.CV])
8. Joint localization and classification of breast tumors on ultrasound images using a novel auxiliary attention-based framework. (arXiv:2210.05762v1 [eess.IV])
9. Match Cutting: Finding Cuts with Smooth Visual Transitions. (arXiv:2210.05766v1 [cs.CV])
10. Deep Active Ensemble Sampling For Image Classification. (arXiv:2210.05770v1 [cs.CV])
11. Towards Discriminative and Transferable One-Stage Few-Shot Object Detectors. (arXiv:2210.05783v1 [cs.CV])
12. Transfer Learning with Joint Fine-Tuning for Multimodal Sentiment Analysis. (arXiv:2210.05790v1 [cs.LG])
13. Robustify Transformers with Robust Kernel Density Estimation. (arXiv:2210.05794v1 [cs.LG])
14. Continuous conditional video synthesis by neural processes. (arXiv:2210.05810v1 [cs.CV])
15. Underspecification in Scene Description-to-Depiction Tasks. (arXiv:2210.05815v1 [cs.CV])
16. Controllable Radiance Fields for Dynamic Face Synthesis. (arXiv:2210.05825v1 [cs.CV])
17. AMICO: Amodal Instance Composition. (arXiv:2210.05828v1 [cs.CV])
18. SaiT: Sparse Vision Transformers through Adaptive Token Pruning. (arXiv:2210.05832v1 [cs.CV])
19. Effectiveness of the Recent Advances in Capsule Networks. (arXiv:2210.05834v1 [cs.CV])
20. Synthetic Power Analyses: Empirical Evaluation and Application to Cognitive Neuroimaging. (arXiv:2210.05835v1 [cs.CV])
21. LiveSeg: Unsupervised Multimodal Temporal Segmentation of Long Livestream Videos. (arXiv:2210.05840v1 [cs.CV])
22. SegViT: Semantic Segmentation with Plain Vision Transformers. (arXiv:2210.05844v1 [cs.CV])
23. SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models. (arXiv:2210.05861v1 [cs.CV])
24. Deep Learning for Iris Recognition: A Survey. (arXiv:2210.05866v1 [cs.CV])
25. LACV-Net: Semantic Segmentation of Large-Scale Point Cloud Scene via Local Adaptive and Comprehensive VLAD. (arXiv:2210.05870v1 [cs.CV])
26. Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation. (arXiv:2210.05872v1 [cs.CV])
27. Learning by Asking Questions for Knowledge-based Novel Object Recognition. (arXiv:2210.05879v1 [cs.CV])
28. Point Cloud Scene Completion with Joint Color and Semantic Estimation from Single RGB-D Image. (arXiv:2210.05891v1 [cs.CV])
29. DG-STGCN: Dynamic Spatial-Temporal Modeling for Skeleton-based Action Recognition. (arXiv:2210.05895v1 [cs.CV])
30. Common Corruption Robustness of Point Cloud Detectors: Benchmark and **Enhancement**. (arXiv:2210.05896v1 [cs.CV])
31. A Lower Bound of Hash Codes' Performance. (arXiv:2210.05899v1 [cs.CV])
32. PSNet: Parallel Symmetric Network for Video Salient Object Detection. (arXiv:2210.05912v1 [cs.CV])
33. Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v1 [cs.CL])
34. Solving combinational optimization problems with evolutionary single-pixel imaging. (arXiv:2210.05923v1 [cs.CV])
35. Robust Models are less Over-Confident. (arXiv:2210.05938v1 [cs.CV])
36. Decomposed Knowledge Distillation for Class-Incremental Semantic Segmentation. (arXiv:2210.05941v1 [cs.CV])
37. Dynamic Clustering Network for Unsupervised Semantic Segmentation. (arXiv:2210.05944v1 [cs.CV])
38. Self-Supervised Equivariant Regularization Reconciles Multiple Instance Learning: Joint Referable Diabetic Retinopathy Classification and Lesion Segmentation. (arXiv:2210.05946v1 [eess.IV])
39. ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors. (arXiv:2210.05950v1 [cs.CV])
40. 3D Brain and Heart Volume Generative Models: A Survey. (arXiv:2210.05952v1 [eess.IV])
41. Projective Transformation Rectification for Camera-captured Chest X-ray Photograph Interpretation with Synthetic Data. (arXiv:2210.05954v1 [cs.CV])
42. Towards Theoretically Inspired Neural Initialization Optimization. (arXiv:2210.05956v1 [cs.LG])
43. Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets. (arXiv:2210.05958v1 [cs.CV])
44. Efficient Image Super-Resolution using Vast-Receptive-Field Attention. (arXiv:2210.05960v1 [eess.IV])
45. Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation. (arXiv:2210.05968v1 [cs.CV])
46. Human Joint Kinematics Diffusion-Refinement for Stochastic Motion Prediction. (arXiv:2210.05976v1 [cs.CV])
47. RING++: Roto-translation Invariant Gram for Global Localization on a Sparse Scan Map. (arXiv:2210.05984v1 [cs.RO])
48. GGViT:Multistream Vision Transformer Network in Face2Face Facial Reenactment Detection. (arXiv:2210.05990v1 [cs.CV])
49. Distilling Knowledge from Language Models for Video-based Action Anticipation. (arXiv:2210.05991v1 [cs.CV])
50. Line Search-Based Feature Transformation for Fast, Stable, and Tunable Content-Style Control in Photorealistic Style Transfer. (arXiv:2210.05996v1 [cs.CV])
51. Estimating the Pose of a Euro Pallet with an RGB Camera based on Synthetic Training Data. (arXiv:2210.06001v1 [cs.CV])
52. Face Super-Resolution with Progressive Embedding of Multi-scale Face Priors. (arXiv:2210.06002v1 [cs.CV])
53. BEV Lane Det: Fast Lane Detection on BEV Ground. (arXiv:2210.06006v1 [cs.CV])
54. BoxMask: Revisiting Bounding Box Supervision for Video Object Detection. (arXiv:2210.06008v1 [cs.CV])
55. VL4Pose: Active Learning Through Out-Of-Distribution Detection For Pose Estimation. (arXiv:2210.06028v1 [cs.CV])
56. Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning. (arXiv:2210.06031v1 [cs.CV])
57. Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning. (arXiv:2210.06044v1 [cs.CV])
58. Teeth3DS: a benchmark for teeth segmentation and labeling from intra-oral 3D scans. (arXiv:2210.06094v1 [cs.CV])
59. M$^3$Video: Masked Motion Modeling for Self-Supervised Video Representation Learning. (arXiv:2210.06096v1 [cs.CV])
60. Reconstructing Personalized Semantic Facial NeRF Models From Monocular Video. (arXiv:2210.06108v1 [cs.GR])
61. Uplift and Upsample: Efficient 3D Human Pose Estimation with Uplifting Transformers. (arXiv:2210.06110v1 [cs.CV])
62. Efficient Gaussian Process Model on Class-Imbalanced Datasets for Generalized Zero-Shot Learning. (arXiv:2210.06120v1 [cs.CV])
63. Regularized Graph Structure Learning with Semantic Knowledge for Multi-variates Time-Series Forecasting. (arXiv:2210.06126v1 [cs.LG])
64. Can Artificial Intelligence Reconstruct Ancient Mosaics?. (arXiv:2210.06145v1 [cs.CV])
65. VCSE: Time-Domain Visual-Contextual Speaker Extraction Network. (arXiv:2210.06177v1 [cs.CV])
66. Convolutional Neural Network-Based Image Watermarking using Discrete Wavelet Transform. (arXiv:2210.06179v1 [eess.IV])
67. Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules. (arXiv:2210.06184v1 [cs.CV])
68. Gotcha: A Challenge-Response System for Real-Time Deepfake Detection. (arXiv:2210.06186v1 [cs.CR])
69. Anomaly Detection using Generative Models and Sum-Product Networks in Mammography Scans. (arXiv:2210.06188v1 [cs.CV])
70. Pose-Guided Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2210.06192v1 [cs.CV])
71. Hierarchical Instance Mixing across Domains in Aerial Segmentation. (arXiv:2210.06216v1 [cs.CV])
72. Latency-aware Spatial-wise Dynamic Networks. (arXiv:2210.06223v1 [cs.CV])
73. On the Generalizability of ECG-based Stress Detection Models. (arXiv:2210.06225v1 [cs.LG])
74. Explore Contextual Information for 3D Scene Graph Generation. (arXiv:2210.06240v1 [cs.CV])
75. What can we learn about a generated image corrupting its latent representation?. (arXiv:2210.06257v1 [cs.CV])
76. Event-based Non-Rigid Reconstruction from Contours. (arXiv:2210.06270v1 [cs.CV])
77. Visual Prompting for Adversarial Robustness. (arXiv:2210.06284v1 [cs.CV])
78. An Efficient and Robust Object-Level Cooperative Perception Framework for Connected and Automated Driving. (arXiv:2210.06289v1 [cs.MA])
79. Two-stream Network for ECG Signal Classification. (arXiv:2210.06293v1 [eess.SP])
80. SeKron: A Decomposition Method Supporting Many Factorization Structures. (arXiv:2210.06299v1 [cs.CV])
81. FontTransformer: Few-shot High-resolution Chinese Glyph Image Synthesis via Stacked Transformers. (arXiv:2210.06301v1 [cs.CV])
82. Semantic Cross Attention for Few-shot Learning. (arXiv:2210.06311v1 [cs.CV])
83. Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers. (arXiv:2210.06313v1 [cs.LG])
84. AISFormer: Amodal Instance Segmentation with Transformer. (arXiv:2210.06323v1 [cs.CV])
85. CoRRECT: A Deep Unfolding Framework for Motion-Corrected Quantitative R2* Mapping. (arXiv:2210.06330v1 [eess.IV])
86. ViewBirdiformer: Learning to recover ground-plane crowd trajectories and ego-motion from a single ego-centric view. (arXiv:2210.06332v1 [cs.CV])
87. A Self-attention Guided Multi-scale Gradient GAN for Diversified X-ray Image Synthesis. (arXiv:2210.06334v1 [eess.IV])
88. A deep learning network with differentiable dynamic programming for retina OCT surface segmentation. (arXiv:2210.06335v1 [eess.IV])
89. Self-Attention Message Passing for Contrastive Few-Shot Learning. (arXiv:2210.06339v1 [cs.CV])
90. MFFN: Multi-view Feature Fusion Network for Camouflaged Object Detection. (arXiv:2210.06361v1 [cs.CV])
91. A Comparative Study on 1.5T-3T MRI Conversion through Deep Neural Network Models. (arXiv:2210.06362v1 [eess.IV])
92. AdaNorm: Adaptive Gradient Norm Correction based Optimizer for CNNs. (arXiv:2210.06364v1 [cs.CV])
93. A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v1 [cs.CV])
94. Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild. (arXiv:2007.12107v2 [cs.CV] UPDATED)
95. Learning Independent Instance Maps for Crowd Localization. (arXiv:2012.04164v3 [cs.CV] UPDATED)
96. Deep Image Harmonization by Bridging the Reality Gap. (arXiv:2103.17104v3 [cs.CV] UPDATED)
97. TransLoc3D : Point Cloud based Large-scale Place Recognition using Adaptive Receptive Fields. (arXiv:2105.11605v3 [cs.CV] UPDATED)
98. Deep Hierarchical Super Resolution for Scientific Data. (arXiv:2107.00462v2 [eess.IV] UPDATED)
99. Saliency Guided Experience Packing for Replay in Continual Learning. (arXiv:2109.04954v2 [cs.LG] UPDATED)
100. TaylorSwiftNet: Taylor Driven Temporal Modeling for Swift Future Frame Prediction. (arXiv:2110.14392v2 [cs.CV] UPDATED)
101. SwAMP: Swapped Assignment of Multi-Modal Pairs for Cross-Modal Retrieval. (arXiv:2111.05814v2 [cs.LG] UPDATED)
102. Deep Probability Estimation. (arXiv:2111.10734v4 [cs.LG] UPDATED)
103. In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v3 [cs.LG] UPDATED)
104. Understanding Deep Contrastive Learning via Coordinate-wise Optimization. (arXiv:2201.12680v5 [cs.LG] UPDATED)
105. Student Becomes Decathlon Master in Retinal Vessel Segmentation via Dual-teacher Multi-target Domain Adaptation. (arXiv:2203.03631v3 [eess.IV] UPDATED)
106. Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?. (arXiv:2203.09081v3 [cs.LG] UPDATED)
107. Deep learning for laboratory earthquake prediction and autoregressive forecasting of fault zone stress. (arXiv:2203.13313v3 [physics.geo-ph] UPDATED)
108. Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution. (arXiv:2204.04218v3 [eess.IV] UPDATED)
109. High-Efficiency Lossy Image Coding Through Adaptive Neighborhood Information Aggregation. (arXiv:2204.11448v2 [eess.IV] UPDATED)
110. DANBO: Disentangled Articulated Neural Body Representations via Graph Neural Networks. (arXiv:2205.01666v2 [cs.CV] UPDATED)
111. Multiview Textured Mesh Recovery by Differentiable Rendering. (arXiv:2205.12468v3 [cs.CV] UPDATED)
112. AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition. (arXiv:2205.13535v2 [cs.CV] UPDATED)
113. MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction. (arXiv:2206.00665v2 [cs.CV] UPDATED)
114. ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v2 [cs.CV] UPDATED)
115. PatchComplete: Learning Multi-Resolution Patch Priors for 3D Shape Completion on Unseen Categories. (arXiv:2206.04916v2 [cs.CV] UPDATED)
116. GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v2 [cs.CV] UPDATED)
117. Object Scene Representation Transformer. (arXiv:2206.06922v2 [cs.CV] UPDATED)
118. VCT: A Video Compression Transformer. (arXiv:2206.07307v2 [cs.CV] UPDATED)
119. Behavior Transformers: Cloning $k$ modes with one stone. (arXiv:2206.11251v2 [cs.LG] UPDATED)
120. A Neuromorphic Vision-Based Measurement for Robust Relative Localization in Future Space Exploration Missions. (arXiv:2206.11541v2 [cs.CV] UPDATED)
121. Delving into Sequential Patches for Deepfake Detection. (arXiv:2207.02803v3 [cs.CV] UPDATED)
122. Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection. (arXiv:2207.07783v3 [cs.CV] UPDATED)
123. Self-supervised learning with rotation-invariant kernels. (arXiv:2208.00789v3 [cs.CV] UPDATED)
124. P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting. (arXiv:2208.02812v2 [cs.CV] UPDATED)
125. BabyNet: A Lightweight Network for Infant Reaching Action Recognition in Unconstrained Environments to Support Future Pediatric Rehabilitation Applications. (arXiv:2208.04950v2 [cs.CV] UPDATED)
126. Patching open-vocabulary models by interpolating weights. (arXiv:2208.05592v2 [cs.CV] UPDATED)
127. Category-Level Pose Retrieval with Contrastive Features Learnt with Occlusion Augmentation. (arXiv:2208.06195v3 [cs.CV] UPDATED)
128. EZNAS: Evolving Zero Cost Proxies For Neural Architecture Scoring. (arXiv:2209.07413v2 [cs.LG] UPDATED)
129. Imbalanced Node Processing Method in Graph Neural Network Classification Task. (arXiv:2209.08514v3 [cs.LG] UPDATED)
130. NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes. (arXiv:2209.08776v6 [cs.CV] UPDATED)
131. Multi-dataset Training of Transformers for Robust Action Recognition. (arXiv:2209.12362v3 [cs.CV] UPDATED)
132. CrossDTR: Cross-view and Depth-guided Transformers for 3D Object Detection. (arXiv:2209.13507v2 [cs.CV] UPDATED)
133. On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks. (arXiv:2210.02191v2 [cs.LG] UPDATED)
134. Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v2 [cs.CV] UPDATED)
135. Contrastive Learning Approach for Semi-Supervised Seismic Facies Identification Using High-Confidence Representations. (arXiv:2210.04776v2 [cs.CV] UPDATED)
136. LMQFormer: A Laplace-Prior-Guided Mask Query Transformer for Lightweight Snow Removal. (arXiv:2210.04787v3 [cs.CV] UPDATED)
137. Learning with an Evolving Class Ontology. (arXiv:2210.04993v2 [cs.CV] UPDATED)
138. TriangleNet: Edge Prior Augmented Network for Semantic Segmentation through Cross-Task Consistency. (arXiv:2210.05152v2 [cs.CV] UPDATED)
139. Evaluating Unsupervised Denoising Requires Unsupervised Metrics. (arXiv:2210.05553v2 [cs.CV] UPDATED)
140. Global Spectral Filter Memory Network for Video Object Segmentation. (arXiv:2210.05567v2 [cs.CV] UPDATED)
141. Motion Aware Self-Supervision for Generic Event Boundary Detection. (arXiv:2210.05574v2 [cs.CV] UPDATED)
142. Point Transformer V2: Grouped Vector Attention and Partition-based Pooling. (arXiv:2210.05666v2 [cs.CV] UPDATED)
143. Understanding Embodied Reference with Touch-Line Transformer. (arXiv:2210.05668v2 [cs.CV] UPDATED)
## eess.IV
---
**25** new papers in eess.IV:-) 
1. Performance Deterioration of Deep Learning Models after Clinical Deployment: A Case Study with Auto-segmentation for Definitive Prostate Cancer Radiotherapy. (arXiv:2210.05673v1 [eess.IV])
2. Joint localization and classification of breast tumors on ultrasound images using a novel auxiliary attention-based framework. (arXiv:2210.05762v1 [eess.IV])
3. Self-Supervised Equivariant Regularization Reconciles Multiple Instance Learning: Joint Referable Diabetic Retinopathy Classification and Lesion Segmentation. (arXiv:2210.05946v1 [eess.IV])
4. 3D Brain and Heart Volume Generative Models: A Survey. (arXiv:2210.05952v1 [eess.IV])
5. Efficient Image Super-Resolution using Vast-Receptive-Field Attention. (arXiv:2210.05960v1 [eess.IV])
6. Estimating the Pose of a Euro Pallet with an RGB Camera based on Synthetic Training Data. (arXiv:2210.06001v1 [cs.CV])
7. Can We "Sense" the Call of The Ocean? Current Advances in Remote Sensing Computational Imaging for Marine Debris Monitoring. (arXiv:2210.06090v1 [eess.IV])
8. Convolutional Neural Network-Based Image Watermarking using Discrete Wavelet Transform. (arXiv:2210.06179v1 [eess.IV])
9. Pose-Guided Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2210.06192v1 [cs.CV])
10. What can we learn about a generated image corrupting its latent representation?. (arXiv:2210.06257v1 [cs.CV])
11. Transmission of high-definition video signals underwater using surface electromagnetic waves. (arXiv:2210.06296v1 [eess.SP])
12. CoRRECT: A Deep Unfolding Framework for Motion-Corrected Quantitative R2* Mapping. (arXiv:2210.06330v1 [eess.IV])
13. A Self-attention Guided Multi-scale Gradient GAN for Diversified X-ray Image Synthesis. (arXiv:2210.06334v1 [eess.IV])
14. A deep learning network with differentiable dynamic programming for retina OCT surface segmentation. (arXiv:2210.06335v1 [eess.IV])
15. A Comparative Study on 1.5T-3T MRI Conversion through Deep Neural Network Models. (arXiv:2210.06362v1 [eess.IV])
16. The Extreme Cardiac MRI Analysis Challenge under Respiratory Motion (CMRxMotion). (arXiv:2210.06385v1 [eess.IV])
17. Revisiting neutron propagation-based phase contrast imaging and tomography: use of phase retrieval to amplify the effective degree of brilliance. (arXiv:1909.11186v3 [eess.IV] UPDATED)
18. Deep Hierarchical Super Resolution for Scientific Data. (arXiv:2107.00462v2 [eess.IV] UPDATED)
19. Student Becomes Decathlon Master in Retinal Vessel Segmentation via Dual-teacher Multi-target Domain Adaptation. (arXiv:2203.03631v3 [eess.IV] UPDATED)
20. Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution. (arXiv:2204.04218v3 [eess.IV] UPDATED)
21. High-Efficiency Lossy Image Coding Through Adaptive Neighborhood Information Aggregation. (arXiv:2204.11448v2 [eess.IV] UPDATED)
22. VCT: A Video Compression Transformer. (arXiv:2206.07307v2 [cs.CV] UPDATED)
23. Learning Correspondency in Frequency Domain by a Latent-Space Similarity Loss for Multispectral Pansharpening. (arXiv:2207.08602v2 [eess.IV] UPDATED)
24. BabyNet: A Lightweight Network for Infant Reaching Action Recognition in Unconstrained Environments to Support Future Pediatric Rehabilitation Applications. (arXiv:2208.04950v2 [cs.CV] UPDATED)
25. Contrastive Learning Approach for Semi-Supervised Seismic Facies Identification Using High-Confidence Representations. (arXiv:2210.04776v2 [cs.CV] UPDATED)
## cs.LG
---
**284** new papers in cs.LG:-) 
1. iMedBot: A Web-based Intelligent Agent for Healthcare Related Prediction and Deep Learning. (arXiv:2210.05671v1 [cs.LG])
2. Unsupervised detection of structural damage using Variational Autoencoder and a One-Class Support Vector Machine. (arXiv:2210.05674v1 [cs.LG])
3. Transformers generalize differently from information stored in context vs in weights. (arXiv:2210.05675v1 [cs.CL])
4. Towards Consistency and Complementarity: A Multiview Graph Information Bottleneck Approach. (arXiv:2210.05676v1 [cs.LG])
5. Application of Deep Learning on Single-Cell RNA-sequencing Data Analysis: A Review. (arXiv:2210.05677v1 [q-bio.GN])
6. Neural Importance Sampling for Rapid and Reliable Gravitational-Wave Inference. (arXiv:2210.05686v1 [gr-qc])
7. Dynamic Ensemble Size Adjustment for Memory Constrained Mondrian Forest. (arXiv:2210.05704v1 [cs.LG])
8. Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers. (arXiv:2210.05709v1 [cs.CL])
9. Predictive Event Segmentation and Representation with Neural Networks: A Self-Supervised Model Assessed by Psychological Experiments. (arXiv:2210.05710v1 [q-bio.NC])
10. Visual Language Maps for Robot Navigation. (arXiv:2210.05714v1 [cs.RO])
11. Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge. (arXiv:2210.05723v1 [cs.AI])
12. TetGAN: A Convolutional Neural Network for Tetrahedral Mesh Generation. (arXiv:2210.05735v1 [cs.CV])
13. Context-aware Bayesian choice models. (arXiv:2210.05737v1 [stat.ML])
14. Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v1 [cs.LG])
15. On RKHS Choices for Assessing Graph Generators via Kernel Stein Statistics. (arXiv:2210.05746v1 [stat.ML])
16. Toward Sustainable Continual Learning: Detection and Knowledge Repurposing of Similar Tasks. (arXiv:2210.05751v1 [cs.CV])
17. Match Cutting: Finding Cuts with Smooth Visual Transitions. (arXiv:2210.05766v1 [cs.CV])
18. Vote'n'Rank: Revision of Benchmarking with Social Choice Theory. (arXiv:2210.05769v1 [cs.LG])
19. C-Mixup: Improving Generalization in Regression. (arXiv:2210.05775v1 [cs.LG])
20. Gradient-Guided Importance Sampling for Learning Binary Energy-Based Models. (arXiv:2210.05782v1 [cs.LG])
21. Trading Off Resource Budgets for Improved Regret Bounds. (arXiv:2210.05789v1 [cs.LG])
22. Transfer Learning with Joint Fine-Tuning for Multimodal Sentiment Analysis. (arXiv:2210.05790v1 [cs.LG])
23. Comparison of Soft and Hard Target RNN-T Distillation for Large-scale ASR. (arXiv:2210.05793v1 [cs.LG])
24. Robustify Transformers with Robust Kernel Density Estimation. (arXiv:2210.05794v1 [cs.LG])
25. Linkless Link Prediction via Relational Distillation. (arXiv:2210.05801v1 [cs.LG])
26. Exploration via Elliptical Episodic Bonuses. (arXiv:2210.05805v1 [cs.LG])
27. Deep Counterfactual Estimation with Categorical Background Variables. (arXiv:2210.05811v1 [cs.LG])
28. Finding and Listing Front-door Adjustment Sets. (arXiv:2210.05816v1 [stat.ME])
29. Short-term prediction of stream turbidity using surrogate data and a meta-model approach. (arXiv:2210.05821v1 [stat.ML])
30. AMICO: Amodal Instance Composition. (arXiv:2210.05828v1 [cs.CV])
31. Social-Group-Agnostic Word Embedding Debiasing via the Stereotype Content Model. (arXiv:2210.05831v1 [cs.CL])
32. Parameter estimation of the homodyned K distribution based on neural networks and trainable fractional-order moments. (arXiv:2210.05833v1 [cs.LG])
33. Synthetic Power Analyses: Empirical Evaluation and Application to Cognitive Neuroimaging. (arXiv:2210.05835v1 [cs.CV])
34. A composable machine-learning approach for steady-state simulations on high-resolution grids. (arXiv:2210.05837v1 [cs.LG])
35. Contrastive introspection (ConSpec) to rapidly identify invariant steps for success. (arXiv:2210.05845v1 [cs.LG])
36. FasterRisk: Fast and Accurate Interpretable Risk Scores. (arXiv:2210.05846v1 [cs.LG])
37. SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models. (arXiv:2210.05861v1 [cs.CV])
38. Multi-Content Time-Series Popularity Prediction with Multiple-Model Transformers in MEC Networks. (arXiv:2210.05874v1 [cs.LG])
39. Statistical Modeling of Soft Error Influence on Neural Networks. (arXiv:2210.05876v1 [cs.LG])
40. Pathology Steered Stratification Network for Subtype Identification in Alzheimer's Disease. (arXiv:2210.05880v1 [q-bio.QM])
41. Deterioration Prediction using Time-Series of Three Vital Signs and Current Clinical Features Amongst COVID-19 Patients. (arXiv:2210.05881v1 [cs.LG])
42. Building Heterogeneous Cloud System for Machine Learning Inference. (arXiv:2210.05889v1 [cs.DC])
43. Common Corruption Robustness of Point Cloud Detectors: Benchmark and **Enhancement**. (arXiv:2210.05896v1 [cs.CV])
44. Travel the Same Path: A Novel TSP Solving Strategy. (arXiv:2210.05906v1 [cs.LG])
45. Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v1 [cs.CL])
46. Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v1 [cs.LG])
47. Boosting Graph Neural Networks via Adaptive Knowledge Distillation. (arXiv:2210.05920v1 [cs.LG])
48. A Unified Framework for Alternating Offline Model Training and Policy Learning. (arXiv:2210.05922v1 [cs.LG])
49. Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning. (arXiv:2210.05927v1 [cs.LG])
50. Few-shot Backdoor Attacks via Neural Tangent Kernels. (arXiv:2210.05929v1 [cs.LG])
51. Explaining Online Reinforcement Learning Decisions of Self-Adaptive Systems. (arXiv:2210.05931v1 [cs.LG])
52. Optimizing Evaluation Metrics for Multi-Task Learning via the Alternating Direction Method of Multipliers. (arXiv:2210.05935v1 [cs.LG])
53. Equal Experience in Recommender Systems. (arXiv:2210.05936v1 [cs.LG])
54. Adaptive Dual Channel Convolution Hypergraph Representation Learning for Technological Intellectual Property. (arXiv:2210.05947v1 [cs.IR])
55. Classification by estimating the cumulative distribution function for small data. (arXiv:2210.05953v1 [cs.LG])
56. Projective Transformation Rectification for Camera-captured Chest X-ray Photograph Interpretation with Synthetic Data. (arXiv:2210.05954v1 [cs.CV])
57. Identifiability and Asymptotics in Learning Homogeneous Linear ODE Systems from Discrete Observations. (arXiv:2210.05955v1 [stat.ML])
58. Towards Theoretically Inspired Neural Initialization Optimization. (arXiv:2210.05956v1 [cs.LG])
59. Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets. (arXiv:2210.05958v1 [cs.CV])
60. JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional Networks. (arXiv:2210.05959v1 [cs.LG])
61. Resolving the Approximability of Offline and Online Non-monotone DR-Submodular Maximization over General Convex Sets. (arXiv:2210.05965v1 [cs.DS])
62. Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation. (arXiv:2210.05968v1 [cs.CV])
63. Unsupervised Learning of Equivariant Structure from Sequences. (arXiv:2210.05972v1 [cs.LG])
64. Clustering Embedding Tables, Without First Learning Them. (arXiv:2210.05974v1 [cs.LG])
65. BORA: Bayesian Optimization for Resource Allocation. (arXiv:2210.05977v1 [cs.LG])
66. Efficient Offline Policy Optimization with a Learned Model. (arXiv:2210.05980v1 [cs.LG])
67. A Momentum Accelerated Adaptive Cubic Regularization Method for Nonconvex Optimization. (arXiv:2210.05987v1 [math.OC])
68. CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction. (arXiv:2210.05988v1 [eess.SP])
69. Distilling Knowledge from Language Models for Video-based Action Anticipation. (arXiv:2210.05991v1 [cs.CV])
70. Feasible and Desirable Counterfactual Generation by Preserving Human Defined Constraints. (arXiv:2210.05993v1 [cs.LG])
71. SARAH-based Variance-reduced Algorithm for Stochastic Finite-sum Cocoercive Variational Inequalities. (arXiv:2210.05994v1 [math.OC])
72. Generative Adversarial Nets: Can we generate a new dataset based on only one training set?. (arXiv:2210.06005v1 [cs.LG])
73. JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VA. (arXiv:2210.06007v1 [cs.SD])
74. Energy Consumption-Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v1 [cs.LG])
75. Modular Flows: Differential Molecular Generation. (arXiv:2210.06032v1 [cs.LG])
76. Guaranteed Conservation of Momentum for Learning Particle-based Fluid Dynamics. (arXiv:2210.06036v1 [cs.LG])
77. Reinforcement Learning with Automated Auxiliary Loss Search. (arXiv:2210.06041v1 [cs.LG])
78. ControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters. (arXiv:2210.06063v1 [cs.GR])
79. E3Bind: An End-to-End Equivariant Network for Protein-Ligand Docking. (arXiv:2210.06069v1 [q-bio.BM])
80. Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity. (arXiv:2210.06077v1 [cs.LG])
81. Outlier-Insensitive Kalman Filtering Using NUV Priors. (arXiv:2210.06083v1 [eess.SP])
82. Annihilation of Spurious Minima in Two-Layer ReLU Networks. (arXiv:2210.06088v1 [cs.LG])
83. When are Local Queries Useful for Robust Learning?. (arXiv:2210.06089v1 [cs.LG])
84. SpecRNet: Towards Faster and More Accessible Audio DeepFake Detection. (arXiv:2210.06105v1 [cs.SD])
85. Fast Bayesian Updates for Deep Learning with a Use Case in Active Learning. (arXiv:2210.06112v1 [cs.LG])
86. Towards Mining Creative Thinking Patterns from Educational Data. (arXiv:2210.06118v1 [cs.IR])
87. Regularized Graph Structure Learning with Semantic Knowledge for Multi-variates Time-Series Forecasting. (arXiv:2210.06126v1 [cs.LG])
88. Differentially Private Bootstrap: New Privacy Analysis and Inference Strategies. (arXiv:2210.06140v1 [stat.ML])
89. On the Importance of Gradient Norm in PAC-Bayesian Bounds. (arXiv:2210.06143v1 [cs.LG])
90. Digital twins of nonlinear dynamical systems. (arXiv:2210.06144v1 [nlin.AO])
91. Aergia: Leveraging Heterogeneity in Federated Learning Systems. (arXiv:2210.06154v1 [cs.LG])
92. Privacy of federated QR decomposition using additive secure multiparty computation. (arXiv:2210.06163v1 [cs.CR])
93. The Role of Exploration for Task Transfer in Reinforcement Learning. (arXiv:2210.06168v1 [cs.LG])
94. Contrastive Neural Ratio Estimation. (arXiv:2210.06170v1 [stat.ML])
95. Learning to Optimize Quasi-Newton Methods. (arXiv:2210.06171v1 [cs.LG])
96. Exploring Efficient-tuning Methods in Self-supervised Speech Models. (arXiv:2210.06175v1 [eess.AS])
97. Transfer Learning on Heterogeneous Feature Spaces for Treatment Effects Estimation. (arXiv:2210.06183v1 [cs.LG])
98. Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules. (arXiv:2210.06184v1 [cs.CV])
99. Anomaly Detection using Generative Models and Sum-Product Networks in Mammography Scans. (arXiv:2210.06188v1 [cs.CV])
100. Diffusion Models for Causal Discovery via Topological Ordering. (arXiv:2210.06201v1 [cs.LG])
101. On Divergence Measures for Bayesian Pseudocoresets. (arXiv:2210.06205v1 [cs.LG])
102. Probabilistic Inverse Modeling: An Application in Hydrology. (arXiv:2210.06213v1 [cs.LG])
103. On the Generalizability of ECG-based Stress Detection Models. (arXiv:2210.06225v1 [cs.LG])
104. Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics. (arXiv:2210.06226v1 [stat.ML])
105. FCT-GAN: Enhancing Table Synthesis via Fourier Transform. (arXiv:2210.06239v1 [cs.LG])
106. Entity Aware Negative Sampling with Auxiliary Loss of False Negative Prediction for Knowledge Graph Embedding. (arXiv:2210.06242v1 [cs.LG])
107. What can we learn about a generated image corrupting its latent representation?. (arXiv:2210.06257v1 [cs.CV])
108. Predicting housing prices and analyzing real estate market in the Chicago suburbs using Machine Learning. (arXiv:2210.06261v1 [cs.LG])
109. Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning. (arXiv:2210.06274v1 [cs.LG])
110. Task Compass: Scaling Multi-task Pre-training with Task Prefix. (arXiv:2210.06277v1 [cs.CL])
111. Language Models are Realistic Tabular Data Generators. (arXiv:2210.06280v1 [cs.LG])
112. Visual Prompting for Adversarial Robustness. (arXiv:2210.06284v1 [cs.CV])
113. Smart Cup: An impedance sensing based fluid intake monitoring system for beverages classification and freshness detection. (arXiv:2210.06285v1 [eess.SP])
114. Self-supervised Learning for Label-Efficient Sleep Stage Classification: A Comprehensive Evaluation. (arXiv:2210.06286v1 [eess.SP])
115. An Energy-Efficient Spiking Neural Network for Finger Velocity Decoding for Implantable Brain-Machine Interface. (arXiv:2210.06287v1 [eess.SP])
116. fAux: Testing Individual Fairness via Gradient Alignment. (arXiv:2210.06288v1 [stat.ML])
117. The evolution of AI approaches for motor imagery EEG-based BCIs. (arXiv:2210.06290v1 [eess.SP])
118. ECG for high-throughput screening of multiple diseases: Proof-of-concept using multi-diagnosis deep learning from population-based datasets. (arXiv:2210.06291v1 [eess.SP])
119. A review on Epileptic Seizure Detection using Machine Learning. (arXiv:2210.06292v1 [eess.SP])
120. Two-stream Network for ECG Signal Classification. (arXiv:2210.06293v1 [eess.SP])
121. Indoor Localization with Robust Global Channel Charting: A Time-Distance-Based Approach. (arXiv:2210.06294v1 [eess.SP])
122. Transfer learning on electromyography (EMG) tasks: approaches and beyond. (arXiv:2210.06295v1 [eess.SP])
123. Multimodality Multi-Lead ECG Arrhythmia Classification using Self-Supervised Learning. (arXiv:2210.06297v1 [eess.SP])
124. Cross Task Neural Architecture Search for EEG Signal Classifications. (arXiv:2210.06298v1 [eess.SP])
125. Generalised Mutual Information for Discriminative Clustering. (arXiv:2210.06300v1 [stat.ML])
126. Maximum entropy exploration in contextual bandits with neural networks and energy based models. (arXiv:2210.06302v1 [cs.LG])
127. Determining band structure parameters of two-dimensional materials by deep learning. (arXiv:2210.06310v1 [cond-mat.mes-hall])
128. Semantic Cross Attention for Few-shot Learning. (arXiv:2210.06311v1 [cs.CV])
129. Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers. (arXiv:2210.06313v1 [cs.LG])
130. SQuId: Measuring Speech Naturalness in Many Languages. (arXiv:2210.06324v1 [cs.CL])
131. Betting the system: Using lineups to predict football scores. (arXiv:2210.06327v1 [cs.LG])
132. Momentum Aggregation for Private Non-convex ERM. (arXiv:2210.06328v1 [cs.LG])
133. CoRRECT: A Deep Unfolding Framework for Motion-Corrected Quantitative R2* Mapping. (arXiv:2210.06330v1 [eess.IV])
134. A Self-attention Guided Multi-scale Gradient GAN for Diversified X-ray Image Synthesis. (arXiv:2210.06334v1 [eess.IV])
135. Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors. (arXiv:2210.06340v1 [cs.CL])
136. TaskMix: Data Augmentation for Meta-Learning of Spoken Intent Understanding. (arXiv:2210.06341v1 [cs.CL])
137. Variational Open-Domain Question Answering. (arXiv:2210.06345v1 [cs.CL])
138. Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network. (arXiv:2210.06346v1 [cs.CL])
139. CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations. (arXiv:2210.06350v1 [cs.LG])
140. Automatic Discovery of Composite SPMD Partitioning Strategies in PartIR. (arXiv:2210.06352v1 [cs.NE])
141. Russian Web Tables: A Public Corpus of Web Tables for Russian Language Based on Wikipedia. (arXiv:2210.06353v1 [cs.CL])
142. A Comparative Study on 1.5T-3T MRI Conversion through Deep Neural Network Models. (arXiv:2210.06362v1 [eess.IV])
143. A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v1 [cs.CV])
144. Robust Streaming PCA. (arXiv:1902.03223v3 [stat.ML] UPDATED)
145. Finite Sample Analysis Of Dynamic Regression Parameter Learning. (arXiv:1906.05591v4 [cs.LG] UPDATED)
146. Distributional Random Forests: Heterogeneity Adjustment and Multivariate Distributional Regression. (arXiv:2005.14458v3 [stat.ML] UPDATED)
147. MACE: A Flexible Framework for Membership Privacy Estimation in Generative Models. (arXiv:2009.05683v5 [cs.CR] UPDATED)
148. Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v5 [cs.IT] UPDATED)
149. Generalised correlated batched bandits via the ARC algorithm with application to dynamic pricing. (arXiv:2102.04263v2 [math.OC] UPDATED)
150. Meta-Learning Dynamics Forecasting Using Task Inference. (arXiv:2102.10271v5 [cs.LG] UPDATED)
151. Is Simple Uniform Sampling Effective for Center-Based Clustering with Outliers: When and Why?. (arXiv:2103.00558v4 [cs.LG] UPDATED)
152. SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning. (arXiv:2105.15013v5 [cs.LG] UPDATED)
153. Efficient and Modular Implicit Differentiation. (arXiv:2105.15183v5 [cs.LG] UPDATED)
154. An Analytical Theory of Curriculum Learning in Teacher-Student Networks. (arXiv:2106.08068v2 [cs.LG] UPDATED)
155. Deep Hierarchical Super Resolution for Scientific Data. (arXiv:2107.00462v2 [eess.IV] UPDATED)
156. Saliency Guided Experience Packing for Replay in Continual Learning. (arXiv:2109.04954v2 [cs.LG] UPDATED)
157. Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks. (arXiv:2109.08844v3 [stat.ML] UPDATED)
158. Efficient Identification of Butterfly Sparse Matrix Factorizations. (arXiv:2110.01230v5 [cs.LG] UPDATED)
159. Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v5 [cs.LG] UPDATED)
160. Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits. (arXiv:2110.08627v2 [cs.LG] UPDATED)
161. Collage: Seamless Integration of Deep Learning Backends with Automatic Placement. (arXiv:2111.00655v2 [cs.LG] UPDATED)
162. SwAMP: Swapped Assignment of Multi-Modal Pairs for Cross-Modal Retrieval. (arXiv:2111.05814v2 [cs.LG] UPDATED)
163. Deep Probability Estimation. (arXiv:2111.10734v4 [cs.LG] UPDATED)
164. Guaranteed Nonlinear Tracking in the Presence of DNN-Learned Dynamics With Contraction Metrics and Disturbance Estimation. (arXiv:2112.08222v4 [eess.SY] UPDATED)
165. A Robust Initialization of Residual Blocks for Effective ResNet Training without Batch Normalization. (arXiv:2112.12299v2 [cs.LG] UPDATED)
166. Multi-modal Attention Network for Stock Movements Prediction. (arXiv:2112.13593v5 [cs.LG] UPDATED)
167. SCROLLS: Standardized CompaRison Over Long Language Sequences. (arXiv:2201.03533v2 [cs.CL] UPDATED)
168. In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v3 [cs.LG] UPDATED)
169. Synthesizing explainable counterfactual policies for algorithmic recourse with program synthesis. (arXiv:2201.07135v2 [cs.LG] UPDATED)
170. Neural Implicit Surface Evolution using Differential Equations. (arXiv:2201.09636v3 [cs.LG] UPDATED)
171. Unifying and Boosting Gradient-Based Training-Free Neural Architecture Search. (arXiv:2201.09785v2 [cs.LG] UPDATED)
172. Generalization Error Bounds on Deep Learning with Markov Datasets. (arXiv:2201.11059v4 [stat.ML] UPDATED)
173. Neural Approximation of Graph Topological Features. (arXiv:2201.12032v3 [cs.LG] UPDATED)
174. Understanding Deep Contrastive Learning via Coordinate-wise Optimization. (arXiv:2201.12680v5 [cs.LG] UPDATED)
175. Learning on Arbitrary Graph Topologies via Predictive Coding. (arXiv:2201.13180v3 [cs.LG] UPDATED)
176. Trajectory balance: Improved credit assignment in GFlowNets. (arXiv:2201.13259v2 [cs.LG] UPDATED)
177. Understanding Cross-Domain Few-Shot Learning Based on Domain Similarity and Few-Shot Difficulty. (arXiv:2202.01339v3 [cs.LG] UPDATED)
178. Meta-Reinforcement Learning with Self-Modifying Networks. (arXiv:2202.02363v3 [cs.LG] UPDATED)
179. MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural Networks. (arXiv:2202.02365v2 [cs.LG] UPDATED)
180. On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error. (arXiv:2202.03618v3 [math.OC] UPDATED)
181. Generating Training Data with Language Models: Towards Zero-Shot Language Understanding. (arXiv:2202.04538v2 [cs.CL] UPDATED)
182. A Characterization of Semi-Supervised Adversarially-Robust PAC Learnability. (arXiv:2202.05420v2 [cs.LG] UPDATED)
183. Escaping Saddle Points with Bias-Variance Reduced Local Perturbed SGD for Communication Efficient Nonconvex Distributed Learning. (arXiv:2202.06083v3 [cs.LG] UPDATED)
184. Supported Policy Optimization for Offline Reinforcement Learning. (arXiv:2202.06239v3 [cs.LG] UPDATED)
185. Extended Unconstrained Features Model for Exploring Deep Neural Collapse. (arXiv:2202.08087v3 [cs.LG] UPDATED)
186. Open-Ended Reinforcement Learning with Neural Reward Functions. (arXiv:2202.08266v2 [cs.LG] UPDATED)
187. From Optimization Dynamics to Generalization Bounds via {\L}ojasiewicz Gradient Inequality. (arXiv:2202.10670v3 [stat.ML] UPDATED)
188. Policy Evaluation for Temporal and/or Spatial Dependent Experiments in Ride-sourcing Platforms. (arXiv:2202.10887v3 [stat.ME] UPDATED)
189. AdaPT: Fast Emulation of Approximate DNN Accelerators in PyTorch. (arXiv:2203.04071v2 [cs.LG] UPDATED)
190. Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?. (arXiv:2203.09081v3 [cs.LG] UPDATED)
191. Alleviating Adversarial Attacks on Variational Autoencoders with MCMC. (arXiv:2203.09940v2 [cs.LG] UPDATED)
192. Reinforcement learning for automatic quadrilateral mesh generation: a soft actor-critic approach. (arXiv:2203.11203v2 [cs.LG] UPDATED)
193. Bellman Residual Orthogonalization for Offline Reinforcement Learning. (arXiv:2203.12786v3 [cs.LG] UPDATED)
194. Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v3 [cs.CL] UPDATED)
195. Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution. (arXiv:2204.04218v3 [eess.IV] UPDATED)
196. On the Representation Collapse of Sparse Mixture of Experts. (arXiv:2204.09179v3 [cs.CL] UPDATED)
197. Scalable Sensitivity and Uncertainty Analysis for Causal-Effect Estimates of Continuous-Valued Interventions. (arXiv:2204.10022v4 [cs.LG] UPDATED)
198. Competitive Physics Informed Networks. (arXiv:2204.11144v2 [cs.LG] UPDATED)
199. Scalable particle-based alternatives to EM. (arXiv:2204.12965v2 [stat.CO] UPDATED)
200. Counterfactual harm. (arXiv:2204.12993v4 [cs.AI] UPDATED)
201. Generative Adversarial Neural Operators. (arXiv:2205.03017v2 [cs.LG] UPDATED)
202. Efficient Risk-Averse Reinforcement Learning. (arXiv:2205.05138v2 [cs.LG] UPDATED)
203. Deep Architecture Connectivity Matters for Its Convergence: A Fine-Grained Analysis. (arXiv:2205.05662v2 [cs.LG] UPDATED)
204. Optimal Comparator Adaptive Online Learning with Switching Cost. (arXiv:2205.06846v3 [cs.LG] UPDATED)
205. Trajectory Inference via Mean-field Langevin in Path Space. (arXiv:2205.07146v4 [math.OC] UPDATED)
206. Generalization Bounds on Multi-Kernel Learning with Mixed Datasets. (arXiv:2205.07313v2 [cs.LG] UPDATED)
207. Adversarial random forests for density estimation and generative modelling. (arXiv:2205.09435v2 [stat.ML] UPDATED)
208. Learning Energy Networks with Generalized Fenchel-Young Losses. (arXiv:2205.09589v2 [cs.LG] UPDATED)
209. Spiking Neural Operators for Scientific Machine Learning. (arXiv:2205.10130v2 [cs.NE] UPDATED)
210. Active Learning Through a Covering Lens. (arXiv:2205.11320v2 [cs.LG] UPDATED)
211. uGLAD: Sparse graph recovery by optimizing deep unrolled networks. (arXiv:2205.11610v2 [cs.LG] UPDATED)
212. Learning Interacting Dynamical Systems with Latent Gaussian Process ODEs. (arXiv:2205.11894v3 [cs.LG] UPDATED)
213. Non-stationary Bandits with Knapsacks. (arXiv:2205.12427v2 [cs.LG] UPDATED)
214. Recipe for a General, Powerful, Scalable Graph Transformer. (arXiv:2205.12454v3 [cs.LG] UPDATED)
215. Fast Stochastic Composite Minimization and an Accelerated Frank-Wolfe Algorithm under Parallelization. (arXiv:2205.12751v2 [math.OC] UPDATED)
216. Selective Classification Via Neural Network Training Dynamics. (arXiv:2205.13532v3 [cs.LG] UPDATED)
217. Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games. (arXiv:2205.13746v3 [math.OC] UPDATED)
218. What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v5 [cs.LG] UPDATED)
219. Non-Stationary Bandits under Recharging Payoffs: Improved Planning with Sublinear Regret. (arXiv:2205.14790v2 [cs.LG] UPDATED)
220. CoNSoLe: Convex Neural Symbolic Learning. (arXiv:2206.00257v2 [cs.LG] UPDATED)
221. An $\alpha$-No-Regret Algorithm For Graphical Bilinear Bandits. (arXiv:2206.00466v2 [cs.LG] UPDATED)
222. One Positive Label is Sufficient: Single-Positive Multi-Label Learning with Label **Enhancement**. (arXiv:2206.00517v4 [cs.LG] UPDATED)
223. Mean Estimation in High-Dimensional Binary Markov Gaussian Mixture Models. (arXiv:2206.02455v3 [math.ST] UPDATED)
224. UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder. (arXiv:2206.02512v3 [eess.AS] UPDATED)
225. Infinite Recommendation Networks: A Data-Centric Approach. (arXiv:2206.02626v3 [cs.IR] UPDATED)
226. Effects of Safety State Augmentation on Safe Exploration. (arXiv:2206.02675v2 [cs.LG] UPDATED)
227. Group Meritocratic Fairness in Linear Contextual Bandits. (arXiv:2206.03150v2 [stat.ML] UPDATED)
228. STable: Table Generation Framework for Encoder-Decoder Models. (arXiv:2206.04045v2 [cs.CL] UPDATED)
229. Deep Surrogate Assisted Generation of Environments. (arXiv:2206.04199v3 [cs.AI] UPDATED)
230. ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v2 [cs.CV] UPDATED)
231. NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs. (arXiv:2206.04910v2 [cs.LG] UPDATED)
232. Mathematical Theory of Bayesian Statistics for Unknown Information Source. (arXiv:2206.05630v3 [cs.LG] UPDATED)
233. GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v2 [cs.CV] UPDATED)
234. Modeling the Machine Learning Multiverse. (arXiv:2206.05985v2 [cs.LG] UPDATED)
235. Matching Pursuit Based Scheduling for Over-the-Air Federated Learning. (arXiv:2206.06679v2 [cs.IT] UPDATED)
236. Object Scene Representation Transformer. (arXiv:2206.06922v2 [cs.CV] UPDATED)
237. VCT: A Video Compression Transformer. (arXiv:2206.07307v2 [cs.CV] UPDATED)
238. Learning to Accelerate Partial Differential Equations via Latent Global Evolution. (arXiv:2206.07681v2 [cs.LG] UPDATED)
239. Interaction-Grounded Learning with Action-inclusive Feedback. (arXiv:2206.08364v2 [cs.LG] UPDATED)
240. On the Limitations of Stochastic Pre-processing Defenses. (arXiv:2206.09491v3 [cs.LG] UPDATED)
241. EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine. (arXiv:2206.10558v2 [cs.LG] UPDATED)
242. Behavior Transformers: Cloning $k$ modes with one stone. (arXiv:2206.11251v2 [cs.LG] UPDATED)
243. pyKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models. (arXiv:2206.11460v3 [cs.LG] UPDATED)
244. Learning sparse features can lead to overfitting in neural networks. (arXiv:2206.12314v2 [stat.ML] UPDATED)
245. Can Push-forward Generative Models Fit Multimodal Distributions?. (arXiv:2206.14476v2 [stat.ML] UPDATED)
246. ZeroC: A Neuro-Symbolic Model for Zero-shot Concept Recognition and Acquisition at Inference Time. (arXiv:2206.15049v3 [cs.LG] UPDATED)
247. TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. (arXiv:2207.01848v3 [cs.LG] UPDATED)
248. Information Compression and Performance Evaluation of Tic-Tac-Toe's Evaluation Function Using Singular Value Decomposition. (arXiv:2207.02449v4 [cs.LG] UPDATED)
249. Scalable and Privacy-enhanced Graph Generative Model for Graph Neural Networks. (arXiv:2207.04396v2 [cs.LG] UPDATED)
250. Graph Neural Network Bandits. (arXiv:2207.06456v2 [cs.LG] UPDATED)
251. HouseX: A Fine-grained House Music Dataset and its Potential in the Music Industry. (arXiv:2207.11690v2 [cs.SD] UPDATED)
252. p-Adic Statistical Field Theory and Deep Belief Networks. (arXiv:2207.13877v3 [math-ph] UPDATED)
253. Graph Neural Networks for Channel Decoding. (arXiv:2207.14742v2 [cs.IT] UPDATED)
254. Simulation and application of COVID-19 compartment model using physics-informed neural network. (arXiv:2208.02433v4 [q-bio.QM] UPDATED)
255. P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting. (arXiv:2208.02812v2 [cs.CV] UPDATED)
256. Information bottleneck theory of high-dimensional regression: relevancy, efficiency and optimality. (arXiv:2208.03848v2 [cs.IT] UPDATED)
257. BabyNet: A Lightweight Network for Infant Reaching Action Recognition in Unconstrained Environments to Support Future Pediatric Rehabilitation Applications. (arXiv:2208.04950v2 [cs.CV] UPDATED)
258. NOTE: Robust Continual Test-time Adaptation Against Temporal Correlation. (arXiv:2208.05117v2 [cs.LG] UPDATED)
259. Patching open-vocabulary models by interpolating weights. (arXiv:2208.05592v2 [cs.CV] UPDATED)
260. Real-world-robustness of tree-based classifiers. (arXiv:2208.10354v2 [cs.LG] UPDATED)
261. Minimax-Optimal Multi-Agent RL in Markov Games With a Generative Model. (arXiv:2208.10458v2 [cs.LG] UPDATED)
262. On the Implicit Bias in Deep-Learning Algorithms. (arXiv:2208.12591v2 [cs.LG] UPDATED)
263. Empirical Gateaux Derivatives for Causal Inference. (arXiv:2208.13701v3 [stat.ME] UPDATED)
264. EZNAS: Evolving Zero Cost Proxies For Neural Architecture Scoring. (arXiv:2209.07413v2 [cs.LG] UPDATED)
265. Imbalanced Node Processing Method in Graph Neural Network Classification Task. (arXiv:2209.08514v3 [cs.LG] UPDATED)
266. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v4 [cs.CY] UPDATED)
267. Magnitude and Angle Dynamics in Training Single ReLU Neurons. (arXiv:2209.13394v2 [cs.LG] UPDATED)
268. Neural Integral Equations. (arXiv:2209.15190v2 [cs.LG] UPDATED)
269. Beyond Bayes-optimality: meta-learning what you know you don't know. (arXiv:2209.15618v2 [cs.AI] UPDATED)
270. Neural Graphical Models. (arXiv:2210.00453v3 [cs.LG] UPDATED)
271. Teaching Yourself:Graph Self-Distillation on Neighborhood for Node Classification. (arXiv:2210.02097v2 [cs.LG] UPDATED)
272. On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks. (arXiv:2210.02191v2 [cs.LG] UPDATED)
273. On Optimal Learning Under Targeted Data Poisoning. (arXiv:2210.02713v2 [cs.LG] UPDATED)
274. Advancing Model Pruning via Bi-level Optimization. (arXiv:2210.04092v2 [cs.LG] UPDATED)
275. Towards Real-Time Temporal Graph Learning. (arXiv:2210.04114v2 [cs.LG] UPDATED)
276. Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v2 [stat.ML] UPDATED)
277. Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue. (arXiv:2210.04443v2 [cs.LG] UPDATED)
278. CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure. (arXiv:2210.04633v2 [cs.SE] UPDATED)
279. A Hybrid Active-Passive Approach to Imbalanced Nonstationary Data Stream Classification. (arXiv:2210.04949v2 [cs.LG] UPDATED)
280. Learning with an Evolving Class Ontology. (arXiv:2210.04993v2 [cs.CV] UPDATED)
281. Planning Assembly Sequence with Graph Transformer. (arXiv:2210.05236v2 [cs.AI] UPDATED)
282. Motion Aware Self-Supervision for Generic Event Boundary Detection. (arXiv:2210.05574v2 [cs.CV] UPDATED)
283. Dilated FCN: Listening Longer to Hear Better. (arXiv:1907.11956v1 [cs.SD] CROSS LISTED)
284. COVID-19-related Nepali Tweets Classification in a Low Resource Setting. (arXiv:2210.05425v1 [cs.CL] CROSS LISTED)
## cs.AI
---
**123** new papers in cs.AI:-) 
1. Unsupervised detection of structural damage using Variational Autoencoder and a One-Class Support Vector Machine. (arXiv:2210.05674v1 [cs.LG])
2. Transformers generalize differently from information stored in context vs in weights. (arXiv:2210.05675v1 [cs.CL])
3. Towards Consistency and Complementarity: A Multiview Graph Information Bottleneck Approach. (arXiv:2210.05676v1 [cs.LG])
4. Predictive Event Segmentation and Representation with Neural Networks: A Self-Supervised Model Assessed by Psychological Experiments. (arXiv:2210.05710v1 [q-bio.NC])
5. Visual Language Maps for Robot Navigation. (arXiv:2210.05714v1 [cs.RO])
6. Relational Embeddings for Language Independent Stance Detection. (arXiv:2210.05715v1 [cs.CL])
7. Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge. (arXiv:2210.05723v1 [cs.AI])
8. Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v1 [cs.LG])
9. Decoupled Context Processing for Context Augmented Language Modeling. (arXiv:2210.05758v1 [cs.CL])
10. Applying FrameNet to Chinese(Poetry). (arXiv:2210.05772v1 [cs.CL])
11. Bil-DOS: A Bi-lingual Dialogue Ordering System (for Subway). (arXiv:2210.05773v1 [cs.CL])
12. Gradient-Guided Importance Sampling for Learning Binary Energy-Based Models. (arXiv:2210.05782v1 [cs.LG])
13. Towards Discriminative and Transferable One-Stage Few-Shot Object Detectors. (arXiv:2210.05783v1 [cs.CV])
14. Exploration via Elliptical Episodic Bonuses. (arXiv:2210.05805v1 [cs.LG])
15. Deep Counterfactual Estimation with Categorical Background Variables. (arXiv:2210.05811v1 [cs.LG])
16. Finding and Listing Front-door Adjustment Sets. (arXiv:2210.05816v1 [stat.ME])
17. Controllable Radiance Fields for Dynamic Face Synthesis. (arXiv:2210.05825v1 [cs.CV])
18. AMICO: Amodal Instance Composition. (arXiv:2210.05828v1 [cs.CV])
19. Social-Group-Agnostic Word Embedding Debiasing via the Stereotype Content Model. (arXiv:2210.05831v1 [cs.CL])
20. Synthetic Power Analyses: Empirical Evaluation and Application to Cognitive Neuroimaging. (arXiv:2210.05835v1 [cs.CV])
21. Contrastive introspection (ConSpec) to rapidly identify invariant steps for success. (arXiv:2210.05845v1 [cs.LG])
22. SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models. (arXiv:2210.05861v1 [cs.CV])
23. Deep Learning for Iris Recognition: A Survey. (arXiv:2210.05866v1 [cs.CV])
24. Perplexity from PLM Is Unreliable for Evaluating Text Quality. (arXiv:2210.05892v1 [cs.CL])
25. Travel the Same Path: A Novel TSP Solving Strategy. (arXiv:2210.05906v1 [cs.LG])
26. Enemy Spotted: in-game gun sound dataset for gunshot classification and localization. (arXiv:2210.05917v1 [cs.SD])
27. Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v1 [cs.LG])
28. A Unified Framework for Alternating Offline Model Training and Policy Learning. (arXiv:2210.05922v1 [cs.LG])
29. Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning. (arXiv:2210.05927v1 [cs.LG])
30. Explaining Online Reinforcement Learning Decisions of Self-Adaptive Systems. (arXiv:2210.05931v1 [cs.LG])
31. Projective Transformation Rectification for Camera-captured Chest X-ray Photograph Interpretation with Synthetic Data. (arXiv:2210.05954v1 [cs.CV])
32. JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional Networks. (arXiv:2210.05959v1 [cs.LG])
33. BORA: Bayesian Optimization for Resource Allocation. (arXiv:2210.05977v1 [cs.LG])
34. Probabilities Are Not Enough: Formal Controller Synthesis for Stochastic Dynamical Models with Epistemic Uncertainty. (arXiv:2210.05989v1 [eess.SY])
35. JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VA. (arXiv:2210.06007v1 [cs.SD])
36. Phantom -- An RL-driven framework for agent-based modeling of complex economic systems and markets. (arXiv:2210.06012v1 [cs.AI])
37. Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics. (arXiv:2210.06023v1 [cs.CL])
38. Question Answering Over Biological Knowledge Graph via Amazon Alexa. (arXiv:2210.06040v1 [cs.AI])
39. Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning. (arXiv:2210.06044v1 [cs.CV])
40. Using Massive Multilingual Pre-Trained Language Models Towards Real Zero-Shot Neural Machine Translation in Clinical Domain. (arXiv:2210.06068v1 [cs.CL])
41. Federated Continual Learning for Text Classification via Selective Inter-client Transfer. (arXiv:2210.06101v1 [cs.CL])
42. THUEE system description for NIST 2020 SRE CTS challenge. (arXiv:2210.06111v1 [cs.SD])
43. Can Artificial Intelligence Reconstruct Ancient Mosaics?. (arXiv:2210.06145v1 [cs.CV])
44. ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding. (arXiv:2210.06155v1 [cs.CL])
45. The Role of Exploration for Task Transfer in Reinforcement Learning. (arXiv:2210.06168v1 [cs.LG])
46. Gotcha: A Challenge-Response System for Real-Time Deepfake Detection. (arXiv:2210.06186v1 [cs.CR])
47. Diffusion Models for Causal Discovery via Topological Ordering. (arXiv:2210.06201v1 [cs.LG])
48. On the Generalizability of ECG-based Stress Detection Models. (arXiv:2210.06225v1 [cs.LG])
49. Quasi-symbolic explanatory NLI via disentanglement: A geometrical examination. (arXiv:2210.06230v1 [cs.CL])
50. Entity Aware Negative Sampling with Auxiliary Loss of False Negative Prediction for Knowledge Graph Embedding. (arXiv:2210.06242v1 [cs.LG])
51. Task Compass: Scaling Multi-task Pre-training with Task Prefix. (arXiv:2210.06277v1 [cs.CL])
52. TwiRGCN: Temporally Weighted Graph Convolution for Question Answering over Temporal Knowledge Graphs. (arXiv:2210.06281v1 [cs.CL])
53. The evolution of AI approaches for motor imagery EEG-based BCIs. (arXiv:2210.06290v1 [eess.SP])
54. Two-stream Network for ECG Signal Classification. (arXiv:2210.06293v1 [eess.SP])
55. Transfer learning on electromyography (EMG) tasks: approaches and beyond. (arXiv:2210.06295v1 [eess.SP])
56. Multimodality Multi-Lead ECG Arrhythmia Classification using Self-Supervised Learning. (arXiv:2210.06297v1 [eess.SP])
57. Cross Task Neural Architecture Search for EEG Signal Classifications. (arXiv:2210.06298v1 [eess.SP])
58. Generalised Mutual Information for Discriminative Clustering. (arXiv:2210.06300v1 [stat.ML])
59. Changing the Representation: Examining Language Representation for Neural Sign Language Production. (arXiv:2210.06312v1 [cs.CL])
60. Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning. (arXiv:2210.06316v1 [cs.AI])
61. A Self-attention Guided Multi-scale Gradient GAN for Diversified X-ray Image Synthesis. (arXiv:2210.06334v1 [eess.IV])
62. Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors. (arXiv:2210.06340v1 [cs.CL])
63. TaskMix: Data Augmentation for Meta-Learning of Spoken Intent Understanding. (arXiv:2210.06341v1 [cs.CL])
64. Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v1 [cs.CL])
65. CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations. (arXiv:2210.06350v1 [cs.LG])
66. Text-to-Audio Grounding Based Novel Metric for Evaluating Audio Caption Similarity. (arXiv:2210.06354v1 [cs.CL])
67. A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v1 [cs.CV])
68. SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning. (arXiv:2105.15013v5 [cs.LG] UPDATED)
69. BF-QC: Belief Functions on Quantum Circuits. (arXiv:2107.03930v2 [quant-ph] UPDATED)
70. The Tensor Brain: A Unified Theory of Perception, Memory and Semantic Decoding. (arXiv:2109.13392v4 [cs.AI] UPDATED)
71. Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits. (arXiv:2110.08627v2 [cs.LG] UPDATED)
72. Collage: Seamless Integration of Deep Learning Backends with Automatic Placement. (arXiv:2111.00655v2 [cs.LG] UPDATED)
73. Deep Probability Estimation. (arXiv:2111.10734v4 [cs.LG] UPDATED)
74. Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples. (arXiv:2111.10962v2 [cs.CL] UPDATED)
75. SCROLLS: Standardized CompaRison Over Long Language Sequences. (arXiv:2201.03533v2 [cs.CL] UPDATED)
76. In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v3 [cs.LG] UPDATED)
77. Unifying and Boosting Gradient-Based Training-Free Neural Architecture Search. (arXiv:2201.09785v2 [cs.LG] UPDATED)
78. Meta-Reinforcement Learning with Self-Modifying Networks. (arXiv:2202.02363v3 [cs.LG] UPDATED)
79. Supported Policy Optimization for Offline Reinforcement Learning. (arXiv:2202.06239v3 [cs.LG] UPDATED)
80. Open-Ended Reinforcement Learning with Neural Reward Functions. (arXiv:2202.08266v2 [cs.LG] UPDATED)
81. Reinforcement learning for automatic quadrilateral mesh generation: a soft actor-critic approach. (arXiv:2203.11203v2 [cs.LG] UPDATED)
82. Frequency and Multi-Scale Selective Kernel Attention for Speaker Verification. (arXiv:2204.01005v4 [eess.AS] UPDATED)
83. Counterfactual harm. (arXiv:2204.12993v4 [cs.AI] UPDATED)
84. Adversarial random forests for density estimation and generative modelling. (arXiv:2205.09435v2 [stat.ML] UPDATED)
85. Risk-Driven Design of Perception Systems. (arXiv:2205.10677v2 [cs.RO] UPDATED)
86. What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v5 [cs.LG] UPDATED)
87. Human-AI Shared Control via Policy Dissection. (arXiv:2206.00152v5 [cs.RO] UPDATED)
88. CoNSoLe: Convex Neural Symbolic Learning. (arXiv:2206.00257v2 [cs.LG] UPDATED)
89. Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules. (arXiv:2206.00823v2 [cs.NE] UPDATED)
90. UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder. (arXiv:2206.02512v3 [eess.AS] UPDATED)
91. Effects of Safety State Augmentation on Safe Exploration. (arXiv:2206.02675v2 [cs.LG] UPDATED)
92. Deep Surrogate Assisted Generation of Environments. (arXiv:2206.04199v3 [cs.AI] UPDATED)
93. ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v2 [cs.CV] UPDATED)
94. NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs. (arXiv:2206.04910v2 [cs.LG] UPDATED)
95. GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v2 [cs.CV] UPDATED)
96. Object Scene Representation Transformer. (arXiv:2206.06922v2 [cs.CV] UPDATED)
97. Interaction-Grounded Learning with Action-inclusive Feedback. (arXiv:2206.08364v2 [cs.LG] UPDATED)
98. EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine. (arXiv:2206.10558v2 [cs.LG] UPDATED)
99. Behavior Transformers: Cloning $k$ modes with one stone. (arXiv:2206.11251v2 [cs.LG] UPDATED)
100. pyKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models. (arXiv:2206.11460v3 [cs.LG] UPDATED)
101. Parametrically Retargetable Decision-Makers Tend To Seek Power. (arXiv:2206.13477v2 [cs.AI] UPDATED)
102. Information Compression and Performance Evaluation of Tic-Tac-Toe's Evaluation Function Using Singular Value Decomposition. (arXiv:2207.02449v4 [cs.LG] UPDATED)
103. Scalable and Privacy-enhanced Graph Generative Model for Graph Neural Networks. (arXiv:2207.04396v2 [cs.LG] UPDATED)
104. GriddlyJS: A Web IDE for Reinforcement Learning. (arXiv:2207.06105v2 [cs.AI] UPDATED)
105. Graph Neural Network Bandits. (arXiv:2207.06456v2 [cs.LG] UPDATED)
106. Self-supervised learning with rotation-invariant kernels. (arXiv:2208.00789v3 [cs.CV] UPDATED)
107. P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting. (arXiv:2208.02812v2 [cs.CV] UPDATED)
108. WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v4 [cs.CL] UPDATED)
109. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v4 [cs.CY] UPDATED)
110. Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions. (arXiv:2209.15565v2 [cs.CL] UPDATED)
111. Beyond Bayes-optimality: meta-learning what you know you don't know. (arXiv:2209.15618v2 [cs.AI] UPDATED)
112. Neural Graphical Models. (arXiv:2210.00453v3 [cs.LG] UPDATED)
113. On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks. (arXiv:2210.02191v2 [cs.LG] UPDATED)
114. Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v2 [cs.CV] UPDATED)
115. Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v2 [cs.CL] UPDATED)
116. Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue. (arXiv:2210.04443v2 [cs.LG] UPDATED)
117. CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure. (arXiv:2210.04633v2 [cs.SE] UPDATED)
118. Extracting or Guessing? Improving Faithfulness of Event Temporal Relation Extraction. (arXiv:2210.04992v2 [cs.CL] UPDATED)
119. Learning with an Evolving Class Ontology. (arXiv:2210.04993v2 [cs.CV] UPDATED)
120. Planning Assembly Sequence with Graph Transformer. (arXiv:2210.05236v2 [cs.AI] UPDATED)
121. PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction. (arXiv:2210.05245v2 [cs.CL] UPDATED)
122. Motion Aware Self-Supervision for Generic Event Boundary Detection. (arXiv:2210.05574v2 [cs.CV] UPDATED)
123. COVID-19-related Nepali Tweets Classification in a Low Resource Setting. (arXiv:2210.05425v1 [cs.CL] CROSS LISTED)

