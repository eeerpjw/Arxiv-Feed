# Your interest papers
---
## cs.CV
---
### NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving. (arXiv:2209.14499v1 [cs.CV])
- Authors : Alexander Popov, Patrik Gebhardt, Ke Chen, Ryan Oldja, Heeseok Lee, Shane Murray, Ruchi Bhargava, Nikolai Smolyanskiy
- Link : [http://arxiv.org/abs/2209.14499](http://arxiv.org/abs/2209.14499)
> ABSTRACT  :  Detecting obstacles is crucial for safe and efficient autonomous driving. To this end, we present NVRadarNet, a deep neural network (DNN) that detects dynamic obstacles and drivable free space using automotive RADAR sensors. The network utilizes temporally accumulated data from multiple RADAR sensors to detect dynamic obstacles and compute their orientation in a top-down bird's-eye view (BEV). The network also regresses drivable free space to detect unclassified obstacles. Our DNN is the first of its kind to utilize sparse RADAR signals in order to perform obstacle and free space detection in **real time** from RADAR data only. The network has been successfully used for perception on our autonomous vehicles in real self-driving scenarios. The network runs faster than **real time** on an embedded GPU and shows good generalization across geographic regions.  
### R2C-GAN: Restore-to-Classify GANs for Blind X-Ray **Restoration** and COVID-19 Classification. (arXiv:2209.14770v1 [eess.IV])
- Authors : Mete Ahishali, Aysen Degerli, Serkan Kiranyaz, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj
- Link : [http://arxiv.org/abs/2209.14770](http://arxiv.org/abs/2209.14770)
> ABSTRACT  :  **Restoration** of poor quality images with a blended set of artifacts plays a vital role for a reliable diagnosis. Existing studies have focused on specific **restoration** problems such as image deblurring, denoising, and **exposure** correction where there is usually a strong assumption on the artifact type and severity. As a pioneer study in blind X-ray **restoration**, we propose a joint model for generic image **restoration** and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). Such a jointly optimized model keeps any disease intact after the **restoration**. Therefore, this will naturally lead to a higher diagnosis performance thanks to the improved X-ray image quality. To accomplish this crucial objective, we define the **restoration** task as an Image-to-Image translation problem from poor quality having noisy, blurry, or over/under-exposed images to high quality image domain. The proposed R2C-GAN model is able to learn forward and inverse transforms between the two domains using unpaired training samples. Simultaneously, the joint classification preserves the disease label during **restoration**. Moreover, the R2C-GANs are equipped with operational layers/neurons reducing the network depth and further boosting both **restoration** and classification performances. The proposed joint model is extensively evaluated over the QaTa-COV19 dataset for Coronavirus Disease 2019 (COVID-19) classification. The proposed **restoration** approach achieves over 90% F1-Score which is significantly higher than the performance of any deep model. Moreover, in the qualitative analysis, the **restoration** performance of R2C-GANs is approved by a group of medical doctors. We share the software implementation at https://github.com/meteahishali/R2C-GAN.  
### GDIP: Gated Differentiable Image Processing for Object-Detection in Adverse Conditions. (arXiv:2209.14922v1 [cs.CV])
- Authors : Sanket Kalwar, Dhruv Patel, Aakash Aanegola, Krishna Reddy, Sourav Garg, Madhava Krishna
- Link : [http://arxiv.org/abs/2209.14922](http://arxiv.org/abs/2209.14922)
> ABSTRACT  :  Detecting objects under adverse weather and lighting conditions is crucial for the safe and continuous operation of an autonomous vehicle, and remains an unsolved problem. We present a Gated Differentiable Image Processing (GDIP) block, a domain-agnostic network architecture, which can be plugged into existing object detection networks (e.g., Yolo) and trained end-to-end with adverse condition images such as those captured under fog and **low light**ing. Our proposed GDIP block learns to enhance images directly through the downstream object detection loss. This is achieved by learning parameters of multiple image pre-processing (IP) techniques that operate concurrently, with their outputs combined using weights learned through a novel gating mechanism. We further improve GDIP through a multi-stage guidance procedure for progressive image **enhancement**. Finally, trading off accuracy for speed, we propose a variant of GDIP that can be used as a regularizer for training Yolo, which eliminates the need for GDIP-based image **enhancement** during inference, resulting in higher throughput and plausible real-world deployment. We demonstrate significant improvement in detection performance over several state-of-the-art methods through quantitative and qualitative studies on synthetic datasets such as PascalVOC, and real-world foggy (RTTS) and **low-light**ing (Ex**Dark**) datasets.  
### Dilated Neighborhood Attention Transformer. (arXiv:2209.15001v1 [cs.CV])
- Authors : Ali Hassani, Humphrey Shi
- Link : [http://arxiv.org/abs/2209.15001](http://arxiv.org/abs/2209.15001)
> ABSTRACT  :  Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or **Swin** Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over attention-based baselines such as NAT and **Swin**, as well as modern convolutional baseline ConvNeXt. Our Large model is ahead of its **Swin** counterpart by 1.5% box AP in COCO object detection, 1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation, and faster in throughput. We believe combinations of NA and DiNA have the potential to empower various tasks beyond those presented in this paper. To support and encourage research in this direction, in vision and beyond, we open-source our project at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.  
### Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v4 [cs.CV] UPDATED)
- Authors : Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, Hang Xu
- Link : [http://arxiv.org/abs/2202.06767](http://arxiv.org/abs/2202.06767)
> ABSTRACT  :  Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/**Swin**T) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, $Wukong_{ViT-L}$ achieves an average accuracy of 73.03%. For the image-text retrieval task, it achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et al. More information can be referred to: https://wukong-dataset.github.io/wukong-dataset/.  
### PnP-ReG: Learned Regularizing Gradient for Plug-and-Play Gradient Descent. (arXiv:2204.13940v2 [eess.IV] UPDATED)
- Authors : Rita Fermanian, Mikael Le, Christine Guillemot
- Link : [http://arxiv.org/abs/2204.13940](http://arxiv.org/abs/2204.13940)
> ABSTRACT  :  The Plug-and-Play (PnP) framework makes it possible to integrate advanced image denoising priors into optimization algorithms, to efficiently solve a variety of image **restoration** tasks generally formulated as Maximum A Posteriori (MAP) estimation problems. The Plug-and-Play alternating direction method of multipliers (ADMM) and the Regularization by Denoising (RED) algorithms are two examples of such methods that made a breakthrough in image **restoration**. However, while the former method only applies to proximal algorithms, it has recently been shown that there exists no regularization that explains the RED algorithm when the denoisers lack Jacobian symmetry, which happen to be the case of most practical denoisers. To the best of our knowledge, there exists no method for training a network that directly represents the gradient of a regularizer, which can be directly used in Plug-and-Play gradient-based algorithms. We show that it is possible to train a network directly modeling the gradient of a MAP regularizer while jointly training the corresponding MAP denoiser. We use this network in gradient-based optimization methods and obtain better results comparing to other generic Plug-and-Play approaches. We also show that the regularizer can be used as a pre-trained network for unrolled gradient descent. Lastly, we show that the resulting denoiser allows for a better convergence of the Plug-and-Play ADMM.  
## eess.IV
---
### R2C-GAN: Restore-to-Classify GANs for Blind X-Ray **Restoration** and COVID-19 Classification. (arXiv:2209.14770v1 [eess.IV])
- Authors : Mete Ahishali, Aysen Degerli, Serkan Kiranyaz, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj
- Link : [http://arxiv.org/abs/2209.14770](http://arxiv.org/abs/2209.14770)
> ABSTRACT  :  **Restoration** of poor quality images with a blended set of artifacts plays a vital role for a reliable diagnosis. Existing studies have focused on specific **restoration** problems such as image deblurring, denoising, and **exposure** correction where there is usually a strong assumption on the artifact type and severity. As a pioneer study in blind X-ray **restoration**, we propose a joint model for generic image **restoration** and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). Such a jointly optimized model keeps any disease intact after the **restoration**. Therefore, this will naturally lead to a higher diagnosis performance thanks to the improved X-ray image quality. To accomplish this crucial objective, we define the **restoration** task as an Image-to-Image translation problem from poor quality having noisy, blurry, or over/under-exposed images to high quality image domain. The proposed R2C-GAN model is able to learn forward and inverse transforms between the two domains using unpaired training samples. Simultaneously, the joint classification preserves the disease label during **restoration**. Moreover, the R2C-GANs are equipped with operational layers/neurons reducing the network depth and further boosting both **restoration** and classification performances. The proposed joint model is extensively evaluated over the QaTa-COV19 dataset for Coronavirus Disease 2019 (COVID-19) classification. The proposed **restoration** approach achieves over 90% F1-Score which is significantly higher than the performance of any deep model. Moreover, in the qualitative analysis, the **restoration** performance of R2C-GANs is approved by a group of medical doctors. We share the software implementation at https://github.com/meteahishali/R2C-GAN.  
### PnP-ReG: Learned Regularizing Gradient for Plug-and-Play Gradient Descent. (arXiv:2204.13940v2 [eess.IV] UPDATED)
- Authors : Rita Fermanian, Mikael Le, Christine Guillemot
- Link : [http://arxiv.org/abs/2204.13940](http://arxiv.org/abs/2204.13940)
> ABSTRACT  :  The Plug-and-Play (PnP) framework makes it possible to integrate advanced image denoising priors into optimization algorithms, to efficiently solve a variety of image **restoration** tasks generally formulated as Maximum A Posteriori (MAP) estimation problems. The Plug-and-Play alternating direction method of multipliers (ADMM) and the Regularization by Denoising (RED) algorithms are two examples of such methods that made a breakthrough in image **restoration**. However, while the former method only applies to proximal algorithms, it has recently been shown that there exists no regularization that explains the RED algorithm when the denoisers lack Jacobian symmetry, which happen to be the case of most practical denoisers. To the best of our knowledge, there exists no method for training a network that directly represents the gradient of a regularizer, which can be directly used in Plug-and-Play gradient-based algorithms. We show that it is possible to train a network directly modeling the gradient of a MAP regularizer while jointly training the corresponding MAP denoiser. We use this network in gradient-based optimization methods and obtain better results comparing to other generic Plug-and-Play approaches. We also show that the regularizer can be used as a pre-trained network for unrolled gradient descent. Lastly, we show that the resulting denoiser allows for a better convergence of the Plug-and-Play ADMM.  
## cs.LG
---
### NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving. (arXiv:2209.14499v1 [cs.CV])
- Authors : Alexander Popov, Patrik Gebhardt, Ke Chen, Ryan Oldja, Heeseok Lee, Shane Murray, Ruchi Bhargava, Nikolai Smolyanskiy
- Link : [http://arxiv.org/abs/2209.14499](http://arxiv.org/abs/2209.14499)
> ABSTRACT  :  Detecting obstacles is crucial for safe and efficient autonomous driving. To this end, we present NVRadarNet, a deep neural network (DNN) that detects dynamic obstacles and drivable free space using automotive RADAR sensors. The network utilizes temporally accumulated data from multiple RADAR sensors to detect dynamic obstacles and compute their orientation in a top-down bird's-eye view (BEV). The network also regresses drivable free space to detect unclassified obstacles. Our DNN is the first of its kind to utilize sparse RADAR signals in order to perform obstacle and free space detection in **real time** from RADAR data only. The network has been successfully used for perception on our autonomous vehicles in real self-driving scenarios. The network runs faster than **real time** on an embedded GPU and shows good generalization across geographic regions.  
### R2C-GAN: Restore-to-Classify GANs for Blind X-Ray **Restoration** and COVID-19 Classification. (arXiv:2209.14770v1 [eess.IV])
- Authors : Mete Ahishali, Aysen Degerli, Serkan Kiranyaz, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj
- Link : [http://arxiv.org/abs/2209.14770](http://arxiv.org/abs/2209.14770)
> ABSTRACT  :  **Restoration** of poor quality images with a blended set of artifacts plays a vital role for a reliable diagnosis. Existing studies have focused on specific **restoration** problems such as image deblurring, denoising, and **exposure** correction where there is usually a strong assumption on the artifact type and severity. As a pioneer study in blind X-ray **restoration**, we propose a joint model for generic image **restoration** and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). Such a jointly optimized model keeps any disease intact after the **restoration**. Therefore, this will naturally lead to a higher diagnosis performance thanks to the improved X-ray image quality. To accomplish this crucial objective, we define the **restoration** task as an Image-to-Image translation problem from poor quality having noisy, blurry, or over/under-exposed images to high quality image domain. The proposed R2C-GAN model is able to learn forward and inverse transforms between the two domains using unpaired training samples. Simultaneously, the joint classification preserves the disease label during **restoration**. Moreover, the R2C-GANs are equipped with operational layers/neurons reducing the network depth and further boosting both **restoration** and classification performances. The proposed joint model is extensively evaluated over the QaTa-COV19 dataset for Coronavirus Disease 2019 (COVID-19) classification. The proposed **restoration** approach achieves over 90% F1-Score which is significantly higher than the performance of any deep model. Moreover, in the qualitative analysis, the **restoration** performance of R2C-GANs is approved by a group of medical doctors. We share the software implementation at https://github.com/meteahishali/R2C-GAN.  
### Dilated Neighborhood Attention Transformer. (arXiv:2209.15001v1 [cs.CV])
- Authors : Ali Hassani, Humphrey Shi
- Link : [http://arxiv.org/abs/2209.15001](http://arxiv.org/abs/2209.15001)
> ABSTRACT  :  Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or **Swin** Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over attention-based baselines such as NAT and **Swin**, as well as modern convolutional baseline ConvNeXt. Our Large model is ahead of its **Swin** counterpart by 1.5% box AP in COCO object detection, 1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation, and faster in throughput. We believe combinations of NA and DiNA have the potential to empower various tasks beyond those presented in this paper. To support and encourage research in this direction, in vision and beyond, we open-source our project at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.  
### Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v4 [cs.CV] UPDATED)
- Authors : Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, Hang Xu
- Link : [http://arxiv.org/abs/2202.06767](http://arxiv.org/abs/2202.06767)
> ABSTRACT  :  Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/**Swin**T) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, $Wukong_{ViT-L}$ achieves an average accuracy of 73.03%. For the image-text retrieval task, it achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et al. More information can be referred to: https://wukong-dataset.github.io/wukong-dataset/.  
### PnP-ReG: Learned Regularizing Gradient for Plug-and-Play Gradient Descent. (arXiv:2204.13940v2 [eess.IV] UPDATED)
- Authors : Rita Fermanian, Mikael Le, Christine Guillemot
- Link : [http://arxiv.org/abs/2204.13940](http://arxiv.org/abs/2204.13940)
> ABSTRACT  :  The Plug-and-Play (PnP) framework makes it possible to integrate advanced image denoising priors into optimization algorithms, to efficiently solve a variety of image **restoration** tasks generally formulated as Maximum A Posteriori (MAP) estimation problems. The Plug-and-Play alternating direction method of multipliers (ADMM) and the Regularization by Denoising (RED) algorithms are two examples of such methods that made a breakthrough in image **restoration**. However, while the former method only applies to proximal algorithms, it has recently been shown that there exists no regularization that explains the RED algorithm when the denoisers lack Jacobian symmetry, which happen to be the case of most practical denoisers. To the best of our knowledge, there exists no method for training a network that directly represents the gradient of a regularizer, which can be directly used in Plug-and-Play gradient-based algorithms. We show that it is possible to train a network directly modeling the gradient of a MAP regularizer while jointly training the corresponding MAP denoiser. We use this network in gradient-based optimization methods and obtain better results comparing to other generic Plug-and-Play approaches. We also show that the regularizer can be used as a pre-trained network for unrolled gradient descent. Lastly, we show that the resulting denoiser allows for a better convergence of the Plug-and-Play ADMM.  
## cs.AI
---
### Dilated Neighborhood Attention Transformer. (arXiv:2209.15001v1 [cs.CV])
- Authors : Ali Hassani, Humphrey Shi
- Link : [http://arxiv.org/abs/2209.15001](http://arxiv.org/abs/2209.15001)
> ABSTRACT  :  Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or **Swin** Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over attention-based baselines such as NAT and **Swin**, as well as modern convolutional baseline ConvNeXt. Our Large model is ahead of its **Swin** counterpart by 1.5% box AP in COCO object detection, 1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation, and faster in throughput. We believe combinations of NA and DiNA have the potential to empower various tasks beyond those presented in this paper. To support and encourage research in this direction, in vision and beyond, we open-source our project at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.  
# Paper List
---
## cs.CV
---
**115** new papers in cs.CV:-) 
1. The Change You Want to See. (arXiv:2209.14341v1 [cs.CV])
2. Semantic Segmentation of Vegetation in Remote Sensing Imagery Using Deep Learning. (arXiv:2209.14364v1 [cs.CV])
3. UNesT: Local Spatial Representation Learning with Hierarchical Transformer for Efficient Medical Segmentation. (arXiv:2209.14378v1 [cs.CV])
4. Assessing Coarse-to-Fine Deep Learning Models for Optic Disc and Cup Segmentation in Fundus Images. (arXiv:2209.14383v1 [cs.CV])
5. Feature Decoupling in Self-supervised Representation Learning for Open Set Recognition. (arXiv:2209.14385v1 [cs.CV])
6. Variational Bayes for robust radar single object tracking. (arXiv:2209.14397v1 [eess.SP])
7. RADACS: Towards Higher-Order Reasoning using Action Recognition in Autonomous Vehicles. (arXiv:2209.14408v1 [cs.CV])
8. Category-Level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondences. (arXiv:2209.14419v1 [cs.CV])
9. View-Invariant Localization using Semantic Objects in Changing Environments. (arXiv:2209.14426v1 [cs.RO])
10. Increasing the Accuracy of a Neural Network Using Frequency Selective Mesh-to-Grid Resampling. (arXiv:2209.14431v1 [cs.CV])
11. Efficient Medical Image Assessment via Self-supervised Learning. (arXiv:2209.14434v1 [cs.CV])
12. Out-of-Distribution Detection for LiDAR-based 3D Object Detection. (arXiv:2209.14435v1 [cs.CV])
13. GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v1 [cs.LG])
14. Visual Detection of Diver Attentiveness for Underwater Human-Robot Interaction. (arXiv:2209.14447v1 [cs.RO])
15. Synthesizing Annotated Image and Video Data Using a Rendering-Based Pipeline for Improved License Plate Recognition. (arXiv:2209.14448v1 [cs.CV])
16. CompNet: A Designated Model to Handle Combinations of Images and Designed features. (arXiv:2209.14454v1 [cs.CV])
17. Machine Learning for Optical Motion Capture-driven Musculoskeletal Modeling from Inertial Motion Capture Data. (arXiv:2209.14456v1 [cs.LG])
18. Reducing Positional Variance in Cross-sectional Abdominal CT Slices with Deep Conditional Generative Models. (arXiv:2209.14467v1 [eess.IV])
19. medigan: A Python Library of Pretrained Generative Models for Enriched Data Access in Medical Imaging. (arXiv:2209.14472v1 [eess.IV])
20. Intrinsic Dimensionality Estimation within Tight Localities: A Theoretical and Experimental Analysis. (arXiv:2209.14475v1 [cs.LG])
21. Semantics-Guided Object Removal for Facial Images: with Broad Applicability and Robust Style Preservation. (arXiv:2209.14479v1 [cs.CV])
22. Re-Imagen: Retrieval-Augmented Text-to-Image Generator. (arXiv:2209.14491v1 [cs.CV])
23. Teaching Where to Look: Attention Similarity Knowledge Distillation for Low Resolution Face Recognition. (arXiv:2209.14498v1 [cs.CV])
24. NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving. (arXiv:2209.14499v1 [cs.CV])
25. Self-Configurable Stabilized Real-Time Detection Learning for Autonomous Driving Applications. (arXiv:2209.14525v1 [cs.CV])
26. Motion and Appearance Adaptation for Cross-Domain Motion Transfer. (arXiv:2209.14529v1 [cs.CV])
27. NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction. (arXiv:2209.14540v1 [eess.IV])
28. Regularizing Neural Network Training via Identity-wise Discriminative Feature Suppression. (arXiv:2209.14553v1 [cs.CV])
29. Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation. (arXiv:2209.14566v1 [eess.IV])
30. Correcting the Sub-optimal Bit Allocation. (arXiv:2209.14575v1 [cs.CV])
31. Spatial Moment Pooling Improves Neural Image Assessment. (arXiv:2209.14583v1 [cs.CV])
32. PerSign: Personalized Bangladeshi Sign Letters Synthesis. (arXiv:2209.14591v1 [cs.CV])
33. Denoising MCMC for Accelerating Diffusion-Based Generative Models. (arXiv:2209.14593v1 [cs.LG])
34. Online pseudo labeling for polyp segmentation with momentum networks. (arXiv:2209.14599v1 [cs.CV])
35. Exploring Cross-Point Embeddings for 3D Dense Uncertainty Estimation. (arXiv:2209.14602v1 [cs.RO])
36. Dataset Distillation for Medical Dataset Sharing. (arXiv:2209.14603v1 [cs.CR])
37. Spherical Image Inpainting with Frame Transformation and Data-driven Prior Deep Networks. (arXiv:2209.14604v1 [eess.IV])
38. Dataset Distillation using Parameter Pruning. (arXiv:2209.14609v1 [cs.CV])
39. Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. (arXiv:2209.14610v1 [cs.LG])
40. Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v1 [cs.LG])
41. Compressed Gastric Image Generation Based on Soft-Label Dataset Distillation for Medical Data Sharing. (arXiv:2209.14635v1 [cs.CV])
42. Increasing Model Generalizability for Unsupervised Domain Adaptation. (arXiv:2209.14644v1 [cs.LG])
43. Bounded Future MS-TCN++ for surgical gesture recognition. (arXiv:2209.14647v1 [cs.CV])
44. Correlated Feature Aggregation by Region Helps Distinguish Aggressive from Indolent Clear Cell Renal Cell Carcinoma Subtypes on CT. (arXiv:2209.14657v1 [eess.IV])
45. Diffusion Posterior Sampling for General Noisy Inverse Problems. (arXiv:2209.14687v1 [stat.ML])
46. Prompt-guided Scene Generation for 3D Zero-Shot Learning. (arXiv:2209.14690v1 [cs.CV])
47. Digital and Physical Face Attacks: Reviewing and One Step Further. (arXiv:2209.14692v1 [cs.CV])
48. Creative Painting with Latent Diffusion Models. (arXiv:2209.14697v1 [cs.CV])
49. Facial Landmark Predictions with Applications to Metaverse. (arXiv:2209.14698v1 [cs.CV])
50. Low-Resolution Action Recognition for Tiny Actions Challenge. (arXiv:2209.14711v1 [cs.CV])
51. In Search of Projectively Equivariant Neural Networks. (arXiv:2209.14719v1 [cs.CV])
52. Hyper-Representations as Generative Models: Sampling Unseen Neural Network Weights. (arXiv:2209.14733v1 [cs.LG])
53. Dataset Complexity Assessment Based on Cumulative Maximum Scaled Area Under Laplacian Spectrum. (arXiv:2209.14743v1 [cs.CV])
54. A Multi-Agent Framework for the Asynchronous and Collaborative Extension of Multitask ML Systems. (arXiv:2209.14745v1 [cs.LG])
55. Speeding Up Action Recognition Using Dynamic Accumulation of Residuals in Compressed Domain. (arXiv:2209.14757v1 [cs.CV])
56. R2C-GAN: Restore-to-Classify GANs for Blind X-Ray **Restoration** and COVID-19 Classification. (arXiv:2209.14770v1 [eess.IV])
57. RECALL: Rehearsal-free Continual Learning for Object Classification. (arXiv:2209.14774v1 [cs.CV])
58. Batch Normalization Explained. (arXiv:2209.14778v1 [cs.LG])
59. A case study of spatiotemporal forecasting techniques for weather forecasting. (arXiv:2209.14782v1 [cs.LG])
60. Training \beta-VAE by Aggregating a Learned Gaussian Posterior with a Decoupled Decoder. (arXiv:2209.14783v1 [cs.CV])
61. Make-A-Video: Text-to-Video Generation without Text-Video Data. (arXiv:2209.14792v1 [cs.CV])
62. SymmNeRF: Learning to Explore Symmetry Prior for Single-View View Synthesis. (arXiv:2209.14819v1 [cs.CV])
63. Denoising Diffusion Probabilistic Models for Styled Walking Synthesis. (arXiv:2209.14828v1 [cs.CV])
64. Lightweight Monocular Depth Estimation with an Edge Guided Network. (arXiv:2209.14829v1 [cs.CV])
65. Access Control with Encrypted Feature Maps for Object Detection Models. (arXiv:2209.14831v1 [cs.CV])
66. Federated Stain Normalization for Computational Pathology. (arXiv:2209.14849v1 [eess.IV])
67. Meta Knowledge Condensation for Federated Learning. (arXiv:2209.14851v1 [cs.LG])
68. 4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object Proposal Generation and Aggregation. (arXiv:2209.14858v1 [cs.CV])
69. Bridging the Gap to Real-World Object-Centric Learning. (arXiv:2209.14860v1 [cs.CV])
70. Mask-Guided Image Person Removal with Data Synthesis. (arXiv:2209.14890v1 [cs.CV])
71. Evaluating the temporal understanding of neural networks on event-based action recognition with DVS-Gesture-Chain. (arXiv:2209.14915v1 [cs.CV])
72. Human Motion Diffusion Model. (arXiv:2209.14916v1 [cs.CV])
73. GDIP: Gated Differentiable Image Processing for Object-Detection in Adverse Conditions. (arXiv:2209.14922v1 [cs.CV])
74. Domain-Unified Prompt Representations for Source-Free Domain Generalization. (arXiv:2209.14926v1 [cs.CV])
75. Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus. (arXiv:2209.14927v1 [cs.CV])
76. Contrastive Unsupervised Learning of World Model with Invariant Causal Features. (arXiv:2209.14932v1 [cs.LG])
77. EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual and Language Learning. (arXiv:2209.14941v1 [cs.CV])
78. EiHi Net: Out-of-Distribution Generalization Paradigm. (arXiv:2209.14946v1 [cs.CV])
79. DirectTracker: 3D Multi-Object Tracking Using Direct Image Alignment and Photometric Bundle Adjustment. (arXiv:2209.14965v1 [cs.CV])
80. Transfer Learning with Pretrained Remote Sensing Transformers. (arXiv:2209.14969v1 [cs.CV])
81. 3D Rendering Framework for Data Augmentation in Optical Character Recognition. (arXiv:2209.14970v1 [cs.CV])
82. Hyperspectral Remote Sensing Benchmark Database for Oil Spill Detection with an Isolation Forest-Guided Unsupervised Detector. (arXiv:2209.14971v1 [cs.CV])
83. Deep Unfolding for Iterative Stripe Noise Removal. (arXiv:2209.14973v1 [eess.IV])
84. Greybox XAI: a Neural-Symbolic learning framework to produce interpretable predictions for image classification. (arXiv:2209.14974v1 [cs.CV])
85. DreamFusion: Text-to-3D using 2D Diffusion. (arXiv:2209.14988v1 [cs.CV])
86. REST: REtrieve & Self-Train for generative action recognition. (arXiv:2209.15000v1 [cs.CV])
87. Dilated Neighborhood Attention Transformer. (arXiv:2209.15001v1 [cs.CV])
88. Effective Vision Transformer Training: A Data-Centric Perspective. (arXiv:2209.15006v1 [cs.CV])
89. Understanding Collapse in Non-Contrastive Learning. (arXiv:2209.15007v1 [cs.LG])
90. Lip-reading with Densely Connected Temporal Convolutional Networks. (arXiv:2009.14233v3 [cs.CV] UPDATED)
91. Self-Supervised Learning for Gastritis Detection with Gastric X-ray Images. (arXiv:2104.02864v3 [cs.CV] UPDATED)
92. Dataset Summarization by K Principal Concepts. (arXiv:2104.03952v2 [cs.CV] UPDATED)
93. Comparison of Consecutive and Re-stained Sections for Image Registration in Histopathology. (arXiv:2106.13150v2 [eess.IV] UPDATED)
94. Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v4 [cs.LG] UPDATED)
95. Similarity Contrastive Estimation for Self-Supervised Soft Contrastive Learning. (arXiv:2111.14585v2 [cs.CV] UPDATED)
96. PreViTS: Contrastive Pretraining with Video Tracking Supervision. (arXiv:2112.00804v2 [cs.CV] UPDATED)
97. From Coarse to Fine-grained Concept based Discrimination for Phrase Detection. (arXiv:2112.03237v2 [cs.CV] UPDATED)
98. DiffuseMorph: Unsupervised Deformable Image Registration Using Diffusion Model. (arXiv:2112.05149v2 [eess.IV] UPDATED)
99. Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v4 [cs.CV] UPDATED)
100. Ensembles of Vision Transformers as a New Paradigm for Automated Classification in Ecology. (arXiv:2203.01726v3 [cs.CV] UPDATED)
101. Few-Shot Object Detection with Fully Cross-Transformer. (arXiv:2203.15021v2 [cs.CV] UPDATED)
102. ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v4 [cs.CV] UPDATED)
103. PnP-ReG: Learned Regularizing Gradient for Plug-and-Play Gradient Descent. (arXiv:2204.13940v2 [eess.IV] UPDATED)
104. Sparse Visual Counterfactual Explanations in Image Space. (arXiv:2205.07972v2 [cs.CV] UPDATED)
105. DynPL-SVO: A New Method Using Point and Line Features for Stereo Visual Odometry in Dynamic Scenes. (arXiv:2205.08207v2 [cs.CV] UPDATED)
106. DSLA: Dynamic smooth label assignment for efficient anchor-free object detection. (arXiv:2208.00817v2 [cs.CV] UPDATED)
107. Pyramidal Denoising Diffusion Probabilistic Models. (arXiv:2208.01864v2 [cs.CV] UPDATED)
108. Prompt-Matched Semantic Segmentation. (arXiv:2208.10159v2 [cs.CV] UPDATED)
109. Constraining Representations Yields Models That Know What They Don't Know. (arXiv:2208.14488v2 [cs.LG] UPDATED)
110. Training Strategies for Improved Lip-reading. (arXiv:2209.01383v3 [cs.CV] UPDATED)
111. Distribution Aware Metrics for Conditional Natural Language Generation. (arXiv:2209.07518v2 [cs.CL] UPDATED)
112. Look where you look! Saliency-guided Q-networks for visual RL tasks. (arXiv:2209.09203v2 [cs.LG] UPDATED)
113. A Morphology Focused Diffusion Probabilistic Model for Synthesis of Histopathology Images. (arXiv:2209.13167v2 [eess.IV] UPDATED)
114. Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v2 [eess.IV] UPDATED)
115. Multi-scale Attention Network for Single Image Super-Resolution. (arXiv:2209.14145v2 [eess.IV] UPDATED)
## eess.IV
---
**22** new papers in eess.IV:-) 
1. Increasing the Accuracy of a Neural Network Using Frequency Selective Mesh-to-Grid Resampling. (arXiv:2209.14431v1 [cs.CV])
2. Out-of-Distribution Detection for LiDAR-based 3D Object Detection. (arXiv:2209.14435v1 [cs.CV])
3. Synthesizing Annotated Image and Video Data Using a Rendering-Based Pipeline for Improved License Plate Recognition. (arXiv:2209.14448v1 [cs.CV])
4. Machine Learning for Optical Motion Capture-driven Musculoskeletal Modeling from Inertial Motion Capture Data. (arXiv:2209.14456v1 [cs.LG])
5. Reducing Positional Variance in Cross-sectional Abdominal CT Slices with Deep Conditional Generative Models. (arXiv:2209.14467v1 [eess.IV])
6. medigan: A Python Library of Pretrained Generative Models for Enriched Data Access in Medical Imaging. (arXiv:2209.14472v1 [eess.IV])
7. NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction. (arXiv:2209.14540v1 [eess.IV])
8. Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation. (arXiv:2209.14566v1 [eess.IV])
9. Dataset Distillation for Medical Dataset Sharing. (arXiv:2209.14603v1 [cs.CR])
10. Spherical Image Inpainting with Frame Transformation and Data-driven Prior Deep Networks. (arXiv:2209.14604v1 [eess.IV])
11. Correlated Feature Aggregation by Region Helps Distinguish Aggressive from Indolent Clear Cell Renal Cell Carcinoma Subtypes on CT. (arXiv:2209.14657v1 [eess.IV])
12. R2C-GAN: Restore-to-Classify GANs for Blind X-Ray **Restoration** and COVID-19 Classification. (arXiv:2209.14770v1 [eess.IV])
13. Federated Stain Normalization for Computational Pathology. (arXiv:2209.14849v1 [eess.IV])
14. 3D Rendering Framework for Data Augmentation in Optical Character Recognition. (arXiv:2209.14970v1 [cs.CV])
15. Deep Unfolding for Iterative Stripe Noise Removal. (arXiv:2209.14973v1 [eess.IV])
16. Comparison of Consecutive and Re-stained Sections for Image Registration in Histopathology. (arXiv:2106.13150v2 [eess.IV] UPDATED)
17. DiffuseMorph: Unsupervised Deformable Image Registration Using Diffusion Model. (arXiv:2112.05149v2 [eess.IV] UPDATED)
18. Unsupervised Learning From Incomplete Measurements for Inverse Problems. (arXiv:2201.12151v4 [stat.ML] UPDATED)
19. PnP-ReG: Learned Regularizing Gradient for Plug-and-Play Gradient Descent. (arXiv:2204.13940v2 [eess.IV] UPDATED)
20. A Morphology Focused Diffusion Probabilistic Model for Synthesis of Histopathology Images. (arXiv:2209.13167v2 [eess.IV] UPDATED)
21. Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v2 [eess.IV] UPDATED)
22. Multi-scale Attention Network for Single Image Super-Resolution. (arXiv:2209.14145v2 [eess.IV] UPDATED)
## cs.LG
---
**198** new papers in cs.LG:-) 
1. Using Multivariate Linear Regression for Biochemical Oxygen Demand Prediction in Waste Water. (arXiv:2209.14297v1 [q-bio.OT])
2. Scalably learning quantum many-body Hamiltonians from dynamical data. (arXiv:2209.14328v1 [quant-ph])
3. Text Independent Speaker Identification System for Access Control. (arXiv:2209.14335v1 [eess.AS])
4. Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning. (arXiv:2209.14344v1 [cs.LG])
5. Generalized Kernel Regularized Least Squares. (arXiv:2209.14355v1 [stat.ML])
6. Improving alignment of dialogue agents via targeted human judgements. (arXiv:2209.14375v1 [cs.LG])
7. Feature Decoupling in Self-supervised Representation Learning for Open Set Recognition. (arXiv:2209.14385v1 [cs.CV])
8. Downstream Datasets Make Surprisingly Good Pretraining Corpora. (arXiv:2209.14389v1 [cs.CL])
9. Neighborhood Gradient Clustering: An Efficient Decentralized Learning Method for Non-IID Data Distributions. (arXiv:2209.14390v1 [cs.LG])
10. Variational Bayes for robust radar single object tracking. (arXiv:2209.14397v1 [eess.SP])
11. FIRE: A Failure-Adaptive Reinforcement Learning Framework for Edge Computing Migrations. (arXiv:2209.14399v1 [cs.NI])
12. Learning to Explain Graph Neural Networks. (arXiv:2209.14402v1 [cs.LG])
13. Biological connectomes as a representation for the architecture of artificial neural networks. (arXiv:2209.14406v1 [cs.NE])
14. RADACS: Towards Higher-Order Reasoning using Action Recognition in Autonomous Vehicles. (arXiv:2209.14408v1 [cs.CV])
15. Applying Machine Learning for Duplicate Detection, Throttling and Prioritization of Equipment Commissioning Audits at Fulfillment Network. (arXiv:2209.14409v1 [cs.LG])
16. Masked Multi-Step Multivariate Time Series Forecasting with Future Information. (arXiv:2209.14413v1 [cs.LG])
17. Optimistic Posterior Sampling for Reinforcement Learning with Few Samples and Tight Guarantees. (arXiv:2209.14414v1 [stat.ML])
18. Minimax Optimal Kernel Operator Learning via Multilevel Training. (arXiv:2209.14430v1 [cs.LG])
19. Out-of-Distribution Detection for LiDAR-based 3D Object Detection. (arXiv:2209.14435v1 [cs.CV])
20. Breaking Time Invariance: Assorted-Time Normalization for RNNs. (arXiv:2209.14439v1 [cs.LG])
21. GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v1 [cs.LG])
22. Parameterized Quantum Circuits with Quantum Kernels for Machine Learning: A Hybrid Quantum-Classical Approach. (arXiv:2209.14449v1 [quant-ph])
23. Machine Learning for Optical Motion Capture-driven Musculoskeletal Modeling from Inertial Motion Capture Data. (arXiv:2209.14456v1 [cs.LG])
24. The Chamber Ensemble Generator: Limitless High-Quality MIR Data via Generative Modeling. (arXiv:2209.14458v1 [cs.SD])
25. Neural Methods for Logical Reasoning Over Knowledge Graphs. (arXiv:2209.14464v1 [cs.AI])
26. medigan: A Python Library of Pretrained Generative Models for Enriched Data Access in Medical Imaging. (arXiv:2209.14472v1 [eess.IV])
27. Intrinsic Dimensionality Estimation within Tight Localities: A Theoretical and Experimental Analysis. (arXiv:2209.14475v1 [cs.LG])
28. Semantics-Guided Object Removal for Facial Images: with Broad Applicability and Robust Style Preservation. (arXiv:2209.14479v1 [cs.CV])
29. Hierarchical Training of Deep Ensemble Policies for Reinforcement Learning in Continuous Spaces. (arXiv:2209.14488v1 [cs.LG])
30. Re-Imagen: Retrieval-Augmented Text-to-Image Generator. (arXiv:2209.14491v1 [cs.CV])
31. NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving. (arXiv:2209.14499v1 [cs.CV])
32. Bidirectional Language Models Are Also Few-shot Learners. (arXiv:2209.14500v1 [cs.LG])
33. On Quantum Speedups for Nonconvex Optimization via Quantum Tunneling Walks. (arXiv:2209.14501v1 [quant-ph])
34. How Does Value Distribution in Distributional Reinforcement Learning Help Optimization?. (arXiv:2209.14513v1 [cs.LG])
35. How Powerful is Implicit Denoising in Graph Neural Networks. (arXiv:2209.14514v1 [cs.LG])
36. Label driven Knowledge Distillation for Federated Learning with non-IID Data. (arXiv:2209.14520v1 [cs.LG])
37. Low-Stabilizer-Complexity Quantum States Are Not Pseudorandom. (arXiv:2209.14530v1 [quant-ph])
38. Feature Selection via the Intervened Interpolative Decomposition and its Application in Diversifying Quantitative Strategies. (arXiv:2209.14532v1 [cs.LG])
39. Convergence of the mini-batch SIHT algorithm. (arXiv:2209.14536v1 [cs.LG])
40. A Secure Federated Learning Framework for Residential Short Term Load Forecasting. (arXiv:2209.14547v1 [cs.CR])
41. Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling. (arXiv:2209.14548v1 [cs.LG])
42. Regularizing Neural Network Training via Identity-wise Discriminative Feature Suppression. (arXiv:2209.14553v1 [cs.CV])
43. Computational Complexity of Sub-linear Convergent Algorithms. (arXiv:2209.14558v1 [cs.LG])
44. Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation. (arXiv:2209.14566v1 [eess.IV])
45. Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies. (arXiv:2209.14568v1 [stat.ML])
46. Rectified Flow: A Marginal Preserving Approach to Optimal Transport. (arXiv:2209.14577v1 [stat.ML])
47. Denoising MCMC for Accelerating Diffusion-Based Generative Models. (arXiv:2209.14593v1 [cs.LG])
48. Bayesian Neural Network Versus Ex-Post Calibration For Prediction Uncertainty. (arXiv:2209.14594v1 [cs.LG])
49. Dynamic Surrogate Switching: Sample-Efficient Search for Factorization Machine Configurations in Online Recommendations. (arXiv:2209.14598v1 [cs.LG])
50. Dataset Distillation for Medical Dataset Sharing. (arXiv:2209.14603v1 [cs.CR])
51. Dataset Distillation using Parameter Pruning. (arXiv:2209.14609v1 [cs.CV])
52. Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. (arXiv:2209.14610v1 [cs.LG])
53. Proportional Multicalibration. (arXiv:2209.14613v1 [cs.LG])
54. Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v1 [cs.LG])
55. An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation. (arXiv:2209.14627v1 [cs.CL])
56. Compressed Gastric Image Generation Based on Soft-Label Dataset Distillation for Medical Data Sharing. (arXiv:2209.14635v1 [cs.CV])
57. Increasing Model Generalizability for Unsupervised Domain Adaptation. (arXiv:2209.14644v1 [cs.LG])
58. Causal inference in drug discovery and development. (arXiv:2209.14664v1 [q-bio.QM])
59. Towards Equalised Odds as Fairness Metric in Academic Performance Prediction. (arXiv:2209.14670v1 [cs.LG])
60. Diffusion Posterior Sampling for General Noisy Inverse Problems. (arXiv:2209.14687v1 [stat.ML])
61. Creative Painting with Latent Diffusion Models. (arXiv:2209.14697v1 [cs.CV])
62. Facial Landmark Predictions with Applications to Metaverse. (arXiv:2209.14698v1 [cs.CV])
63. FastPacket: Towards Pre-trained Packets Embedding based on FastText for next-generation NIDS. (arXiv:2209.14727v1 [cs.CR])
64. Hyper-Representations as Generative Models: Sampling Unseen Neural Network Weights. (arXiv:2209.14733v1 [cs.LG])
65. DiGress: Discrete Denoising diffusion for graph generation. (arXiv:2209.14734v1 [cs.LG])
66. Optimal Stopping with Gaussian Processes. (arXiv:2209.14738v1 [stat.ML])
67. Learning Gradient-based Mixup towards Flatter Minima for Domain Generalization. (arXiv:2209.14742v1 [cs.LG])
68. Dataset Complexity Assessment Based on Cumulative Maximum Scaled Area Under Laplacian Spectrum. (arXiv:2209.14743v1 [cs.CV])
69. A Multi-Agent Framework for the Asynchronous and Collaborative Extension of Multitask ML Systems. (arXiv:2209.14745v1 [cs.LG])
70. Non-contrastive approaches to similarity learning: positive examples are all you need. (arXiv:2209.14750v1 [cs.AI])
71. Model Zoos: A Dataset of Diverse Populations of Neural Network Models. (arXiv:2209.14764v1 [cs.LG])
72. R2C-GAN: Restore-to-Classify GANs for Blind X-Ray **Restoration** and COVID-19 Classification. (arXiv:2209.14770v1 [eess.IV])
73. Batch Normalization Explained. (arXiv:2209.14778v1 [cs.LG])
74. Learning Parsimonious Dynamics for Generalization in Reinforcement Learning. (arXiv:2209.14781v1 [cs.LG])
75. A case study of spatiotemporal forecasting techniques for weather forecasting. (arXiv:2209.14782v1 [cs.LG])
76. Sparse PCA With Multiple Components. (arXiv:2209.14790v1 [math.OC])
77. Make-A-Video: Text-to-Video Generation without Text-Video Data. (arXiv:2209.14792v1 [cs.CV])
78. polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics. (arXiv:2209.14803v1 [cond-mat.mtrl-sci])
79. Analyzing Diffusion as Serial Reproduction. (arXiv:2209.14821v1 [cs.LG])
80. Trading off Quality for Efficiency of Community Detection: An Inductive Method across Graphs. (arXiv:2209.14825v1 [cs.SI])
81. Towards Lightweight Black-Box Attacks against Deep Neural Networks. (arXiv:2209.14826v1 [cs.LG])
82. On the Convergence of AdaGrad on $\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration. (arXiv:2209.14827v1 [cs.LG])
83. Denoising Diffusion Probabilistic Models for Styled Walking Synthesis. (arXiv:2209.14828v1 [cs.CV])
84. Access Control with Encrypted Feature Maps for Object Detection Models. (arXiv:2209.14831v1 [cs.CV])
85. Meta Knowledge Condensation for Federated Learning. (arXiv:2209.14851v1 [cs.LG])
86. META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for Unbounded Functions. (arXiv:2209.14853v1 [cs.LG])
87. Continuous PDE Dynamics Forecasting with Implicit Neural Representations. (arXiv:2209.14855v1 [cs.LG])
88. Bridging the Gap to Real-World Object-Centric Learning. (arXiv:2209.14860v1 [cs.CV])
89. Neural Networks Efficiently Learn Low-Dimensional Representations with SGD. (arXiv:2209.14863v1 [stat.ML])
90. Sequential Attention for Feature Selection. (arXiv:2209.14881v1 [cs.LG])
91. Joint Embedding Self-Supervised Learning in the Kernel Regime. (arXiv:2209.14884v1 [cs.LG])
92. Joint Optimization of Energy Consumption and Completion Time in Federated Learning. (arXiv:2209.14900v1 [cs.LG])
93. Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations. (arXiv:2209.14905v1 [cs.LG])
94. Patients' Severity States Classification based on Electronic Health Record (EHR) Data using Multiple Machine Learning and Deep Learning Approaches. (arXiv:2209.14907v1 [cs.LG])
95. Evaluating the temporal understanding of neural networks on event-based action recognition with DVS-Gesture-Chain. (arXiv:2209.14915v1 [cs.CV])
96. Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus. (arXiv:2209.14927v1 [cs.CV])
97. Graph Anomaly Detection with Graph Neural Networks: Current Status and Challenges. (arXiv:2209.14930v1 [cs.LG])
98. Contrastive Unsupervised Learning of World Model with Invariant Causal Features. (arXiv:2209.14932v1 [cs.LG])
99. Training Normalizing Flows from Dependent Data. (arXiv:2209.14933v1 [cs.LG])
100. Does Zero-Shot Reinforcement Learning Exist?. (arXiv:2209.14935v1 [cs.LG])
101. NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizers. (arXiv:2209.14937v1 [math.OC])
102. Reinforcement Learning Algorithms: An Overview and Classification. (arXiv:2209.14940v1 [cs.LG])
103. Statistical Learning and Inverse Problems: An Stochastic Gradient Approach. (arXiv:2209.14967v1 [stat.ML])
104. Hyperspectral Remote Sensing Benchmark Database for Oil Spill Detection with an Isolation Forest-Guided Unsupervised Detector. (arXiv:2209.14971v1 [cs.CV])
105. Deep Unfolding for Iterative Stripe Noise Removal. (arXiv:2209.14973v1 [eess.IV])
106. Greybox XAI: a Neural-Symbolic learning framework to produce interpretable predictions for image classification. (arXiv:2209.14974v1 [cs.CV])
107. Causal Inference via Nonlinear Variable Decorrelation for Healthcare Applications. (arXiv:2209.14975v1 [cs.LG])
108. Transformer Meets Boundary Value Inverse Problems. (arXiv:2209.14977v1 [cs.LG])
109. Enumeration of max-pooling responses with generalized permutohedra. (arXiv:2209.14978v1 [math.CO])
110. Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging. (arXiv:2209.14981v1 [cs.LG])
111. No Free Lunch in "Privacy for Free: How does Dataset Condensation Help Privacy". (arXiv:2209.14987v1 [cs.LG])
112. DreamFusion: Text-to-3D using 2D Diffusion. (arXiv:2209.14988v1 [cs.CV])
113. Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms. (arXiv:2209.14990v1 [cs.LG])
114. Equivariant maps from invariant functions. (arXiv:2209.14991v1 [stat.ML])
115. Multiple Modes for Continual Learning. (arXiv:2209.14996v1 [cs.LG])
116. Optimistic MLE -- A Generic Model-based Algorithm for Partially Observable Sequential Decision Making. (arXiv:2209.14997v1 [cs.LG])
117. REST: REtrieve & Self-Train for generative action recognition. (arXiv:2209.15000v1 [cs.CV])
118. Dilated Neighborhood Attention Transformer. (arXiv:2209.15001v1 [cs.CV])
119. Understanding Collapse in Non-Contrastive Learning. (arXiv:2209.15007v1 [cs.LG])
120. Single-Node Attacks for Fooling Graph Neural Networks. (arXiv:2011.03574v2 [cs.LG] UPDATED)
121. A Survey on Multimodal Disinformation Detection. (arXiv:2103.12541v2 [cs.MM] UPDATED)
122. Dataset Summarization by K Principal Concepts. (arXiv:2104.03952v2 [cs.CV] UPDATED)
123. Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle. (arXiv:2105.14559v2 [cs.LG] UPDATED)
124. Graph Neural Networks in Network Neuroscience. (arXiv:2106.03535v2 [cs.LG] UPDATED)
125. LaplaceNet: A Hybrid Graph-Energy Neural Network for Deep Semi-Supervised Classification. (arXiv:2106.04527v3 [cs.LG] UPDATED)
126. A Decision Support System for Safer Airplane Landings: Predicting Runway Conditions Using XGBoost and Explainable AI. (arXiv:2107.04010v2 [cs.CY] UPDATED)
127. Learning Causal Models from Conditional Moment Restrictions by Importance Weighting. (arXiv:2108.01312v2 [econ.EM] UPDATED)
128. Spectral Bias in Practice: The Role of Function Frequency in Generalization. (arXiv:2110.02424v4 [cs.LG] UPDATED)
129. Extracting Dynamical Models from Data. (arXiv:2110.06917v3 [cs.LG] UPDATED)
130. Heterogeneous Graph-Based Multimodal Brain Network Learning. (arXiv:2110.08465v5 [cs.LG] UPDATED)
131. Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v4 [cs.LG] UPDATED)
132. From Kepler to Newton: Explainable AI for Science Discovery. (arXiv:2111.12210v6 [cs.AI] UPDATED)
133. False Data Injection Threats in Active Distribution Systems: A Comprehensive Survey. (arXiv:2111.14251v2 [cs.CR] UPDATED)
134. Similarity Contrastive Estimation for Self-Supervised Soft Contrastive Learning. (arXiv:2111.14585v2 [cs.CV] UPDATED)
135. DiffuseMorph: Unsupervised Deformable Image Registration Using Diffusion Model. (arXiv:2112.05149v2 [eess.IV] UPDATED)
136. D-HYPR: Harnessing Neighborhood Modeling and Asymmetry Preservation for Digraph Representation Learning. (arXiv:2112.11734v2 [cs.LG] UPDATED)
137. Revisiting Global Pooling through the Lens of Optimal Transport. (arXiv:2201.09191v2 [cs.LG] UPDATED)
138. Unsupervised Learning From Incomplete Measurements for Inverse Problems. (arXiv:2201.12151v4 [stat.ML] UPDATED)
139. Distributional Reinforcement Learning via Sinkhorn Iterations. (arXiv:2202.00769v3 [cs.LG] UPDATED)
140. Algorithms that get old : the case of generative deep neural networks. (arXiv:2202.03008v3 [stat.ML] UPDATED)
141. Physics-informed neural networks for solving parametric magnetostatic problems. (arXiv:2202.04041v2 [cs.CE] UPDATED)
142. Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v4 [cs.CV] UPDATED)
143. On the influence of stochastic roundoff errors on the convergence of the gradient descent method with low-precision floating-point computation. (arXiv:2202.12276v2 [cs.LG] UPDATED)
144. Ensembles of Vision Transformers as a New Paradigm for Automated Classification in Ecology. (arXiv:2203.01726v3 [cs.CV] UPDATED)
145. The Role of Local Steps in Local SGD. (arXiv:2203.06798v3 [cs.LG] UPDATED)
146. STCGAT: A Spatio-temporal Causal Graph Attention Network for traffic flow prediction in Intelligent Transportation Systems. (arXiv:2203.10749v3 [cs.LG] UPDATED)
147. Gradient flows and randomised thresholding: sparse inversion and classification. (arXiv:2203.11555v2 [math.NA] UPDATED)
148. GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets. (arXiv:2204.02782v2 [cs.LG] UPDATED)
149. Surface Similarity Parameter: A New Machine Learning Loss Metric for Oscillatory Spatio-Temporal Data. (arXiv:2204.06843v2 [cs.LG] UPDATED)
150. ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v4 [cs.CV] UPDATED)
151. PnP-ReG: Learned Regularizing Gradient for Plug-and-Play Gradient Descent. (arXiv:2204.13940v2 [eess.IV] UPDATED)
152. Adversarial confound regression and uncertainty measurements to classify heterogeneous clinical MRI in Mass General Brigham. (arXiv:2205.02885v2 [cs.LG] UPDATED)
153. Predicting hot-electron free energies from ground-state data. (arXiv:2205.05591v2 [cond-mat.mtrl-sci] UPDATED)
154. Power and limitations of single-qubit native quantum neural networks. (arXiv:2205.07848v2 [quant-ph] UPDATED)
155. Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems. (arXiv:2205.09809v2 [cs.LG] UPDATED)
156. Mirror Descent Maximizes Generalized Margin and Can Be Implemented Efficiently. (arXiv:2205.12808v2 [cs.LG] UPDATED)
157. Variance-Aware Sparse Linear Bandits. (arXiv:2205.13450v2 [cs.LG] UPDATED)
158. Efficient Approximation of Gromov-Wasserstein Distance using Importance Sparsification. (arXiv:2205.13573v2 [cs.LG] UPDATED)
159. Fast Nonlinear Vector Quantile Regression. (arXiv:2205.14977v2 [stat.CO] UPDATED)
160. Fool SHAP with Stealthily Biased Sampling. (arXiv:2205.15419v2 [cs.LG] UPDATED)
161. VC Theoretical Explanation of Double Descent. (arXiv:2205.15549v3 [stat.ML] UPDATED)
162. Differentiable Invariant Causal Discovery. (arXiv:2205.15638v4 [cs.LG] UPDATED)
163. Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning. (arXiv:2206.01342v2 [cs.LG] UPDATED)
164. The Survival Bandit Problem. (arXiv:2206.03019v2 [cs.LG] UPDATED)
165. On Transfer Learning in Functional Linear Regression. (arXiv:2206.04277v2 [stat.ML] UPDATED)
166. Evolutionary Echo State Network: evolving reservoirs in the Fourier space. (arXiv:2206.04951v2 [cs.NE] UPDATED)
167. Differentiable and Transportable Structure Learning. (arXiv:2206.06354v2 [cs.LG] UPDATED)
168. Deep Isolation Forest for Anomaly Detection. (arXiv:2206.06602v2 [cs.LG] UPDATED)
169. ComENet: Towards Complete and Efficient Message Passing for 3D Molecular Graphs. (arXiv:2206.08515v3 [cs.LG] UPDATED)
170. FedorAS: Federated Architecture Search under system heterogeneity. (arXiv:2206.11239v3 [cs.LG] UPDATED)
171. Robustness to corruption in pre-trained Bayesian neural networks. (arXiv:2206.12361v2 [cs.LG] UPDATED)
172. A Causal Approach to Detecting Multivariate Time-series Anomalies and Root Causes. (arXiv:2206.15033v2 [cs.LG] UPDATED)
173. A New Index for Clustering Evaluation Based on Density Estimation. (arXiv:2207.01294v3 [cs.LG] UPDATED)
174. Signed Network Embedding with Application to Simultaneous Detection of Communities and Anomalies. (arXiv:2207.09324v2 [cs.SI] UPDATED)
175. Pyramidal Denoising Diffusion Probabilistic Models. (arXiv:2208.01864v2 [cs.CV] UPDATED)
176. Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v2 [cs.LG] UPDATED)
177. Constraining Representations Yields Models That Know What They Don't Know. (arXiv:2208.14488v2 [cs.LG] UPDATED)
178. Training Strategies for Improved Lip-reading. (arXiv:2209.01383v3 [cs.CV] UPDATED)
179. Depression Symptoms Modelling from Social Media Text: A Semi-supervised Learning Approach. (arXiv:2209.02765v3 [cs.CL] UPDATED)
180. Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation. (arXiv:2209.06620v2 [cs.LG] UPDATED)
181. Distribution Aware Metrics for Conditional Natural Language Generation. (arXiv:2209.07518v2 [cs.CL] UPDATED)
182. Inducing Early Neural Collapse in Deep Neural Networks for Improved Out-of-Distribution Detection. (arXiv:2209.08378v2 [cs.LG] UPDATED)
183. DeepTOP: Deep Threshold-Optimal Policy for MDPs and RMABs. (arXiv:2209.08646v2 [cs.LG] UPDATED)
184. Look where you look! Saliency-guided Q-networks for visual RL tasks. (arXiv:2209.09203v2 [cs.LG] UPDATED)
185. Revisiting Embeddings for Graph Neural Networks. (arXiv:2209.09338v3 [cs.LG] UPDATED)
186. Streaming Encoding Algorithms for Scalable Hyperdimensional Computing. (arXiv:2209.09868v3 [cs.LG] UPDATED)
187. An NWDAF Approach to 5G Core Network Signaling Traffic: Analysis and Characterization. (arXiv:2209.10428v2 [cs.NI] UPDATED)
188. Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows. (arXiv:2209.10873v2 [cs.LG] UPDATED)
189. Concept Activation Regions: A Generalized Framework For Concept-Based Explanations. (arXiv:2209.11222v2 [cs.LG] UPDATED)
190. PiFold: Toward effective and efficient protein inverse folding. (arXiv:2209.12643v2 [cs.AI] UPDATED)
191. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v2 [cs.CY] UPDATED)
192. DAMO-NLP at NLPCC-2022 Task 2: Knowledge Enhanced Robust NER for Speech Entity Linking. (arXiv:2209.13187v2 [cs.CL] UPDATED)
193. Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v2 [eess.IV] UPDATED)
194. Watch What You Pretrain For: Targeted, Transferable Adversarial Examples on Self-Supervised Speech Recognition models. (arXiv:2209.13523v2 [cs.LG] UPDATED)
195. Hierarchical Sliced Wasserstein Distance. (arXiv:2209.13570v3 [stat.ML] UPDATED)
196. DVGAN: Stabilize Wasserstein GAN training for time-domain Gravitational Wave physics. (arXiv:2209.13592v2 [astro-ph.IM] UPDATED)
197. Falsification before Extrapolation in Causal Effect Estimation. (arXiv:2209.13708v2 [cs.LG] UPDATED)
198. Multilingual Search with Subword TF-IDF. (arXiv:2209.14281v2 [cs.CL] UPDATED)
## cs.AI
---
**98** new papers in cs.AI:-) 
1. Audio Barlow Twins: Self-Supervised Audio Representation Learning. (arXiv:2209.14345v1 [cs.SD])
2. Semantic Segmentation of Vegetation in Remote Sensing Imagery Using Deep Learning. (arXiv:2209.14364v1 [cs.CV])
3. Feature Decoupling in Self-supervised Representation Learning for Open Set Recognition. (arXiv:2209.14385v1 [cs.CV])
4. Learning to Explain Graph Neural Networks. (arXiv:2209.14402v1 [cs.LG])
5. Biological connectomes as a representation for the architecture of artificial neural networks. (arXiv:2209.14406v1 [cs.NE])
6. Applying Machine Learning for Duplicate Detection, Throttling and Prioritization of Equipment Commissioning Audits at Fulfillment Network. (arXiv:2209.14409v1 [cs.LG])
7. Masked Multi-Step Multivariate Time Series Forecasting with Future Information. (arXiv:2209.14413v1 [cs.LG])
8. Efficient Medical Image Assessment via Self-supervised Learning. (arXiv:2209.14434v1 [cs.CV])
9. Out-of-Distribution Detection for LiDAR-based 3D Object Detection. (arXiv:2209.14435v1 [cs.CV])
10. GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v1 [cs.LG])
11. CompNet: A Designated Model to Handle Combinations of Images and Designed features. (arXiv:2209.14454v1 [cs.CV])
12. Constrained Dynamic Movement Primitives for Safe Learning of Motor Skills. (arXiv:2209.14461v1 [cs.RO])
13. Neural Methods for Logical Reasoning Over Knowledge Graphs. (arXiv:2209.14464v1 [cs.AI])
14. Lazy Probabilistic Roadmaps Revisited. (arXiv:2209.14471v1 [cs.RO])
15. Re-Imagen: Retrieval-Augmented Text-to-Image Generator. (arXiv:2209.14491v1 [cs.CV])
16. A Two-Stage Method for Chinese AMR Parsing. (arXiv:2209.14512v1 [cs.CL])
17. Label driven Knowledge Distillation for Federated Learning with non-IID Data. (arXiv:2209.14520v1 [cs.LG])
18. Motion and Appearance Adaptation for Cross-Domain Motion Transfer. (arXiv:2209.14529v1 [cs.CV])
19. A Secure Federated Learning Framework for Residential Short Term Load Forecasting. (arXiv:2209.14547v1 [cs.CR])
20. Regularizing Neural Network Training via Identity-wise Discriminative Feature Suppression. (arXiv:2209.14553v1 [cs.CV])
21. Bayesian Neural Network Versus Ex-Post Calibration For Prediction Uncertainty. (arXiv:2209.14594v1 [cs.LG])
22. Dynamic Surrogate Switching: Sample-Efficient Search for Factorization Machine Configurations in Online Recommendations. (arXiv:2209.14598v1 [cs.LG])
23. Dataset Distillation using Parameter Pruning. (arXiv:2209.14609v1 [cs.CV])
24. Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. (arXiv:2209.14610v1 [cs.LG])
25. An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation. (arXiv:2209.14627v1 [cs.CL])
26. Compressed Gastric Image Generation Based on Soft-Label Dataset Distillation for Medical Data Sharing. (arXiv:2209.14635v1 [cs.CV])
27. Bounded Future MS-TCN++ for surgical gesture recognition. (arXiv:2209.14647v1 [cs.CV])
28. Domain-aware Self-supervised Pre-training for Label-Efficient Meme Analysis. (arXiv:2209.14667v1 [cs.CL])
29. Towards Equalised Odds as Fairness Metric in Academic Performance Prediction. (arXiv:2209.14670v1 [cs.LG])
30. A canonical correlation-based framework for performance analysis of radio access networks. (arXiv:2209.14684v1 [cs.NI])
31. Diffusion Posterior Sampling for General Noisy Inverse Problems. (arXiv:2209.14687v1 [stat.ML])
32. Creative Painting with Latent Diffusion Models. (arXiv:2209.14697v1 [cs.CV])
33. TruEyes: Utilizing Microtasks in Mobile Apps for Crowdsourced Labeling of Machine Learning Datasets. (arXiv:2209.14708v1 [cs.HC])
34. Dataset Complexity Assessment Based on Cumulative Maximum Scaled Area Under Laplacian Spectrum. (arXiv:2209.14743v1 [cs.CV])
35. A Multi-Agent Framework for the Asynchronous and Collaborative Extension of Multitask ML Systems. (arXiv:2209.14745v1 [cs.LG])
36. Non-contrastive approaches to similarity learning: positive examples are all you need. (arXiv:2209.14750v1 [cs.AI])
37. RECALL: Rehearsal-free Continual Learning for Object Classification. (arXiv:2209.14774v1 [cs.CV])
38. Batch Normalization Explained. (arXiv:2209.14778v1 [cs.LG])
39. Make-A-Video: Text-to-Video Generation without Text-Video Data. (arXiv:2209.14792v1 [cs.CV])
40. polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics. (arXiv:2209.14803v1 [cond-mat.mtrl-sci])
41. Named Entity Recognition in Industrial Tables using Tabular Language Models. (arXiv:2209.14812v1 [cs.AI])
42. Denoising Diffusion Probabilistic Models for Styled Walking Synthesis. (arXiv:2209.14828v1 [cs.CV])
43. Continuous PDE Dynamics Forecasting with Implicit Neural Representations. (arXiv:2209.14855v1 [cs.LG])
44. Accelerating Laboratory Automation Through Robot Skill Learning For Sample Scraping. (arXiv:2209.14875v1 [cs.RO])
45. Repairing Bugs in Python Assignments Using Large Language Models. (arXiv:2209.14876v1 [cs.SE])
46. Joint Embedding Self-Supervised Learning in the Kernel Regime. (arXiv:2209.14884v1 [cs.LG])
47. Learning Low-Frequency Motion Control for Robust and Dynamic Robot Locomotion. (arXiv:2209.14887v1 [cs.RO])
48. DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing. (arXiv:2209.14901v1 [cs.CL])
49. Patients' Severity States Classification based on Electronic Health Record (EHR) Data using Multiple Machine Learning and Deep Learning Approaches. (arXiv:2209.14907v1 [cs.LG])
50. Graph Modeling in Computer Assisted Automotive Development. (arXiv:2209.14910v1 [cs.AI])
51. Graph Anomaly Detection with Graph Neural Networks: Current Status and Challenges. (arXiv:2209.14930v1 [cs.LG])
52. Contrastive Unsupervised Learning of World Model with Invariant Causal Features. (arXiv:2209.14932v1 [cs.LG])
53. Reinforcement Learning Algorithms: An Overview and Classification. (arXiv:2209.14940v1 [cs.LG])
54. EiHi Net: Out-of-Distribution Generalization Paradigm. (arXiv:2209.14946v1 [cs.CV])
55. Greybox XAI: a Neural-Symbolic learning framework to produce interpretable predictions for image classification. (arXiv:2209.14974v1 [cs.CV])
56. Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging. (arXiv:2209.14981v1 [cs.LG])
57. Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms. (arXiv:2209.14990v1 [cs.LG])
58. Optimistic MLE -- A Generic Model-based Algorithm for Partially Observable Sequential Decision Making. (arXiv:2209.14997v1 [cs.LG])
59. REST: REtrieve & Self-Train for generative action recognition. (arXiv:2209.15000v1 [cs.CV])
60. Dilated Neighborhood Attention Transformer. (arXiv:2209.15001v1 [cs.CV])
61. Compositional Semantic Parsing with Large Language Models. (arXiv:2209.15003v1 [cs.CL])
62. Understanding Collapse in Non-Contrastive Learning. (arXiv:2209.15007v1 [cs.LG])
63. Causal Bandits without prior knowledge using separating sets. (arXiv:2009.07916v2 [cs.AI] UPDATED)
64. Single-Node Attacks for Fooling Graph Neural Networks. (arXiv:2011.03574v2 [cs.LG] UPDATED)
65. Morality, Machines and the Interpretation Problem: A value-based, Wittgensteinian approach to building Moral Agents. (arXiv:2103.02728v2 [cs.AI] UPDATED)
66. A Survey on Multimodal Disinformation Detection. (arXiv:2103.12541v2 [cs.MM] UPDATED)
67. Have a break from making decisions, have a MARS: The Multi-valued Action Reasoning System. (arXiv:2109.03283v3 [cs.AI] UPDATED)
68. Hyperseed: Unsupervised Learning with Vector Symbolic Architectures. (arXiv:2110.08343v2 [cs.AI] UPDATED)
69. From Kepler to Newton: Explainable AI for Science Discovery. (arXiv:2111.12210v6 [cs.AI] UPDATED)
70. False Data Injection Threats in Active Distribution Systems: A Comprehensive Survey. (arXiv:2111.14251v2 [cs.CR] UPDATED)
71. Similarity Contrastive Estimation for Self-Supervised Soft Contrastive Learning. (arXiv:2111.14585v2 [cs.CV] UPDATED)
72. D-HYPR: Harnessing Neighborhood Modeling and Asymmetry Preservation for Digraph Representation Learning. (arXiv:2112.11734v2 [cs.LG] UPDATED)
73. Hierarchical Reinforcement Learning with AI Planning Models. (arXiv:2203.00669v2 [cs.AI] UPDATED)
74. STCGAT: A Spatio-temporal Causal Graph Attention Network for traffic flow prediction in Intelligent Transportation Systems. (arXiv:2203.10749v3 [cs.LG] UPDATED)
75. Few-Shot Object Detection with Fully Cross-Transformer. (arXiv:2203.15021v2 [cs.CV] UPDATED)
76. Power and limitations of single-qubit native quantum neural networks. (arXiv:2205.07848v2 [quant-ph] UPDATED)
77. Variance-Aware Sparse Linear Bandits. (arXiv:2205.13450v2 [cs.LG] UPDATED)
78. Human-AI Shared Control via Policy Dissection. (arXiv:2206.00152v3 [cs.RO] UPDATED)
79. Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning. (arXiv:2206.01342v2 [cs.LG] UPDATED)
80. Evolutionary Echo State Network: evolving reservoirs in the Fourier space. (arXiv:2206.04951v2 [cs.NE] UPDATED)
81. Robustness to corruption in pre-trained Bayesian neural networks. (arXiv:2206.12361v2 [cs.LG] UPDATED)
82. A Causal Approach to Detecting Multivariate Time-series Anomalies and Root Causes. (arXiv:2206.15033v2 [cs.LG] UPDATED)
83. Breaking Bad News in the Era of Artificial Intelligence and Algorithmic Medicine: An Exploration of Disclosure and its Ethical Justification using the Hedonic Calculus. (arXiv:2207.01431v2 [cs.CY] UPDATED)
84. Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v2 [cs.LG] UPDATED)
85. Constraining Representations Yields Models That Know What They Don't Know. (arXiv:2208.14488v2 [cs.LG] UPDATED)
86. Depression Symptoms Modelling from Social Media Text: A Semi-supervised Learning Approach. (arXiv:2209.02765v3 [cs.CL] UPDATED)
87. Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation. (arXiv:2209.06620v2 [cs.LG] UPDATED)
88. Distribution Aware Metrics for Conditional Natural Language Generation. (arXiv:2209.07518v2 [cs.CL] UPDATED)
89. DeepTOP: Deep Threshold-Optimal Policy for MDPs and RMABs. (arXiv:2209.08646v2 [cs.LG] UPDATED)
90. Look where you look! Saliency-guided Q-networks for visual RL tasks. (arXiv:2209.09203v2 [cs.LG] UPDATED)
91. Generate rather than Retrieve: Large Language Models are Strong Context Generators. (arXiv:2209.10063v2 [cs.CL] UPDATED)
92. Concept Activation Regions: A Generalized Framework For Concept-Based Explanations. (arXiv:2209.11222v2 [cs.LG] UPDATED)
93. PiFold: Toward effective and efficient protein inverse folding. (arXiv:2209.12643v2 [cs.AI] UPDATED)
94. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v2 [cs.CY] UPDATED)
95. DAMO-NLP at NLPCC-2022 Task 2: Knowledge Enhanced Robust NER for Speech Entity Linking. (arXiv:2209.13187v2 [cs.CL] UPDATED)
96. Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v2 [eess.IV] UPDATED)
97. Multilingual Search with Subword TF-IDF. (arXiv:2209.14281v2 [cs.CL] UPDATED)
98. Numerical Integration and Dynamic Discretization in Heuristic Search Planning over Hybrid Domains. (arXiv:1703.04232v1 [cs.AI] CROSS LISTED)

