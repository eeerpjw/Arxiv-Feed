# Your interest papers
---
## cs.CV
---
### Practical Blind Denoising via **Swin**-Conv-UNet and Data Synthesis. (arXiv:2203.13278v1 [cs.CV])
- Authors : Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2203.13278](http://arxiv.org/abs/2203.13278)
> ABSTRACT  :  While recent years have witnessed a dramatic upsurge of exploiting deep neural networks toward solving image denoising, existing methods mostly rely on simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG compression noise and camera sensor noise, and a general-purpose blind denoising method for real images remains unsolved. In this paper, we attempt to solve this problem from the perspective of network architecture design and training data synthesis. Specifically, for the network architecture design, we propose a swin-conv block to incorporate the local modeling ability of residual convolutional layer and non-local modeling ability of swin transformer block, and then plug it as the main building block into the widely-used image-to-image translation UNet architecture. For the training data synthesis, we design a practical noise degradation model which takes into consideration different kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and processed camera sensor noises) and resizing, and also involves a random shuffle strategy and a double degradation strategy. Extensive experiments on AGWN removal and real image denoising demonstrate that the new network architecture design achieves state-of-the-art performance and the new degradation model can help to significantly improve the practicability. We believe our work can provide useful insights into current denoising research.  
### Vision Transformer Compression with Structured Pruning and Low Rank Approximation. (arXiv:2203.13444v1 [cs.CV])
- Authors : Ankur Kumar
- Link : [http://arxiv.org/abs/2203.13444](http://arxiv.org/abs/2203.13444)
> ABSTRACT  :  Transformer architecture has gained popularity due to its ability to scale with large dataset. Consequently, there is a need to reduce the model size and latency, especially for on-device deployment. We focus on vision transformer proposed for image recognition task (Dosovitskiy et al., 2021), and explore the application of different compression techniques such as low rank approximation and pruning for this purpose. Specifically, we investigate a structured pruning method proposed recently in Zhu et al. (2021) and find that mostly feedforward blocks are pruned with this approach, that too, with severe degradation in accuracy. We propose a hybrid compression approach to mitigate this where we compress the attention blocks using low rank approximation and use the previously mentioned pruning with a lower rate for feedforward blocks in each transformer layer. Our technique results in 50% compression with 14% relative increase in classification error whereas we obtain 44% compression with 20% relative increase in error when only pruning is applied. We propose further **enhancement**s to bridge the accuracy gap but leave it as a future work.  
### Facial Expression Recognition with **Swin** Transformer. (arXiv:2203.13472v1 [cs.CV])
- Authors : Hwa Kim, Namho Kim, Chee Sun
- Link : [http://arxiv.org/abs/2203.13472](http://arxiv.org/abs/2203.13472)
> ABSTRACT  :  The task of recognizing human facial expressions plays a vital role in various human-related systems, including health care and medical fields. With the recent success of deep learning and the accessibility of a large amount of annotated data, facial expression recognition research has been mature enough to be utilized in real-world scenarios with audio-visual datasets. In this paper, we introduce **Swin** transformer-based facial expression approach for an in-the-wild audio-visual dataset of the Aff-Wild2 Expression dataset. Specifically, we employ a three-stream network (i.e., Visual stream, Temporal stream, and Audio stream) for the audio-visual videos to fuse the multi-modal information into facial expression recognition. Experimental results on the Aff-Wild2 dataset show the effectiveness of our proposed multi-modal approaches.  
### SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance. (arXiv:2203.13535v1 [cs.MM])
- Authors : Xinchi Zhou, Dongzhan Zhou, Wanli Ouyang, Hang Zhou, Ziwei Liu, Di Hu
- Link : [http://arxiv.org/abs/2203.13535](http://arxiv.org/abs/2203.13535)
> ABSTRACT  :  Recent years have witnessed the success of deep learning on the visual sound separation task. However, existing works follow similar settings where the training and testing datasets share the same musical instrument categories, which to some extent limits the versatility of this task. In this work, we focus on a more general and challenging scenario, namely the separation of unknown musical instruments, where the categories in training and testing phases have no overlap with each other. To tackle this new setting, we propose the Separation-with-Consistency (SeCo) framework, which can accomplish the separation on unknown categories by exploiting the consistency constraints. Furthermore, to capture richer characteristics of the novel melodies, we devise an online matching strategy, which can bring stable **enhancement**s with no cost of extra parameters. Experiments demonstrate that our SeCo framework exhibits strong adaptation ability on the novel musical categories and outperforms the baseline methods by a significant margin.  
### Searching for Network Width with **Bilateral**ly Coupled Network. (arXiv:2203.13714v1 [cs.CV])
- Authors : Xiu Su, Shan You, Jiyang Xie, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu
- Link : [http://arxiv.org/abs/2203.13714](http://arxiv.org/abs/2203.13714)
> ABSTRACT  :  Searching for a more compact network width recently serves as an effective way of channel pruning for the deployment of convolutional neural networks (CNNs) under hardware constraints. To fulfill the searching, a one-shot supernet is usually leveraged to efficiently evaluate the performance \wrt~different network widths. However, current methods mainly follow a \textit{unilaterally augmented} (UA) principle for the evaluation of each width, which induces the training unfairness of channels in supernet. In this paper, we introduce a new supernet called **Bilateral**ly Coupled Network (BCNet) to address this issue. In BCNet, each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we propose to reduce the redundant search space and present the BCNetV2 as the enhanced supernet to ensure rigorous training fairness over channels. Furthermore, we leverage a stochastic complementary strategy for training the BCNet, and propose a prior initial population sampling method to boost the performance of the evolutionary search. We also propose the first open-source width benchmark on macro structures named Channel-Bench-Macro for the better comparison of width search algorithms. Extensive experiments on benchmark CIFAR-10 and ImageNet datasets indicate that our method can achieve state-of-the-art or competing performance over other baseline methods. Moreover, our method turns out to further boost the performance of NAS models by refining their network widths. For example, with the same FLOPs budget, our obtained EfficientNet-B0 achieves 77.53\% Top-1 accuracy on ImageNet dataset, surpassing the performance of original setting by 0.65\%.  
### FReSCO: Flow Reconstruction and Segmentation for low latency Cardiac Output monitoring using deep artifact suppression and segmentation. (arXiv:2203.13729v1 [cs.CV])
- Authors : Olivier Jaubert, Javier Montalt, James Brown, Daniel K**night**, Simon Arridge, Jennifer Steeden, Vivek Muthurangu
- Link : [http://arxiv.org/abs/2203.13729](http://arxiv.org/abs/2203.13729)
> ABSTRACT  :  Purpose: **Real-time** monitoring of cardiac output (CO) requires low latency reconstruction and segmentation of real-time phase contrast MR (PCMR), which has previously been difficult to perform. Here we propose a deep learning framework for 'Flow Reconstruction and Segmentation for low latency Cardiac Output monitoring' (FReSCO).    Methods: Deep artifact suppression and segmentation U-Nets were independently trained. Breath hold spiral PCMR data (n=516) was synthetically undersampled using a variable density spiral sampling pattern and gridded to create aliased data for training of the artifact suppression U-net. A subset of the data (n=96) was segmented and used to train the segmentation U-net. **Real-time** spiral PCMR was prospectively acquired and then reconstructed and segmented using the trained models (FReSCO) at low latency at the scanner in 10 healthy subjects during rest, exercise and recovery periods. CO obtained via FReSCO was compared to a reference rest CO and rest and exercise Compressed Sensing (CS) CO.    Results: FReSCO was demonstrated prospectively at the scanner. Beat-to-beat heartrate, stroke volume and CO could be visualized with a mean latency of 622ms. No significant differences were noted when compared to reference at rest (Bias = -0.21+-0.50 L/min, p=0.246) or CS at peak exercise (Bias=0.12+-0.48 L/min, p=0.458).    Conclusion: FReSCO was successfully demonstrated for real-time monitoring of CO during exercise and could provide a convenient tool for assessment of the hemodynamic response to a range of stressors.  
### Visual-based Safe Landing for UAVs in Populated Areas: **Real-time** Validation in Virtual Environments. (arXiv:2203.13792v1 [cs.RO])
- Authors : Hector Tovanche, Javier Gonzalez, Angel Flores, Diego Mercado
- Link : [http://arxiv.org/abs/2203.13792](http://arxiv.org/abs/2203.13792)
> ABSTRACT  :  Safe autonomous landing for Unmanned Aerial Vehicles (UAVs) in populated areas is a crucial aspect for successful urban deployment, particularly in emergency landing situations. Nonetheless, validating autonomous landing in real scenarios is a challenging task involving a high risk of injuring people. In this work, we propose a framework for real-time safe and thorough evaluation of vision-based autonomous landing in populated scenarios, using photo-realistic virtual environments. We propose to use the Unreal graphics engine coupled with the AirSim plugin for drone's simulation, and evaluate autonomous landing strategies based on visual detection of Safe Landing Zones (SLZ) in populated scenarios. Then, we study two different criteria for selecting the "best" SLZ, and evaluate them during autonomous landing of a virtual drone in different scenarios and conditions, under different distributions of people in urban scenes, including moving people. We evaluate different metrics to quantify the performance of the landing strategies, establishing a baseline for comparison with future works in this challenging task, and analyze them through an important number of randomized iterations. The study suggests that the use of the autonomous landing algorithms considerably helps to prevent accidents involving humans, which may allow to unleash the full potential of drones in urban environments near to people.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v7 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
### ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v2 [cs.CV] UPDATED)
- Authors : Kailin Li, Lixin Yang, Xinyu Zhan, Jun Lv, Wenqiang Xu, Jiefeng Li, Cewu Lu
- Link : [http://arxiv.org/abs/2109.05488](http://arxiv.org/abs/2109.05488)
> ABSTRACT  :  Estimating the articulated 3D hand-object pose from a single RGB image is a highly ambiguous and challenging problem, requiring large-scale datasets that contain diverse hand poses, object types, and camera viewpoints. Most real-world datasets lack these diversities. In contrast, data synthesis can easily ensure those diversities separately. However, constructing both valid and diverse hand-object interactions and efficiently learning from the vast synthetic data is still challenging. To address the above issues, we propose ArtiBoost, a lightweight online data **enhancement** method. ArtiBoost can cover diverse hand-object poses and camera viewpoints through sampling in a Composited hand-object Configuration and Viewpoint space (CCV-space) and can adaptively enrich the current hard-discernable items by loss-feedback and sample re-weighting. ArtiBoost alternatively performs data exploration and synthesis within a learning pipeline, and those synthetic data are blended into real-world source data for training. We apply ArtiBoost on a simple learning baseline network and witness the performance boost on several hand-object benchmarks. Our models and code are available at https://github.com/lixiny/ArtiBoost.  
### Global-Local Context Network for Person Search. (arXiv:2112.02500v2 [cs.CV] UPDATED)
- Authors : Peng Zheng, Jie Qin, Yichao Yan, Shengcai Liao, Bingbing Ni, Xiaogang Cheng, Ling Shao
- Link : [http://arxiv.org/abs/2112.02500](http://arxiv.org/abs/2112.02500)
> ABSTRACT  :  Person search aims to jointly localize and identify a query person from natural, uncropped images, which has been actively studied in the computer vision community over the past few years. In this paper, we delve into the rich context information globally and locally surrounding the target person, which we refer to scene and group context, respectively. Unlike previous works that treat the two types of context individually, we exploit them in a unified global-local context network (GLCNet) with the intuitive aim of feature **enhancement**. Specifically, re-ID embeddings and context features are enhanced simultaneously in a multi-stage fashion, ultimately leading to enhanced, discriminative features for person search. We conduct the experiments on two person search benchmarks (i.e., CUHK-SYSU and PRW) as well as extend our approach to a more challenging setting (i.e., character search on MovieNet). Extensive experimental results demonstrate the consistent improvement of the proposed GLCNet over the state-of-the-art methods on the three datasets. Our source codes, pre-trained models, and the new setting for character search are available at: https://github.com/ZhengPeng7/GLCNet.  
### Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v3 [eess.IV] UPDATED)
- Authors : Ziwei Luo, Haibin Huang, Lei Yu, Youwei Li, Haoqiang Fan, Shuaicheng Liu
- Link : [http://arxiv.org/abs/2202.07508](http://arxiv.org/abs/2202.07508)
> ABSTRACT  :  In this paper, we tackle the problem of blind image super-resolution(SR) with a reformulated degradation model and two novel modules. Following the common practices of blind SR, our method proposes to improve both the kernel estimation as well as the kernel-based high-resolution image **restoration**. To be more specific, we first reformulate the degradation model such that the deblurring kernel estimation can be transferred into the low-resolution space. On top of this, we introduce a dynamic deep linear filter module. Instead of learning a fixed kernel for all images, it can adaptively generate deblurring kernel weights conditional on the input and yield a more robust kernel estimation. Subsequently, a deep constrained least square filtering module is applied to generate clean features based on the reformulation and estimated kernel. The deblurred feature and the low input image feature are then fed into a dual-path structured SR network and restore the final high-resolution result. To evaluate our method, we further conduct evaluations on several benchmarks, including Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed method achieves better accuracy and visual improvements against state-of-the-art methods.  
### Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization. (arXiv:2203.07740v2 [cs.CV] UPDATED)
- Authors : Yabin Zhang, Minghan Li, Ruihuang Li, Kui Jia, **Lei Zhang**
- Link : [http://arxiv.org/abs/2203.07740](http://arxiv.org/abs/2203.07740)
> ABSTRACT  :  Arbitrary style transfer (AST) and domain generalization (DG) are important yet challenging visual learning tasks, which can be cast as a feature distribution matching problem. With the assumption of Gaussian feature distribution, conventional feature distribution matching methods usually match the mean and standard deviation of features. However, the feature distributions of real-world data are usually much more complicated than Gaussian, which cannot be accurately matched by using only the first-order and second-order statistics, while it is computationally prohibitive to use high-order statistics for distribution matching. In this work, we, for the first time to our best knowledge, propose to perform Exact Feature Distribution Matching (EFDM) by exactly matching the empirical Cumulative Distribution Functions (eCDFs) of image features, which could be implemented by applying the Exact Histogram Matching (EHM) in the image feature space. Particularly, a fast EHM algorithm, named Sort-Matching, is employed to perform EFDM in a plug-and-play manner with minimal cost. The effectiveness of our proposed EFDM method is verified on a variety of AST and DG tasks, demonstrating new state-of-the-art results. Codes are available at https://github.com/YBZh/EFDM.  
### A Preliminary Research on Space Situational Awareness Based on Event Cameras. (arXiv:2203.13093v2 [cs.CV] UPDATED)
- Authors : Kun Xiao, Pengju Li, Guohui Wang, Zhi Li, Yi Chen, Yongfeng Xie, Yuqiang Fang
- Link : [http://arxiv.org/abs/2203.13093](http://arxiv.org/abs/2203.13093)
> ABSTRACT  :  Event camera is a new type of sensor that is different from traditional cameras. Each pixel is triggered asynchronously by an event. The trigger event is the change of the brightness irradiated on the pixel. If the increment or decrement is higher than a certain threshold, the event is output. Compared with traditional cameras, event cameras have the advantages of high temporal resolution, low latency, **high dynamic range**, low bandwidth and low power consumption. We carried out a series of observation experiments in a simulated space lighting environment. The experimental results show that the event camera can give full play to the above advantages in space situational awareness. This article first introduces the basic principles of the event camera, then analyzes its advantages and disadvantages, then introduces the observation experiment and analyzes the experimental results, and finally, a workflow of space situational awareness based on event cameras is given.  
## eess.IV
---
### Practical Blind Denoising via **Swin**-Conv-UNet and Data Synthesis. (arXiv:2203.13278v1 [cs.CV])
- Authors : Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2203.13278](http://arxiv.org/abs/2203.13278)
> ABSTRACT  :  While recent years have witnessed a dramatic upsurge of exploiting deep neural networks toward solving image denoising, existing methods mostly rely on simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG compression noise and camera sensor noise, and a general-purpose blind denoising method for real images remains unsolved. In this paper, we attempt to solve this problem from the perspective of network architecture design and training data synthesis. Specifically, for the network architecture design, we propose a swin-conv block to incorporate the local modeling ability of residual convolutional layer and non-local modeling ability of swin transformer block, and then plug it as the main building block into the widely-used image-to-image translation UNet architecture. For the training data synthesis, we design a practical noise degradation model which takes into consideration different kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and processed camera sensor noises) and resizing, and also involves a random shuffle strategy and a double degradation strategy. Extensive experiments on AGWN removal and real image denoising demonstrate that the new network architecture design achieves state-of-the-art performance and the new degradation model can help to significantly improve the practicability. We believe our work can provide useful insights into current denoising research.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v7 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
### Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v3 [eess.IV] UPDATED)
- Authors : Ziwei Luo, Haibin Huang, Lei Yu, Youwei Li, Haoqiang Fan, Shuaicheng Liu
- Link : [http://arxiv.org/abs/2202.07508](http://arxiv.org/abs/2202.07508)
> ABSTRACT  :  In this paper, we tackle the problem of blind image super-resolution(SR) with a reformulated degradation model and two novel modules. Following the common practices of blind SR, our method proposes to improve both the kernel estimation as well as the kernel-based high-resolution image **restoration**. To be more specific, we first reformulate the degradation model such that the deblurring kernel estimation can be transferred into the low-resolution space. On top of this, we introduce a dynamic deep linear filter module. Instead of learning a fixed kernel for all images, it can adaptively generate deblurring kernel weights conditional on the input and yield a more robust kernel estimation. Subsequently, a deep constrained least square filtering module is applied to generate clean features based on the reformulation and estimated kernel. The deblurred feature and the low input image feature are then fed into a dual-path structured SR network and restore the final high-resolution result. To evaluate our method, we further conduct evaluations on several benchmarks, including Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed method achieves better accuracy and visual improvements against state-of-the-art methods.  
## cs.LG
---
### BDDM: **Bilateral** Denoising Diffusion Models for Fast and High-Quality Speech Synthesis. (arXiv:2203.13508v1 [eess.AS])
- Authors : Jun Wang, Dan Su, Dong Yu
- Link : [http://arxiv.org/abs/2203.13508](http://arxiv.org/abs/2203.13508)
> ABSTRACT  :  Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new **bilateral** denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel **bilateral** modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm.  
### Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v1 [cs.SD])
- Authors : Shin Lee, Yuan Chen, Yu Tsao, Min Wang
- Link : [http://arxiv.org/abs/2203.13696](http://arxiv.org/abs/2203.13696)
> ABSTRACT  :  Compensation for channel mismatch and noise interference is essential for robust automatic speech recognition. Enhanced speech has been introduced into the multi-condition training of acoustic models to improve their generalization ability. In this paper, a noise-aware training framework based on two cascaded neural structures is proposed to jointly optimize speech **enhancement** and speech recognition. The feature **enhancement** module is composed of a multi-task autoencoder, where noisy speech is decomposed into clean speech and noise. By concatenating its enhanced, noise-aware, and noisy features for each frame, the acoustic-modeling module maps each feature-augmented frame into a triphone state by optimizing the lattice-free maximum mutual information and cross entropy between the predicted and actual state sequences. On top of the factorized time delay neural network (TDNN-F) and its convolutional variant (CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared with the best existing systems that use bigram and trigram language models for decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based system also outperforms the baseline CNN-TDNNF system on the AMI task.  
### Searching for Network Width with **Bilateral**ly Coupled Network. (arXiv:2203.13714v1 [cs.CV])
- Authors : Xiu Su, Shan You, Jiyang Xie, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu
- Link : [http://arxiv.org/abs/2203.13714](http://arxiv.org/abs/2203.13714)
> ABSTRACT  :  Searching for a more compact network width recently serves as an effective way of channel pruning for the deployment of convolutional neural networks (CNNs) under hardware constraints. To fulfill the searching, a one-shot supernet is usually leveraged to efficiently evaluate the performance \wrt~different network widths. However, current methods mainly follow a \textit{unilaterally augmented} (UA) principle for the evaluation of each width, which induces the training unfairness of channels in supernet. In this paper, we introduce a new supernet called **Bilateral**ly Coupled Network (BCNet) to address this issue. In BCNet, each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we propose to reduce the redundant search space and present the BCNetV2 as the enhanced supernet to ensure rigorous training fairness over channels. Furthermore, we leverage a stochastic complementary strategy for training the BCNet, and propose a prior initial population sampling method to boost the performance of the evolutionary search. We also propose the first open-source width benchmark on macro structures named Channel-Bench-Macro for the better comparison of width search algorithms. Extensive experiments on benchmark CIFAR-10 and ImageNet datasets indicate that our method can achieve state-of-the-art or competing performance over other baseline methods. Moreover, our method turns out to further boost the performance of NAS models by refining their network widths. For example, with the same FLOPs budget, our obtained EfficientNet-B0 achieves 77.53\% Top-1 accuracy on ImageNet dataset, surpassing the performance of original setting by 0.65\%.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v7 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
## cs.AI
---
### Facial Expression Recognition with **Swin** Transformer. (arXiv:2203.13472v1 [cs.CV])
- Authors : Hwa Kim, Namho Kim, Chee Sun
- Link : [http://arxiv.org/abs/2203.13472](http://arxiv.org/abs/2203.13472)
> ABSTRACT  :  The task of recognizing human facial expressions plays a vital role in various human-related systems, including health care and medical fields. With the recent success of deep learning and the accessibility of a large amount of annotated data, facial expression recognition research has been mature enough to be utilized in real-world scenarios with audio-visual datasets. In this paper, we introduce **Swin** transformer-based facial expression approach for an in-the-wild audio-visual dataset of the Aff-Wild2 Expression dataset. Specifically, we employ a three-stream network (i.e., Visual stream, Temporal stream, and Audio stream) for the audio-visual videos to fuse the multi-modal information into facial expression recognition. Experimental results on the Aff-Wild2 dataset show the effectiveness of our proposed multi-modal approaches.  
### BDDM: **Bilateral** Denoising Diffusion Models for Fast and High-Quality Speech Synthesis. (arXiv:2203.13508v1 [eess.AS])
- Authors : Jun Wang, Dan Su, Dong Yu
- Link : [http://arxiv.org/abs/2203.13508](http://arxiv.org/abs/2203.13508)
> ABSTRACT  :  Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new **bilateral** denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel **bilateral** modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm.  
### Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v1 [cs.SD])
- Authors : Shin Lee, Yuan Chen, Yu Tsao, Min Wang
- Link : [http://arxiv.org/abs/2203.13696](http://arxiv.org/abs/2203.13696)
> ABSTRACT  :  Compensation for channel mismatch and noise interference is essential for robust automatic speech recognition. Enhanced speech has been introduced into the multi-condition training of acoustic models to improve their generalization ability. In this paper, a noise-aware training framework based on two cascaded neural structures is proposed to jointly optimize speech **enhancement** and speech recognition. The feature **enhancement** module is composed of a multi-task autoencoder, where noisy speech is decomposed into clean speech and noise. By concatenating its enhanced, noise-aware, and noisy features for each frame, the acoustic-modeling module maps each feature-augmented frame into a triphone state by optimizing the lattice-free maximum mutual information and cross entropy between the predicted and actual state sequences. On top of the factorized time delay neural network (TDNN-F) and its convolutional variant (CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared with the best existing systems that use bigram and trigram language models for decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based system also outperforms the baseline CNN-TDNNF system on the AMI task.  
### A World-Self Model Towards Understanding Intelligence. (arXiv:2203.13762v1 [cs.AI])
- Authors : Yutao Yue
- Link : [http://arxiv.org/abs/2203.13762](http://arxiv.org/abs/2203.13762)
> ABSTRACT  :  Artificial intelligence has achieved tremendous successes in various tasks, while it is still out of question that there are big gaps between artificial and human intelligence, and the nature of intelligence is still in **dark**ness. In this work we will first stress the importance of scope of discussion and granularity of investigation for this type of research. We will carefully compare human and artificial intelligence, and propose that a certain aspect (Aspect 3) of human intelligence is the key to connect perception and cognition, and the lack of a new model is preventing the understanding and next-level implementation of intelligence. We will present the broader idea of "concept", the principles and mathematical frameworks of the new model World-Self Model (WSM) of intelligence, and finally an unified general framework of intelligence based on WSM. Rather than focusing on solving a specific problem or discussing a certain kind of intelligence, our work is instead towards a better understanding of the nature of the general phenomenon of intelligence, independent of the kind of task or system of investigation.  
### ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v2 [cs.CV] UPDATED)
- Authors : Kailin Li, Lixin Yang, Xinyu Zhan, Jun Lv, Wenqiang Xu, Jiefeng Li, Cewu Lu
- Link : [http://arxiv.org/abs/2109.05488](http://arxiv.org/abs/2109.05488)
> ABSTRACT  :  Estimating the articulated 3D hand-object pose from a single RGB image is a highly ambiguous and challenging problem, requiring large-scale datasets that contain diverse hand poses, object types, and camera viewpoints. Most real-world datasets lack these diversities. In contrast, data synthesis can easily ensure those diversities separately. However, constructing both valid and diverse hand-object interactions and efficiently learning from the vast synthetic data is still challenging. To address the above issues, we propose ArtiBoost, a lightweight online data **enhancement** method. ArtiBoost can cover diverse hand-object poses and camera viewpoints through sampling in a Composited hand-object Configuration and Viewpoint space (CCV-space) and can adaptively enrich the current hard-discernable items by loss-feedback and sample re-weighting. ArtiBoost alternatively performs data exploration and synthesis within a learning pipeline, and those synthetic data are blended into real-world source data for training. We apply ArtiBoost on a simple learning baseline network and witness the performance boost on several hand-object benchmarks. Our models and code are available at https://github.com/lixiny/ArtiBoost.  
# Paper List
---
## cs.CV
---
**136** new papers in cs.CV:-) 
1. Practical Blind Denoising via **Swin**-Conv-UNet and Data Synthesis. (arXiv:2203.13278v1 [cs.CV])
2. Effectively leveraging Multi-modal Features for Movie Genre Classification. (arXiv:2203.13281v1 [cs.CV])
3. Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition. (arXiv:2203.13285v1 [cs.SD])
4. Searching for fingerspelled content in American Sign Language. (arXiv:2203.13291v1 [cs.CV])
5. RayTran: 3D pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers. (arXiv:2203.13296v1 [cs.CV])
6. Multi-modal Multi-label Facial Action Unit Detection with Transformer. (arXiv:2203.13301v1 [cs.CV])
7. Weakly-Supervised Online Action Segmentation in Multi-View Instructional Videos. (arXiv:2203.13309v1 [cs.CV])
8. MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v1 [cs.CV])
9. SharpContour: A Contour-based Boundary Refinement Approach for Efficient and Accurate Instance Segmentation. (arXiv:2203.13312v1 [cs.CV])
10. Deep learning for laboratory earthquake prediction and autoregressive forecasting of fault zone stress. (arXiv:2203.13313v1 [physics.geo-ph])
11. Human Gait Recognition Using Bag of Words Feature Representation Method. (arXiv:2203.13317v1 [cs.CV])
12. NPBG++: Accelerating Neural Point-Based Graphics. (arXiv:2203.13318v1 [cs.CV])
13. Text to Mesh Without 3D Supervision Using Limit Subdivision. (arXiv:2203.13333v1 [cs.CV])
14. Occluded Human Mesh Recovery. (arXiv:2203.13349v1 [cs.CV])
15. FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks. (arXiv:2203.13371v1 [cs.CV])
16. Probing Representation Forgetting in Supervised and Unsupervised Continual Learning. (arXiv:2203.13381v1 [cs.LG])
17. CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation. (arXiv:2203.13387v1 [cs.CV])
18. Point2Seq: Detecting 3D Objects as Sequences. (arXiv:2203.13394v1 [cs.CV])
19. Multi-scale and Cross-scale Contrastive Learning for Semantic Segmentation. (arXiv:2203.13409v1 [cs.CV])
20. Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes. (arXiv:2203.13412v1 [cs.CV])
21. Noisy Boundaries: Lemon or Lemonade for Semi-supervised Instance Segmentation?. (arXiv:2203.13427v1 [cs.CV])
22. Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices. (arXiv:2203.13436v1 [cs.CV])
23. BCOT: A Markerless High-Precision 3D Object Tracking Benchmark. (arXiv:2203.13437v1 [cs.CV])
24. Microstructure Surface Reconstruction from SEM Images: An Alternative to Digital Image Correlation (DIC). (arXiv:2203.13438v1 [cs.GR])
25. 3D GAN Inversion for Controllable Portrait Image Animation. (arXiv:2203.13441v1 [cs.CV])
26. MDAN: Multi-level Dependent Attention Network for Visual Emotion Analysis. (arXiv:2203.13443v1 [cs.CV])
27. Vision Transformer Compression with Structured Pruning and Low Rank Approximation. (arXiv:2203.13444v1 [cs.CV])
28. PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models. (arXiv:2203.13452v1 [cs.CV])
29. CNN LEGO: Disassembling and Assembling Convolutional Neural Network. (arXiv:2203.13453v1 [cs.CV])
30. A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training. (arXiv:2203.13455v1 [cs.LG])
31. Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. (arXiv:2203.13457v1 [cs.LG])
32. PANDORA: Polarization-Aided Neural Decomposition Of Radiance. (arXiv:2203.13458v1 [cs.CV])
33. Semi-supervised and Deep learning Frameworks for Video Classification and Key-frame Identification. (arXiv:2203.13459v1 [cs.CV])
34. Interpretation of Chest x-rays affected by bullets using deep transfer learning. (arXiv:2203.13461v1 [cs.CV])
35. CAD: Co-Adapting Discriminative Features for Improved Few-Shot Classification. (arXiv:2203.13465v1 [cs.CV])
36. RD-Optimized Trit-Plane Coding of Deep Compressed Image Latent Tensors. (arXiv:2203.13467v1 [eess.IV])
37. Interactive Style Transfer: All is Your Palette. (arXiv:2203.13470v1 [cs.CV])
38. Non-Probability Sampling Network for Stochastic Human Trajectory Prediction. (arXiv:2203.13471v1 [cs.CV])
39. Facial Expression Recognition with **Swin** Transformer. (arXiv:2203.13472v1 [cs.CV])
40. Improving Adversarial Transferability with Spatial Momentum. (arXiv:2203.13479v1 [cs.CV])
41. Polarization Multiplexed Diffractive Computing: All-Optical Implementation of a Group of Linear Transformations Through a Polarization-Encoded Diffractive Network. (arXiv:2203.13482v1 [physics.optics])
42. Compare learning: bi-attention network for few-shot learning. (arXiv:2203.13487v1 [cs.CV])
43. Contrastive learning of Class-agnostic Activation Map for Weakly Supervised Object Localization and Semantic Segmentation. (arXiv:2203.13505v1 [cs.CV])
44. Analysis of the Production Strategy of Mask Types in the COVID-19 Environment. (arXiv:2203.13506v1 [cs.CV])
45. Multimodal Pre-training Based on Graph Attention Network for Document Understanding. (arXiv:2203.13530v1 [cs.CV])
46. High-Performance Transformer Tracking. (arXiv:2203.13533v1 [cs.CV])
47. SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance. (arXiv:2203.13535v1 [cs.MM])
48. Efficient Visual Tracking via Hierarchical Cross-Attention Transformer. (arXiv:2203.13537v1 [cs.CV])
49. Deformable Butterfly: A Highly Structured and Sparse Linear Transform. (arXiv:2203.13556v1 [cs.CV])
50. Neural Networks with Divisive normalization for image segmentation with application in cityscapes dataset. (arXiv:2203.13558v1 [cs.CV])
51. A Visual Navigation Perspective for Category-Level Object Pose Estimation. (arXiv:2203.13572v1 [cs.CV])
52. Continual Test-Time Domain Adaptation. (arXiv:2203.13591v1 [cs.CV])
53. Fast Hybrid Image Retargeting. (arXiv:2203.13595v1 [cs.CV])
54. Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints. (arXiv:2203.13601v1 [cs.DB])
55. Rope3D: TheRoadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task. (arXiv:2203.13608v1 [cs.CV])
56. Unsupervised Pre-training for Temporal Action Localization Tasks. (arXiv:2203.13609v1 [cs.CV])
57. Learning to Adapt to Unseen Abnormal Activities under Weak Supervision. (arXiv:2203.13610v1 [cs.CV])
58. Class-Incremental Learning for Action Recognition in Videos. (arXiv:2203.13611v1 [cs.CV])
59. Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v1 [cs.LG])
60. Lightweight Graph Convolutional Networks with Topologically Consistent Magnitude Pruning. (arXiv:2203.13616v1 [cs.LG])
61. Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness. (arXiv:2203.13639v1 [cs.CV])
62. StretchBEV: Stretching Future Instance Prediction Spatially and Temporally. (arXiv:2203.13641v1 [cs.CV])
63. MDsrv -- visual sharing and analysis of molecular dynamics simulations. (arXiv:2203.13658v1 [cs.CV])
64. Adjacent Context Coordination Network for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2203.13664v1 [cs.CV])
65. Dense Continuous-Time Optical Flow from Events and Frames. (arXiv:2203.13674v1 [cs.CV])
66. On the performance of preconditioned methods to solve \(L^p\)-norm phase unwrapping. (arXiv:2203.13675v1 [math.NA])
67. ST-FL: Style Transfer Preprocessing in Federated Learning for COVID-19 Segmentation. (arXiv:2203.13680v1 [eess.IV])
68. Satellite Infrastructure/Mission Tradeoffs. (arXiv:2203.13686v1 [cs.LG])
69. The TerraByte Client: providing access to terabytes of plant data. (arXiv:2203.13691v1 [cs.CV])
70. Implicit Neural Representations for Variable Length Human Motion Generation. (arXiv:2203.13694v1 [cs.CV])
71. Unsupervised Image Deraining: Optimization Model Driven Deep CNN. (arXiv:2203.13699v1 [cs.CV])
72. Clustering Aided Weakly Supervised Training to Detect Anomalous Events in Surveillance Videos. (arXiv:2203.13704v1 [cs.CV])
73. Searching for Network Width with **Bilateral**ly Coupled Network. (arXiv:2203.13714v1 [cs.CV])
74. Stabilizing Adversarially Learned One-Class Novelty Detection Using Pseudo Anomalies. (arXiv:2203.13716v1 [cs.CV])
75. Digital Fingerprinting of Microstructures. (arXiv:2203.13718v1 [cs.CV])
76. Salt Detection Using Segmentation of Seismic Image. (arXiv:2203.13721v1 [cs.CV])
77. FReSCO: Flow Reconstruction and Segmentation for low latency Cardiac Output monitoring using deep artifact suppression and segmentation. (arXiv:2203.13729v1 [cs.CV])
78. Efficient-VDVAE: Less is more. (arXiv:2203.13751v1 [cs.LG])
79. Analysis of the use of color and its emotional relationship in visual creations based on experiences during the context of the COVID-19 pandemic. (arXiv:2203.13770v1 [cs.CV])
80. Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion. (arXiv:2203.13777v1 [cs.CV])
81. Visual-based Safe Landing for UAVs in Populated Areas: **Real-time** Validation in Virtual Environments. (arXiv:2203.13792v1 [cs.RO])
82. Continuous Dynamic-NeRF: Spline-NeRF. (arXiv:2203.13800v1 [cs.CV])
83. Playing Lottery Tickets in Style Transfer Models. (arXiv:2203.13802v1 [cs.CV])
84. Spatially Multi-conditional Image Generation. (arXiv:2203.13812v1 [cs.CV])
85. Versatile Multi-Modal Pre-Training for Human-Centric Perception. (arXiv:2203.13815v1 [cs.CV])
86. AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling. (arXiv:2203.13817v1 [cs.CV])
87. Detection and Tracking of Multiple Mice Using Part Proposal Networks. (arXiv:1906.02831v3 [cs.CV] UPDATED)
88. Which Model to Transfer? Finding the Needle in the Growing Haystack. (arXiv:2010.06402v2 [cs.LG] UPDATED)
89. Understanding and Increasing Efficiency of Frank-Wolfe Adversarial Training. (arXiv:2012.12368v5 [cs.LG] UPDATED)
90. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v7 [cs.CV] UPDATED)
91. MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens. (arXiv:2105.15168v3 [cs.CV] UPDATED)
92. SIFT Matching by Context Exposed. (arXiv:2106.09584v4 [cs.CV] UPDATED)
93. Multi-level Feature Learning for Contrastive Multi-view Clustering. (arXiv:2106.11193v2 [cs.LG] UPDATED)
94. Recall@k Surrogate Loss with Large Batches and Similarity Mixup. (arXiv:2108.11179v2 [cs.CV] UPDATED)
95. ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v2 [cs.CV] UPDATED)
96. Adversarial Bone Length Attack on Action Recognition. (arXiv:2109.05830v2 [cs.CV] UPDATED)
97. Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering. (arXiv:2109.08029v3 [cs.CV] UPDATED)
98. QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v3 [eess.IV] UPDATED)
99. LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v2 [cs.CV] UPDATED)
100. Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v5 [cs.CV] UPDATED)
101. Trustworthy Long-Tailed Classification. (arXiv:2111.09030v2 [cs.LG] UPDATED)
102. BoxeR: Box-Attention for 2D and 3D Transformers. (arXiv:2111.13087v2 [cs.CV] UPDATED)
103. Adaptive Inverse Transform Sampling For Efficient Vision Transformers. (arXiv:2111.15667v2 [cs.CV] UPDATED)
104. A Structured Dictionary Perspective on Implicit Neural Representations. (arXiv:2112.01917v2 [cs.LG] UPDATED)
105. Global-Local Context Network for Person Search. (arXiv:2112.02500v2 [cs.CV] UPDATED)
106. HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs. (arXiv:2112.02789v3 [cs.CV] UPDATED)
107. DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover's Distance Improves Out-Of-Distribution Face Identification. (arXiv:2112.04016v2 [cs.CV] UPDATED)
108. Feature Statistics Mixing Regularization for Generative Adversarial Networks. (arXiv:2112.04120v2 [cs.CV] UPDATED)
109. Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning. (arXiv:2112.04731v4 [cs.CV] UPDATED)
110. One Sketch for All: One-Shot Personalized Sketch Segmentation. (arXiv:2112.10838v2 [cs.CV] UPDATED)
111. Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v2 [eess.IV] UPDATED)
112. Deep Contrastive Learning is Provably (almost) Principal Component Analysis. (arXiv:2201.12680v2 [cs.LG] UPDATED)
113. Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v3 [eess.IV] UPDATED)
114. Weakly Supervised Object Localization as Domain Adaption. (arXiv:2203.01714v3 [cs.CV] UPDATED)
115. On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v2 [cs.LG] UPDATED)
116. LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network. (arXiv:2203.01824v2 [cs.CV] UPDATED)
117. Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v2 [cs.LG] UPDATED)
118. Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v2 [cs.CV] UPDATED)
119. Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v3 [cs.CV] UPDATED)
120. Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v2 [cs.CV] UPDATED)
121. Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance. (arXiv:2203.06749v2 [cs.CV] UPDATED)
122. Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization. (arXiv:2203.07740v2 [cs.CV] UPDATED)
123. Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v3 [cs.LG] UPDATED)
124. DATA: Domain-Aware and Task-Aware Self-supervised Learning. (arXiv:2203.09041v2 [cs.CV] UPDATED)
125. Towards a Perceptual Model for Estimating the Quality of Visual Speech. (arXiv:2203.10117v2 [cs.SD] UPDATED)
126. Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v2 [eess.IV] UPDATED)
127. Deep Portrait Delighting. (arXiv:2203.12088v3 [cs.CV] UPDATED)
128. Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. (arXiv:2203.12208v2 [cs.CV] UPDATED)
129. Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v2 [cs.CV] UPDATED)
130. RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization. (arXiv:2203.12870v2 [cs.CV] UPDATED)
131. Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v2 [cs.CV] UPDATED)
132. Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v2 [cs.CV] UPDATED)
133. Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory. (arXiv:2203.13055v2 [cs.SD] UPDATED)
134. A Preliminary Research on Space Situational Awareness Based on Event Cameras. (arXiv:2203.13093v2 [cs.CV] UPDATED)
135. IA-FaceS: A Bidirectional Method for Semantic Face Editing. (arXiv:2203.13097v2 [cs.CV] UPDATED)
136. WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation. (arXiv:2203.12917v1 [cs.CV] CROSS LISTED)
## eess.IV
---
**15** new papers in eess.IV:-) 
1. Practical Blind Denoising via **Swin**-Conv-UNet and Data Synthesis. (arXiv:2203.13278v1 [cs.CV])
2. MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v1 [cs.CV])
3. RD-Optimized Trit-Plane Coding of Deep Compressed Image Latent Tensors. (arXiv:2203.13467v1 [eess.IV])
4. ST-FL: Style Transfer Preprocessing in Federated Learning for COVID-19 Segmentation. (arXiv:2203.13680v1 [eess.IV])
5. Satellite Infrastructure/Mission Tradeoffs. (arXiv:2203.13686v1 [cs.LG])
6. Unsupervised Image Deraining: Optimization Model Driven Deep CNN. (arXiv:2203.13699v1 [cs.CV])
7. Salt Detection Using Segmentation of Seismic Image. (arXiv:2203.13721v1 [cs.CV])
8. Playing Lottery Tickets in Style Transfer Models. (arXiv:2203.13802v1 [cs.CV])
9. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v7 [cs.CV] UPDATED)
10. QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v3 [eess.IV] UPDATED)
11. Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v5 [cs.CV] UPDATED)
12. Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v2 [eess.IV] UPDATED)
13. Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v3 [eess.IV] UPDATED)
14. Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v2 [eess.IV] UPDATED)
15. Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v2 [cs.CV] UPDATED)
## cs.LG
---
**144** new papers in cs.LG:-) 
1. Quantum Feature Selection. (arXiv:2203.13261v1 [quant-ph])
2. Precipitaion Nowcasting using Deep Neural Network. (arXiv:2203.13263v1 [cs.LG])
3. Shoring Up the Foundations: Fusing Model Embeddings and Weak Supervision. (arXiv:2203.13270v1 [stat.ML])
4. On Exploiting Layerwise Gradient Statistics for Effective Training of Deep Neural Networks. (arXiv:2203.13273v1 [cs.LG])
5. A Manifold View of Adversarial Risk. (arXiv:2203.13277v1 [cs.LG])
6. Local optimisation of Nystr\"om samples through stochastic gradient descent. (arXiv:2203.13284v1 [stat.ML])
7. Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition. (arXiv:2203.13285v1 [cs.SD])
8. Learning Spatiotemporal Chaos Using Next-Generation Reservoir Computing. (arXiv:2203.13294v1 [cs.LG])
9. Mix and Match: Learning-free Controllable Text Generation using Energy Language Models. (arXiv:2203.13299v1 [cs.CL])
10. Tackling Online One-Class Incremental Learning by Removing Negative Contrasts. (arXiv:2203.13307v1 [cs.LG])
11. Human Gait Recognition Using Bag of Words Feature Representation Method. (arXiv:2203.13317v1 [cs.CV])
12. Remember and Forget Experience Replay for Multi-Agent Reinforcement Learning. (arXiv:2203.13319v1 [cs.LG])
13. Addressing Client Drift in Federated Continual Learning with Adaptive Optimization. (arXiv:2203.13321v1 [cs.LG])
14. Text to Mesh Without 3D Supervision Using Limit Subdivision. (arXiv:2203.13333v1 [cs.CV])
15. Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation. (arXiv:2203.13339v1 [cs.CL])
16. Linking Emergent and Natural Languages via Corpus Transfer. (arXiv:2203.13344v1 [cs.CL])
17. Does human speech follow Benford's Law?. (arXiv:2203.13352v1 [cs.CL])
18. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v1 [cs.IR])
19. Deep reinforcement learning for optimal well control in subsurface systems with uncertain geology. (arXiv:2203.13375v1 [physics.comp-ph])
20. Statistic Selection and MCMC for Differentially Private Bayesian Estimation. (arXiv:2203.13377v1 [stat.ME])
21. Email Summarization to Assist Users in Phishing Identification. (arXiv:2203.13380v1 [cs.CR])
22. Probing Representation Forgetting in Supervised and Unsupervised Continual Learning. (arXiv:2203.13381v1 [cs.LG])
23. Automated Algorithm Selection: from Feature-Based to Feature-Free Approaches. (arXiv:2203.13392v1 [cs.LG])
24. Qualitative neural network approximation over R and C: Elementary proofs for analytic and polynomial activation. (arXiv:2203.13410v1 [cs.LG])
25. Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers. (arXiv:2203.13411v1 [cs.RO])
26. Amortized Projection Optimization for Sliced Wasserstein Generative Models. (arXiv:2203.13417v1 [stat.ML])
27. Learning Losses for Strategic Classification. (arXiv:2203.13421v1 [cs.LG])
28. Modeling Attrition in Recommender Systems with Departing Bandits. (arXiv:2203.13423v1 [cs.LG])
29. Dealing with Sparse Rewards Using Graph Neural Networks. (arXiv:2203.13424v1 [cs.LG])
30. Risk-Aware Off-Road Navigation via a Learned Speed Distribution Map. (arXiv:2203.13429v1 [cs.RO])
31. Nash Neural Networks : Inferring Utilities from Optimal Behaviour. (arXiv:2203.13432v1 [cs.LG])
32. 3D GAN Inversion for Controllable Portrait Image Animation. (arXiv:2203.13441v1 [cs.CV])
33. Randomized Policy Optimization for Optimal Stopping. (arXiv:2203.13446v1 [math.OC])
34. A Comparative Evaluation of Machine Learning Algorithms for the Prediction of R/C Buildings' Seismic Damage. (arXiv:2203.13449v1 [cs.LG])
35. A Comparative Survey of Deep Active Learning. (arXiv:2203.13450v1 [cs.LG])
36. A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training. (arXiv:2203.13455v1 [cs.LG])
37. Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. (arXiv:2203.13457v1 [cs.LG])
38. From MIM-Based GAN to Anomaly Detection:Event Probability Influence on Generative Adversarial Networks. (arXiv:2203.13464v1 [cs.LG])
39. Non-Probability Sampling Network for Stochastic Human Trajectory Prediction. (arXiv:2203.13471v1 [cs.CV])
40. A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v1 [cs.LG])
41. MKQ-BERT: Quantized BERT with 4-bits Weights and Activations. (arXiv:2203.13483v1 [cs.LG])
42. Machine-Learning Based Objective Function Selection for Community Detection. (arXiv:2203.13495v1 [cs.SI])
43. Supplemental Material: Lifelong Generative Modelling Using Dynamic Expansion Graph Model. (arXiv:2203.13503v1 [cs.LG])
44. Analysis of the Production Strategy of Mask Types in the COVID-19 Environment. (arXiv:2203.13506v1 [cs.CV])
45. BDDM: **Bilateral** Denoising Diffusion Models for Fast and High-Quality Speech Synthesis. (arXiv:2203.13508v1 [eess.AS])
46. Sparse Federated Learning with Hierarchical Personalization Models. (arXiv:2203.13517v1 [cs.LG])
47. Generalization bounds for learning under graph-dependence: A survey. (arXiv:2203.13534v1 [cs.LG])
48. Feature extraction using Spectral Clustering for Gene Function Prediction. (arXiv:2203.13551v1 [cs.LG])
49. Preprocessing Reward Functions for Interpretability. (arXiv:2203.13553v1 [cs.LG])
50. Deformable Butterfly: A Highly Structured and Sparse Linear Transform. (arXiv:2203.13556v1 [cs.CV])
51. Neural Networks with Divisive normalization for image segmentation with application in cityscapes dataset. (arXiv:2203.13558v1 [cs.CV])
52. An Intelligent End-to-End Neural Architecture Search Framework for Electricity Forecasting Model Development. (arXiv:2203.13563v1 [cs.LG])
53. $p$-Generalized Probit Regression and Scalable Maximum Likelihood Estimation via Sketching and Coresets. (arXiv:2203.13568v1 [cs.DS])
54. Improving Question Answering over Knowledge Graphs Using Graph Summarization. (arXiv:2203.13570v1 [cs.LG])
55. Unsupervised Learning of Temporal Abstractions with Slot-based Transformers. (arXiv:2203.13573v1 [cs.LG])
56. Impact of Dataset on Acoustic Models for Automatic Speech Recognition. (arXiv:2203.13590v1 [cs.LG])
57. Fast and computationally efficient generative adversarial network algorithm for unmanned aerial vehicle-based network coverage optimization. (arXiv:2203.13607v1 [cs.LG])
58. Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v1 [cs.LG])
59. Lightweight Graph Convolutional Networks with Topologically Consistent Magnitude Pruning. (arXiv:2203.13616v1 [cs.LG])
60. EmotionNAS: Two-stream Architecture Search for Speech Emotion Recognition. (arXiv:2203.13617v1 [eess.AS])
61. StretchBEV: Stretching Future Instance Prediction Spatially and Temporally. (arXiv:2203.13641v1 [cs.CV])
62. Understanding the Difficulty of Training Physics-Informed Neural Networks on Dynamical Systems. (arXiv:2203.13648v1 [cs.LG])
63. HYDRA: Competing convolutional kernels for fast and accurate time series classification. (arXiv:2203.13652v1 [cs.LG])
64. Gransformer: Transformer-based Graph Generation. (arXiv:2203.13655v1 [cs.LG])
65. Common Failure Modes of Subcluster-based Sampling in Dirichlet Process Gaussian Mixture Models -- and a Deep-learning Solution. (arXiv:2203.13661v1 [cs.LG])
66. FedGradNorm: Personalized Federated Gradient-Normalized Multi-Task Learning. (arXiv:2203.13663v1 [cs.LG])
67. ST-FL: Style Transfer Preprocessing in Federated Learning for COVID-19 Segmentation. (arXiv:2203.13680v1 [eess.IV])
68. Learning to Mediate Disparities Towards Pragmatic Communication. (arXiv:2203.13685v1 [cs.CL])
69. Satellite Infrastructure/Mission Tradeoffs. (arXiv:2203.13686v1 [cs.LG])
70. Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v1 [cs.SD])
71. Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v1 [cs.SD])
72. LAMBDA: Covering the Solution Set of Black-Box Inequality by Search Space Quantization. (arXiv:2203.13708v1 [cs.LG])
73. Searching for Network Width with **Bilateral**ly Coupled Network. (arXiv:2203.13714v1 [cs.CV])
74. Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning. (arXiv:2203.13733v1 [cs.RO])
75. High Dimensional Quantum Learning With Small Quantum Computers. (arXiv:2203.13739v1 [quant-ph])
76. Efficient-VDVAE: Less is more. (arXiv:2203.13751v1 [cs.LG])
77. Fast fluorescence lifetime imaging analysis via extreme learning machine. (arXiv:2203.13754v1 [physics.bio-ph])
78. JAX-FLUIDS: A fully-differentiable high-order computational fluid dynamics solver for compressible two-phase flows. (arXiv:2203.13760v1 [physics.flu-dyn])
79. Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion. (arXiv:2203.13777v1 [cs.CV])
80. L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models. (arXiv:2203.13778v1 [cs.CL])
81. Origins of Low-dimensional Adversarial Perturbations. (arXiv:2203.13779v1 [stat.ML])
82. Ensemble Spectral Prediction (ESP) Model for Metabolite Annotation. (arXiv:2203.13783v1 [cs.LG])
83. A Hybrid Framework for Sequential Data Prediction with End-to-End Optimization. (arXiv:2203.13787v1 [stat.ML])
84. FLUTE: A Scalable, Extensible Framework for High-Performance Federated Learning Simulations. (arXiv:2203.13789v1 [cs.LG])
85. Asynchronous Decentralized SGD with Quantized and Local Updates. (arXiv:1910.12308v4 [cs.LG] UPDATED)
86. Principal Fairness for Human and Algorithmic Decision-Making. (arXiv:2005.10400v5 [cs.CY] UPDATED)
87. Which Model to Transfer? Finding the Needle in the Growing Haystack. (arXiv:2010.06402v2 [cs.LG] UPDATED)
88. Understanding and Increasing Efficiency of Frank-Wolfe Adversarial Training. (arXiv:2012.12368v5 [cs.LG] UPDATED)
89. On Transportation of Mini-batches: A Hierarchical Approach. (arXiv:2102.05912v4 [stat.ML] UPDATED)
90. Reinforcement Learning, Bit by Bit. (arXiv:2103.04047v7 [cs.LG] UPDATED)
91. Split Computing and Early Exiting for Deep Learning Applications: Survey and Research Challenges. (arXiv:2103.04505v4 [eess.SP] UPDATED)
92. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v7 [cs.CV] UPDATED)
93. From Distributed Machine Learning to Federated Learning: A Survey. (arXiv:2104.14362v4 [cs.DC] UPDATED)
94. CrossWalk: Fairness-enhanced Node Representation Learning. (arXiv:2105.02725v2 [cs.LG] UPDATED)
95. CrystalCandle: A User-Facing Model Explainer for Narrative Explanations. (arXiv:2105.12941v2 [stat.ML] UPDATED)
96. MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens. (arXiv:2105.15168v3 [cs.CV] UPDATED)
97. Photonic Differential Privacy with Direct Feedback Alignment. (arXiv:2106.03645v2 [cs.LG] UPDATED)
98. Learning from Multiple Noisy Partial Labelers. (arXiv:2106.04530v2 [cs.LG] UPDATED)
99. Verifiable and Compositional Reinforcement Learning Systems. (arXiv:2106.05864v2 [cs.LG] UPDATED)
100. Multi-level Feature Learning for Contrastive Multi-view Clustering. (arXiv:2106.11193v2 [cs.LG] UPDATED)
101. Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v2 [cs.CL] UPDATED)
102. Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior. (arXiv:2107.09234v2 [cs.LG] UPDATED)
103. State, global and local parameter estimation using local ensemble Kalman filters: applications to online machine learning of chaotic dynamics. (arXiv:2107.11253v4 [stat.ML] UPDATED)
104. Structure Learning for Directed Trees. (arXiv:2108.08871v3 [stat.ML] UPDATED)
105. Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v3 [cs.CL] UPDATED)
106. Adversarial Bone Length Attack on Action Recognition. (arXiv:2109.05830v2 [cs.CV] UPDATED)
107. Uncertainty-Aware Machine Translation Evaluation. (arXiv:2109.06352v2 [cs.CL] UPDATED)
108. Fast TreeSHAP: Accelerating SHAP Value Computation for Trees. (arXiv:2109.09847v2 [cs.LG] UPDATED)
109. FlowVocoder: A small Footprint Neural Vocoder based Normalizing flow for Speech Synthesis. (arXiv:2109.13675v2 [cs.SD] UPDATED)
110. Physics informed neural networks for continuum micromechanics. (arXiv:2110.07374v2 [cs.LG] UPDATED)
111. Auctions Between Regret-Minimizing Agents. (arXiv:2110.11855v3 [cs.GT] UPDATED)
112. QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v3 [eess.IV] UPDATED)
113. Multi-Task Neural Processes. (arXiv:2110.14953v5 [cs.LG] UPDATED)
114. An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v4 [cs.CL] UPDATED)
115. Distribution Compression in Near-linear Time. (arXiv:2111.07941v3 [stat.ML] UPDATED)
116. LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v2 [cs.CV] UPDATED)
117. Trustworthy Long-Tailed Classification. (arXiv:2111.09030v2 [cs.LG] UPDATED)
118. GHRS: Graph-based Hybrid Recommendation System with Application to Movie Recommendation. (arXiv:2111.11293v2 [cs.IR] UPDATED)
119. A Structured Dictionary Perspective on Implicit Neural Representations. (arXiv:2112.01917v2 [cs.LG] UPDATED)
120. DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover's Distance Improves Out-Of-Distribution Face Identification. (arXiv:2112.04016v2 [cs.CV] UPDATED)
121. Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning. (arXiv:2112.04731v4 [cs.CV] UPDATED)
122. Score-Based Generative Modeling with Critically-Damped Langevin Diffusion. (arXiv:2112.07068v4 [stat.ML] UPDATED)
123. Total Energy Shaping with Neural Interconnection and Damping Assignment -- Passivity Based Control. (arXiv:2112.12999v2 [eess.SY] UPDATED)
124. Bounding Training Data Reconstruction in Private (Deep) Learning. (arXiv:2201.12383v2 [cs.LG] UPDATED)
125. Deep Contrastive Learning is Provably (almost) Principal Component Analysis. (arXiv:2201.12680v2 [cs.LG] UPDATED)
126. Deep discriminative to kernel generative modeling. (arXiv:2201.13001v4 [cs.LG] UPDATED)
127. From Generalisation Error to Transportation-cost Inequalities and Back. (arXiv:2202.03956v3 [cs.IT] UPDATED)
128. Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines. (arXiv:2202.08679v3 [cs.LG] UPDATED)
129. Prune and Tune Ensembles: Low-Cost Ensemble Learning With Sparse Independent Subnetworks. (arXiv:2202.11782v2 [cs.LG] UPDATED)
130. Weakly Supervised Object Localization as Domain Adaption. (arXiv:2203.01714v3 [cs.CV] UPDATED)
131. On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v2 [cs.LG] UPDATED)
132. T-Cal: An optimal test for the calibration of predictive models. (arXiv:2203.01850v2 [stat.ML] UPDATED)
133. Evolving symbolic density functionals. (arXiv:2203.02540v3 [cs.NE] UPDATED)
134. Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v2 [cs.LG] UPDATED)
135. Contrastive Conditional Neural Processes. (arXiv:2203.03978v2 [cs.LG] UPDATED)
136. Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v3 [cs.CV] UPDATED)
137. Data Smells in Public Datasets. (arXiv:2203.08007v3 [cs.SE] UPDATED)
138. Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v3 [cs.LG] UPDATED)
139. Thompson Sampling on Asymmetric $\alpha$-Stable Bandits. (arXiv:2203.10214v2 [stat.ML] UPDATED)
140. Deep Portrait Delighting. (arXiv:2203.12088v3 [cs.CV] UPDATED)
141. A Deep Learning Approach to Probabilistic Forecasting of Weather. (arXiv:2203.12529v2 [stat.ML] UPDATED)
142. Bioformers: Embedding Transformers for Ultra-Low Power sEMG-based Gesture Recognition. (arXiv:2203.12932v2 [eess.SP] UPDATED)
143. Explainable Artificial Intelligence for Exhaust Gas Temperature of Turbofan Engines. (arXiv:2203.13108v2 [cs.LG] UPDATED)
144. Safe Neurosymbolic Learning with Differentiable Symbolic Execution. (arXiv:2203.07671v1 [cs.LG] CROSS LISTED)
## cs.AI
---
**60** new papers in cs.AI:-) 
1. Computing Optimal Location of Microphone for Improved Speech Recognition. (arXiv:2203.13259v1 [eess.AS])
2. Interpretability of Neural Network With Physiological Mechanisms. (arXiv:2203.13262v1 [cs.NE])
3. Precipitaion Nowcasting using Deep Neural Network. (arXiv:2203.13263v1 [cs.LG])
4. Tackling Online One-Class Incremental Learning by Removing Negative Contrasts. (arXiv:2203.13307v1 [cs.LG])
5. MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v1 [cs.CV])
6. Remember and Forget Experience Replay for Multi-Agent Reinforcement Learning. (arXiv:2203.13319v1 [cs.LG])
7. Addressing Client Drift in Federated Continual Learning with Adaptive Optimization. (arXiv:2203.13321v1 [cs.LG])
8. Linking Emergent and Natural Languages via Corpus Transfer. (arXiv:2203.13344v1 [cs.CL])
9. Occluded Human Mesh Recovery. (arXiv:2203.13349v1 [cs.CV])
10. Predicting Personas Using Mechanic Frequencies and Game State Traces. (arXiv:2203.13351v1 [cs.AI])
11. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v1 [cs.IR])
12. Gender and Racial Stereotype Detection in Legal Opinion Word Embeddings. (arXiv:2203.13369v1 [cs.CL])
13. Probing Representation Forgetting in Supervised and Unsupervised Continual Learning. (arXiv:2203.13381v1 [cs.LG])
14. Automated Algorithm Selection: from Feature-Based to Feature-Free Approaches. (arXiv:2203.13392v1 [cs.LG])
15. Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers. (arXiv:2203.13411v1 [cs.RO])
16. Automatic Song Translation for Tonal Languages. (arXiv:2203.13420v1 [cs.CL])
17. Dealing with Sparse Rewards Using Graph Neural Networks. (arXiv:2203.13424v1 [cs.LG])
18. Risk-Aware Off-Road Navigation via a Learned Speed Distribution Map. (arXiv:2203.13429v1 [cs.RO])
19. Component-wise Analysis of Automatically Designed Multiobjective Algorithms on Constrained Problems. (arXiv:2203.13447v1 [cs.NE])
20. A Comparative Evaluation of Machine Learning Algorithms for the Prediction of R/C Buildings' Seismic Damage. (arXiv:2203.13449v1 [cs.LG])
21. Semi-supervised and Deep learning Frameworks for Video Classification and Key-frame Identification. (arXiv:2203.13459v1 [cs.CV])
22. From MIM-Based GAN to Anomaly Detection:Event Probability Influence on Generative Adversarial Networks. (arXiv:2203.13464v1 [cs.LG])
23. Facial Expression Recognition with **Swin** Transformer. (arXiv:2203.13472v1 [cs.CV])
24. BDDM: **Bilateral** Denoising Diffusion Models for Fast and High-Quality Speech Synthesis. (arXiv:2203.13508v1 [eess.AS])
25. EnHDC: Ensemble Learning for Brain-Inspired Hyperdimensional Computing. (arXiv:2203.13542v1 [cs.NE])
26. Personalize Web Searching Strategies Classification and Comparison. (arXiv:2203.13561v1 [cs.IR])
27. Unsupervised Learning of Temporal Abstractions with Slot-based Transformers. (arXiv:2203.13573v1 [cs.LG])
28. Fast Hybrid Image Retargeting. (arXiv:2203.13595v1 [cs.CV])
29. Learning Relational Rules from Rewards. (arXiv:2203.13599v1 [cs.AI])
30. Formal Semantics and Formally Verified Validation for Temporal Planning. (arXiv:2203.13604v1 [cs.AI])
31. Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v1 [cs.LG])
32. LQoCo: Learning to Optimize Cache Capacity Overloading in Storage Systems. (arXiv:2203.13678v1 [cs.AR])
33. Learning to Mediate Disparities Towards Pragmatic Communication. (arXiv:2203.13685v1 [cs.CL])
34. Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v1 [cs.SD])
35. The TerraByte Client: providing access to terabytes of plant data. (arXiv:2203.13691v1 [cs.CV])
36. Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v1 [cs.SD])
37. LAMBDA: Covering the Solution Set of Black-Box Inequality by Search Space Quantization. (arXiv:2203.13708v1 [cs.LG])
38. Code Smells for Machine Learning Applications. (arXiv:2203.13746v1 [cs.SE])
39. A World-Self Model Towards Understanding Intelligence. (arXiv:2203.13762v1 [cs.AI])
40. An Audit of Misinformation Filter Bubbles on YouTube: Bubble Bursting and Recent Behavior Changes. (arXiv:2203.13769v1 [cs.IR])
41. Ensemble Spectral Prediction (ESP) Model for Metabolite Annotation. (arXiv:2203.13783v1 [cs.LG])
42. Reinforcement Learning, Bit by Bit. (arXiv:2103.04047v7 [cs.LG] UPDATED)
43. From Distributed Machine Learning to Federated Learning: A Survey. (arXiv:2104.14362v4 [cs.DC] UPDATED)
44. Verifiable and Compositional Reinforcement Learning Systems. (arXiv:2106.05864v2 [cs.LG] UPDATED)
45. Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v3 [cs.CL] UPDATED)
46. ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v2 [cs.CV] UPDATED)
47. Adversarial Bone Length Attack on Action Recognition. (arXiv:2109.05830v2 [cs.CV] UPDATED)
48. Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering. (arXiv:2109.08029v3 [cs.CV] UPDATED)
49. Auctions Between Regret-Minimizing Agents. (arXiv:2110.11855v3 [cs.GT] UPDATED)
50. GHRS: Graph-based Hybrid Recommendation System with Application to Movie Recommendation. (arXiv:2111.11293v2 [cs.IR] UPDATED)
51. Evaluating importance of nodes in complex networks with local volume information dimension. (arXiv:2111.13585v2 [cs.SI] UPDATED)
52. Frequency Fitness Assignment: Optimization without a Bias for Good Solutions can be Efficient. (arXiv:2112.00229v3 [cs.NE] UPDATED)
53. InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation. (arXiv:2112.01589v3 [cs.CL] UPDATED)
54. Deep discriminative to kernel generative modeling. (arXiv:2201.13001v4 [cs.LG] UPDATED)
55. Leveraging Experience in Lifelong Multi-Agent Pathfinding. (arXiv:2202.04382v2 [cs.MA] UPDATED)
56. Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v2 [cs.LG] UPDATED)
57. Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v2 [cs.CV] UPDATED)
58. Explainable Artificial Intelligence for Exhaust Gas Temperature of Turbofan Engines. (arXiv:2203.13108v2 [cs.LG] UPDATED)
59. Safe Neurosymbolic Learning with Differentiable Symbolic Execution. (arXiv:2203.07671v1 [cs.LG] CROSS LISTED)
60. WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation. (arXiv:2203.12917v1 [cs.CV] CROSS LISTED)

