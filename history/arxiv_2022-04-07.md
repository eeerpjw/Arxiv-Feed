# Your interest papers
---
## cs.CV
---
### Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])
- Authors : Chaewon Kim, Jaeho Lee, Jinwoo Shin
- Link : [http://arxiv.org/abs/2204.02405](http://arxiv.org/abs/2204.02405)
> ABSTRACT  :  Recent denoising algorithms based on the "blind-spot" strategy show impressive blind image denoising performances, without utilizing any external dataset. While the methods excel in recovering highly contaminated images, we observe that such algorithms are often less effective under a low-noise or real noise regime. To address this gap, we propose an alternative denoising strategy that leverages the architectural inductive bias of **implicit neural representation**s (INRs), based on our two findings: (1) INR tends to fit the low-frequency clean image signal faster than the high-frequency noise, and (2) INR layers that are closer to the output play more critical roles in fitting higher-frequency parts. Building on these observations, we propose a denoising algorithm that maximizes the innate denoising capability of INRs by penalizing the growth of deeper layer weights. We show that our method outperforms existing zero-shot denoising methods under an extensive set of low-noise or real-noise scenarios.  
### MixFormer: Mixing Features across Windows and Dimensions. (arXiv:2204.02557v1 [cs.CV])
- Authors : Qiang Chen, Qiman Wu, Jian Wang, Qinghao Hu, Tao Hu, Errui Ding, Jian Cheng, Jingdong Wang
- Link : [http://arxiv.org/abs/2204.02557](http://arxiv.org/abs/2204.02557)
> ABSTRACT  :  While local-window self-attention performs notably in vision tasks, it suffers from limited receptive field and weak modeling capability issues. This is mainly because it performs self-attention within non-overlapped windows and shares weights on the channel dimension. We propose MixFormer to find a solution. First, we combine local-window self-attention with depth-wise convolution in a parallel design, modeling cross-window connections to enlarge the receptive fields. Second, we propose bi-directional interactions across branches to provide complementary clues in the channel and spatial dimensions. These two designs are integrated to achieve efficient feature mixing among windows and dimensions. Our MixFormer provides competitive results on image classification with EfficientNet and shows better results than RegNet and **Swin** Transformer. Performance in downstream tasks outperforms its alternatives by significant margins with less computational costs in 5 dense prediction tasks on MS COCO, ADE20k, and LVIS. Code is available at \url{https://github.com/PaddlePaddle/PaddleClas}.  
### Squeeze**NeRF**: Further factorized Fast**NeRF** for memory-efficient inference. (arXiv:2204.02585v1 [cs.CV])
- Authors : Krishna Wadhwani, Tamaki Kojima
- Link : [http://arxiv.org/abs/2204.02585](http://arxiv.org/abs/2204.02585)
> ABSTRACT  :  Neural Radiance Fields (**NeRF**) has emerged as the state-of-the-art method for novel view generation of complex scenes, but is very slow during inference. Recently, there have been multiple works on speeding up **NeRF** inference, but the state of the art methods for real-time **NeRF** inference rely on caching the neural network output, which occupies several giga-bytes of disk space that limits their real-world applicability. As caching the neural network of original **NeRF** network is not feasible, Garbin et.al. proposed "Fast**NeRF**" which factorizes the problem into 2 sub-networks - one which depends only on the 3D coordinate of a sample point and one which depends only on the 2D camera viewing direction. Although this factorization enables them to reduce the cache size and perform inference at over 200 frames per second, the memory overhead is still substantial. In this work, we propose Squeeze**NeRF**, which is more than 60 times memory-efficient than the sparse cache of Fast**NeRF** and is still able to render at more than 190 frames per second on a high spec GPU during inference.  
### CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v1 [cs.LG])
- Authors : Emanuel Slany, Yannik Ott, Stephan Scheele, Jan Paulus, Ute Schmid
- Link : [http://arxiv.org/abs/2204.02661](http://arxiv.org/abs/2204.02661)
> ABSTRACT  :  Would you trust physicians if they cannot explain their decisions to you? Medical diagnostics using machine learning gained enormously in importance within the last decade. However, without further **enhancement**s many state-of-the-art machine learning methods are not suitable for medical application. The most important reasons are insufficient data set quality and the black-box behavior of machine learning algorithms such as Deep Learning models. Consequently, end-users cannot correct the model's decisions and the corresponding explanations. The latter is crucial for the trustworthiness of machine learning in the medical domain. The research field explainable interactive machine learning searches for methods that address both shortcomings. This paper extends the explainable and interactive CAIPI algorithm and provides an interface to simplify human-in-the-loop approaches for image classification. The interface enables the end-user (1) to investigate and (2) to correct the model's prediction and explanation, and (3) to influence the data set quality. After CAIPI optimization with only a single counterexample per iteration, the model achieves an accuracy of $97.48\%$ on the Medical MNIST and $95.02\%$ on the Fashion MNIST. This accuracy is approximately equal to state-of-the-art Deep Learning optimization procedures. Besides, CAIPI reduces the labeling effort by approximately $80\%$.  
### Semi-DRDNet Semi-supervised Detail-recovery Image Deraining Network via Unpaired Contrastive Learning. (arXiv:2204.02772v1 [cs.CV])
- Authors : Yiyang Shen, Sen Deng, **Wenhan Yang**, Mingqiang Wei, Haoran Xie, XiaoPing Zhang, Jing Qin, Meng Wang
- Link : [http://arxiv.org/abs/2204.02772](http://arxiv.org/abs/2204.02772)
> ABSTRACT  :  The intricacy of rainy image contents often leads cutting-edge deraining models to image degradation including remnant rain, wrongly-removed details, and distorted appearance. Such degradation is further exacerbated when applying the models trained on synthetic data to real-world rainy images. We raise an intriguing question -- if leveraging both accessible unpaired clean/rainy yet real-world images and additional detail repair guidance, can improve the generalization ability of a deraining model? To answer it, we propose a semi-supervised detail-recovery image deraining network (termed as Semi-DRDNet). Semi-DRDNet consists of three branches: 1) for removing rain streaks without remnants, we present a \textit{squeeze-and-excitation} (SE)-based rain residual network; 2) for encouraging the lost details to return, we construct a \textit{structure detail context aggregation} (SDCAB)-based detail repair network; to our knowledge, this is the first time; and 3) for bridging the domain gap, we develop a novel contrastive regularization network to learn from unpaired positive (clean) and negative (rainy) yet real-world images. As a semi-supervised learning paradigm, Semi-DRDNet operates smoothly on both synthetic and real-world rainy data in terms of deraining robustness and detail accuracy. Comparisons on four datasets show clear visual and numerical improvements of our Semi-DRDNet over thirteen state-of-the-arts.  
### Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v1 [cs.CV])
- Authors : Zhiqi Kang, Mostafa Sadeghi, Radu Horaud, Jacob Donley, Anurag Kumar, Xavier Alameda
- Link : [http://arxiv.org/abs/2204.02810](http://arxiv.org/abs/2204.02810)
> ABSTRACT  :  Face frontalization consists of synthesizing a frontally-viewed face from an arbitrarily-viewed one. The main contribution of this paper is a frontalization methodology that preserves non-rigid facial deformations in order to boost the performance of visually assisted speech communication. The method alternates between the estimation of (i)~the rigid transformation (scale, rotation, and translation) and (ii)~the non-rigid deformation between an arbitrarily-viewed face and a face model. The method has two important merits: it can deal with non-Gaussian errors in the data and it incorporates a dynamical face deformation model. For that purpose, we use the generalized Student t-distribution in combination with a linear dynamic system in order to account for both rigid head motions and time-varying facial deformations caused by speech production. We propose to use the zero-mean normalized cross-correlation (ZNCC) score to evaluate the ability of the method to preserve facial expressions. The method is thoroughly evaluated and compared with several state of the art methods, either based on traditional geometric models or on deep learning. Moreover, we show that the method, when incorporated into deep learning pipelines, namely lip reading and speech **enhancement**, improves word recognition and speech intelligibilty scores by a considerable margin. Supplemental material is accessible at https://team.inria.fr/robotlearn/research/facefrontalization-benchmark/  
### An Empirical Study of Remote Sensing Pretraining. (arXiv:2204.02825v1 [cs.CV])
- Authors : Di Wang, Jing Zhang, Bo Du, Song Xia, Dacheng Tao
- Link : [http://arxiv.org/abs/2204.02825](http://arxiv.org/abs/2204.02825)
> ABSTRACT  :  Deep learning has largely reshaped remote sensing research for aerial image understanding. Nevertheless, most of existing deep models are initialized with ImageNet pretrained weights, where the natural images inevitably presents a large domain gap relative to the aerial images, probably limiting the finetuning performance on downstream aerial scene tasks. This issue motivates us to conduct an empirical study of remote sensing pretraining (RSP). To this end, we train different networks from scratch with the help of the largest remote sensing scene recognition dataset up to now-MillionAID, to obtain the remote sensing pretrained backbones, including both convolutional neural networks (CNN) and vision transformers such as **Swin** and ViTAE, which have shown promising performance on computer vision tasks. Then, we investigate the impact of ImageNet pretraining (IMP) and RSP on a series of downstream tasks including scene recognition, semantic segmentation, object detection, and change detection using the CNN and vision transformers backbones. We have some empirical findings as follows. First, vision transformers generally outperforms CNN backbones, where ViTAE achieves the best performance, owing to its strong representation capacity by introducing intrinsic inductive bias from convolutions to transformers. Second, both IMP and RSP help deliver better performance, where IMP enjoys a versatility by learning more universal representations from diverse images belonging to much more categories while RSP is distinctive in perceiving remote sensing related semantics. Third, RSP mitigates the data discrepancy of IMP for remote sensing but may still suffer from the task discrepancy, where downstream tasks require different representations from the scene recognition task. These findings call for further research efforts on both large-scale pretraining datasets and effective pretraining methods.  
### Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection. (arXiv:2204.02964v1 [cs.CV])
- Authors : Yuxin Fang, Shusheng Yang, Shijie Wang, Yixiao Ge, Ying Shan, Xinggang Wang
- Link : [http://arxiv.org/abs/2204.02964](http://arxiv.org/abs/2204.02964)
> ABSTRACT  :  We present an approach to efficiently and effectively adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection, which is based on our two novel observations: (i) A MIM pre-trained vanilla ViT can work surprisingly well in the challenging object-level recognition scenario even with random sampled partial observations, e.g., only 25% ~ 50% of the input sequence. (ii) In order to construct multi-scale representations for object detection, a random initialized compact convolutional stem supplants the pre-trained large kernel patchify stem, and its intermediate features can naturally serve as the higher resolution inputs of a feature pyramid without upsampling. While the pre-trained ViT is only regarded as the third-stage of our detector's backbone instead of the whole feature extractor, resulting in a ConvNet-ViT hybrid architecture. The proposed detector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform hierarchical **Swin** Transformer by 2.3 box AP and 2.5 mask AP on COCO, and achieve even better results compared with other adapted vanilla ViT using a more modest fine-tuning recipe while converging 2.8x faster. Code and pre-trained models are available at \url{https://github.com/hustvl/MIMDet}.  
### Semantically Adversarial Learnable Filters. (arXiv:2008.06069v3 [cs.CV] UPDATED)
- Authors : Ali Shahin, Changjae Oh, Andrea Cavallaro
- Link : [http://arxiv.org/abs/2008.06069](http://arxiv.org/abs/2008.06069)
> ABSTRACT  :  We present an adversarial framework to craft perturbations that mislead classifiers by accounting for the image content and the semantics of the labels. The proposed framework combines a structure loss and a semantic adversarial loss in a multi-task objective function to train a fully convolutional neural network. The structure loss helps generate perturbations whose type and magnitude are defined by a target image processing filter. The semantic adversarial loss considers groups of (semantic) labels to craft perturbations that prevent the filtered image {from} being classified with a label in the same group. We validate our framework with three different target filters, namely detail **enhancement**, log transformation and gamma correction filters; and evaluate the adversarially filtered images against three classifiers, ResNet50, ResNet18 and AlexNet, pre-trained on ImageNet. We show that the proposed framework generates filtered images with a high success rate, robustness, and transferability to unseen classifiers. We also discuss objective and subjective evaluations of the adversarial perturbations.  
### **NeRF**--: Neural Radiance Fields Without Known Camera Parameters. (arXiv:2102.07064v4 [cs.CV] UPDATED)
- Authors : Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian
- Link : [http://arxiv.org/abs/2102.07064](http://arxiv.org/abs/2102.07064)
> ABSTRACT  :  Considering the problem of novel view synthesis (NVS) from only a set of 2D images, we simplify the training process of Neural Radiance Field (**NeRF**) on forward-facing scenes by removing the requirement of known or pre-computed camera parameters, including both intrinsics and 6DoF poses. To this end, we propose **NeRF**$--$, with three contributions: First, we show that the camera parameters can be jointly optimised as learnable parameters with **NeRF** training, through a photometric reconstruction; Second, to benchmark the camera parameter estimation and the quality of novel view renderings, we introduce a new dataset of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset (BLEFF); Third, we conduct extensive analyses to understand the training behaviours under various camera motions, and show that in most scenarios, the joint optimisation pipeline can recover accurate camera parameters and achieve comparable novel view synthesis quality as those trained with COLMAP pre-computed camera parameters. Our code and data are available at https://nerfmm.active.vision.  
### On the Connection between Local Attention and Dynamic Depth-wise Convolution. (arXiv:2106.04263v4 [cs.CV] UPDATED)
- Authors : Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming Cheng, **Jiaying Liu**, Jingdong Wang
- Link : [http://arxiv.org/abs/2106.04263](http://arxiv.org/abs/2106.04263)
> ABSTRACT  :  Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as weight computation. Sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. Weight sharing: the connection weights for one position are shared across channels or within each group of channels. Dynamic weight: the connection weights are dynamically predicted according to each image instance. We point out that local attention resembles depth-wise convolution and its dynamic version in sparse connectivity. The main difference lies in weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions. We empirically observe that the models based on depth-wise convolution and the dynamic variant with lower computation complexity perform on-par with or sometimes slightly better than **Swin** Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. These observations suggest that Local Vision Transformer takes advantage of two regularization forms and dynamic weight to increase the network capacity. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT.  
### NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v3 [cs.CV] UPDATED)
- Authors : Jerrick Liu, Nathan Inkawhich, Oliver Nina, Radu Timofte, Sahil Jain, Bob Lee, Yuru Duan, Wei Wei, **Lei Zhang**, Songzheng Xu, Yuxuan Sun, Jiaqi Tang, Xueli Geng, Mengru Ma, Gongzhe Li, Xueli Geng, Huanqia Cai, Chengxue Cai, Sol Cummings, Casian Miron, Alexandru Pasarica, Yen Yang, Min Hsu, Jiarui Cai, Jie Mei, Ying Yeh, Neng Hwang, Michael Xin, Zhongkai Shangguan, Zihe Zheng, Xu Yifei, Lehan Yang, Kele Xu, Min Feng
- Link : [http://arxiv.org/abs/2107.01189](http://arxiv.org/abs/2107.01189)
> ABSTRACT  :  In this paper, we introduce the first Challenge on Multi-modal Aerial View Object Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at CVPR. This challenge is composed of two different tracks using EO andSAR imagery. Both EO and SAR sensors possess different advantages and drawbacks. The purpose of this competition is to analyze how to use both sets of sensory information in complementary ways. We discuss the top methods submitted for this competition and evaluate their results on our blind test set. Our challenge results show significant improvement of more than 15% accuracy from our current baselines for each track of the competition  
### **NeRF**ReN: Neural Radiance Fields with Reflections. (arXiv:2111.15234v2 [cs.CV] UPDATED)
- Authors : Chen Guo, Di Kang, Linchao Bao, Yu He, Hai Zhang
- Link : [http://arxiv.org/abs/2111.15234](http://arxiv.org/abs/2111.15234)
> ABSTRACT  :  Neural Radiance Fields (**NeRF**) has achieved unprecedented view synthesis quality using coordinate-based neural scene representations. However, **NeRF**'s view dependency can only handle simple reflections like highlights but cannot deal with complex reflections such as those from glass and mirrors. In these scenarios, **NeRF** models the virtual image as real geometries which leads to inaccurate depth estimation, and produces blurry renderings when the multi-view consistency is violated as the reflected objects may only be seen under some of the viewpoints. To overcome these issues, we introduce **NeRF**ReN, which is built upon **NeRF** to model scenes with reflections. Specifically, we propose to split a scene into transmitted and reflected components, and model the two components with separate neural radiance fields. Considering that this decomposition is highly under-constrained, we exploit geometric priors and apply carefully-designed training strategies to achieve reasonable decomposition results. Experiments on various self-captured scenes show that our method achieves high-quality novel view synthesis and physically sound depth estimation results while enabling scene editing applications.  
### Medium Transmission Map Matters for Learning to Restore Real-World Underwater Images. (arXiv:2203.09414v2 [cs.CV] UPDATED)
- Authors : Yan Kai, Liang Lanyue, Zheng Ziqiang, Wang Guoqing, Yang Yang
- Link : [http://arxiv.org/abs/2203.09414](http://arxiv.org/abs/2203.09414)
> ABSTRACT  :  Underwater visual perception is essentially important for underwater exploration, archeology, ecosystem and so on. The low illumination, light reflections, scattering, absorption and suspended particles inevitably lead to the critically degraded underwater image quality, which causes great challenges on recognizing the objects from the underwater images. The existing underwater **enhancement** methods that aim to promote the underwater visibility, heavily suffer from the poor image **restoration** performance and generalization ability. To reduce the difficulty of underwater image **enhancement**, we introduce the media transmission map as guidance to assist in image **enhancement**. We formulate the interaction between the underwater visual images and the transmission map to obtain better **enhancement** results. Even with simple and lightweight network configuration, the proposed method can achieve advanced results of 22.6 dB on the challenging Test-R90 with an impressive 30 times faster than the existing models. Comprehensive experimental results have demonstrated the superiority and potential on underwater perception. Paper's code is offered on: https://github.com/GroupG-yk/MTUR-Net.  
### Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment. (arXiv:2203.14557v2 [cs.CV] UPDATED)
- Authors : Junyong You
- Link : [http://arxiv.org/abs/2203.14557](http://arxiv.org/abs/2203.14557)
> ABSTRACT  :  Visual (image, video) quality assessments can be modelled by visual features in different domains, e.g., spatial, frequency, and temporal domains. Perceptual mechanisms in the human visual system (HVS) play a crucial role in generation of quality perception. This paper proposes a general framework for no-reference visual quality assessment using efficient windowed transformer architectures. A lightweight module for multi-stage channel attention is integrated into **Swin** (shifted window) Transformer. Such module can represent appropriate perceptual mechanisms in image quality assessment (IQA) to build an accurate IQA model. Meanwhile, representative features for image quality perception in the spatial and frequency domains can also be derived from the IQA model, which are then fed into another windowed transformer architecture for video quality assessment (VQA). The VQA model efficiently reuses attention information across local windows to tackle the issue of expensive time and memory complexities of original transformer. Experimental results on both large-scale IQA and VQA databases demonstrate that the proposed quality assessment models outperform other state-of-the-art models by large margins. The complete source code will be published on Github.  
## eess.IV
---
### Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])
- Authors : Chaewon Kim, Jaeho Lee, Jinwoo Shin
- Link : [http://arxiv.org/abs/2204.02405](http://arxiv.org/abs/2204.02405)
> ABSTRACT  :  Recent denoising algorithms based on the "blind-spot" strategy show impressive blind image denoising performances, without utilizing any external dataset. While the methods excel in recovering highly contaminated images, we observe that such algorithms are often less effective under a low-noise or real noise regime. To address this gap, we propose an alternative denoising strategy that leverages the architectural inductive bias of **implicit neural representation**s (INRs), based on our two findings: (1) INR tends to fit the low-frequency clean image signal faster than the high-frequency noise, and (2) INR layers that are closer to the output play more critical roles in fitting higher-frequency parts. Building on these observations, we propose a denoising algorithm that maximizes the innate denoising capability of INRs by penalizing the growth of deeper layer weights. We show that our method outperforms existing zero-shot denoising methods under an extensive set of low-noise or real-noise scenarios.  
### CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v1 [cs.LG])
- Authors : Emanuel Slany, Yannik Ott, Stephan Scheele, Jan Paulus, Ute Schmid
- Link : [http://arxiv.org/abs/2204.02661](http://arxiv.org/abs/2204.02661)
> ABSTRACT  :  Would you trust physicians if they cannot explain their decisions to you? Medical diagnostics using machine learning gained enormously in importance within the last decade. However, without further **enhancement**s many state-of-the-art machine learning methods are not suitable for medical application. The most important reasons are insufficient data set quality and the black-box behavior of machine learning algorithms such as Deep Learning models. Consequently, end-users cannot correct the model's decisions and the corresponding explanations. The latter is crucial for the trustworthiness of machine learning in the medical domain. The research field explainable interactive machine learning searches for methods that address both shortcomings. This paper extends the explainable and interactive CAIPI algorithm and provides an interface to simplify human-in-the-loop approaches for image classification. The interface enables the end-user (1) to investigate and (2) to correct the model's prediction and explanation, and (3) to influence the data set quality. After CAIPI optimization with only a single counterexample per iteration, the model achieves an accuracy of $97.48\%$ on the Medical MNIST and $95.02\%$ on the Fashion MNIST. This accuracy is approximately equal to state-of-the-art Deep Learning optimization procedures. Besides, CAIPI reduces the labeling effort by approximately $80\%$.  
## cs.LG
---
### Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])
- Authors : Chaewon Kim, Jaeho Lee, Jinwoo Shin
- Link : [http://arxiv.org/abs/2204.02405](http://arxiv.org/abs/2204.02405)
> ABSTRACT  :  Recent denoising algorithms based on the "blind-spot" strategy show impressive blind image denoising performances, without utilizing any external dataset. While the methods excel in recovering highly contaminated images, we observe that such algorithms are often less effective under a low-noise or real noise regime. To address this gap, we propose an alternative denoising strategy that leverages the architectural inductive bias of **implicit neural representation**s (INRs), based on our two findings: (1) INR tends to fit the low-frequency clean image signal faster than the high-frequency noise, and (2) INR layers that are closer to the output play more critical roles in fitting higher-frequency parts. Building on these observations, we propose a denoising algorithm that maximizes the innate denoising capability of INRs by penalizing the growth of deeper layer weights. We show that our method outperforms existing zero-shot denoising methods under an extensive set of low-noise or real-noise scenarios.  
### CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v1 [cs.LG])
- Authors : Emanuel Slany, Yannik Ott, Stephan Scheele, Jan Paulus, Ute Schmid
- Link : [http://arxiv.org/abs/2204.02661](http://arxiv.org/abs/2204.02661)
> ABSTRACT  :  Would you trust physicians if they cannot explain their decisions to you? Medical diagnostics using machine learning gained enormously in importance within the last decade. However, without further **enhancement**s many state-of-the-art machine learning methods are not suitable for medical application. The most important reasons are insufficient data set quality and the black-box behavior of machine learning algorithms such as Deep Learning models. Consequently, end-users cannot correct the model's decisions and the corresponding explanations. The latter is crucial for the trustworthiness of machine learning in the medical domain. The research field explainable interactive machine learning searches for methods that address both shortcomings. This paper extends the explainable and interactive CAIPI algorithm and provides an interface to simplify human-in-the-loop approaches for image classification. The interface enables the end-user (1) to investigate and (2) to correct the model's prediction and explanation, and (3) to influence the data set quality. After CAIPI optimization with only a single counterexample per iteration, the model achieves an accuracy of $97.48\%$ on the Medical MNIST and $95.02\%$ on the Fashion MNIST. This accuracy is approximately equal to state-of-the-art Deep Learning optimization procedures. Besides, CAIPI reduces the labeling effort by approximately $80\%$.  
### How Do Graph Networks Generalize to Large and Diverse Molecular Systems?. (arXiv:2204.02782v1 [cs.LG])
- Authors : Johannes Gasteiger, Muhammed Shuaibi, Anuroop Sriram, Zachary Ulissi, Lawrence Zitnick, Abhishek Das
- Link : [http://arxiv.org/abs/2204.02782](http://arxiv.org/abs/2204.02782)
> ABSTRACT  :  The predominant method of demonstrating progress of atomic graph neural networks are benchmarks on small and limited datasets. The implicit hypothesis behind this approach is that progress on these narrow datasets generalize to the large diversity of chemistry. This generalizability would be very helpful for research, but currently remains untested. In this work we test this assumption by identifying four aspects of complexity in which many datasets are lacking: 1. Chemical diversity (number of different elements), 2. system size (number of atoms per sample), 3. dataset size (number of data samples), and 4. domain shift (similarity of the training and test set). We introduce multiple subsets of the large Open Catalyst 2020 (OC20) dataset to independently investigate each of these aspects. We then perform 21 ablation studies and sensitivity analyses on 9 datasets testing both previously proposed and new model **enhancement**s. We find that some improvements are consistent between datasets, but many are not and some even have opposite effects. Based on this analysis, we identify a smaller dataset that correlates well with the full OC20 dataset, and propose the GemNet-OC model, which outperforms the previous state-of-the-art on OC20 by 16%, while reducing training time by a factor of 10. Overall, our findings challenge the common belief that graph neural networks work equally well independent of dataset size and diversity, and suggest that caution must be exercised when making generalizations based on narrow datasets.  
### Semantically Adversarial Learnable Filters. (arXiv:2008.06069v3 [cs.CV] UPDATED)
- Authors : Ali Shahin, Changjae Oh, Andrea Cavallaro
- Link : [http://arxiv.org/abs/2008.06069](http://arxiv.org/abs/2008.06069)
> ABSTRACT  :  We present an adversarial framework to craft perturbations that mislead classifiers by accounting for the image content and the semantics of the labels. The proposed framework combines a structure loss and a semantic adversarial loss in a multi-task objective function to train a fully convolutional neural network. The structure loss helps generate perturbations whose type and magnitude are defined by a target image processing filter. The semantic adversarial loss considers groups of (semantic) labels to craft perturbations that prevent the filtered image {from} being classified with a label in the same group. We validate our framework with three different target filters, namely detail **enhancement**, log transformation and gamma correction filters; and evaluate the adversarially filtered images against three classifiers, ResNet50, ResNet18 and AlexNet, pre-trained on ImageNet. We show that the proposed framework generates filtered images with a high success rate, robustness, and transferability to unseen classifiers. We also discuss objective and subjective evaluations of the adversarial perturbations.  
### NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v3 [cs.CV] UPDATED)
- Authors : Jerrick Liu, Nathan Inkawhich, Oliver Nina, Radu Timofte, Sahil Jain, Bob Lee, Yuru Duan, Wei Wei, **Lei Zhang**, Songzheng Xu, Yuxuan Sun, Jiaqi Tang, Xueli Geng, Mengru Ma, Gongzhe Li, Xueli Geng, Huanqia Cai, Chengxue Cai, Sol Cummings, Casian Miron, Alexandru Pasarica, Yen Yang, Min Hsu, Jiarui Cai, Jie Mei, Ying Yeh, Neng Hwang, Michael Xin, Zhongkai Shangguan, Zihe Zheng, Xu Yifei, Lehan Yang, Kele Xu, Min Feng
- Link : [http://arxiv.org/abs/2107.01189](http://arxiv.org/abs/2107.01189)
> ABSTRACT  :  In this paper, we introduce the first Challenge on Multi-modal Aerial View Object Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at CVPR. This challenge is composed of two different tracks using EO andSAR imagery. Both EO and SAR sensors possess different advantages and drawbacks. The purpose of this competition is to analyze how to use both sets of sensory information in complementary ways. We discuss the top methods submitted for this competition and evaluate their results on our blind test set. Our challenge results show significant improvement of more than 15% accuracy from our current baselines for each track of the competition  
## cs.AI
---
### Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])
- Authors : Chaewon Kim, Jaeho Lee, Jinwoo Shin
- Link : [http://arxiv.org/abs/2204.02405](http://arxiv.org/abs/2204.02405)
> ABSTRACT  :  Recent denoising algorithms based on the "blind-spot" strategy show impressive blind image denoising performances, without utilizing any external dataset. While the methods excel in recovering highly contaminated images, we observe that such algorithms are often less effective under a low-noise or real noise regime. To address this gap, we propose an alternative denoising strategy that leverages the architectural inductive bias of **implicit neural representation**s (INRs), based on our two findings: (1) INR tends to fit the low-frequency clean image signal faster than the high-frequency noise, and (2) INR layers that are closer to the output play more critical roles in fitting higher-frequency parts. Building on these observations, we propose a denoising algorithm that maximizes the innate denoising capability of INRs by penalizing the growth of deeper layer weights. We show that our method outperforms existing zero-shot denoising methods under an extensive set of low-noise or real-noise scenarios.  
### "Does it come in black?" CLIP-like models are zero-shot recommenders. (arXiv:2204.02473v1 [cs.IR])
- Authors : Patrick John, Jacopo Tagliabue, Federico Bianchi, Ciro Greco, Diogo Goncalves
- Link : [http://arxiv.org/abs/2204.02473](http://arxiv.org/abs/2204.02473)
> ABSTRACT  :  Product discovery is a crucial component for online shopping. However, item-to-item recommendations today do not allow users to explore changes along selected dimensions: given a query item, can a model suggest something similar but in a different color? We consider item recommendations of the comparative nature (e.g. "something **dark**er") and show how CLIP-based models can support this use case in a zero-shot manner. Leveraging a large model built for fashion, we introduce GradREC and its industry potential, and offer a first rounded assessment of its strength and weaknesses.  
# Paper List
---
## cs.CV
---
**128** new papers in cs.CV:-) 
1. Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification. (arXiv:2204.02399v1 [cs.LG])
2. Explainable Deep Learning Algorithm for Distinguishing Incomplete Kawasaki Disease by Coronary Artery Lesions on Echocardiographic Imaging. (arXiv:2204.02403v1 [eess.IV])
3. Hospital-Agnostic Image Representation Learning in Digital Pathology. (arXiv:2204.02404v1 [eess.IV])
4. Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])
5. A deep learning framework for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography. (arXiv:2204.02406v1 [eess.IV])
6. Texturify: Generating Textures on 3D Shape Surfaces. (arXiv:2204.02411v1 [cs.CV])
7. CHORE: Contact, Human and Object REconstruction from a single RGB image. (arXiv:2204.02445v1 [cs.CV])
8. Detecting Cloud-Based Phishing Attacks by Combining Deep Learning Models. (arXiv:2204.02446v1 [cs.CR])
9. Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis. (arXiv:2204.02448v1 [cs.HC])
10. Federated Cross Learning for Medical Image Segmentation. (arXiv:2204.02450v1 [eess.IV])
11. Learning Optimal K-space Acquisition and Reconstruction using Physics-Informed Neural Networks. (arXiv:2204.02480v1 [eess.IV])
12. Adversarial Robustness through the Lens of Convolutional Filters. (arXiv:2204.02481v1 [cs.CV])
13. Training-Free Robust Multimodal Learning via Sample-Wise Jacobian Regularization. (arXiv:2204.02485v1 [cs.CV])
14. Text2LIVE: Text-Driven Layered Image and Video Editing. (arXiv:2204.02491v1 [cs.CV])
15. Leveraging Disentangled Representations to Improve Vision-Based Keystroke Inference Attacks Under Low Data. (arXiv:2204.02494v1 [cs.CV])
16. Depth-Guided Sparse Structure-from-Motion for Movies and TV Shows. (arXiv:2204.02509v1 [cs.CV])
17. Emphasis on the Minimization of False Negatives or False Positives in Binary Classification. (arXiv:2204.02526v1 [cs.LG])
18. Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation. (arXiv:2204.02547v1 [cs.CV])
19. Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation. (arXiv:2204.02548v1 [cs.CV])
20. RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection. (arXiv:2204.02553v1 [cs.CV])
21. MixFormer: Mixing Features across Windows and Dimensions. (arXiv:2204.02557v1 [cs.CV])
22. Gait Recognition in the Wild with Dense 3D Representations and A Benchmark. (arXiv:2204.02569v1 [cs.CV])
23. Detecting key Soccer match events to create highlights using Computer Vision. (arXiv:2204.02573v1 [cs.CV])
24. FocalClick: Towards Practical Interactive Image Segmentation. (arXiv:2204.02574v1 [cs.CV])
25. Banana Sub-Family Classification and Quality Prediction using Computer Vision. (arXiv:2204.02581v1 [cs.CV])
26. Squeeze**NeRF**: Further factorized Fast**NeRF** for memory-efficient inference. (arXiv:2204.02585v1 [cs.CV])
27. Learning to Anticipate Future with Dynamic Context Removal. (arXiv:2204.02587v1 [cs.CV])
28. Contextual Attention Mechanism, SRGAN Based Inpainting System for Eliminating Interruptions from Images. (arXiv:2204.02591v1 [cs.CV])
29. Fine-Grained Predicates Learning for Scene Graph Generation. (arXiv:2204.02597v1 [cs.CV])
30. Face recognition in a transformed domain. (arXiv:2204.02608v1 [cs.CV])
31. Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification. (arXiv:2204.02611v1 [cs.CV])
32. Towards Robust Adaptive Object Detection under Noisy Annotations. (arXiv:2204.02620v1 [cs.CV])
33. IterVM: Iterative Vision Modeling Module for Scene Text Recognition. (arXiv:2204.02630v1 [cs.CV])
34. Super-resolved multi-temporal segmentation with deep permutation-invariant networks. (arXiv:2204.02631v1 [eess.IV])
35. The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models. (arXiv:2204.02641v1 [cs.CV])
36. CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v1 [cs.LG])
37. Towards An End-to-End Framework for Flow-Guided Video Inpainting. (arXiv:2204.02663v1 [eess.IV])
38. Faster-TAD: Towards Temporal Action Detection with Proposal Generation and Classification in a Unified Network. (arXiv:2204.02674v1 [cs.CV])
39. Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition. (arXiv:2204.02675v1 [cs.CV])
40. PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model. (arXiv:2204.02681v1 [cs.CV])
41. Domain-Agnostic Prior for Transfer Semantic Segmentation. (arXiv:2204.02684v1 [cs.CV])
42. SEAL: A Large-scale Video Dataset of Multi-grained Spatio-temporally Action Localization. (arXiv:2204.02688v1 [cs.CV])
43. Aesthetic Text Logo Synthesis via Content-aware Layout Inferring. (arXiv:2204.02701v1 [cs.CV])
44. Georeferencing of Photovoltaic Modules from Aerial Infrared Videos using Structure-from-Motion. (arXiv:2204.02733v1 [cs.CV])
45. Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network. (arXiv:2204.02738v1 [cs.CV])
46. Universal Representations: A Unified Look at Multiple Task and Domain Learning. (arXiv:2204.02744v1 [cs.CV])
47. BFRnet: A deep learning-based MR background field removal method for QSM of the brain containing significant pathological susceptibility sources. (arXiv:2204.02760v1 [q-bio.QM])
48. Semi-DRDNet Semi-supervised Detail-recovery Image Deraining Network via Unpaired Contrastive Learning. (arXiv:2204.02772v1 [cs.CV])
49. 3D face reconstruction with dense landmarks. (arXiv:2204.02776v1 [cs.CV])
50. A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v1 [eess.IV])
51. Implicit Motion-Compensated Network for Unsupervised Video Object Segmentation. (arXiv:2204.02791v1 [cs.CV])
52. A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition. (arXiv:2204.02803v1 [cs.CV])
53. Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v1 [cs.CV])
54. BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy for Source-free Domain Adaptation. (arXiv:2204.02811v1 [cs.CV])
55. ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks. (arXiv:2204.02824v1 [cs.CV])
56. An Empirical Study of Remote Sensing Pretraining. (arXiv:2204.02825v1 [cs.CV])
57. CCAT-NET: A Novel Transformer Based Semi-supervised Framework for Covid-19 Lung Lesion Segmentation. (arXiv:2204.02839v1 [eess.IV])
58. Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and Development. (arXiv:2204.02842v1 [q-bio.QM])
59. Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training. (arXiv:2204.02844v1 [cs.CV])
60. KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v1 [cs.CV])
61. Influence of Color Spaces for Deep Learning Image Colorization. (arXiv:2204.02850v1 [cs.CV])
62. Retrieval-based Spatially Adaptive Normalization for Semantic Image Synthesis. (arXiv:2204.02854v1 [cs.CV])
63. Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning. (arXiv:2204.02863v1 [cs.RO])
64. ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v1 [cs.CV])
65. Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2204.02887v1 [cs.CV])
66. DBF: Dynamic Belief Fusion for Combining Multiple Object Detectors. (arXiv:2204.02890v1 [cs.CV])
67. End-to-End Instance Edge Detection. (arXiv:2204.02898v1 [cs.CV])
68. An Empirical Study of End-to-End Temporal Action Detection. (arXiv:2204.02932v1 [cs.CV])
69. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v1 [cs.LG])
70. S-R2F2U-Net: A single-stage model for teeth segmentation. (arXiv:2204.02939v1 [cs.CV])
71. Intervertebral Disc Labeling With Learning Shape Information, A Look Once Approach. (arXiv:2204.02943v1 [cs.CV])
72. "The Pedestrian next to the Lamppost" Adaptive Object Graphs for Better Instantaneous Mapping. (arXiv:2204.02944v1 [cs.CV])
73. Video Demoireing with Relation-Based Temporal Consistency. (arXiv:2204.02957v1 [cs.CV])
74. LEAD: Self-Supervised Landmark Estimation by Aligning Distributions of Feature Similarity. (arXiv:2204.02958v1 [cs.CV])
75. Simple and Effective Synthesis of Indoor 3D Scenes. (arXiv:2204.02960v1 [cs.CV])
76. SMU-Net: Style matching U-Net for brain tumor segmentation with missing modalities. (arXiv:2204.02961v1 [cs.CV])
77. Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection. (arXiv:2204.02964v1 [cs.CV])
78. LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification. (arXiv:2204.02965v1 [cs.CV])
79. Temporal Alignment Networks for Long-term Video. (arXiv:2204.02968v1 [cs.CV])
80. ME R-CNN: Multi-Expert R-CNN for Object Detection. (arXiv:1704.01069v3 [cs.CV] UPDATED)
81. Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion. (arXiv:1902.03871v5 [cs.NE] UPDATED)
82. ktrain: A Low-Code Library for Augmented Machine Learning. (arXiv:2004.10703v5 [cs.LG] UPDATED)
83. Semantically Adversarial Learnable Filters. (arXiv:2008.06069v3 [cs.CV] UPDATED)
84. Video action recognition for lane-change classification and prediction of surrounding vehicles. (arXiv:2101.05043v2 [cs.CV] UPDATED)
85. **NeRF**--: Neural Radiance Fields Without Known Camera Parameters. (arXiv:2102.07064v4 [cs.CV] UPDATED)
86. Learning Calibrated-Guidance for Object Detection in Aerial Images. (arXiv:2103.11399v4 [cs.CV] UPDATED)
87. Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification. (arXiv:2104.01546v4 [cs.CV] UPDATED)
88. Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v4 [eess.IV] UPDATED)
89. FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration. (arXiv:2106.03479v3 [cs.CV] UPDATED)
90. On the Connection between Local Attention and Dynamic Depth-wise Convolution. (arXiv:2106.04263v4 [cs.CV] UPDATED)
91. Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v2 [cs.LG] UPDATED)
92. Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v3 [cs.LG] UPDATED)
93. NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v3 [cs.CV] UPDATED)
94. Detect and Locate: Exposing Face Manipulation by Semantic- and Noise-level Telltales. (arXiv:2107.05821v2 [cs.CV] UPDATED)
95. Accelerating Video Object Segmentation with Compressed Video. (arXiv:2107.12192v3 [cs.CV] UPDATED)
96. WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. (arXiv:2111.02403v3 [eess.IV] UPDATED)
97. It's About Time: Analog Clock Reading in the Wild. (arXiv:2111.09162v4 [cs.CV] UPDATED)
98. L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v11 [cs.CV] UPDATED)
99. Neural Fields in Visual Computing and Beyond. (arXiv:2111.11426v4 [cs.CV] UPDATED)
100. Non-invasive hemodynamic analysis for aortic regurgitation using computational fluid dynamics and deep learning. (arXiv:2111.11660v2 [cs.CV] UPDATED)
101. An Image Patch is a Wave: Phase-Aware Vision MLP. (arXiv:2111.12294v5 [cs.CV] UPDATED)
102. UDA-COPE: Unsupervised Domain Adaptation for Category-level Object Pose Estimation. (arXiv:2111.12580v2 [cs.CV] UPDATED)
103. **NeRF**ReN: Neural Radiance Fields with Reflections. (arXiv:2111.15234v2 [cs.CV] UPDATED)
104. PolyWorld: Polygonal Building Extraction with Graph Neural Networks in Satellite Images. (arXiv:2111.15491v3 [cs.CV] UPDATED)
105. LAVT: Language-Aware Vision Transformer for Referring Image Segmentation. (arXiv:2112.02244v2 [cs.CV] UPDATED)
106. Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects. (arXiv:2112.11347v2 [cs.CV] UPDATED)
107. Cross Modal Retrieval with Querybank Normalisation. (arXiv:2112.12777v2 [cs.CV] UPDATED)
108. Improving Deep Image Matting via Local Smoothness Assumption. (arXiv:2112.13809v2 [cs.CV] UPDATED)
109. Multimodal registration of FISH and nanoSIMS images using convolutional neural network models. (arXiv:2201.05545v2 [cs.CV] UPDATED)
110. vCLIMB: A Novel Video Class Incremental Learning Benchmark. (arXiv:2201.09381v2 [cs.CV] UPDATED)
111. Automatic Recognition and Digital Documentation of Cultural Heritage Hemispherical Domes using Images. (arXiv:2201.10015v2 [cs.CV] UPDATED)
112. Fast Online Video Super-Resolution with Deformable Attention Pyramid. (arXiv:2202.01731v2 [eess.IV] UPDATED)
113. Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval. (arXiv:2202.06014v2 [cs.CV] UPDATED)
114. X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. (arXiv:2203.00843v3 [cs.CV] UPDATED)
115. SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters. (arXiv:2203.04746v2 [cs.CV] UPDATED)
116. Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?. (arXiv:2203.08392v2 [cs.CV] UPDATED)
117. Medium Transmission Map Matters for Learning to Restore Real-World Underwater Images. (arXiv:2203.09414v2 [cs.CV] UPDATED)
118. Expression Classification using Concatenation of Deep Neural Network for the 3rd ABAW3 Competition. (arXiv:2203.12899v2 [cs.CV] UPDATED)
119. CenterLoc3D: Monocular 3D Vehicle Localization Network for Roadside Surveillance Cameras. (arXiv:2203.14550v2 [cs.CV] UPDATED)
120. Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment. (arXiv:2203.14557v2 [cs.CV] UPDATED)
121. Classification of Hyperspectral Images Using SVM with Shape-adaptive Reconstruction and Smoothed Total Variation. (arXiv:2203.15619v2 [cs.CV] UPDATED)
122. A Unified Framework for Domain Adaptive Pose Estimation. (arXiv:2204.00172v2 [cs.CV] UPDATED)
123. MRI-based Multi-task Decoupling Learning for Alzheimer's Disease Detection and MMSE Score Prediction: A Multi-site Validation. (arXiv:2204.01708v2 [eess.IV] UPDATED)
124. On Explaining Multimodal Hateful Meme Detection Models. (arXiv:2204.01734v2 [cs.CV] UPDATED)
125. Unified Implicit Neural Stylization. (arXiv:2204.01943v2 [cs.CV] UPDATED)
126. Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation. (arXiv:2204.01971v2 [cs.CV] UPDATED)
127. Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition. (arXiv:2204.02148v2 [cs.CV] UPDATED)
128. CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV] CROSS LISTED)
## eess.IV
---
**22** new papers in eess.IV:-) 
1. Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification. (arXiv:2204.02399v1 [cs.LG])
2. Explainable Deep Learning Algorithm for Distinguishing Incomplete Kawasaki Disease by Coronary Artery Lesions on Echocardiographic Imaging. (arXiv:2204.02403v1 [eess.IV])
3. Hospital-Agnostic Image Representation Learning in Digital Pathology. (arXiv:2204.02404v1 [eess.IV])
4. Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])
5. A deep learning framework for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography. (arXiv:2204.02406v1 [eess.IV])
6. Imaging Conductivity from Current Density Magnitude using Neural Networks. (arXiv:2204.02441v1 [math.NA])
7. Federated Cross Learning for Medical Image Segmentation. (arXiv:2204.02450v1 [eess.IV])
8. PDE-constrained shape registration to characterize biological growth and morphogenesis from imaging data. (arXiv:2204.02451v1 [physics.bio-ph])
9. Learning Optimal K-space Acquisition and Reconstruction using Physics-Informed Neural Networks. (arXiv:2204.02480v1 [eess.IV])
10. Super-resolved multi-temporal segmentation with deep permutation-invariant networks. (arXiv:2204.02631v1 [eess.IV])
11. CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v1 [cs.LG])
12. Towards An End-to-End Framework for Flow-Guided Video Inpainting. (arXiv:2204.02663v1 [eess.IV])
13. BFRnet: A deep learning-based MR background field removal method for QSM of the brain containing significant pathological susceptibility sources. (arXiv:2204.02760v1 [q-bio.QM])
14. A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v1 [eess.IV])
15. CCAT-NET: A Novel Transformer Based Semi-supervised Framework for Covid-19 Lung Lesion Segmentation. (arXiv:2204.02839v1 [eess.IV])
16. Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training. (arXiv:2204.02844v1 [cs.CV])
17. Statistical Dependencies Beyond Linear Correlations in Light Scattered by Disordered Media. (arXiv:2011.08336v2 [physics.optics] UPDATED)
18. Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v4 [eess.IV] UPDATED)
19. WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. (arXiv:2111.02403v3 [eess.IV] UPDATED)
20. Fast Online Video Super-Resolution with Deformable Attention Pyramid. (arXiv:2202.01731v2 [eess.IV] UPDATED)
21. Expression Classification using Concatenation of Deep Neural Network for the 3rd ABAW3 Competition. (arXiv:2203.12899v2 [cs.CV] UPDATED)
22. MRI-based Multi-task Decoupling Learning for Alzheimer's Disease Detection and MMSE Score Prediction: A Multi-site Validation. (arXiv:2204.01708v2 [eess.IV] UPDATED)
## cs.LG
---
**138** new papers in cs.LG:-) 
1. Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification. (arXiv:2204.02399v1 [cs.LG])
2. Comment on "Black Box Prediction Methods in Sports Medicine Deserve a Red Card for Reckless Practice: A Change of Tactics is Needed to Advance Athlete Care". (arXiv:2204.02402v1 [cs.LG])
3. Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])
4. OccamNets: Mitigating Dataset Bias by Favoring Simpler Hypotheses. (arXiv:2204.02426v1 [cs.LG])
5. Imaging Conductivity from Current Density Magnitude using Neural Networks. (arXiv:2204.02441v1 [math.NA])
6. Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis. (arXiv:2204.02448v1 [cs.HC])
7. Improving Voice Trigger Detection with Metric Learning. (arXiv:2204.02455v1 [cs.SD])
8. Configuration Path Control. (arXiv:2204.02471v1 [cs.RO])
9. Generative Enriched Sequential Learning (ESL) Approach for Molecular Design via Augmented Domain Knowledge. (arXiv:2204.02474v1 [q-bio.BM])
10. Adversarial Robustness through the Lens of Convolutional Filters. (arXiv:2204.02481v1 [cs.CV])
11. Training-Free Robust Multimodal Learning via Sample-Wise Jacobian Regularization. (arXiv:2204.02485v1 [cs.CV])
12. Discovering and forecasting extreme events via active learning in neural operators. (arXiv:2204.02488v1 [cs.LG])
13. Pareto-optimal clustering with the primal deterministic information bottleneck. (arXiv:2204.02489v1 [cs.LG])
14. Privacy-Preserving Federated Learning via System Immersion and Random Matrix Encryption. (arXiv:2204.02497v1 [cs.LG])
15. Deep Graphic FBSDEs for Opinion Dynamics Stochastic Control. (arXiv:2204.02506v1 [cs.MA])
16. In-Pocket 3D Graphs Enhance Ligand-Target Compatibility in Generative Small-Molecule Creation. (arXiv:2204.02513v1 [q-bio.BM])
17. Service resource allocation problem in the IoT driven personalized healthcare information platform. (arXiv:2204.02521v1 [cs.LG])
18. Emphasis on the Minimization of False Negatives or False Positives in Binary Classification. (arXiv:2204.02526v1 [cs.LG])
19. Prosodic Alignment for off-screen automatic dubbing. (arXiv:2204.02530v1 [cs.CL])
20. Continuous LWE is as Hard as LWE & Applications to Learning Gaussian Mixtures. (arXiv:2204.02550v1 [cs.CR])
21. DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning. (arXiv:2204.02558v1 [cs.AI])
22. FairNeuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons. (arXiv:2204.02567v1 [cs.LG])
23. Optimal Sublinear Sampling of Spanning Trees and Determinantal Point Processes via Average-Case Entropic Independence. (arXiv:2204.02570v1 [cs.DS])
24. Greedier is Better: Selecting Multiple Neighbors per Iteration for Sparse Subspace Clustering. (arXiv:2204.02572v1 [cs.LG])
25. PAGP: A physics-assisted Gaussian process framework with active learning for forward and inverse problems of partial differential equations. (arXiv:2204.02583v1 [stat.ML])
26. Nonlinear gradient mappings and stochastic optimization: A general framework with applications to heavy-tail noise. (arXiv:2204.02593v1 [math.OC])
27. Consensual Aggregation on Random Projected High-dimensional Features for Regression. (arXiv:2204.02606v1 [stat.ML])
28. Efficient Test-Time Model Adaptation without Forgetting. (arXiv:2204.02610v1 [cs.LG])
29. Data-Driven Approach for Log Instruction Quality Assessment. (arXiv:2204.02618v1 [cs.SE])
30. Attention-based CNN-LSTM and XGBoost hybrid model for stock prediction. (arXiv:2204.02623v1 [q-fin.ST])
31. Bridging the Gap of AutoGraph between Academia and Industry: Analysing AutoGraph Challenge at KDD Cup 2020. (arXiv:2204.02625v1 [cs.LG])
32. Federated Reinforcement Learning with Environment Heterogeneity. (arXiv:2204.02634v1 [cs.LG])
33. Failure Identification from Unstable Log Data using Deep Learning. (arXiv:2204.02636v1 [cs.SE])
34. Spatio-Temporal Dynamic Graph Relation Learning for Urban Metro Flow Prediction. (arXiv:2204.02650v1 [cs.LG])
35. CHIEF: Clustering with Higher-order Motifs in Big Networks. (arXiv:2204.02656v1 [cs.SI])
36. CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v1 [cs.LG])
37. Accelerating Backward Aggregation in GCN Training with Execution Path Preparing on GPUs. (arXiv:2204.02662v1 [cs.LG])
38. Double Descent in Random Feature Models: Precise Asymptotic Analysis for General Convex Regularization. (arXiv:2204.02678v1 [stat.ML])
39. Beyond Separability: Analyzing the Linear Transferability of Contrastive Representations to Related Subpopulations. (arXiv:2204.02683v1 [cs.LG])
40. Learning to Adapt Clinical Sequences with Residual Mixture of Experts. (arXiv:2204.02687v1 [cs.LG])
41. Customizable End-to-end Optimization of Online Neural Network-supported Dereverberation for Hearing Devices. (arXiv:2204.02694v1 [eess.AS])
42. VNIbCReg: VIbCReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series. (arXiv:2204.02697v1 [cs.LG])
43. Fundamental limits to learning closed-form mathematical models from data. (arXiv:2204.02704v1 [cs.LG])
44. Distilling Robust and Non-Robust Features in Adversarial Examples by Information Bottleneck. (arXiv:2204.02735v1 [cs.LG])
45. Neural Network-augmented Kalman Filtering for Robust Online Speech Dereverberation in Noisy Reverberant Environments. (arXiv:2204.02741v1 [eess.AS])
46. Data-Centric Green AI: An Exploratory Empirical Study. (arXiv:2204.02766v1 [cs.LG])
47. Walk this Way! Entity Walks and Property Walks for RDF2vec. (arXiv:2204.02777v1 [cs.LG])
48. A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v1 [eess.IV])
49. How Do Graph Networks Generalize to Large and Diverse Molecular Systems?. (arXiv:2204.02782v1 [cs.LG])
50. Reinforcement Learning Agents in Colonel Blotto. (arXiv:2204.02785v1 [cs.AI])
51. Classification of NEQR Processed Classical Images using Quantum Neural Networks (QNN). (arXiv:2204.02797v1 [quant-ph])
52. Dimensionality Expansion and Transfer Learning for Next Generation Energy Management Systems. (arXiv:2204.02802v1 [cs.LG])
53. A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition. (arXiv:2204.02803v1 [cs.CV])
54. Federated Self-supervised Speech Representations: Are We There Yet?. (arXiv:2204.02804v1 [cs.SD])
55. High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize. (arXiv:2204.02833v1 [math.OC])
56. KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v1 [cs.CV])
57. PAnDR: Fast Adaptation to New Environments from Offline Experiences via Decoupling Policy and Environment Representations. (arXiv:2204.02877v1 [cs.LG])
58. Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks. (arXiv:2204.02892v1 [cs.CL])
59. Efficient Bayesian Network Structure Learning via Parameterized Local Search on Topological Orderings. (arXiv:2204.02902v1 [cs.DS])
60. Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask. (arXiv:2204.02908v1 [cs.CL])
61. A survey on recently proposed activation functions for Deep Learning. (arXiv:2204.02921v1 [cs.LG])
62. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v1 [cs.LG])
63. Marrying Fairness and Explainability in Supervised Learning. (arXiv:2204.02947v1 [cs.LG])
64. Guaranteed Bounds for Posterior Inference in Universal Probabilistic Programming. (arXiv:2204.02948v1 [cs.PL])
65. Simple and Effective Synthesis of Indoor 3D Scenes. (arXiv:2204.02960v1 [cs.CV])
66. LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification. (arXiv:2204.02965v1 [cs.CV])
67. Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion. (arXiv:1902.03871v5 [cs.NE] UPDATED)
68. Model Pruning Enables Efficient Federated Learning on Edge Devices. (arXiv:1909.12326v5 [cs.LG] UPDATED)
69. Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction. (arXiv:1911.09419v3 [cs.LG] UPDATED)
70. CATVI: Conditional and Adaptively Truncated Variational Inference for Hierarchical Bayesian Nonparametric Models. (arXiv:2001.04508v2 [stat.ML] UPDATED)
71. ktrain: A Low-Code Library for Augmented Machine Learning. (arXiv:2004.10703v5 [cs.LG] UPDATED)
72. Spectral Ranking with Covariates. (arXiv:2005.04035v3 [stat.ML] UPDATED)
73. Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing. (arXiv:2006.09365v5 [cs.LG] UPDATED)
74. Semantically Adversarial Learnable Filters. (arXiv:2008.06069v3 [cs.CV] UPDATED)
75. Fair and Useful Cohort Selection. (arXiv:2009.02207v2 [cs.DS] UPDATED)
76. Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism. (arXiv:2009.04544v4 [cs.LG] UPDATED)
77. MLPerf Mobile Inference Benchmark. (arXiv:2012.02328v4 [cs.LG] UPDATED)
78. Deep reinforcement learning for portfolio management. (arXiv:2012.13773v7 [q-fin.CP] UPDATED)
79. Tactical Optimism and Pessimism for Deep Reinforcement Learning. (arXiv:2102.03765v5 [cs.LG] UPDATED)
80. Decision Making in Monopoly using a Hybrid Deep Reinforcement Learning Approach. (arXiv:2103.00683v4 [cs.LG] UPDATED)
81. The Shapley Value of coalition of variables provides better explanations. (arXiv:2103.13342v3 [stat.ML] UPDATED)
82. Fast Design Space Exploration of Nonlinear Systems: Part I. (arXiv:2104.01747v6 [cs.LG] UPDATED)
83. Emotional Contagion-Aware Deep Reinforcement Learning for Antagonistic Crowd Simulation. (arXiv:2105.00854v2 [cs.LG] UPDATED)
84. SimNet: Accurate and High-Performance Computer Architecture Simulation using Deep Learning. (arXiv:2105.05821v3 [cs.AR] UPDATED)
85. DMInet: An Accurate and Highly Flexible Deep Learning Framework for Drug Membrane Interaction with Membrane Selectivity. (arXiv:2105.13928v2 [physics.bio-ph] UPDATED)
86. Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v6 [cs.LG] UPDATED)
87. Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (arXiv:2106.05087v4 [cs.LG] UPDATED)
88. Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v2 [cs.LG] UPDATED)
89. Exploring Robust Architectures for Deep Artificial Neural Networks. (arXiv:2106.15850v2 [cs.LG] UPDATED)
90. Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v3 [cs.LG] UPDATED)
91. NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v3 [cs.CV] UPDATED)
92. Relational graph convolutional networks for predicting blood-brain barrier penetration of drug molecules. (arXiv:2107.06773v2 [q-bio.QM] UPDATED)
93. Learning Linearized Assignment Flows for Image Labeling. (arXiv:2108.02571v2 [cs.LG] UPDATED)
94. FedZKT: Zero-Shot Knowledge Transfer towards Resource-Constrained Federated Learning with Heterogeneous On-Device Models. (arXiv:2109.03775v2 [cs.LG] UPDATED)
95. The Grammar-Learning Trajectories of Neural Language Models. (arXiv:2109.06096v3 [cs.CL] UPDATED)
96. Adversarially Regularized Policy Learning Guided by Trajectory Optimization. (arXiv:2109.07627v3 [cs.RO] UPDATED)
97. Lifelong Robotic Reinforcement Learning by Retaining Experiences. (arXiv:2109.09180v2 [cs.LG] UPDATED)
98. Contrastive Learning of Subject-Invariant EEG Representations for Cross-Subject Emotion Recognition. (arXiv:2109.09559v2 [cs.HC] UPDATED)
99. Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding. (arXiv:2110.14170v2 [cs.LG] UPDATED)
100. Convergence and Stability of the Stochastic Proximal Point Algorithm with Momentum. (arXiv:2111.06171v4 [math.OC] UPDATED)
101. Federated Learning for Internet of Things: Applications, Challenges, and Opportunities. (arXiv:2111.07494v4 [cs.LG] UPDATED)
102. It's About Time: Analog Clock Reading in the Wild. (arXiv:2111.09162v4 [cs.CV] UPDATED)
103. L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v11 [cs.CV] UPDATED)
104. Neural Fields in Visual Computing and Beyond. (arXiv:2111.11426v4 [cs.CV] UPDATED)
105. Explainable Natural Language Processing with Matrix Product States. (arXiv:2112.08628v2 [cond-mat.dis-nn] UPDATED)
106. Agent Smith: Teaching Question Answering to Jill Watson. (arXiv:2112.13677v2 [cs.LG] UPDATED)
107. A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level. (arXiv:2112.15594v3 [cs.LG] UPDATED)
108. AttentionLight: Rethinking queue length and attention mechanism for traffic signal control. (arXiv:2201.00006v2 [cs.LG] UPDATED)
109. Transfer RL across Observation Feature Spaces via Model-Based Regularization. (arXiv:2201.00248v3 [cs.LG] UPDATED)
110. Challenges in Migrating Imperative Deep Learning Programs to Graph Execution: An Empirical Study. (arXiv:2201.09953v3 [cs.SE] UPDATED)
111. Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v4 [cs.CL] UPDATED)
112. Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning. (arXiv:2201.13425v3 [cs.LG] UPDATED)
113. Importance Weighting Approach in Kernel Bayes' Rule. (arXiv:2202.02474v2 [stat.ML] UPDATED)
114. On the complexity of All $\varepsilon$-Best Arms Identification. (arXiv:2202.06280v2 [stat.ML] UPDATED)
115. Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning with Provable Convergence. (arXiv:2202.12183v2 [cs.LG] UPDATED)
116. SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters. (arXiv:2203.04746v2 [cs.CV] UPDATED)
117. FedSyn: Synthetic Data Generation using Federated Learning. (arXiv:2203.05931v2 [stat.ML] UPDATED)
118. The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning. (arXiv:2203.06498v4 [cs.LG] UPDATED)
119. Adaptive Model Predictive Control by Learning Classifiers. (arXiv:2203.06783v2 [cs.RO] UPDATED)
120. On Exploiting Layerwise Gradient Statistics for Effective Training of Deep Neural Networks. (arXiv:2203.13273v2 [cs.LG] UPDATED)
121. Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey. (arXiv:2203.14009v2 [cs.LG] UPDATED)
122. On the Neural Tangent Kernel Analysis of Randomly Pruned Wide Neural Networks. (arXiv:2203.14328v2 [cs.LG] UPDATED)
123. Deep Learning and Artificial General Intelligence: Still a Long Way to Go. (arXiv:2203.14963v2 [cs.LG] UPDATED)
124. Improving the Learnability of Machine Learning APIs by Semi-Automated API Wrapping. (arXiv:2203.15491v2 [cs.SE] UPDATED)
125. Hypergraph Convolutional Networks via Equivalency between Hypergraphs and Undirected Graphs. (arXiv:2203.16939v2 [cs.LG] UPDATED)
126. Quantum-Aided Meta-Learning for Bayesian Binary Neural Networks via Born Machines. (arXiv:2203.17089v2 [quant-ph] UPDATED)
127. A Unified Framework for Domain Adaptive Pose Estimation. (arXiv:2204.00172v2 [cs.CV] UPDATED)
128. Deep learning, stochastic gradient descent and diffusion maps. (arXiv:2204.01365v2 [stat.ML] UPDATED)
129. Langevin Diffusion: An Almost Universal Algorithm for Private Euclidean (Convex) Optimization. (arXiv:2204.01585v2 [cs.LG] UPDATED)
130. Deep-Ensemble-Based Uncertainty Quantification in Spatiotemporal Graph Neural Networks for Traffic Forecasting. (arXiv:2204.01618v2 [cs.LG] UPDATED)
131. A high-order tensor completion algorithm based on Fully-Connected Tensor Network weighted optimization. (arXiv:2204.01732v2 [cs.LG] UPDATED)
132. Automating Reinforcement Learning with Example-based Resets. (arXiv:2204.02041v2 [cs.LG] UPDATED)
133. GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes. (arXiv:2204.02112v2 [stat.ME] UPDATED)
134. SemanticCAP: Chromatin Accessibility Prediction Enhanced by Features Learning from a Language Model. (arXiv:2204.02130v2 [q-bio.GN] UPDATED)
135. Hybrid Predictive Coding: Inferring, Fast and Slow. (arXiv:2204.02169v2 [q-bio.NC] UPDATED)
136. Penalised FTRL With Time-Varying Constraints. (arXiv:2204.02197v2 [cs.LG] UPDATED)
137. Test Against High-Dimensional Uncertainties: Accelerated Evaluation of Autonomous Vehicles with Deep Importance Sampling. (arXiv:2204.02351v2 [cs.LG] UPDATED)
138. CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV] CROSS LISTED)
## cs.AI
---
**72** new papers in cs.AI:-) 
1. Hospital-Agnostic Image Representation Learning in Digital Pathology. (arXiv:2204.02404v1 [eess.IV])
2. Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])
3. A deep learning framework for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography. (arXiv:2204.02406v1 [eess.IV])
4. AAAI SSS-22 Symposium on Closing the Assessment Loop: Communicating Proficiency and Intent in Human-Robot Teaming. (arXiv:2204.02437v1 [cs.AI])
5. Detecting Cloud-Based Phishing Attacks by Combining Deep Learning Models. (arXiv:2204.02446v1 [cs.CR])
6. Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis. (arXiv:2204.02448v1 [cs.HC])
7. BeeTS: Smart Distributed Sensor Tuple Spaces combined with Agents using Bluetooth and IP Broadcasting. (arXiv:2204.02464v1 [cs.NI])
8. "Does it come in black?" CLIP-like models are zero-shot recommenders. (arXiv:2204.02473v1 [cs.IR])
9. Adversarial Robustness through the Lens of Convolutional Filters. (arXiv:2204.02481v1 [cs.CV])
10. Efficient Pragmatic Program Synthesis with Informative Specifications. (arXiv:2204.02495v1 [cs.AI])
11. Inferring Rewards from Language in Context. (arXiv:2204.02515v1 [cs.CL])
12. Improving Zero-Shot Event Extraction via Sentence Simplification. (arXiv:2204.02531v1 [cs.CL])
13. DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning. (arXiv:2204.02558v1 [cs.AI])
14. FairNeuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons. (arXiv:2204.02567v1 [cs.LG])
15. Detecting key Soccer match events to create highlights using Computer Vision. (arXiv:2204.02573v1 [cs.CV])
16. Banana Sub-Family Classification and Quality Prediction using Computer Vision. (arXiv:2204.02581v1 [cs.CV])
17. Contextual Attention Mechanism, SRGAN Based Inpainting System for Eliminating Interruptions from Images. (arXiv:2204.02591v1 [cs.CV])
18. Distributed Transition Systems with Tags for Privacy Analysis. (arXiv:2204.02602v1 [cs.CL])
19. Bridging the Gap of AutoGraph between Academia and Industry: Analysing AutoGraph Challenge at KDD Cup 2020. (arXiv:2204.02625v1 [cs.LG])
20. A Weakly Supervised Propagation Model for Rumor Verification and Stance Detection with Multiple Instance Learning. (arXiv:2204.02626v1 [cs.CL])
21. DeFTA: A Plug-and-Play Decentralized Replacement for FedAvg. (arXiv:2204.02632v1 [cs.DC])
22. DAGAM: Data Augmentation with Generation And Modification. (arXiv:2204.02633v1 [cs.CL])
23. Spatio-Temporal Dynamic Graph Relation Learning for Urban Metro Flow Prediction. (arXiv:2204.02650v1 [cs.LG])
24. Disentangling the Computational Complexity of Network Untangling. (arXiv:2204.02668v1 [cs.DS])
25. PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model. (arXiv:2204.02681v1 [cs.CV])
26. Language Model for Text Analytic in Cybersecurity. (arXiv:2204.02685v1 [cs.CL])
27. Learning to Adapt Clinical Sequences with Residual Mixture of Experts. (arXiv:2204.02687v1 [cs.LG])
28. Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings. (arXiv:2204.02710v1 [cs.CL])
29. Adversarial Learning to Reason in an Arbitrary Logic. (arXiv:2204.02737v1 [cs.AI])
30. Data-Centric Green AI: An Exploratory Empirical Study. (arXiv:2204.02766v1 [cs.LG])
31. Walk this Way! Entity Walks and Property Walks for RDF2vec. (arXiv:2204.02777v1 [cs.LG])
32. A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v1 [eess.IV])
33. Reinforcement Learning Agents in Colonel Blotto. (arXiv:2204.02785v1 [cs.AI])
34. A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition. (arXiv:2204.02803v1 [cs.CV])
35. KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v1 [cs.CV])
36. Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning. (arXiv:2204.02863v1 [cs.RO])
37. ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v1 [cs.CV])
38. PAnDR: Fast Adaptation to New Environments from Offline Experiences via Decoupling Policy and Environment Representations. (arXiv:2204.02877v1 [cs.LG])
39. A Cognitive Framework for Delegation Between Error-Prone AI and Human Agents. (arXiv:2204.02889v1 [cs.HC])
40. Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask. (arXiv:2204.02908v1 [cs.CL])
41. A survey on recently proposed activation functions for Deep Learning. (arXiv:2204.02921v1 [cs.LG])
42. Beam Search: Faster and Monotonic. (arXiv:2204.02929v1 [cs.AI])
43. Simple and Effective Synthesis of Indoor 3D Scenes. (arXiv:2204.02960v1 [cs.CV])
44. Notes on Abstract Argumentation Theory. (arXiv:1806.07709v4 [cs.AI] UPDATED)
45. Video action recognition for lane-change classification and prediction of surrounding vehicles. (arXiv:2101.05043v2 [cs.CV] UPDATED)
46. Decision Making in Monopoly using a Hybrid Deep Reinforcement Learning Approach. (arXiv:2103.00683v4 [cs.LG] UPDATED)
47. Fast Design Space Exploration of Nonlinear Systems: Part I. (arXiv:2104.01747v6 [cs.LG] UPDATED)
48. Exploring Robust Architectures for Deep Artificial Neural Networks. (arXiv:2106.15850v2 [cs.LG] UPDATED)
49. Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v3 [cs.LG] UPDATED)
50. Playtesting: What is Beyond Personas. (arXiv:2107.11965v2 [cs.AI] UPDATED)
51. The Grammar-Learning Trajectories of Neural Language Models. (arXiv:2109.06096v3 [cs.CL] UPDATED)
52. Lifelong Robotic Reinforcement Learning by Retaining Experiences. (arXiv:2109.09180v2 [cs.LG] UPDATED)
53. Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding. (arXiv:2110.14170v2 [cs.LG] UPDATED)
54. Agent Smith: Teaching Question Answering to Jill Watson. (arXiv:2112.13677v2 [cs.LG] UPDATED)
55. A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level. (arXiv:2112.15594v3 [cs.LG] UPDATED)
56. AttentionLight: Rethinking queue length and attention mechanism for traffic signal control. (arXiv:2201.00006v2 [cs.LG] UPDATED)
57. Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v2 [cs.CL] UPDATED)
58. Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning. (arXiv:2201.13425v3 [cs.LG] UPDATED)
59. Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning with Provable Convergence. (arXiv:2202.12183v2 [cs.LG] UPDATED)
60. Distributed Control using Reinforcement Learning with Temporal-Logic-Based Reward Shaping. (arXiv:2203.04172v2 [cs.AI] UPDATED)
61. SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters. (arXiv:2203.04746v2 [cs.CV] UPDATED)
62. Adaptive Model Predictive Control by Learning Classifiers. (arXiv:2203.06783v2 [cs.RO] UPDATED)
63. Do DNNs trained on Natural Images organize visual features into Gestalts?. (arXiv:2203.07302v2 [cs.AI] UPDATED)
64. ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps. (arXiv:2203.09127v2 [cs.CL] UPDATED)
65. Self-Consistency Improves Chain of Thought Reasoning in Language Models. (arXiv:2203.11171v2 [cs.CL] UPDATED)
66. Duality-Induced Regularizer for Semantic Matching Knowledge Graph Embeddings. (arXiv:2203.12949v2 [cs.CL] UPDATED)
67. CenterLoc3D: Monocular 3D Vehicle Localization Network for Roadside Surveillance Cameras. (arXiv:2203.14550v2 [cs.CV] UPDATED)
68. Deep Learning and Artificial General Intelligence: Still a Long Way to Go. (arXiv:2203.14963v2 [cs.LG] UPDATED)
69. Domain Knowledge Driven Pseudo Labels for Interpretable Goal-Conditioned Interactive Trajectory Prediction. (arXiv:2203.15112v2 [cs.RO] UPDATED)
70. Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation. (arXiv:2204.01971v2 [cs.CV] UPDATED)
71. Hybrid Predictive Coding: Inferring, Fast and Slow. (arXiv:2204.02169v2 [q-bio.NC] UPDATED)
72. CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV] CROSS LISTED)

