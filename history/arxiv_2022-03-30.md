# Your interest papers
---
## cs.CV
---
### New pyramidal hybrid textural and deep features based automatic skin cancer classification model: Ensemble **Dark**Net and textural feature extractor. (arXiv:2203.15090v1 [cs.CV])
- Authors : Mehmet Baygin, Turker Tuncer, Sengul Dogan
- Link : [http://arxiv.org/abs/2203.15090](http://arxiv.org/abs/2203.15090)
> ABSTRACT  :  Background: Skin cancer is one of the widely seen cancer worldwide and automatic classification of skin cancer can be benefited dermatology clinics for an accurate diagnosis. Hence, a machine learning-based automatic skin cancer detection model must be developed. Material and Method: This research interests to overcome automatic skin cancer detection problem. A colored skin cancer image dataset is used. This dataset contains 3297 images with two classes. An automatic multilevel textural and deep features-based model is presented. Multilevel fuse feature generation using discrete wavelet transform (DWT), local phase quantization (LPQ), local binary pattern (LBP), pre-trained **Dark**Net19, and **Dark**Net53 are utilized to generate features of the skin cancer images, top 1000 features are selected threshold value-based neighborhood component analysis (NCA). The chosen top 1000 features are classified using the 10-fold cross-validation technique. Results: To obtain results, ten-fold cross-validation is used and 91.54% classification accuracy results are obtained by using the recommended pyramidal hybrid feature generator and NCA selector-based model. Further, various training and testing separation ratios (90:10, 80:20, 70:30, 60:40, 50:50) are used and the maximum classification rate is calculated as 95.74% using the 90:10 separation ratio. Conclusions: The findings and accuracies calculated are denoted that this model can be used in dermatology and pathology clinics to simplify the skin cancer detection process and help physicians.  
### Rethinking Semantic Segmentation: A Prototype View. (arXiv:2203.15102v1 [cs.CV])
- Authors : Tianfei Zhou, Wenguan Wang, Ender Konukoglu, Luc Van
- Link : [http://arxiv.org/abs/2203.15102](http://arxiv.org/abs/2203.15102)
> ABSTRACT  :  Prevalent semantic segmentation solutions, despite their different network designs (FCN based or attention based) and mask decoding strategies (parametric softmax based or pixel-query based), can be placed in one category, by considering the softmax weights or query vectors as learnable class prototypes. In light of this prototype view, this study uncovers several limitations of such parametric segmentation regime, and proposes a nonparametric alternative based on non-learnable prototypes. Instead of prior methods learning a single weight/query vector for each class in a fully parametric manner, our model represents each class as a set of non-learnable prototypes, relying solely on the mean features of several training pixels within that class. The dense prediction is thus achieved by nonparametric nearest prototype retrieving. This allows our model to directly shape the pixel embedding space, by optimizing the arrangement between embedded pixels and anchored prototypes. It is able to handle arbitrary number of classes with a constant amount of learnable parameters. We empirically show that, with FCN based and attention based segmentation models (i.e., HRNet, **Swin**, SegFormer) and backbones (i.e., ResNet, HRNet, **Swin**, MiT), our nonparametric framework yields compelling results over several datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in the large-vocabulary situation. We expect this work will provoke a rethink of the current de facto semantic segmentation model design.  
### Coarse to Fine: Image **Restoration** Boosted by Multi-Scale Low-Rank Tensor Completion. (arXiv:2203.15189v1 [cs.CV])
- Authors : Rui Lin, Cong Chen, Ngai Wong
- Link : [http://arxiv.org/abs/2203.15189](http://arxiv.org/abs/2203.15189)
> ABSTRACT  :  Existing low-rank tensor completion (LRTC) approaches aim at restoring a partially observed tensor by imposing a global low-rank constraint on the underlying completed tensor. However, such a global rank assumption suffers the trade-off between restoring the originally details-lacking parts and neglecting the potentially complex objects, making the completion performance unsatisfactory on both sides. To address this problem, we propose a novel and practical strategy for image **restoration** that restores the partially observed tensor in a coarse-to-fine (C2F) manner, which gets rid of such trade-off by searching proper local ranks for both low- and high-rank parts. Extensive experiments are conducted to demonstrate the superiority of the proposed C2F scheme. The codes are available at: https://github.com/RuiLin0212/C2FLRTC.  
### Panoptic **NeRF**: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation. (arXiv:2203.15224v1 [cs.CV])
- Authors : Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, Lanyun Zhu, Xiaowei Zhou, Andreas Geiger, Yiyi Liao
- Link : [http://arxiv.org/abs/2203.15224](http://arxiv.org/abs/2203.15224)
> ABSTRACT  :  Large-scale training data with high-quality annotations is critical for training semantic and instance segmentation models. Unfortunately, pixel-wise annotation is labor-intensive and costly, raising the demand for more efficient labeling strategies. In this work, we present a novel 3D-to-2D label transfer method, Panoptic **NeRF**, which aims for obtaining per-pixel 2D semantic and instance labels from easy-to-obtain coarse 3D bounding primitives. Our method utilizes **NeRF** as a differentiable tool to unify coarse 3D annotations and 2D semantic cues transferred from existing datasets. We demonstrate that this combination allows for improved geometry guided by semantic information, enabling rendering of accurate semantic maps across multiple views. Furthermore, this fusion process resolves label ambiguity of the coarse 3D annotations and filters noise in the 2D predictions. By inferring in 3D space and rendering to 2D labels, our 2D semantic and instance labels are multi-view consistent by design. Experimental results show that Panoptic **NeRF** outperforms existing semantic and instance label transfer methods in terms of accuracy and multi-view consistency on challenging urban scenes of the KITTI-360 dataset.  
### Harmonizing Pathological and Normal Pixels for Pseudo-healthy Synthesis. (arXiv:2203.15347v1 [eess.IV])
- Authors : Yunlong Zhang, Xin Lin, Yihong Zhuang, Yue Huang, Xinghao Ding, Guisheng Wang, Lin Yang, Yizhou Yu
- Link : [http://arxiv.org/abs/2203.15347](http://arxiv.org/abs/2203.15347)
> ABSTRACT  :  Synthesizing a subject-specific pathology-free image from a pathological image is valuable for algorithm development and clinical practice. In recent years, several approaches based on the Generative Adversarial Network (GAN) have achieved promising results in pseudo-healthy synthesis. However, the discriminator (i.e., a classifier) in the GAN cannot accurately identify lesions and further hampers from generating admirable pseudo-healthy images. To address this problem, we present a new type of discriminator, the segmentor, to accurately locate the lesions and improve the visual quality of pseudo-healthy images. Then, we apply the generated images into medical image **enhancement** and utilize the enhanced results to cope with the low contrast problem existing in medical image segmentation. Furthermore, a reliable metric is proposed by utilizing two attributes of label noise to measure the health of synthetic images. Comprehensive experiments on the T2 modality of BraTS demonstrate that the proposed method substantially outperforms the state-of-the-art methods. The method achieves better performance than the existing methods with only 30\% of the training data. The effectiveness of the proposed method is also demonstrated on the LiTS and the T1 modality of BraTS. The code and the pre-trained model of this study are publicly available at https://github.com/Au3C2/Generator-Versus-Segmentor.  
### End-to-End Transformer Based Model for Image Captioning. (arXiv:2203.15350v1 [cs.CV])
- Authors : Yiyu Wang, Jungang Xu, Yingfei Sun
- Link : [http://arxiv.org/abs/2203.15350](http://arxiv.org/abs/2203.15350)
> ABSTRACT  :  CNN-LSTM based architectures have played an important role in image captioning, but limited by the training efficiency and expression ability, researchers began to explore the CNN-Transformer based models and achieved great success. Meanwhile, almost all recent works adopt Faster R-CNN as the backbone encoder to extract region-level features from given images. However, Faster R-CNN needs a pre-training on an additional dataset, which divides the image captioning task into two stages and limits its potential applications. In this paper, we build a pure Transformer-based model, which integrates image captioning into one stage and realizes end-to-end training. Firstly, we adopt **Swin**Transformer to replace Faster R-CNN as the backbone encoder to extract grid-level features from given images; Then, referring to Transformer, we build a refining encoder and a decoder. The refining encoder refines the grid features by capturing the intra-relationship between them, and the decoder decodes the refined features into captions word by word. Furthermore, in order to increase the interaction between multi-modal (vision and language) features to enhance the modeling capability, we calculate the mean pooling of grid features as the global feature, then introduce it into refining encoder to refine with grid features together, and add a pre-fusion process of refined global feature and generated words in decoder. To validate the effectiveness of our proposed model, we conduct experiments on MSCOCO dataset. The experimental results compared to existing published works demonstrate that our model achieves new state-of-the-art performances of 138.2% (single model) and 141.0% (ensemble of 4 models) CIDEr scores on `Karpathy' offline test split and 136.0% (c5) and 138.3% (c40) CIDEr scores on the official online test server. Trained models and source code will be released.  
### Nested Collaborative Learning for Long-Tailed Visual Recognition. (arXiv:2203.15359v1 [cs.CV])
- Authors : Jun Li, Zichang Tan, Jun Wan, Zhen Lei, Guodong Guo
- Link : [http://arxiv.org/abs/2203.15359](http://arxiv.org/abs/2203.15359)
> ABSTRACT  :  The networks trained on the long-tailed dataset vary remarkably, despite the same training settings, which shows the great uncertainty in long-tailed learning. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL), which tackles the problem by collaboratively learning multiple experts together. NCL consists of two core components, namely Nested Individual Learning (NIL) and Nested Balanced Online Distillation (NBOD), which focus on the individual supervised learning for each single expert and the knowledge transferring among multiple experts, respectively. To learn representations more thoroughly, both NIL and NBOD are formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Regarding the learning in the partial perspective, we specifically select the negative categories with high predicted scores as the hard categories by using a proposed Hard Category Mining (HCM). In the NCL, the learning from two perspectives is nested, highly related and complementary, and helps the network to capture not only global and robust features but also meticulous distinguishing ability. Moreover, self-supervision is further utilized for feature **enhancement**. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether by using a single model or an ensemble.  
### RGB-D Neural Radiance Fields: Local Sampling for Faster Training. (arXiv:2203.15587v1 [cs.CV])
- Authors : Arnab Dey
- Link : [http://arxiv.org/abs/2203.15587](http://arxiv.org/abs/2203.15587)
> ABSTRACT  :  Learning a 3D representation of a scene has been a challenging problem for decades in computer vision. Recent advances in **implicit neural representation** from images using neural radiance fields(**NeRF**) have shown promising results. Some of the limitations of previous **NeRF** based methods include longer training time, and inaccurate underlying geometry. The proposed method takes advantage of RGB-D data to reduce training time by leveraging depth sensing to improve local sampling. This paper proposes a depth-guided local sampling strategy and a smaller neural network architecture to achieve faster training time without compromising quality.  
### Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v3 [cs.CV] UPDATED)
- Authors : Wencan Zhang, Mariella Dimiccoli
- Link : [http://arxiv.org/abs/2012.05567](http://arxiv.org/abs/2012.05567)
> ABSTRACT  :  Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/**night**). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.  
### MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v3 [cs.CV] UPDATED)
- Authors : Qi Yang, Yujie Zhang, Siheng Chen, Yiling Xu, Jun Sun, Zhan Ma
- Link : [http://arxiv.org/abs/2103.02850](http://arxiv.org/abs/2103.02850)
> ABSTRACT  :  In this paper, we propose a new distortion quantification method for point clouds, the multiscale potential energy discrepancy (MPED). Currently, there is a lack of effective distortion quantification for a variety of point cloud perception tasks. Specifically, for dense point clouds, a distortion quantification method is used to predict human subjective scores and optimize the selection of human perception tasks parameters, such as compression and **enhancement**. For sparse point clouds, a distortion quantification methods is work as a loss function to guide the training of deep neural networks for unsupervised learning tasks (e.g., point cloud reconstruction, completion and upsampling). Therefore, an effective distortion quantification should be differentiable, distortion discriminable and have a low computational complexity. However, current distortion quantification cannot satisfy all three conditions. To fill this gap, we propose a new point cloud feature description method, the point potential energy (PPE), inspired by the classical physics. We regard the point clouds are systems that have potential energy and the distortion can change the total potential energy. By evaluating at various neighborhood sizes, the proposed MPED achieves global-local tradeoffs, capturing distortion in a multiscale fashion. We further theoretically show that classical Chamfer distance is a special case of our MPED. Extensive experiments show the proposed MPED superior to current methods on both human and machine perception tasks. Our code is avaliable at https://github.com/Qi-Yangsjtu/MPED.  
### MogFace: Towards a Deeper Appreciation on Face Detection. (arXiv:2103.11139v5 [cs.CV] UPDATED)
- Authors : Yang Liu, Fei Wang, Jiankang Deng, Zhipeng Zhou, Baigui Sun, Hao Li
- Link : [http://arxiv.org/abs/2103.11139](http://arxiv.org/abs/2103.11139)
> ABSTRACT  :  Benefiting from the pioneering design of generic object detectors, significant achievements have been made in the field of face detection. Typically, the architectures of the backbone, feature pyramid layer, and detection head module within the face detector all assimilate the excellent experience from general object detectors. However, several effective methods, including label assignment and scale-level data augmentation strategy, fail to maintain consistent superiority when applying on the face detector directly. Concretely, the former strategy involves a vast body of hyper-parameters and the latter one suffers from the challenge of scale distribution bias between different detection tasks, which both limit their generalization abilities. Furthermore, in order to provide accurate face bounding boxes for facial down-stream tasks, the face detector imperatively requires the elimination of false alarms. As a result, practical solutions on label assignment, scale-level data augmentation, and reducing false alarms are necessary for advancing face detectors. In this paper, we focus on resolving three aforementioned challenges that exiting methods are difficult to finish off and present a novel face detector, termed MogFace. In our Mogface, three key components, Adaptive Online Incremental Anchor Mining Strategy, Selective Scale **Enhancement** Strategy and Hierarchical Context-Aware Module, are separately proposed to boost the performance of face detectors. Finally, to the best of our knowledge, our MogFace is the best face detector on the Wider Face leader-board, achieving all champions across different testing scenarios. The code is available at \url{https://github.com/damo-cv/MogFace}.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v8 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
### NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v2 [cs.CV] UPDATED)
- Authors : Jerrick Liu, Nathan Inkawhich, Oliver Nina, Radu Timofte, Sahil Jain, Bob Lee, Yuru Duan, Wei Wei, **Lei Zhang**, Songzheng Xu, Yuxuan Sun, Jiaqi Tang, Xueli Geng, Mengru Ma, Gongzhe Li, Xueli Geng, Huanqia Cai, Chengxue Cai, Sol Cummings, Casian Miron, Alexandru Pasarica, Yen Yang, Min Hsu, Jiarui Cai, Jie Mei, Ying Yeh, Neng Hwang, Michael Xin, Zhongkai Shangguan, Zihe Zheng, Xu Yifei, Lehan Yang, Kele Xu, Min Feng
- Link : [http://arxiv.org/abs/2107.01189](http://arxiv.org/abs/2107.01189)
> ABSTRACT  :  In this paper, we introduce the first Challenge on Multi-modal Aerial View Object Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at CVPR. This challenge is composed of two different tracks using EO andSAR imagery. Both EO and SAR sensors possess different advantages and drawbacks. The purpose of this competition is to analyze how to use both sets of sensory information in complementary ways. We discuss the top methods submitted for this competition and evaluate their results on our blind test set. Our challenge results show significant improvement of more than 15% accuracy from our current baselines for each track of the competition  
### Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v4 [cs.CV] UPDATED)
- Authors : Manuel Schultheiss, Philipp Schmette, Thorsten Sellerer, Rafael Schick, Kirsten Taphorn, Korbinian Mechlem, Lorenz Birnbacher, Bernhard Renger, Franz Pfeiffer, Daniela Pfeiffer
- Link : [http://arxiv.org/abs/2110.12509](http://arxiv.org/abs/2110.12509)
> ABSTRACT  :  Estimating the lung depth on x-ray images could provide both an accurate opportunistic lung volume estimation during clinical routine and improve image contrast in modern structural chest imaging techniques like x-ray **dark**-field imaging. We present a method based on a convolutional neural network that allows a per-pixel lung thickness estimation and subsequent total lung capacity estimation. The network was trained and validated using 5250 simulated radiographs generated from 525 real CT scans. The network was evaluated on a test set of 131 synthetic radiographs and a retrospective evaluation was performed on another test set of 45 standard clinical radiographs. The standard clinical radiographs were obtained from 45 patients, who got a CT examination between July 1, 2021 and September 1, 2021 and a chest x-ray 6 month before or after the CT. For 45 standard clinical radiographs, the mean-absolute error between the estimated lung volume and groundtruth volume was 0.75 liter with a positive correlation (r = 0.78). When accounting for the patient diameter, the error decreases to 0.69 liter with a positive correlation (r = 0.83). Additionally, we predicted the lung thicknesses on the synthetic test set, where the mean-absolute error between the total volumes was 0.19 liter with a positive correlation (r = 0.99). The results show, that creation of lung thickness maps and estimation of approximate total lung volume is possible from standard clinical radiographs.  
### Revisiting LiDAR Registration and Reconstruction: A Range Image Perspective. (arXiv:2112.02779v2 [cs.CV] UPDATED)
- Authors : Wei Dong, Kwonyoung Ryu, Michael Kaess, Jaesik Park
- Link : [http://arxiv.org/abs/2112.02779](http://arxiv.org/abs/2112.02779)
> ABSTRACT  :  Spinning LiDAR data are prevalent for 3D vision tasks. Since LiDAR data is presented in the form of point clouds, expensive 3D operations are usually required. This paper revisits spinning LiDAR scan formation and presents a cylindrical range image representation with a ray-wise projection/unprojection model. It is built upon raw scans and supports lossless conversion from 2D to 3D, allowing fast 2D operations, including 2D index-based neighbor search and downsampling. We then propose, to the best of our knowledge, the first multi-scale registration and dense signed distance function (SDF) reconstruction system for LiDAR range images. We further collect a dataset of indoor and outdoor LiDAR scenes in the posed range image format. A comprehensive evaluation of registration and reconstruction is conducted on the proposed dataset and the KITTI dataset. Experiments demonstrate that our approach outperforms surface reconstruction baselines and achieves similar performance to state-of-the-art LiDAR registration methods, including a modern learning-based registration approach. Thanks to the simplicity, our registration runs at 100Hz and SDF reconstruction in **real time**. The dataset and a modularized C++/Python toolbox will be released.  
### Mega-**NeRF**: Scalable Construction of Large-Scale **NeRF**s for Virtual Fly-Throughs. (arXiv:2112.10703v2 [cs.CV] UPDATED)
- Authors : Haithem Turki, Deva Ramanan, Mahadev Satyanarayanan
- Link : [http://arxiv.org/abs/2112.10703](http://arxiv.org/abs/2112.10703)
> ABSTRACT  :  We use neural radiance fields (**NeRF**s) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drones. In contrast to single object scenes (on which **NeRF**s are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thousands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) prohibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs.    To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different **NeRF** submodules that can be trained in parallel.    We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12%. We also evaluate recent **NeRF** fast renderers on top of Mega-**NeRF** and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional **NeRF** rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.  
### DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v2 [cs.CV] UPDATED)
- Authors : Feng Li, Hao Zhang, Shilong Liu, Jian Guo, **Lei Zhang**
- Link : [http://arxiv.org/abs/2203.01305](http://arxiv.org/abs/2203.01305)
> ABSTRACT  :  We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) training and offer a deepened understanding of the slow convergence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable improvement. As a result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and $50$ epochs of training respectively) among DETR-like methods with ResNet-$50$ backbone. Compared with the baseline under the same setting, DN-DETR achieves comparable performance with $50\%$ training epochs. Code is available at \url{https://github.com/FengLi-ust/DN-DETR}.  
### DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v2 [cs.CV] UPDATED)
- Authors : Hao Zhang, Feng Li, Shilong Liu, **Lei Zhang**, Hang Su, Jun Zhu, Yeung Shum
- Link : [http://arxiv.org/abs/2203.03605](http://arxiv.org/abs/2203.03605)
> ABSTRACT  :  We present DINO (\textbf{D}ETR with \textbf{I}mproved de\textbf{N}oising anch\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $48.3$AP in $12$ epochs and $51.0$AP in $36$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\textbf{+4.9}$\textbf{AP} and $\textbf{+2.4}$\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a **Swin**L backbone, DINO obtains the best results on both COCO \texttt{val2017} ($\textbf{63.2}$\textbf{AP}) and \texttt{test-dev} (\textbf{$\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \url{https://github.com/IDEACVR/DINO}.  
### **Real-time** Object Detection for Streaming Perception. (arXiv:2203.12338v2 [cs.CV] UPDATED)
- Authors : Jinrong Yang, Songtao Liu, Zeming Li, Xiaoping Li, Jian Sun
- Link : [http://arxiv.org/abs/2203.12338](http://arxiv.org/abs/2203.12338)
> ABSTRACT  :  Autonomous driving requires the model to perceive the environment and (re)act within a low latency for safety. While past works ignore the inevitable changes in the environment after processing, streaming perception is proposed to jointly evaluate the latency and accuracy into a single metric for video online perception. In this paper, instead of searching trade-offs between accuracy and speed like previous works, we point out that endowing real-time models with the ability to predict the future is the key to dealing with this problem. We build a simple and effective framework for streaming perception. It equips a novel DualFlow Perception module (DFP), which includes dynamic and static flows to capture the moving trend and basic detection feature for streaming prediction. Further, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to generate adaptive weights for objects with different moving speeds. Our simple method achieves competitive performance on Argoverse-HD dataset and improves the AP by 4.9% compared to the strong baseline, validating its effectiveness. Our code will be made available at https://github.com/yancie-yjr/StreamYOLO.  
### Practical Blind Denoising via **Swin**-Conv-UNet and Data Synthesis. (arXiv:2203.13278v2 [cs.CV] UPDATED)
- Authors : Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2203.13278](http://arxiv.org/abs/2203.13278)
> ABSTRACT  :  While recent years have witnessed a dramatic upsurge of exploiting deep neural networks toward solving image denoising, existing methods mostly rely on simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG compression noise and camera sensor noise, and a general-purpose blind denoising method for real images remains unsolved. In this paper, we attempt to solve this problem from the perspective of network architecture design and training data synthesis. Specifically, for the network architecture design, we propose a swin-conv block to incorporate the local modeling ability of residual convolutional layer and non-local modeling ability of swin transformer block, and then plug it as the main building block into the widely-used image-to-image translation UNet architecture. For the training data synthesis, we design a practical noise degradation model which takes into consideration different kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and processed camera sensor noises) and resizing, and also involves a random shuffle strategy and a double degradation strategy. Extensive experiments on AGWN removal and real image denoising demonstrate that the new network architecture design achieves state-of-the-art performance and the new degradation model can help to significantly improve the practicability. We believe our work can provide useful insights into current denoising research.  
## eess.IV
---
### Harmonizing Pathological and Normal Pixels for Pseudo-healthy Synthesis. (arXiv:2203.15347v1 [eess.IV])
- Authors : Yunlong Zhang, Xin Lin, Yihong Zhuang, Yue Huang, Xinghao Ding, Guisheng Wang, Lin Yang, Yizhou Yu
- Link : [http://arxiv.org/abs/2203.15347](http://arxiv.org/abs/2203.15347)
> ABSTRACT  :  Synthesizing a subject-specific pathology-free image from a pathological image is valuable for algorithm development and clinical practice. In recent years, several approaches based on the Generative Adversarial Network (GAN) have achieved promising results in pseudo-healthy synthesis. However, the discriminator (i.e., a classifier) in the GAN cannot accurately identify lesions and further hampers from generating admirable pseudo-healthy images. To address this problem, we present a new type of discriminator, the segmentor, to accurately locate the lesions and improve the visual quality of pseudo-healthy images. Then, we apply the generated images into medical image **enhancement** and utilize the enhanced results to cope with the low contrast problem existing in medical image segmentation. Furthermore, a reliable metric is proposed by utilizing two attributes of label noise to measure the health of synthetic images. Comprehensive experiments on the T2 modality of BraTS demonstrate that the proposed method substantially outperforms the state-of-the-art methods. The method achieves better performance than the existing methods with only 30\% of the training data. The effectiveness of the proposed method is also demonstrated on the LiTS and the T1 modality of BraTS. The code and the pre-trained model of this study are publicly available at https://github.com/Au3C2/Generator-Versus-Segmentor.  
### MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v3 [cs.CV] UPDATED)
- Authors : Qi Yang, Yujie Zhang, Siheng Chen, Yiling Xu, Jun Sun, Zhan Ma
- Link : [http://arxiv.org/abs/2103.02850](http://arxiv.org/abs/2103.02850)
> ABSTRACT  :  In this paper, we propose a new distortion quantification method for point clouds, the multiscale potential energy discrepancy (MPED). Currently, there is a lack of effective distortion quantification for a variety of point cloud perception tasks. Specifically, for dense point clouds, a distortion quantification method is used to predict human subjective scores and optimize the selection of human perception tasks parameters, such as compression and **enhancement**. For sparse point clouds, a distortion quantification methods is work as a loss function to guide the training of deep neural networks for unsupervised learning tasks (e.g., point cloud reconstruction, completion and upsampling). Therefore, an effective distortion quantification should be differentiable, distortion discriminable and have a low computational complexity. However, current distortion quantification cannot satisfy all three conditions. To fill this gap, we propose a new point cloud feature description method, the point potential energy (PPE), inspired by the classical physics. We regard the point clouds are systems that have potential energy and the distortion can change the total potential energy. By evaluating at various neighborhood sizes, the proposed MPED achieves global-local tradeoffs, capturing distortion in a multiscale fashion. We further theoretically show that classical Chamfer distance is a special case of our MPED. Extensive experiments show the proposed MPED superior to current methods on both human and machine perception tasks. Our code is avaliable at https://github.com/Qi-Yangsjtu/MPED.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v8 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
### Practical Blind Denoising via **Swin**-Conv-UNet and Data Synthesis. (arXiv:2203.13278v2 [cs.CV] UPDATED)
- Authors : Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2203.13278](http://arxiv.org/abs/2203.13278)
> ABSTRACT  :  While recent years have witnessed a dramatic upsurge of exploiting deep neural networks toward solving image denoising, existing methods mostly rely on simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG compression noise and camera sensor noise, and a general-purpose blind denoising method for real images remains unsolved. In this paper, we attempt to solve this problem from the perspective of network architecture design and training data synthesis. Specifically, for the network architecture design, we propose a swin-conv block to incorporate the local modeling ability of residual convolutional layer and non-local modeling ability of swin transformer block, and then plug it as the main building block into the widely-used image-to-image translation UNet architecture. For the training data synthesis, we design a practical noise degradation model which takes into consideration different kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and processed camera sensor noises) and resizing, and also involves a random shuffle strategy and a double degradation strategy. Extensive experiments on AGWN removal and real image denoising demonstrate that the new network architecture design achieves state-of-the-art performance and the new degradation model can help to significantly improve the practicability. We believe our work can provide useful insights into current denoising research.  
## cs.LG
---
### New pyramidal hybrid textural and deep features based automatic skin cancer classification model: Ensemble **Dark**Net and textural feature extractor. (arXiv:2203.15090v1 [cs.CV])
- Authors : Mehmet Baygin, Turker Tuncer, Sengul Dogan
- Link : [http://arxiv.org/abs/2203.15090](http://arxiv.org/abs/2203.15090)
> ABSTRACT  :  Background: Skin cancer is one of the widely seen cancer worldwide and automatic classification of skin cancer can be benefited dermatology clinics for an accurate diagnosis. Hence, a machine learning-based automatic skin cancer detection model must be developed. Material and Method: This research interests to overcome automatic skin cancer detection problem. A colored skin cancer image dataset is used. This dataset contains 3297 images with two classes. An automatic multilevel textural and deep features-based model is presented. Multilevel fuse feature generation using discrete wavelet transform (DWT), local phase quantization (LPQ), local binary pattern (LBP), pre-trained **Dark**Net19, and **Dark**Net53 are utilized to generate features of the skin cancer images, top 1000 features are selected threshold value-based neighborhood component analysis (NCA). The chosen top 1000 features are classified using the 10-fold cross-validation technique. Results: To obtain results, ten-fold cross-validation is used and 91.54% classification accuracy results are obtained by using the recommended pyramidal hybrid feature generator and NCA selector-based model. Further, various training and testing separation ratios (90:10, 80:20, 70:30, 60:40, 50:50) are used and the maximum classification rate is calculated as 95.74% using the 90:10 separation ratio. Conclusions: The findings and accuracies calculated are denoted that this model can be used in dermatology and pathology clinics to simplify the skin cancer detection process and help physicians.  
### CMGAN: Conformer-based Metric GAN for Speech **Enhancement**. (arXiv:2203.15149v1 [cs.SD])
- Authors : Ruizhe Cao, Sherif Abdulatif, Bin Yang
- Link : [http://arxiv.org/abs/2203.15149](http://arxiv.org/abs/2203.15149)
> ABSTRACT  :  Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech **enhancement** (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset indicates the capability of CMGAN in outperforming various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB.  
### Harmonizing Pathological and Normal Pixels for Pseudo-healthy Synthesis. (arXiv:2203.15347v1 [eess.IV])
- Authors : Yunlong Zhang, Xin Lin, Yihong Zhuang, Yue Huang, Xinghao Ding, Guisheng Wang, Lin Yang, Yizhou Yu
- Link : [http://arxiv.org/abs/2203.15347](http://arxiv.org/abs/2203.15347)
> ABSTRACT  :  Synthesizing a subject-specific pathology-free image from a pathological image is valuable for algorithm development and clinical practice. In recent years, several approaches based on the Generative Adversarial Network (GAN) have achieved promising results in pseudo-healthy synthesis. However, the discriminator (i.e., a classifier) in the GAN cannot accurately identify lesions and further hampers from generating admirable pseudo-healthy images. To address this problem, we present a new type of discriminator, the segmentor, to accurately locate the lesions and improve the visual quality of pseudo-healthy images. Then, we apply the generated images into medical image **enhancement** and utilize the enhanced results to cope with the low contrast problem existing in medical image segmentation. Furthermore, a reliable metric is proposed by utilizing two attributes of label noise to measure the health of synthetic images. Comprehensive experiments on the T2 modality of BraTS demonstrate that the proposed method substantially outperforms the state-of-the-art methods. The method achieves better performance than the existing methods with only 30\% of the training data. The effectiveness of the proposed method is also demonstrated on the LiTS and the T1 modality of BraTS. The code and the pre-trained model of this study are publicly available at https://github.com/Au3C2/Generator-Versus-Segmentor.  
### Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v3 [cs.CV] UPDATED)
- Authors : Wencan Zhang, Mariella Dimiccoli
- Link : [http://arxiv.org/abs/2012.05567](http://arxiv.org/abs/2012.05567)
> ABSTRACT  :  Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/**night**). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v8 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
### NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v2 [cs.CV] UPDATED)
- Authors : Jerrick Liu, Nathan Inkawhich, Oliver Nina, Radu Timofte, Sahil Jain, Bob Lee, Yuru Duan, Wei Wei, **Lei Zhang**, Songzheng Xu, Yuxuan Sun, Jiaqi Tang, Xueli Geng, Mengru Ma, Gongzhe Li, Xueli Geng, Huanqia Cai, Chengxue Cai, Sol Cummings, Casian Miron, Alexandru Pasarica, Yen Yang, Min Hsu, Jiarui Cai, Jie Mei, Ying Yeh, Neng Hwang, Michael Xin, Zhongkai Shangguan, Zihe Zheng, Xu Yifei, Lehan Yang, Kele Xu, Min Feng
- Link : [http://arxiv.org/abs/2107.01189](http://arxiv.org/abs/2107.01189)
> ABSTRACT  :  In this paper, we introduce the first Challenge on Multi-modal Aerial View Object Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at CVPR. This challenge is composed of two different tracks using EO andSAR imagery. Both EO and SAR sensors possess different advantages and drawbacks. The purpose of this competition is to analyze how to use both sets of sensory information in complementary ways. We discuss the top methods submitted for this competition and evaluate their results on our blind test set. Our challenge results show significant improvement of more than 15% accuracy from our current baselines for each track of the competition  
### Mega-**NeRF**: Scalable Construction of Large-Scale **NeRF**s for Virtual Fly-Throughs. (arXiv:2112.10703v2 [cs.CV] UPDATED)
- Authors : Haithem Turki, Deva Ramanan, Mahadev Satyanarayanan
- Link : [http://arxiv.org/abs/2112.10703](http://arxiv.org/abs/2112.10703)
> ABSTRACT  :  We use neural radiance fields (**NeRF**s) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drones. In contrast to single object scenes (on which **NeRF**s are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thousands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) prohibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs.    To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different **NeRF** submodules that can be trained in parallel.    We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12%. We also evaluate recent **NeRF** fast renderers on top of Mega-**NeRF** and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional **NeRF** rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.  
### MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data. (arXiv:2203.12369v3 [cs.SD] UPDATED)
- Authors : George Close, Thomas Hain, Stefan Goetze
- Link : [http://arxiv.org/abs/2203.12369](http://arxiv.org/abs/2203.12369)
> ABSTRACT  :  Training of speech **enhancement** systems often does not incorporate knowledge of human perception and thus can lead to unnatural sounding results. Incorporating psychoacoustically motivated speech perception metrics as part of model training via a predictor network has recently gained interest. However, the performance of such predictors is limited by the distribution of metric scores that appear in the training data. In this work, we propose MetricGAN+/- (an extension of MetricGAN+, one such metric-motivated system) which introduces an additional network - a "de-generator" which attempts to improve the robustness of the prediction network (and by extension of the generator) by ensuring observation of a wider range of metric scores in training. Experimental results on the VoiceBank-DEMAND dataset show relative improvement in PESQ score of 3.8% (3.05 vs 3.22 PESQ score), as well as better generalisation to unseen noise and speech.  
## cs.AI
---
### Requirements Elicitation in Cognitive Service for Recommendation. (arXiv:2203.14958v1 [cs.AI])
- Authors : Bolin Zhang, Zhiying Tu, Yunzhe Xu, Dianhui Chu, Xiaofei Xu
- Link : [http://arxiv.org/abs/2203.14958](http://arxiv.org/abs/2203.14958)
> ABSTRACT  :  Nowadays, cognitive service provides more interactive way to understand users' requirements via human-machine conversation. In other words, it has to capture users' requirements from their utterance and respond them with the relevant and suitable service resources. To this end, two phases must be applied: I.Sequence planning and **Real-time** detection of user requirement, II.Service resource selection and Response generation. The existing works ignore the potential connection between these two phases. To model their connection, Two-Phase Requirement Elicitation Method is proposed. For the phase I, this paper proposes a user requirement elicitation framework (URef) to plan a potential requirement sequence grounded on user profile and personal knowledge base before the conversation. In addition, it can also predict user's true requirement and judge whether the requirement is completed based on the user's utterance during the conversation. For the phase II, this paper proposes a response generation model based on attention, SaRSNet. It can select the appropriate resource (i.e. knowledge triple) in line with the requirement predicted by URef, and then generates a suitable response for recommendation. The experimental results on the open dataset \emph{DuRecDial} have been significantly improved compared to the baseline, which proves the effectiveness of the proposed methods.  
### CMGAN: Conformer-based Metric GAN for Speech **Enhancement**. (arXiv:2203.15149v1 [cs.SD])
- Authors : Ruizhe Cao, Sherif Abdulatif, Bin Yang
- Link : [http://arxiv.org/abs/2203.15149](http://arxiv.org/abs/2203.15149)
> ABSTRACT  :  Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech **enhancement** (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset indicates the capability of CMGAN in outperforming various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB.  
### MogFace: Towards a Deeper Appreciation on Face Detection. (arXiv:2103.11139v5 [cs.CV] UPDATED)
- Authors : Yang Liu, Fei Wang, Jiankang Deng, Zhipeng Zhou, Baigui Sun, Hao Li
- Link : [http://arxiv.org/abs/2103.11139](http://arxiv.org/abs/2103.11139)
> ABSTRACT  :  Benefiting from the pioneering design of generic object detectors, significant achievements have been made in the field of face detection. Typically, the architectures of the backbone, feature pyramid layer, and detection head module within the face detector all assimilate the excellent experience from general object detectors. However, several effective methods, including label assignment and scale-level data augmentation strategy, fail to maintain consistent superiority when applying on the face detector directly. Concretely, the former strategy involves a vast body of hyper-parameters and the latter one suffers from the challenge of scale distribution bias between different detection tasks, which both limit their generalization abilities. Furthermore, in order to provide accurate face bounding boxes for facial down-stream tasks, the face detector imperatively requires the elimination of false alarms. As a result, practical solutions on label assignment, scale-level data augmentation, and reducing false alarms are necessary for advancing face detectors. In this paper, we focus on resolving three aforementioned challenges that exiting methods are difficult to finish off and present a novel face detector, termed MogFace. In our Mogface, three key components, Adaptive Online Incremental Anchor Mining Strategy, Selective Scale **Enhancement** Strategy and Hierarchical Context-Aware Module, are separately proposed to boost the performance of face detectors. Finally, to the best of our knowledge, our MogFace is the best face detector on the Wider Face leader-board, achieving all champions across different testing scenarios. The code is available at \url{https://github.com/damo-cv/MogFace}.  
### Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v4 [cs.CV] UPDATED)
- Authors : Manuel Schultheiss, Philipp Schmette, Thorsten Sellerer, Rafael Schick, Kirsten Taphorn, Korbinian Mechlem, Lorenz Birnbacher, Bernhard Renger, Franz Pfeiffer, Daniela Pfeiffer
- Link : [http://arxiv.org/abs/2110.12509](http://arxiv.org/abs/2110.12509)
> ABSTRACT  :  Estimating the lung depth on x-ray images could provide both an accurate opportunistic lung volume estimation during clinical routine and improve image contrast in modern structural chest imaging techniques like x-ray **dark**-field imaging. We present a method based on a convolutional neural network that allows a per-pixel lung thickness estimation and subsequent total lung capacity estimation. The network was trained and validated using 5250 simulated radiographs generated from 525 real CT scans. The network was evaluated on a test set of 131 synthetic radiographs and a retrospective evaluation was performed on another test set of 45 standard clinical radiographs. The standard clinical radiographs were obtained from 45 patients, who got a CT examination between July 1, 2021 and September 1, 2021 and a chest x-ray 6 month before or after the CT. For 45 standard clinical radiographs, the mean-absolute error between the estimated lung volume and groundtruth volume was 0.75 liter with a positive correlation (r = 0.78). When accounting for the patient diameter, the error decreases to 0.69 liter with a positive correlation (r = 0.83). Additionally, we predicted the lung thicknesses on the synthetic test set, where the mean-absolute error between the total volumes was 0.19 liter with a positive correlation (r = 0.99). The results show, that creation of lung thickness maps and estimation of approximate total lung volume is possible from standard clinical radiographs.  
### DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v2 [cs.CV] UPDATED)
- Authors : Feng Li, Hao Zhang, Shilong Liu, Jian Guo, **Lei Zhang**
- Link : [http://arxiv.org/abs/2203.01305](http://arxiv.org/abs/2203.01305)
> ABSTRACT  :  We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) training and offer a deepened understanding of the slow convergence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable improvement. As a result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and $50$ epochs of training respectively) among DETR-like methods with ResNet-$50$ backbone. Compared with the baseline under the same setting, DN-DETR achieves comparable performance with $50\%$ training epochs. Code is available at \url{https://github.com/FengLi-ust/DN-DETR}.  
# Paper List
---
## cs.CV
---
**246** new papers in cs.CV:-) 
1. Learning to segment fetal brain tissue from noisy annotations. (arXiv:2203.14962v1 [eess.IV])
2. TL-GAN: Improving Traffic Light Recognition via Data Synthesis for Autonomous Driving. (arXiv:2203.15006v1 [cs.CV])
3. Registering Explicit to Implicit: Towards High-Fidelity Garment mesh Reconstruction from Single Images. (arXiv:2203.15007v1 [cs.CV])
4. Deep Interactive Learning-based ovarian cancer segmentation of H&E-stained whole slide images to study morphological patterns of BRCA mutation. (arXiv:2203.15015v1 [eess.IV])
5. Few-Shot Object Detection with Fully Cross-Transformer. (arXiv:2203.15021v1 [cs.CV])
6. A systematic review and meta-analysis of Digital Elevation Model (DEM) fusion: pre-processing, methods and applications. (arXiv:2203.15026v1 [cs.CV])
7. Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation. (arXiv:2203.15041v1 [cs.RO])
8. A distribution-dependent Mumford-Shah model for unsupervised hyperspectral image segmentation. (arXiv:2203.15058v1 [cs.CV])
9. A Deep Learning Technique using a Sequence of Follow Up X-Rays for Disease classification. (arXiv:2203.15060v1 [cs.CV])
10. Cycle-Consistent Counterfactuals by Latent Transformations. (arXiv:2203.15064v1 [cs.CV])
11. DeepShadow: Neural Shape from Shadow. (arXiv:2203.15065v1 [cs.CV])
12. Face Verification Bypass. (arXiv:2203.15068v1 [cs.CV])
13. Semantic Motion Correction Via Iterative Nonlinear Optimization and Animation. (arXiv:2203.15072v1 [cs.CV])
14. Neurosymbolic hybrid approach to driver collision warning. (arXiv:2203.15076v1 [cs.CV])
15. CD-Net: Histopathology Representation Learning using Pyramidal Context-Detail Network. (arXiv:2203.15078v1 [cs.CV])
16. Iterative, Deep Synthetic Aperture Sonar Image Segmentation. (arXiv:2203.15082v1 [cs.CV])
17. X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval. (arXiv:2203.15086v1 [cs.CV])
18. Learning Optical Flow, Depth, and Scene Flow without Real-World Labels. (arXiv:2203.15089v1 [cs.CV])
19. New pyramidal hybrid textural and deep features based automatic skin cancer classification model: Ensemble **Dark**Net and textural feature extractor. (arXiv:2203.15090v1 [cs.CV])
20. Understanding out-of-distribution accuracies through quantifying difficulty of test samples. (arXiv:2203.15100v1 [cs.LG])
21. Rethinking Semantic Segmentation: A Prototype View. (arXiv:2203.15102v1 [cs.CV])
22. LiDAR Snowfall Simulation for Robust 3D Object Detection. (arXiv:2203.15118v1 [cs.CV])
23. Visual Odometry for RGB-D Cameras. (arXiv:2203.15119v1 [cs.CV])
24. Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v1 [cs.CV])
25. LocalBins: Improving Depth Estimation by Learning Local Distributions. (arXiv:2203.15132v1 [cs.CV])
26. Towards End-to-End Unified Scene Text Detection and Layout Analysis. (arXiv:2203.15143v1 [cs.CV])
27. Learning to Synthesize Volumetric Meshes from Vision-based Tactile Imprints. (arXiv:2203.15155v1 [cs.RO])
28. Practical Aspects of Zero-Shot Learning. (arXiv:2203.15158v1 [cs.LG])
29. CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI. (arXiv:2203.15163v1 [eess.IV])
30. Self-Supervised Light Field Depth Estimation Using Epipolar Plane Images. (arXiv:2203.15171v1 [cs.CV])
31. Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v1 [cs.CV])
32. Unified Transformer Tracker for Object Tracking. (arXiv:2203.15175v1 [cs.CV])
33. Min-Max Similarity: A Contrastive Learning Based Semi-Supervised Learning Network for Surgical Tools Segmentation. (arXiv:2203.15177v1 [cs.CV])
34. Long-term Visual Map Sparsification with Heterogeneous GNN. (arXiv:2203.15182v1 [cs.CV])
35. ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization. (arXiv:2203.15187v1 [cs.CV])
36. Coarse to Fine: Image **Restoration** Boosted by Multi-Scale Low-Rank Tensor Completion. (arXiv:2203.15189v1 [cs.CV])
37. 3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow. (arXiv:2203.15190v1 [cs.CV])
38. AnoDFDNet: A Deep Feature Difference Network for Anomaly Detection. (arXiv:2203.15195v1 [cs.CV])
39. Light Field Depth Estimation Based on Stitched-EPI. (arXiv:2203.15201v1 [cs.CV])
40. SimT: Handling Open-set Noise for Domain Adaptive Semantic Segmentation. (arXiv:2203.15202v1 [cs.CV])
41. Periocular Biometrics and its Relevance to Partially Masked Faces: A Survey. (arXiv:2203.15203v1 [cs.CV])
42. SPAct: Self-supervised Privacy Preservation for Action Recognition. (arXiv:2203.15205v1 [cs.CV])
43. Generalizing Few-Shot NAS with Gradient Matching. (arXiv:2203.15207v1 [cs.CV])
44. Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification. (arXiv:2203.15210v1 [cs.CV])
45. Affine Medical Image Registration with Coarse-to-Fine Vision Transformer. (arXiv:2203.15216v1 [cs.CV])
46. Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection. (arXiv:2203.15221v1 [cs.CV])
47. Panoptic **NeRF**: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation. (arXiv:2203.15224v1 [cs.CV])
48. Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation. (arXiv:2203.15227v1 [cs.CV])
49. SHOP: A Deep Learning Based Pipeline for near Real-Time Detection of Small Handheld Objects Present in Blurry Video. (arXiv:2203.15228v1 [cs.CV])
50. Edge Detection and Deep Learning Based SETI Signal Classification Method. (arXiv:2203.15229v1 [cs.CV])
51. Zero-Query Transfer Attacks on Context-Aware Object Detectors. (arXiv:2203.15230v1 [cs.CV])
52. AutoPoly: Predicting a Polygonal Mesh Construction Sequence from a Silhouette Image. (arXiv:2203.15233v1 [cs.CV])
53. Equivariance Allows Handling Multiple Nuisance Variables When Analyzing Pooled Neuroimaging Datasets. (arXiv:2203.15234v1 [cs.LG])
54. Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian. (arXiv:2203.15235v1 [cs.CV])
55. Semi-Supervised Image-to-Image Translation using Latent Space Mapping. (arXiv:2203.15241v1 [cs.CV])
56. Fine-tuning Image Transformers using Learnable Memory. (arXiv:2203.15243v1 [cs.CV])
57. Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients. (arXiv:2203.15245v1 [cs.CV])
58. Exploring Intra- and Inter-Video Relation for Surgical Semantic Scene Segmentation. (arXiv:2203.15251v1 [cs.CV])
59. Identification and classification of exfoliated graphene flakes from microscopy images using a hierarchical deep convolutional neural network. (arXiv:2203.15252v1 [cs.CV])
60. Efficient Reflectance Capture with a Deep Gated Mixture-of-Experts. (arXiv:2203.15258v1 [cs.CV])
61. Eigencontours: Novel Contour Descriptors Based on Low-Rank Approximation. (arXiv:2203.15259v1 [cs.CV])
62. Interactive Multi-Class Tiny-Object Detection. (arXiv:2203.15266v1 [cs.CV])
63. Vision Transformers in Medical Computer Vision -- A Contemplative Retrospection. (arXiv:2203.15269v1 [eess.IV])
64. MAT: Mask-Aware Transformer for Large Hole Image Inpainting. (arXiv:2203.15270v1 [cs.CV])
65. Sparse Image based Navigation Architecture to Mitigate the need of precise Localization in Mobile Robots. (arXiv:2203.15272v1 [cs.RO])
66. Semantic Line Detection Using Mirror Attention and Comparative Ranking and Matching. (arXiv:2203.15285v1 [cs.CV])
67. Uncertainty-Aware Adaptation for Self-Supervised 3D Human Pose Estimation. (arXiv:2203.15293v1 [cs.CV])
68. Kernel Modulation: A Parameter-Efficient Method for Training Convolutional Neural Networks. (arXiv:2203.15297v1 [cs.CV])
69. Eigenlanes: Data-Driven Lane Descriptors for Structurally Diverse Lanes. (arXiv:2203.15302v1 [cs.CV])
70. Learning-based Point Cloud Registration for 6D Object Pose Estimation in the Real World. (arXiv:2203.15309v1 [cs.CV])
71. Hybrid Routing Transformer for Zero-Shot Learning. (arXiv:2203.15310v1 [cs.CV])
72. In-N-Out Generative Learning for Dense Unsupervised Video Segmentation. (arXiv:2203.15312v1 [cs.CV])
73. Cross-Modality High-Frequency Transformer for MR Image Super-Resolution. (arXiv:2203.15314v1 [cs.CV])
74. Agreement or Disagreement in Noise-tolerant Mutual Learning?. (arXiv:2203.15317v1 [cs.CV])
75. Dressing in the Wild by Watching Dance Videos. (arXiv:2203.15320v1 [cs.CV])
76. Robust Single Image Dehazing Based on Consistent and Contrast-Assisted Reconstruction. (arXiv:2203.15325v1 [cs.CV])
77. CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV])
78. Balanced Multimodal Learning via On-the-fly Gradient Modulation. (arXiv:2203.15332v1 [cs.CV])
79. AnyFace: Free-style Text-to-Face Synthesis and Manipulation. (arXiv:2203.15334v1 [cs.CV])
80. End-to-End Compressed Video Representation Learning for Generic Event Boundary Detection. (arXiv:2203.15336v1 [cs.CV])
81. Infrared and Visible Image Fusion via Interactive Compensatory Attention Adversarial Learning. (arXiv:2203.15337v1 [cs.CV])
82. Task-specific Inconsistency Alignment for Domain Adaptive Object Detection. (arXiv:2203.15345v1 [cs.CV])
83. Harmonizing Pathological and Normal Pixels for Pseudo-healthy Synthesis. (arXiv:2203.15347v1 [eess.IV])
84. End-to-End Transformer Based Model for Image Captioning. (arXiv:2203.15350v1 [cs.CV])
85. SIOD: Single Instance Annotated Per Category Per Image for Object Detection. (arXiv:2203.15353v1 [cs.CV])
86. Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production. (arXiv:2203.15354v1 [cs.CV])
87. Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v1 [cs.CV])
88. Nested Collaborative Learning for Long-Tailed Visual Recognition. (arXiv:2203.15359v1 [cs.CV])
89. Self-Supervised Image Representation Learning with Geometric Set Consistency. (arXiv:2203.15361v1 [cs.CV])
90. Domain Invariant Siamese Attention Mask for Small Object Change Detection via Everyday Indoor Robot Navigation. (arXiv:2203.15362v1 [cs.CV])
91. Face segmentation: A comparison between visible and thermal images. (arXiv:2203.15366v1 [cs.CV])
92. mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v1 [cs.CV])
93. A Style-aware Discriminator for Controllable Image Translation. (arXiv:2203.15375v1 [cs.CV])
94. SepViT: Separable Vision Transformer. (arXiv:2203.15380v1 [cs.CV])
95. Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification. (arXiv:2203.15381v1 [cs.CV])
96. Category Guided Attention Network for Brain Tumor Segmentation in MRI. (arXiv:2203.15383v1 [eess.IV])
97. Efficient Hybrid Network: Inducting Scattering Features. (arXiv:2203.15392v1 [cs.CV])
98. Quantifying Societal Bias Amplification in Image Captioning. (arXiv:2203.15395v1 [cs.CV])
99. Neural Face Video Compression using Multiple Views. (arXiv:2203.15401v1 [cs.CV])
100. TransGAN: a Transductive Adversarial Model for Novelty Detection. (arXiv:2203.15406v1 [cs.LG])
101. AutoCoMet: Smart Neural Architecture Search via Co-Regulated Shaping Reinforcement. (arXiv:2203.15408v1 [cs.LG])
102. Deep Reinforcement Learning for Data-Driven Adaptive Scanning in Ptychography. (arXiv:2203.15413v1 [physics.comp-ph])
103. Long-term Video Frame Interpolation via Feature Propagation. (arXiv:2203.15427v1 [cs.CV])
104. Clean Implicit 3D Structure from Noisy 2D STEM Images. (arXiv:2203.15434v1 [eess.IV])
105. Contextual Information Based Anomaly Detection for a Multi-Scene UAV Aerial Videos. (arXiv:2203.15437v1 [cs.CV])
106. Eventor: An Efficient Event-Based Monocular Multi-View Stereo Accelerator on FPGA Platform. (arXiv:2203.15439v1 [cs.AR])
107. UnShadowNet: Illumination Critic Guided Contrastive Learning For Shadow Removal. (arXiv:2203.15441v1 [cs.CV])
108. Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding. (arXiv:2203.15442v1 [cs.CV])
109. A Naturalistic Database of Thermal Emotional Facial Expressions and Effects of Induced Emotions on Memory. (arXiv:2203.15443v1 [cs.CV])
110. Efficient Virtual View Selection for 3D Hand Pose Estimation. (arXiv:2203.15458v1 [cs.CV])
111. Abstract Flow for Temporal Semantic Segmentation on the Permutohedral Lattice. (arXiv:2203.15469v1 [cs.CV])
112. SAR-ShipNet: SAR-Ship Detection Neural Network via Bidirectional Coordinate Attention and Multi-resolution Feature Fusion. (arXiv:2203.15480v1 [cs.CV])
113. Learning Structured Gaussians to Approximate Deep Ensembles. (arXiv:2203.15485v1 [cs.CV])
114. Powerful Physical Adversarial Examples Against Practical Face Recognition Systems. (arXiv:2203.15498v1 [cs.CR])
115. Analysis of OODA Loop based on Adversarial for Complex Game Environments. (arXiv:2203.15502v1 [cs.AI])
116. Treatment Learning Transformer for Noisy Image Classification. (arXiv:2203.15529v1 [cs.CV])
117. OSOP: A Multi-Stage One Shot Object Pose Estimation Framework. (arXiv:2203.15533v1 [cs.CV])
118. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v1 [cs.CV])
119. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v1 [cs.CV])
120. Image Segmentation with Adaptive Spatial Priors from Joint Registration. (arXiv:2203.15548v1 [cs.CV])
121. Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC. (arXiv:2203.15565v1 [cs.CV])
122. Core Risk Minimization using Salient ImageNet. (arXiv:2203.15566v1 [cs.CV])
123. Learning a Structured Latent Space for Unsupervised Point Cloud Completion. (arXiv:2203.15580v1 [cs.CV])
124. RGB-D Neural Radiance Fields: Local Sampling for Faster Training. (arXiv:2203.15587v1 [cs.CV])
125. Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v1 [cs.LG])
126. DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis. (arXiv:2008.05865v3 [cs.CV] UPDATED)
127. Image Animation with Perturbed Masks. (arXiv:2011.06922v3 [cs.CV] UPDATED)
128. Deep Magnification-Flexible Upsampling over 3D Point Clouds. (arXiv:2011.12745v4 [cs.CV] UPDATED)
129. Towards Accurate Active Camera Localization. (arXiv:2012.04263v2 [cs.CV] UPDATED)
130. Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v3 [cs.CV] UPDATED)
131. Task-Adaptive Negative Class Envision for Few-Shot Open-Set Recognition. (arXiv:2012.13073v2 [cs.CV] UPDATED)
132. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v4 [cs.CV] UPDATED)
133. Generative Adversarial Transformers. (arXiv:2103.01209v4 [cs.CV] UPDATED)
134. MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v3 [cs.CV] UPDATED)
135. On the Whitney extension problem for near isometries and beyond. (arXiv:2103.09748v5 [math.CA] UPDATED)
136. MogFace: Towards a Deeper Appreciation on Face Detection. (arXiv:2103.11139v5 [cs.CV] UPDATED)
137. Selective Output Smoothing Regularization: Regularize Neural Networks by Softening Output Distributions. (arXiv:2103.15383v2 [cs.CV] UPDATED)
138. Opening up Open-World Tracking. (arXiv:2104.11221v2 [cs.CV] UPDATED)
139. RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition. (arXiv:2104.11934v2 [cs.CV] UPDATED)
140. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v8 [cs.CV] UPDATED)
141. Unsupervised Visual Representation Learning by Online Constrained K-Means. (arXiv:2105.11527v3 [cs.CV] UPDATED)
142. Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v5 [cs.CV] UPDATED)
143. DETReg: Unsupervised Pretraining with Region Priors for Object Detection. (arXiv:2106.04550v3 [cs.CV] UPDATED)
144. NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v2 [cs.CV] UPDATED)
145. Region-wise Loss for Biomedical Image Segmentation. (arXiv:2108.01405v2 [eess.IV] UPDATED)
146. Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-Temporal Sparsity. (arXiv:2108.02297v4 [cs.AR] UPDATED)
147. Interpreting Face Inference Models using Hierarchical Network Dissection. (arXiv:2108.10360v2 [cs.CV] UPDATED)
148. Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation via Semantic Knowledge Transfer and Self-Refinement. (arXiv:2109.09477v3 [cs.CV] UPDATED)
149. Early Lane Change Prediction for Automated Driving Systems Using Multi-Task Attention-based Convolutional Neural Networks. (arXiv:2109.10742v2 [cs.CV] UPDATED)
150. IntentVizor: Towards Generic Query Guided Interactive Video Summarization. (arXiv:2109.14834v2 [cs.CV] UPDATED)
151. Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes. (arXiv:2110.05909v3 [cs.CV] UPDATED)
152. Multimodal Dialogue Response Generation. (arXiv:2110.08515v2 [cs.CL] UPDATED)
153. Unsupervised Representation Learning for Binary Networks by Joint Classifier Learning. (arXiv:2110.08851v3 [cs.LG] UPDATED)
154. Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v4 [cs.CV] UPDATED)
155. Industrial Scene Text Detection with Refined Feature-attentive Network. (arXiv:2110.12663v2 [cs.CV] UPDATED)
156. Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v6 [cs.CV] UPDATED)
157. Recurrent Variational Network: A Deep Learning Inverse Problem Solver applied to the task of Accelerated MRI Reconstruction. (arXiv:2111.09639v2 [eess.IV] UPDATED)
158. Universal Captioner: Inducing Content-Style Separation in Vision-and-Language Model Training. (arXiv:2111.12727v2 [cs.CV] UPDATED)
159. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. (arXiv:2111.13152v3 [cs.CV] UPDATED)
160. Towards Principled Disentanglement for Domain Generalization. (arXiv:2111.13839v3 [cs.LG] UPDATED)
161. Adaptive Image Transformations for Transfer-based Adversarial Attack. (arXiv:2111.13844v2 [cs.CV] UPDATED)
162. Instance-wise Occlusion and Depth Orders in Natural Scenes. (arXiv:2111.14562v3 [cs.CV] UPDATED)
163. FaceAtlasAR: Atlas of Facial Acupuncture Points in Augmented Reality. (arXiv:2111.14755v2 [cs.GR] UPDATED)
164. Deep Decomposition for Stochastic Normal-Abnormal Transport. (arXiv:2111.14777v2 [cs.CV] UPDATED)
165. Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v3 [cs.LG] UPDATED)
166. DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation. (arXiv:2111.14887v2 [cs.CV] UPDATED)
167. HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing. (arXiv:2111.15666v2 [cs.CV] UPDATED)
168. Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data. (arXiv:2112.00054v3 [cs.CV] UPDATED)
169. MonoScene: Monocular 3D Semantic Scene Completion. (arXiv:2112.00726v2 [cs.CV] UPDATED)
170. Incremental Learning in Semantic Segmentation from Image Labels. (arXiv:2112.01882v2 [cs.CV] UPDATED)
171. Novel Class Discovery in Semantic Segmentation. (arXiv:2112.01900v2 [cs.CV] UPDATED)
172. SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing. (arXiv:2112.02236v3 [cs.CV] UPDATED)
173. Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations. (arXiv:2112.02290v2 [cs.CV] UPDATED)
174. Toward Practical Monocular Indoor Depth Estimation. (arXiv:2112.02306v2 [cs.CV] UPDATED)
175. Revisiting LiDAR Registration and Reconstruction: A Range Image Perspective. (arXiv:2112.02779v2 [cs.CV] UPDATED)
176. General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v2 [cs.CV] UPDATED)
177. MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection. (arXiv:2112.03902v2 [cs.CV] UPDATED)
178. Vehicle trajectory prediction works, but not everywhere. (arXiv:2112.03909v2 [cs.CV] UPDATED)
179. PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures. (arXiv:2112.05135v3 [cs.LG] UPDATED)
180. Label, Verify, Correct: A Simple Few Shot Object Detection Method. (arXiv:2112.05749v2 [cs.CV] UPDATED)
181. Few-shot Keypoint Detection with Uncertainty Learning for Unseen Species. (arXiv:2112.06183v2 [cs.CV] UPDATED)
182. Lifelong Unsupervised Domain Adaptive Person Re-identification with Coordinated Anti-forgetting and Adaptation. (arXiv:2112.06632v2 [cs.CV] UPDATED)
183. Multi-View Depth Estimation by Fusing Single-View Depth Probability with Multi-View Geometry. (arXiv:2112.08177v2 [cs.CV] UPDATED)
184. Putting People in their Place: Monocular Regression of 3D People in Depth. (arXiv:2112.08274v2 [cs.CV] UPDATED)
185. ICON: Implicit Clothed humans Obtained from Normals. (arXiv:2112.09127v2 [cs.CV] UPDATED)
186. Light Field Neural Rendering. (arXiv:2112.09687v2 [cs.CV] UPDATED)
187. Mega-**NeRF**: Scalable Construction of Large-Scale **NeRF**s for Virtual Fly-Throughs. (arXiv:2112.10703v2 [cs.CV] UPDATED)
188. NinjaDesc: Content-Concealing Visual Descriptors via Adversarial Learning. (arXiv:2112.12785v2 [cs.CV] UPDATED)
189. EM-driven unsupervised learning for efficient motion segmentation. (arXiv:2201.02074v2 [cs.CV] UPDATED)
190. Eye Know You Too: A DenseNet Architecture for End-to-end Eye Movement Biometrics. (arXiv:2201.02110v2 [cs.CV] UPDATED)
191. Stereo Magnification with Multi-Layer Images. (arXiv:2201.05023v2 [cs.CV] UPDATED)
192. Saliency Constrained Arbitrary Image Style Transfer using SIFT and DCNN. (arXiv:2201.05346v3 [cs.CV] UPDATED)
193. SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data. (arXiv:2201.05905v2 [eess.IV] UPDATED)
194. Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?. (arXiv:2201.06346v3 [cs.CV] UPDATED)
195. ShapeFormer: Transformer-based Shape Completion via Sparse Representation. (arXiv:2201.10326v2 [cs.CV] UPDATED)
196. Interactron: Embodied Adaptive Object Detection. (arXiv:2202.00660v2 [cs.CV] UPDATED)
197. Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space. (arXiv:2202.03800v2 [cs.CV] UPDATED)
198. Combining the Silhouette and Skeleton Data for Gait Recognition. (arXiv:2202.10645v2 [cs.CV] UPDATED)
199. Learning with Free Object Segments for Long-Tailed Instance Segmentation. (arXiv:2202.11124v2 [cs.CV] UPDATED)
200. TeachAugment: Data Augmentation Optimization Using Teacher Knowledge. (arXiv:2202.12513v3 [cs.CV] UPDATED)
201. ConvNeXt-backbone HoVerNet for nuclei segmentation and classification. (arXiv:2202.13560v2 [eess.IV] UPDATED)
202. OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion. (arXiv:2203.00838v2 [cs.CV] UPDATED)
203. DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v2 [cs.CV] UPDATED)
204. BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] UPDATED)
205. HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction. (arXiv:2203.01577v2 [cs.CV] UPDATED)
206. Style-ERD: Responsive and Coherent Online Motion Style Transfer. (arXiv:2203.02574v2 [cs.CV] UPDATED)
207. DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v2 [cs.CV] UPDATED)
208. Human-Aware Object Placement for Visual Environment Reconstruction. (arXiv:2203.03609v2 [cs.CV] UPDATED)
209. Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v4 [cs.CV] UPDATED)
210. ChiTransformer:Towards Reliable Stereo from Cues. (arXiv:2203.04554v2 [cs.CV] UPDATED)
211. The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v2 [cs.LG] UPDATED)
212. Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v6 [cs.CV] UPDATED)
213. Learning What Not to Segment: A New Perspective on Few-Shot Segmentation. (arXiv:2203.07615v2 [cs.CV] UPDATED)
214. Implicit Feature Decoupling with Depthwise Quantization. (arXiv:2203.08080v2 [cs.CV] UPDATED)
215. SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v2 [cs.CV] UPDATED)
216. Graph Flow: Cross-layer Graph Flow Distillation for Dual Efficient Medical Image Segmentation. (arXiv:2203.08667v3 [cs.CV] UPDATED)
217. MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v2 [cs.CV] UPDATED)
218. Localizing Visual Sounds the Easy Way. (arXiv:2203.09324v2 [cs.CV] UPDATED)
219. ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding. (arXiv:2203.10886v2 [cs.CV] UPDATED)
220. MixFormer: End-to-End Tracking with Iterative Mixed Attention. (arXiv:2203.11082v2 [cs.CV] UPDATED)
221. WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v2 [cs.CV] UPDATED)
222. Mixed Differential Privacy in Computer Vision. (arXiv:2203.11481v2 [cs.CV] UPDATED)
223. PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo. (arXiv:2203.12082v2 [cs.CV] UPDATED)
224. **Real-time** Object Detection for Streaming Perception. (arXiv:2203.12338v2 [cs.CV] UPDATED)
225. CroMo: Cross-Modal Learning for Monocular Depth Estimation. (arXiv:2203.12485v2 [cs.CV] UPDATED)
226. Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v2 [cs.CV] UPDATED)
227. Learning to generate line drawings that convey geometry and semantics. (arXiv:2203.12691v3 [cs.CV] UPDATED)
228. Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation. (arXiv:2203.12707v2 [cs.CV] UPDATED)
229. Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v3 [cs.CV] UPDATED)
230. Multiple Emotion Descriptors Estimation at the ABAW3 Challenge. (arXiv:2203.12845v2 [cs.CV] UPDATED)
231. Intrinsic Bias Identification on Medical Image Datasets. (arXiv:2203.12872v2 [cs.CV] UPDATED)
232. CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. (arXiv:2203.13009v3 [cs.CV] UPDATED)
233. EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation. (arXiv:2203.13254v2 [cs.CV] UPDATED)
234. Practical Blind Denoising via **Swin**-Conv-UNet and Data Synthesis. (arXiv:2203.13278v2 [cs.CV] UPDATED)
235. Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition. (arXiv:2203.13285v2 [cs.SD] UPDATED)
236. FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable Model from a Hybrid Dataset. (arXiv:2203.14057v2 [cs.CV] UPDATED)
237. Feature Selective Transformer for Semantic Image Segmentation. (arXiv:2203.14124v2 [cs.CV] UPDATED)
238. Audio-Adaptive Activity Recognition Across Video Domains. (arXiv:2203.14240v2 [cs.CV] UPDATED)
239. SuperMVS: Non-Uniform Cost Volume For High-Resolution Multi-View Stereo. (arXiv:2203.14331v2 [cs.CV] UPDATED)
240. Locality-Aware Inter-and Intra-Video Reconstruction for Self-Supervised Correspondence Learning. (arXiv:2203.14333v2 [cs.CV] UPDATED)
241. Deep Hierarchical Semantic Segmentation. (arXiv:2203.14335v2 [cs.CV] UPDATED)
242. MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation. (arXiv:2203.14341v2 [eess.IV] UPDATED)
243. Thin-Plate Spline Motion Model for Image Animation. (arXiv:2203.14367v2 [cs.CV] UPDATED)
244. ARCS: Accurate Rotation and Correspondence Search. (arXiv:2203.14493v2 [cs.CV] UPDATED)
245. Affordance Transfer Learning for Human-Object Interaction Detection. (arXiv:2104.02867v2 [cs.CV] CROSS LISTED)
246. Discovering Human-Object Interaction Concepts via Self-Compositional Learning. (arXiv:2203.14272v1 [cs.CV] CROSS LISTED)
## eess.IV
---
**26** new papers in eess.IV:-) 
1. Learning to segment fetal brain tissue from noisy annotations. (arXiv:2203.14962v1 [eess.IV])
2. Deep Interactive Learning-based ovarian cancer segmentation of H&E-stained whole slide images to study morphological patterns of BRCA mutation. (arXiv:2203.15015v1 [eess.IV])
3. CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI. (arXiv:2203.15163v1 [eess.IV])
4. Min-Max Similarity: A Contrastive Learning Based Semi-Supervised Learning Network for Surgical Tools Segmentation. (arXiv:2203.15177v1 [cs.CV])
5. Edge Detection and Deep Learning Based SETI Signal Classification Method. (arXiv:2203.15229v1 [cs.CV])
6. Identification and classification of exfoliated graphene flakes from microscopy images using a hierarchical deep convolutional neural network. (arXiv:2203.15252v1 [cs.CV])
7. Vision Transformers in Medical Computer Vision -- A Contemplative Retrospection. (arXiv:2203.15269v1 [eess.IV])
8. Harmonizing Pathological and Normal Pixels for Pseudo-healthy Synthesis. (arXiv:2203.15347v1 [eess.IV])
9. Category Guided Attention Network for Brain Tumor Segmentation in MRI. (arXiv:2203.15383v1 [eess.IV])
10. Clean Implicit 3D Structure from Noisy 2D STEM Images. (arXiv:2203.15434v1 [eess.IV])
11. Learning Structured Gaussians to Approximate Deep Ensembles. (arXiv:2203.15485v1 [cs.CV])
12. Treatment Learning Transformer for Noisy Image Classification. (arXiv:2203.15529v1 [cs.CV])
13. Angular Super-Resolution in Diffusion MRI with a 3D Recurrent Convolutional Autoencoder. (arXiv:2203.15598v1 [eess.IV])
14. Photographic Visualization of Weather Forecasts with Generative Adversarial Networks. (arXiv:2203.15601v1 [cs.CV])
15. Synergizing Physics/Model-based and Data-driven Methods for Low-Dose CT. (arXiv:2203.15725v1 [eess.IV])
16. MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v3 [cs.CV] UPDATED)
17. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v8 [cs.CV] UPDATED)
18. Region-wise Loss for Biomedical Image Segmentation. (arXiv:2108.01405v2 [eess.IV] UPDATED)
19. Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v6 [cs.CV] UPDATED)
20. Recurrent Variational Network: A Deep Learning Inverse Problem Solver applied to the task of Accelerated MRI Reconstruction. (arXiv:2111.09639v2 [eess.IV] UPDATED)
21. SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data. (arXiv:2201.05905v2 [eess.IV] UPDATED)
22. ConvNeXt-backbone HoVerNet for nuclei segmentation and classification. (arXiv:2202.13560v2 [eess.IV] UPDATED)
23. ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding. (arXiv:2203.10886v2 [cs.CV] UPDATED)
24. Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation. (arXiv:2203.12707v2 [cs.CV] UPDATED)
25. Practical Blind Denoising via **Swin**-Conv-UNet and Data Synthesis. (arXiv:2203.13278v2 [cs.CV] UPDATED)
26. MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation. (arXiv:2203.14341v2 [eess.IV] UPDATED)
## cs.LG
---
**177** new papers in cs.LG:-) 
1. Domino: Discovering Systematic Errors with Cross-Modal Embeddings. (arXiv:2203.14960v1 [cs.LG])
2. A Deep Learning Approach for Thermal Plume Prediction of Groundwater Heat Pumps. (arXiv:2203.14961v1 [cs.LG])
3. Learning to segment fetal brain tissue from noisy annotations. (arXiv:2203.14962v1 [eess.IV])
4. Deep Learning and Artificial General Intelligence: Still a Long Way to Go. (arXiv:2203.14963v1 [cs.LG])
5. Error Correction Code Transformer. (arXiv:2203.14966v1 [cs.LG])
6. Comparing in context: Improving cosine similarity measures with a metric tensor. (arXiv:2203.14996v1 [cs.CL])
7. DAMNETS: A Deep Autoregressive Model for Generating Markovian Network Time Series. (arXiv:2203.15009v1 [stat.ML])
8. Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks. (arXiv:2203.15030v1 [cs.AI])
9. Learning Parameterized Task Structure for Generalization to Unseen Entities. (arXiv:2203.15034v1 [cs.LG])
10. Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation. (arXiv:2203.15041v1 [cs.RO])
11. AUC Maximization in the Era of Big Data and AI: A Survey. (arXiv:2203.15046v1 [cs.LG])
12. A Deep Learning Technique using a Sequence of Follow Up X-Rays for Disease classification. (arXiv:2203.15060v1 [cs.CV])
13. Cycle-Consistent Counterfactuals by Latent Transformations. (arXiv:2203.15064v1 [cs.CV])
14. User Driven Model Adjustment via Boolean Rule Explanations. (arXiv:2203.15071v1 [cs.AI])
15. Learning Optical Flow, Depth, and Scene Flow without Real-World Labels. (arXiv:2203.15089v1 [cs.CV])
16. New pyramidal hybrid textural and deep features based automatic skin cancer classification model: Ensemble **Dark**Net and textural feature extractor. (arXiv:2203.15090v1 [cs.CV])
17. Improved singing voice separation with chromagram-based pitch-aware remixing. (arXiv:2203.15092v1 [eess.AS])
18. Robust Speaker Recognition with Transformers Using wav2vec 2.0. (arXiv:2203.15095v1 [cs.SD])
19. Understanding out-of-distribution accuracies through quantifying difficulty of test samples. (arXiv:2203.15100v1 [cs.LG])
20. FedADMM: A Federated Primal-Dual Algorithm Allowing Partial Participation. (arXiv:2203.15104v1 [cs.LG])
21. Investigation of Different Calibration Methods for Deep Speaker Embedding based Verification Systems. (arXiv:2203.15106v1 [cs.SD])
22. LiDAR Snowfall Simulation for Robust 3D Object Detection. (arXiv:2203.15118v1 [cs.CV])
23. Toward Deep Learning Based Access Control. (arXiv:2203.15124v1 [cs.CR])
24. Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v1 [cs.CV])
25. CMGAN: Conformer-based Metric GAN for Speech **Enhancement**. (arXiv:2203.15149v1 [cs.SD])
26. A super-polynomial lower bound for learning nonparametric mixtures. (arXiv:2203.15150v1 [cs.LG])
27. A machine learning-based severity prediction tool for diabetic sensorimotor polyneuropathy using Michigan neuropathy screening instrumentations. (arXiv:2203.15151v1 [cs.LG])
28. Practical Aspects of Zero-Shot Learning. (arXiv:2203.15158v1 [cs.LG])
29. An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese Codex. (arXiv:2203.15173v1 [cs.CL])
30. Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v1 [cs.CV])
31. SPAct: Self-supervised Privacy Preservation for Action Recognition. (arXiv:2203.15205v1 [cs.CV])
32. Generalizing Few-Shot NAS with Gradient Matching. (arXiv:2203.15207v1 [cs.CV])
33. OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks. (arXiv:2203.15209v1 [cs.LG])
34. SHOP: A Deep Learning Based Pipeline for near Real-Time Detection of Small Handheld Objects Present in Blurry Video. (arXiv:2203.15228v1 [cs.CV])
35. Zero-Query Transfer Attacks on Context-Aware Object Detectors. (arXiv:2203.15230v1 [cs.CV])
36. Equivariance Allows Handling Multiple Nuisance Variables When Analyzing Pooled Neuroimaging Datasets. (arXiv:2203.15234v1 [cs.LG])
37. Best Arm Identification in Restless Markov Multi-Armed Bandits. (arXiv:2203.15236v1 [stat.ML])
38. NeuraGen-A Low-Resource Neural Network based approach for Gender Classification. (arXiv:2203.15253v1 [cs.SD])
39. Efficient Convex Optimization Requires Superlinear Memory. (arXiv:2203.15260v1 [cs.LG])
40. Vision Transformers in Medical Computer Vision -- A Contemplative Retrospection. (arXiv:2203.15269v1 [eess.IV])
41. A Multi-size Kernel based Adaptive Convolutional Neural Network for Bearing Fault Diagnosis. (arXiv:2203.15275v1 [eess.SP])
42. Mel Frequency Spectral Domain Defenses against Adversarial Attacks on Speech Recognition Systems. (arXiv:2203.15283v1 [eess.AS])
43. A Wavelet, AR and SVM based hybrid method for short-term wind speed prediction. (arXiv:2203.15298v1 [cs.LG])
44. Agreement or Disagreement in Noise-tolerant Mutual Learning?. (arXiv:2203.15317v1 [cs.CV])
45. Evolving Multi-Label Fuzzy Classifier. (arXiv:2203.15318v1 [cs.LG])
46. Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation. (arXiv:2203.15319v1 [cs.CL])
47. syslrn: Learning What to Monitor for Efficient Anomaly Detection. (arXiv:2203.15324v1 [cs.LG])
48. CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV])
49. Harmonizing Pathological and Normal Pixels for Pseudo-healthy Synthesis. (arXiv:2203.15347v1 [eess.IV])
50. Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v1 [cs.CV])
51. Multiclass classification using quantum convolutional neural networks with hybrid quantum-classical learning. (arXiv:2203.15368v1 [quant-ph])
52. Spoofing-Aware Speaker Verification by Multi-Level Fusion. (arXiv:2203.15377v1 [cs.SD])
53. Pareto Set Learning for Neural Multi-objective Combinatorial Optimization. (arXiv:2203.15386v1 [cs.LG])
54. ReIL: A Framework for Reinforced Intervention-based Imitation Learning. (arXiv:2203.15390v1 [cs.RO])
55. Physics-informed deep-learning applications to experimental fluid mechanics. (arXiv:2203.15402v1 [physics.flu-dyn])
56. TransGAN: a Transductive Adversarial Model for Novelty Detection. (arXiv:2203.15406v1 [cs.LG])
57. AutoCoMet: Smart Neural Architecture Search via Co-Regulated Shaping Reinforcement. (arXiv:2203.15408v1 [cs.LG])
58. Deep Reinforcement Learning for Data-Driven Adaptive Scanning in Ptychography. (arXiv:2203.15413v1 [physics.comp-ph])
59. On Reinforcement Learning, Effect Handlers, and the State Monad. (arXiv:2203.15426v1 [cs.PL])
60. Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus. (arXiv:2203.15447v1 [eess.AS])
61. Protein language models trained on multiple sequence alignments learn phylogenetic relationships. (arXiv:2203.15465v1 [q-bio.BM])
62. Machine Composition of Korean Music via Topological Data Analysis and Artificial Neural Network. (arXiv:2203.15468v1 [cs.SD])
63. Abstract Flow for Temporal Semantic Segmentation on the Permutohedral Lattice. (arXiv:2203.15469v1 [cs.CV])
64. Graph similarity learning for change-point detection in dynamic networks. (arXiv:2203.15470v1 [stat.ML])
65. Gaussian Control Barrier Functions : A Non-Parametric Paradigm to Safety. (arXiv:2203.15474v1 [eess.SY])
66. Over-the-Air Federated Learning via Second-Order Optimization. (arXiv:2203.15488v1 [cs.IT])
67. Neural representation of a time optimal, constant acceleration rendezvous. (arXiv:2203.15490v1 [astro-ph.EP])
68. Improving the Learnability of Machine Learning APIs by Semi-Automated API Wrapping. (arXiv:2203.15491v1 [cs.SE])
69. Powerful Physical Adversarial Examples Against Practical Face Recognition Systems. (arXiv:2203.15498v1 [cs.CR])
70. Deep Learning for Encrypted Traffic Classification and Unknown Data Detection. (arXiv:2203.15501v1 [cs.CR])
71. Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning. (arXiv:2203.15506v1 [cs.CR])
72. Improving Contrastive Learning with Model Augmentation. (arXiv:2203.15508v1 [cs.LG])
73. Achieving Guidance in Applied Machine Learning through Software Engineering Techniques. (arXiv:2203.15510v1 [cs.SE])
74. Explaining random forest prediction through diverse rulesets. (arXiv:2203.15511v1 [cs.LG])
75. Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias. (arXiv:2203.15514v1 [cs.AI])
76. Rich Feature Construction for the Optimization-Generalization Dilemma. (arXiv:2203.15516v1 [cs.LG])
77. A multimodal approach for Parkinson disease analysis. (arXiv:2203.15517v1 [q-bio.NC])
78. Learning neural audio features without supervision. (arXiv:2203.15519v1 [cs.SD])
79. Treatment Learning Transformer for Noisy Image Classification. (arXiv:2203.15529v1 [cs.CV])
80. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v1 [cs.CV])
81. Graph Neural Networks are Dynamic Programmers. (arXiv:2203.15544v1 [cs.LG])
82. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v1 [cs.CV])
83. Invariance Learning based on Label Hierarchy. (arXiv:2203.15549v1 [stat.ML])
84. Training Compute-Optimal Large Language Models. (arXiv:2203.15556v1 [cs.CL])
85. Wildfire risk forecast: An optimizable fire danger index. (arXiv:2203.15558v1 [cs.LG])
86. Attacker Attribution of Audio Deepfakes. (arXiv:2203.15563v1 [cs.CR])
87. A Dataset for Speech Emotion Recognition in Greek Theatrical Plays. (arXiv:2203.15568v1 [cs.SD])
88. Circuit encapsulation for efficient quantum computing based on controlled many-body dynamics. (arXiv:2203.15574v1 [quant-ph])
89. Subspace-based Representation and Learning for Phonotactic Spoken Language Recognition. (arXiv:2203.15576v1 [cs.SD])
90. Disentangling speech from surroundings in a neural audio codec. (arXiv:2203.15578v1 [cs.SD])
91. Discovering Governing Equations by Machine Learning implemented with Invariance. (arXiv:2203.15586v1 [cs.LG])
92. Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v1 [cs.LG])
93. On Kernelized Multi-Armed Bandits with Constraints. (arXiv:2203.15589v1 [cs.LG])
94. Modified Multidimensional Scaling and High Dimensional Clustering. (arXiv:1810.10172v5 [stat.ME] UPDATED)
95. Improving Generalization of Deep Neural Networks by Leveraging Margin Distribution. (arXiv:1812.10761v3 [cs.LG] UPDATED)
96. Constant Time Graph Neural Networks. (arXiv:1901.07868v4 [cs.LG] UPDATED)
97. Amortized Rejection Sampling in Universal Probabilistic Programming. (arXiv:1910.09056v3 [cs.LG] UPDATED)
98. xAI-GAN: Enhancing Generative Adversarial Networks via Explainable AI Systems. (arXiv:2002.10438v3 [cs.LG] UPDATED)
99. Ternary Compression for Communication-Efficient Federated Learning. (arXiv:2003.03564v2 [cs.LG] UPDATED)
100. On Multivariate Singular Spectrum Analysis and its Variants. (arXiv:2006.13448v4 [cs.LG] UPDATED)
101. Towards Improving Selective Prediction Ability of NLP Systems. (arXiv:2008.09371v2 [cs.CL] UPDATED)
102. Towards Flexible Sparsity-Aware Modeling: Automatic Tensor Rank Learning Using The Generalized Hyperbolic Prior. (arXiv:2009.02472v2 [cs.LG] UPDATED)
103. Robust Optimization as Data Augmentation for Large-scale Graphs. (arXiv:2010.09891v3 [cs.LG] UPDATED)
104. Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v3 [cs.CV] UPDATED)
105. AutonoML: Towards an Integrated Framework for Autonomous Machine Learning. (arXiv:2012.12600v2 [cs.LG] UPDATED)
106. A Sketching Method for Finding the Closest Point on a Convex Hull. (arXiv:2102.10502v2 [math.DG] UPDATED)
107. Generative Adversarial Transformers. (arXiv:2103.01209v4 [cs.CV] UPDATED)
108. On the Whitney extension problem for near isometries and beyond. (arXiv:2103.09748v5 [math.CA] UPDATED)
109. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v8 [cs.CV] UPDATED)
110. A Generalized Projected Bellman Error for Off-policy Value Estimation in Reinforcement Learning. (arXiv:2104.13844v2 [cs.LG] UPDATED)
111. High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach. (arXiv:2105.02487v2 [stat.ML] UPDATED)
112. Secure Multi-Function Computation with Private Remote Sources. (arXiv:2106.09485v4 [cs.IT] UPDATED)
113. Concentration of Contractive Stochastic Approximation and Reinforcement Learning. (arXiv:2106.14308v3 [cs.LG] UPDATED)
114. Spotting adversarial samples for speaker verification by neural vocoders. (arXiv:2107.00309v3 [cs.SD] UPDATED)
115. NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v2 [cs.CV] UPDATED)
116. Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search. (arXiv:2108.01036v2 [cs.AI] UPDATED)
117. Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-Temporal Sparsity. (arXiv:2108.02297v4 [cs.AR] UPDATED)
118. Structure Learning for Directed Trees. (arXiv:2108.08871v4 [stat.ML] UPDATED)
119. Multiple Hypothesis Testing Framework for Spatial Signals. (arXiv:2108.12314v2 [eess.SP] UPDATED)
120. An Experimental Study of Class Imbalance in Federated Learning. (arXiv:2109.04094v2 [cs.LG] UPDATED)
121. Early Lane Change Prediction for Automated Driving Systems Using Multi-Task Attention-based Convolutional Neural Networks. (arXiv:2109.10742v2 [cs.CV] UPDATED)
122. End-To-End Label Uncertainty Modeling for Speech-based Arousal Recognition Using Bayesian Neural Networks. (arXiv:2110.03299v2 [eess.AS] UPDATED)
123. Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions. (arXiv:2110.05064v3 [cs.LG] UPDATED)
124. Efficient Training of Audio Transformers with Patchout. (arXiv:2110.05069v3 [cs.SD] UPDATED)
125. Unsupervised Source Separation via Bayesian Inference in the Latent Domain. (arXiv:2110.05313v3 [cs.LG] UPDATED)
126. Case-based Reasoning for Better Generalization in Textual Reinforcement Learning. (arXiv:2110.08470v3 [cs.CL] UPDATED)
127. Multimodal Dialogue Response Generation. (arXiv:2110.08515v2 [cs.CL] UPDATED)
128. Unsupervised Representation Learning for Binary Networks by Joint Classifier Learning. (arXiv:2110.08851v3 [cs.LG] UPDATED)
129. Private Language Model Adaptation for Speech Recognition. (arXiv:2110.10026v2 [eess.AS] UPDATED)
130. Quantum field theories, Markov random fields and machine learning. (arXiv:2110.10928v2 [cs.LG] UPDATED)
131. Leveraging Time Irreversibility with Order-Contrastive Pre-training. (arXiv:2111.02599v2 [cs.LG] UPDATED)
132. Characterizing the adversarial vulnerability of speech self-supervised learning. (arXiv:2111.04330v2 [cs.SD] UPDATED)
133. A Teacher-Student Markov Decision Process-based Framework for Online Correctional Learning. (arXiv:2111.07818v2 [cs.LG] UPDATED)
134. GCR: Gradient Coreset Based Replay Buffer Selection For Continual Learning. (arXiv:2111.11210v2 [cs.LG] UPDATED)
135. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. (arXiv:2111.13152v3 [cs.CV] UPDATED)
136. Towards Principled Disentanglement for Domain Generalization. (arXiv:2111.13839v3 [cs.LG] UPDATED)
137. Comparing Machine Learning and Interpolation Methods for Loop-Level Calculations. (arXiv:2111.14788v3 [hep-ph] UPDATED)
138. Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v3 [cs.LG] UPDATED)
139. Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data. (arXiv:2112.00054v3 [cs.CV] UPDATED)
140. What to Learn, and How: Toward Effective Learning from Rationales. (arXiv:2112.00071v2 [cs.LG] UPDATED)
141. Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations. (arXiv:2112.02290v2 [cs.CV] UPDATED)
142. Next Steps: Learning a Disentangled Gait Representation for Versatile Quadruped Locomotion. (arXiv:2112.04809v2 [cs.RO] UPDATED)
143. PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures. (arXiv:2112.05135v3 [cs.LG] UPDATED)
144. Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems. (arXiv:2112.08718v2 [cs.CL] UPDATED)
145. Mega-**NeRF**: Scalable Construction of Large-Scale **NeRF**s for Virtual Fly-Throughs. (arXiv:2112.10703v2 [cs.CV] UPDATED)
146. FedLGA: Towards System-Heterogeneity of Federated Learning via Local Gradient Approximation. (arXiv:2112.11989v2 [cs.LG] UPDATED)
147. Safe Reinforcement Learning with Chance-constrained Model Predictive Control. (arXiv:2112.13941v2 [cs.LG] UPDATED)
148. A sampling-based approach for efficient clustering in large datasets. (arXiv:2112.14793v2 [cs.LG] UPDATED)
149. Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks. (arXiv:2201.06147v2 [cs.LG] UPDATED)
150. NAS-VAD: Neural Architecture Search for Voice Activity Detection. (arXiv:2201.09032v2 [cs.SD] UPDATED)
151. Optimal transport for causal discovery. (arXiv:2201.09366v2 [cs.LG] UPDATED)
152. ShapeFormer: Transformer-based Shape Completion via Sparse Representation. (arXiv:2201.10326v2 [cs.CV] UPDATED)
153. Approximation of Images via Generalized Higher Order Singular Value Decomposition over Finite-dimensional Commutative Semisimple Algebra. (arXiv:2202.00450v3 [cs.LG] UPDATED)
154. PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. (arXiv:2202.01279v3 [cs.LG] UPDATED)
155. A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits. (arXiv:2202.01850v2 [stat.ML] UPDATED)
156. Adversarially-regularized mixed effects deep learning (ARMED) models for improved interpretability, performance, and generalization on clustered data. (arXiv:2202.11783v2 [cs.LG] UPDATED)
157. Style-ERD: Responsive and Coherent Online Motion Style Transfer. (arXiv:2203.02574v2 [cs.CV] UPDATED)
158. The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v2 [cs.LG] UPDATED)
159. SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v2 [cs.CV] UPDATED)
160. Backpropagation through Time and Space: Learning Numerical Methods with Multi-Agent Reinforcement Learning. (arXiv:2203.08937v3 [cs.LG] UPDATED)
161. MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v2 [cs.CV] UPDATED)
162. Localizing Visual Sounds the Easy Way. (arXiv:2203.09324v2 [cs.CV] UPDATED)
163. Enriching Unsupervised User Embedding via Medical Concepts. (arXiv:2203.10627v2 [cs.CL] UPDATED)
164. MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data. (arXiv:2203.12369v3 [cs.SD] UPDATED)
165. Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v2 [cs.CV] UPDATED)
166. Pseudo Label Is Better Than Human Label. (arXiv:2203.12668v2 [cs.LG] UPDATED)
167. Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition. (arXiv:2203.13285v2 [cs.SD] UPDATED)
168. Tuning Particle Accelerators with Safety Constraints using Bayesian Optimization. (arXiv:2203.13968v2 [physics.acc-ph] UPDATED)
169. MQDD: Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain. (arXiv:2203.14093v2 [cs.CL] UPDATED)
170. Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices. (arXiv:2203.14177v2 [cs.LG] UPDATED)
171. MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation. (arXiv:2203.14341v2 [eess.IV] UPDATED)
172. New insights into four-boson renormalization group limit cycles. (arXiv:2203.14597v2 [nucl-th] UPDATED)
173. MSR-NV: Neural Vocoder Using Multiple Sampling Rates. (arXiv:2109.13714v2 [eess.AS] CROSS LISTED)
174. Automated fault tree learning from continuous-valued sensor data: a case study on domestic heaters. (arXiv:2203.07374v1 [cs.LG] CROSS LISTED)
175. Cluster Algebras: Network Science and Machine Learning. (arXiv:2203.13847v1 [math.CO] CROSS LISTED)
176. Discovering Human-Object Interaction Concepts via Self-Compositional Learning. (arXiv:2203.14272v1 [cs.CV] CROSS LISTED)
177. On-the-fly Feature Based Speaker Adaptation for Dysarthric and Elderly Speech Recognition. (arXiv:2203.14593v1 [eess.AS] CROSS LISTED)
## cs.AI
---
**98** new papers in cs.AI:-) 
1. Requirements Elicitation in Cognitive Service for Recommendation. (arXiv:2203.14958v1 [cs.AI])
2. Domino: Discovering Systematic Errors with Cross-Modal Embeddings. (arXiv:2203.14960v1 [cs.LG])
3. Learning to segment fetal brain tissue from noisy annotations. (arXiv:2203.14962v1 [eess.IV])
4. Deep Learning and Artificial General Intelligence: Still a Long Way to Go. (arXiv:2203.14963v1 [cs.LG])
5. Error Correction Code Transformer. (arXiv:2203.14966v1 [cs.LG])
6. Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment. (arXiv:2203.14987v1 [cs.AI])
7. Few-Shot Object Detection with Fully Cross-Transformer. (arXiv:2203.15021v1 [cs.CV])
8. Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks. (arXiv:2203.15030v1 [cs.AI])
9. Learning Parameterized Task Structure for Generalization to Unseen Entities. (arXiv:2203.15034v1 [cs.LG])
10. AUC Maximization in the Era of Big Data and AI: A Survey. (arXiv:2203.15046v1 [cs.LG])
11. Cycle-Consistent Counterfactuals by Latent Transformations. (arXiv:2203.15064v1 [cs.CV])
12. User Driven Model Adjustment via Boolean Rule Explanations. (arXiv:2203.15071v1 [cs.AI])
13. Semantic Motion Correction Via Iterative Nonlinear Optimization and Animation. (arXiv:2203.15072v1 [cs.CV])
14. Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v1 [eess.AS])
15. LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models. (arXiv:2203.15099v1 [cs.AI])
16. Federated Named Entity Recognition. (arXiv:2203.15101v1 [cs.CL])
17. Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions. (arXiv:2203.15103v1 [cs.AI])
18. Domain Knowledge Driven Pseudo Labels for Interpretable Goal-Conditioned Interactive Trajectory Prediction. (arXiv:2203.15112v1 [cs.RO])
19. An Online Approach to Solve the Dynamic Vehicle Routing Problem with Stochastic Trip Requests for Paratransit Services. (arXiv:2203.15127v1 [cs.AI])
20. Separate What You Describe: Language-Queried Audio Source Separation. (arXiv:2203.15147v1 [eess.AS])
21. CMGAN: Conformer-based Metric GAN for Speech **Enhancement**. (arXiv:2203.15149v1 [cs.SD])
22. Learning to Synthesize Volumetric Meshes from Vision-based Tactile Imprints. (arXiv:2203.15155v1 [cs.RO])
23. Practical Aspects of Zero-Shot Learning. (arXiv:2203.15158v1 [cs.LG])
24. Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v1 [cs.CV])
25. Equivariance Allows Handling Multiple Nuisance Variables When Analyzing Pooled Neuroimaging Datasets. (arXiv:2203.15234v1 [cs.LG])
26. Enabling hand gesture customization on wrist-worn devices. (arXiv:2203.15239v1 [cs.HC])
27. Finding Structure and Causality in Linear Programs. (arXiv:2203.15274v1 [cs.AI])
28. A Multi-size Kernel based Adaptive Convolutional Neural Network for Bearing Fault Diagnosis. (arXiv:2203.15275v1 [eess.SP])
29. Accelerating Code Search with Deep Hashing and Code Classification. (arXiv:2203.15287v1 [cs.SE])
30. Agreement or Disagreement in Noise-tolerant Mutual Learning?. (arXiv:2203.15317v1 [cs.CV])
31. Evolving Multi-Label Fuzzy Classifier. (arXiv:2203.15318v1 [cs.LG])
32. Speech Emotion Recognition with Co-Attention based Multi-level Acoustic Information. (arXiv:2203.15326v1 [cs.SD])
33. Balanced Multimodal Learning via On-the-fly Gradient Modulation. (arXiv:2203.15332v1 [cs.CV])
34. Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v1 [cs.CV])
35. A Principle-based Ethical Assurance Argument for AI and Autonomous Systems. (arXiv:2203.15370v1 [cs.CY])
36. ReIL: A Framework for Reinforced Intervention-based Imitation Learning. (arXiv:2203.15390v1 [cs.RO])
37. Learning to act: a Reinforcement Learning approach to recommend the best next activities. (arXiv:2203.15398v1 [cs.AI])
38. AutoCoMet: Smart Neural Architecture Search via Co-Regulated Shaping Reinforcement. (arXiv:2203.15408v1 [cs.LG])
39. Quality Assurance of Generative Dialog Models in an Evolving Conversational Agent Used for Swedish Language Practice. (arXiv:2203.15414v1 [cs.SE])
40. Spatiotemporal Patterns in Neurobiology: An Overview for Future Artificial Intelligence. (arXiv:2203.15415v1 [q-bio.NC])
41. Neural representation of a time optimal, constant acceleration rendezvous. (arXiv:2203.15490v1 [astro-ph.EP])
42. Powerful Physical Adversarial Examples Against Practical Face Recognition Systems. (arXiv:2203.15498v1 [cs.CR])
43. Deep Learning for Encrypted Traffic Classification and Unknown Data Detection. (arXiv:2203.15501v1 [cs.CR])
44. Analysis of OODA Loop based on Adversarial for Complex Game Environments. (arXiv:2203.15502v1 [cs.AI])
45. Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning. (arXiv:2203.15506v1 [cs.CR])
46. Improving Contrastive Learning with Model Augmentation. (arXiv:2203.15508v1 [cs.LG])
47. Explaining random forest prediction through diverse rulesets. (arXiv:2203.15511v1 [cs.LG])
48. Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias. (arXiv:2203.15514v1 [cs.AI])
49. Rich Feature Construction for the Optimization-Generalization Dilemma. (arXiv:2203.15516v1 [cs.LG])
50. Collision-Free Navigation using Evolutionary Symmetrical Neural Networks. (arXiv:2203.15522v1 [cs.RO])
51. Treatment Learning Transformer for Noisy Image Classification. (arXiv:2203.15529v1 [cs.CV])
52. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v1 [cs.CV])
53. Modeling Users' Contextualized Page-wise Feedback for Click-Through Rate Prediction in E-commerce Search. (arXiv:2203.15542v1 [cs.IR])
54. Graph Neural Networks are Dynamic Programmers. (arXiv:2203.15544v1 [cs.LG])
55. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v1 [cs.CV])
56. Wildfire risk forecast: An optimizable fire danger index. (arXiv:2203.15558v1 [cs.LG])
57. Core Risk Minimization using Salient ImageNet. (arXiv:2203.15566v1 [cs.CV])
58. Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v1 [cs.LG])
59. Cross-Media Scientific Research Achievements Retrieval Based on Deep Language Model. (arXiv:2203.15595v1 [cs.IR])
60. Amortized Rejection Sampling in Universal Probabilistic Programming. (arXiv:1910.09056v3 [cs.LG] UPDATED)
61. AutonoML: Towards an Integrated Framework for Autonomous Machine Learning. (arXiv:2012.12600v2 [cs.LG] UPDATED)
62. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v4 [cs.CV] UPDATED)
63. Generative Adversarial Transformers. (arXiv:2103.01209v4 [cs.CV] UPDATED)
64. MogFace: Towards a Deeper Appreciation on Face Detection. (arXiv:2103.11139v5 [cs.CV] UPDATED)
65. RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition. (arXiv:2104.11934v2 [cs.CV] UPDATED)
66. A Generalized Projected Bellman Error for Off-policy Value Estimation in Reinforcement Learning. (arXiv:2104.13844v2 [cs.LG] UPDATED)
67. Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search. (arXiv:2108.01036v2 [cs.AI] UPDATED)
68. Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-Temporal Sparsity. (arXiv:2108.02297v4 [cs.AR] UPDATED)
69. MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction. (arXiv:2110.06651v2 [cs.CL] UPDATED)
70. Multimodal Dialogue Response Generation. (arXiv:2110.08515v2 [cs.CL] UPDATED)
71. Privacy in Open Search: A Review of Challenges and Solutions. (arXiv:2110.10720v3 [cs.CR] UPDATED)
72. Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v4 [cs.CV] UPDATED)
73. GCR: Gradient Coreset Based Replay Buffer Selection For Continual Learning. (arXiv:2111.11210v2 [cs.LG] UPDATED)
74. Universal Captioner: Inducing Content-Style Separation in Vision-and-Language Model Training. (arXiv:2111.12727v2 [cs.CV] UPDATED)
75. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. (arXiv:2111.13152v3 [cs.CV] UPDATED)
76. Instance-wise Occlusion and Depth Orders in Natural Scenes. (arXiv:2111.14562v3 [cs.CV] UPDATED)
77. Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v3 [cs.LG] UPDATED)
78. MonoScene: Monocular 3D Semantic Scene Completion. (arXiv:2112.00726v2 [cs.CV] UPDATED)
79. ICON: Implicit Clothed humans Obtained from Normals. (arXiv:2112.09127v2 [cs.CV] UPDATED)
80. Towards Relatable Explainable AI with the Perceptual Process. (arXiv:2112.14005v3 [cs.HC] UPDATED)
81. Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks. (arXiv:2201.06147v2 [cs.LG] UPDATED)
82. A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits. (arXiv:2202.01850v2 [stat.ML] UPDATED)
83. DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v2 [cs.CV] UPDATED)
84. Human-Aware Object Placement for Visual Environment Reconstruction. (arXiv:2203.03609v2 [cs.CV] UPDATED)
85. Adaptative Perturbation Patterns: Realistic Adversarial Learning for Robust Intrusion Detection. (arXiv:2203.04234v2 [cs.CR] UPDATED)
86. The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v2 [cs.LG] UPDATED)
87. Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v6 [cs.CV] UPDATED)
88. Implicit Feature Decoupling with Depthwise Quantization. (arXiv:2203.08080v2 [cs.CV] UPDATED)
89. Pushing the limits of raw waveform speaker recognition. (arXiv:2203.08488v2 [eess.AS] UPDATED)
90. A Feasibility Study of Answer-Agnostic Question Generation for Education. (arXiv:2203.08685v2 [cs.CL] UPDATED)
91. Probing Factually Grounded Content Transfer with Factual Ablation. (arXiv:2203.10133v2 [cs.CL] UPDATED)
92. Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v2 [cs.CV] UPDATED)
93. Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v3 [cs.CV] UPDATED)
94. MERLIN -- Malware Evasion with Reinforcement LearnINg. (arXiv:2203.12980v3 [cs.CR] UPDATED)
95. Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices. (arXiv:2203.14177v2 [cs.LG] UPDATED)
96. Affordance Transfer Learning for Human-Object Interaction Detection. (arXiv:2104.02867v2 [cs.CV] CROSS LISTED)
97. Automated fault tree learning from continuous-valued sensor data: a case study on domestic heaters. (arXiv:2203.07374v1 [cs.LG] CROSS LISTED)
98. On-the-fly Feature Based Speaker Adaptation for Dysarthric and Elderly Speech Recognition. (arXiv:2203.14593v1 [eess.AS] CROSS LISTED)

