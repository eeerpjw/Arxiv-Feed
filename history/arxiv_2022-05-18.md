# Your interest papers
---
## cs.CV
---
### Privacy **Enhancement** for Cloud-Based Few-Shot Learning. (arXiv:2205.07864v1 [cs.LG])
- Authors : Archit Parnami, Muhammad Usama, Liyue Fan, Minwoo Lee
- Link : [http://arxiv.org/abs/2205.07864](http://arxiv.org/abs/2205.07864)
> ABSTRACT  :  Requiring less data for accurate models, few-shot learning has shown robustness and generality in many application domains. However, deploying few-shot models in untrusted environments may inflict privacy concerns, e.g., attacks or adversaries that may breach the privacy of user-supplied data. This paper studies the privacy **enhancement** for the few-shot learning in an untrusted environment, e.g., the cloud, by establishing a novel privacy-preserved embedding space that preserves the privacy of data and maintains the accuracy of the model. We examine the impact of various image privacy methods such as blurring, pixelization, Gaussian noise, and differentially private pixelization (DP-Pix) on few-shot image classification and propose a method that learns privacy-preserved representation through the joint loss. The empirical results show how privacy-performance trade-off can be negotiated for privacy-enhanced few-shot learning.  
### Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography. (arXiv:2205.07888v1 [eess.IV])
- Authors : Emilien Valat, Katayoun Farrahi, Thomas Blumensath
- Link : [http://arxiv.org/abs/2205.07888](http://arxiv.org/abs/2205.07888)
> ABSTRACT  :  We address the problem of reconstructing X-Ray tomographic images from scarce measurements by interpolating missing acquisitions using a self-supervised approach. To do so, we train shallow neural networks to combine two neighbouring acquisitions into an estimated measurement at an intermediate angle. This procedure yields an enhanced sequence of measurements that can be reconstructed using standard methods, or further enhanced using regularisation approaches.    Unlike methods that improve the sequence of acquisitions using an initial deterministic interpolation followed by machine-learning **enhancement**, we focus on inferring one measurement at once. This allows the method to scale to 3D, the computation to be faster and crucially, the interpolation to be significantly better than the current methods, when they exist. We also establish that a sequence of measurements must be processed as such, rather than as an image or a volume. We do so by comparing interpolation and up-sampling methods, and find that the latter significantly under-perform.    We compare the performance of the proposed method against deterministic interpolation and up-sampling procedures and find that it outperforms them, even when used jointly with a state-of-the-art projection-data **enhancement** approach using machine-learning. These results are obtained for 2D and 3D imaging, on large biomedical datasets, in both projection space and image space.  
### A Linear Comb Filter for Event Flicker Removal. (arXiv:2205.08090v1 [cs.CV])
- Authors : Ziwei Wang, Dingran Yuan, Yonhon Ng, Robert Mahony
- Link : [http://arxiv.org/abs/2205.08090](http://arxiv.org/abs/2205.08090)
> ABSTRACT  :  Event cameras are bio-inspired sensors that capture per-pixel asynchronous intensity change rather than the synchronous absolute intensity frames captured by a classical camera sensor. Such cameras are ideal for robotics applications since they have high temporal resolution, **high dynamic range** and low latency. However, due to their high temporal resolution, event cameras are particularly sensitive to flicker such as from fluorescent or LED lights. During every cycle from bright to **dark**, pixels that image a flickering light source generate many events that provide little or no useful information for a robot, swamping the useful data in the scene. In this paper, we propose a novel linear filter to preprocess event data to remove unwanted flicker events from an event stream. The proposed algorithm achieves over 4.6 times relative improvement in the signal-to-noise ratio when compared to the raw event stream due to the effective removal of flicker from fluorescent lighting. Thus, it is ideally suited to robotics applications that operate in indoor settings or scenes illuminated by flickering light sources.  
### MulT: An End-to-End Multitask Learning Transformer. (arXiv:2205.08303v1 [cs.CV])
- Authors : Deblina Bhattacharjee, Tong Zhang, Mathieu Salzmann
- Link : [http://arxiv.org/abs/2205.08303](http://arxiv.org/abs/2205.08303)
> ABSTRACT  :  We propose an end-to-end Multitask Learning Transformer framework, named MulT, to simultaneously learn multiple high-level vision tasks, including depth estimation, semantic segmentation, reshading, surface normal estimation, 2D keypoint detection, and edge detection. Based on the **Swin** transformer model, our framework encodes the input image into a shared representation and makes predictions for each vision task using task-specific transformer-based decoder heads. At the heart of our approach is a shared attention mechanism modeling the dependencies across the tasks. We evaluate our model on several multitask benchmarks, showing that our MulT framework outperforms both the state-of-the art multitask convolutional neural network models and all the respective single task transformer models. Our experiments further highlight the benefits of sharing attention across all the tasks, and demonstrate that our MulT model is robust and generalizes well to new domains. Our project website is at https://ivrl.github.io/MulT/.  
### Vision Transformer Adapter for Dense Predictions. (arXiv:2205.08534v1 [cs.CV])
- Authors : Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao
- Link : [http://arxiv.org/abs/2205.08534](http://arxiv.org/abs/2205.08534)
> ABSTRACT  :  This work investigates a simple yet powerful adapter for Vision Transformer (ViT). Unlike recent visual transformers that introduce vision-specific inductive biases into their architectures, ViT achieves inferior performance on dense prediction tasks due to lacking prior information of images. To solve this issue, we propose a Vision Transformer Adapter (ViT-Adapter), which can remedy the defects of ViT and achieve comparable performance to vision-specific models by introducing inductive biases via an additional architecture. Specifically, the backbone in our framework is a vanilla transformer that can be pre-trained with multi-modal data. When fine-tuning on downstream tasks, a modality-specific adapter is used to introduce the data and tasks' prior information into the model, making it suitable for these tasks. We verify the effectiveness of our ViT-Adapter on multiple downstream tasks, including object detection, instance segmentation, and semantic segmentation. Notably, when using HTC++, our ViT-Adapter-L yields 60.1 box AP and 52.1 mask AP on COCO test-dev, surpassing **Swin**-L by 1.4 box AP and 1.0 mask AP. For semantic segmentation, our ViT-Adapter-L establishes a new state-of-the-art of 60.5 mIoU on ADE20K val, 0.6 points higher than **Swin**V2-G. We hope that the proposed ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research.  
### An Attack on Facial Soft-biometric Privacy **Enhancement**. (arXiv:2111.12405v2 [cs.CV] UPDATED)
- Authors : Christian Rathgeb, Pawel Drozdowski, Philipp Terh, Christoph Busch
- Link : [http://arxiv.org/abs/2111.12405](http://arxiv.org/abs/2111.12405)
> ABSTRACT  :  In the recent past, different researchers have proposed privacy-enhancing face recognition systems designed to conceal soft-biometric attributes at feature level. These works have reported impressive results, but generally did not consider specific attacks in their analysis of privacy protection. We introduce an attack on said schemes based on two observations: (1) highly similar facial representations usually originate from face images with similar soft-biometric attributes; (2) to achieve high recognition accuracy, robustness against intra-class variations within facial representations has to be retained in their privacy-enhanced versions. The presented attack only requires the privacy-enhancing algorithm as a black-box and a relatively small database of face images with annotated soft-biometric attributes. Firstly, an intercepted privacy-enhanced face representation is compared against the attacker's database. Subsequently, the unknown attribute is inferred from the attributes associated with the highest obtained similarity scores. In the experiments, the attack is applied against two state-of-the-art approaches. The attack is shown to circumvent the privacy **enhancement** to a considerable degree and is able to correctly classify gender with an accuracy of up to approximately 90%. Future works on privacy-enhancing face recognition are encouraged to include the proposed attack in evaluations on the privacy protection.  
### **Swin**BERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v3 [cs.CV] UPDATED)
- Authors : Kevin Lin, Linjie Li, Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, Lijuan Wang
- Link : [http://arxiv.org/abs/2111.13196](http://arxiv.org/abs/2111.13196)
> ABSTRACT  :  The canonical approach to video captioning dictates a caption generation model to learn from offline-extracted dense video features. These feature extractors usually operate on video frames sampled at a fixed frame rate and are often trained on image/video understanding tasks, without adaption to video captioning data. In this work, we present **Swin**BERT, an end-to-end transformer-based model for video captioning, which takes video frame patches directly as inputs, and outputs a natural language description. Instead of leveraging multiple 2D/3D feature extractors, our method adopts a video transformer to encode spatial-temporal representations that can adapt to variable lengths of video input without dedicated design for different frame rates. Based on this model architecture, we show that video captioning can benefit significantly from more densely sampled video frames as opposed to previous successes with sparsely sampled video frames for video-and-language understanding tasks (e.g., video question answering). Moreover, to avoid the inherent redundancy in consecutive video frames, we propose adaptively learning a sparse attention mask and optimizing it for task-specific performance improvement through better long-range video sequence modeling. Through extensive experiments on 5 video captioning datasets, we show that **Swin**BERT achieves across-the-board performance improvements over previous methods, often by a large margin. The learned sparse attention masks in addition push the limit to new state of the arts, and can be transferred between different video lengths and between different datasets. Code is available at https://github.com/microsoft/**Swin**BERT  
### RoVISQ: Reduction of Video Service Quality via Adversarial Attacks on Deep Learning-based Video Compression. (arXiv:2203.10183v2 [cs.CV] UPDATED)
- Authors : Woo Chang, Mojan Javaheripi, Seira Hidano, Farinaz Koushanfar
- Link : [http://arxiv.org/abs/2203.10183](http://arxiv.org/abs/2203.10183)
> ABSTRACT  :  Video compression plays a crucial role in video streaming and classification systems by maximizing the end-user quality of experience (QoE) at a given bandwidth budget. In this paper, we conduct the first systematic study for adversarial attacks on deep learning-based video compression and downstream classification systems. Our attack framework, dubbed RoVISQ, manipulates the Rate-Distortion (R-D) relationship of a video compression model to achieve one or both of the following goals: (1) increasing the network bandwidth, (2) degrading the video quality for end-users. We further devise new objectives for targeted and untargeted attacks to a downstream video classification service. Finally, we design an input-invariant perturbation that universally disrupts video compression and classification systems in **real time**. Unlike previously proposed attacks on video classification, our adversarial perturbations are the first to withstand compression. We empirically show the resilience of RoVISQ attacks against various defenses, i.e., adversarial training, video denoising, and JPEG compression. Our extensive experimental results on various video datasets show RoVISQ attacks deteriorate peak signal-to-noise ratio by up to 5.6dB and the bit-rate by up to 2.4 times while achieving over 90% attack success rate on a downstream classifier.  
## eess.IV
---
### Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography. (arXiv:2205.07888v1 [eess.IV])
- Authors : Emilien Valat, Katayoun Farrahi, Thomas Blumensath
- Link : [http://arxiv.org/abs/2205.07888](http://arxiv.org/abs/2205.07888)
> ABSTRACT  :  We address the problem of reconstructing X-Ray tomographic images from scarce measurements by interpolating missing acquisitions using a self-supervised approach. To do so, we train shallow neural networks to combine two neighbouring acquisitions into an estimated measurement at an intermediate angle. This procedure yields an enhanced sequence of measurements that can be reconstructed using standard methods, or further enhanced using regularisation approaches.    Unlike methods that improve the sequence of acquisitions using an initial deterministic interpolation followed by machine-learning **enhancement**, we focus on inferring one measurement at once. This allows the method to scale to 3D, the computation to be faster and crucially, the interpolation to be significantly better than the current methods, when they exist. We also establish that a sequence of measurements must be processed as such, rather than as an image or a volume. We do so by comparing interpolation and up-sampling methods, and find that the latter significantly under-perform.    We compare the performance of the proposed method against deterministic interpolation and up-sampling procedures and find that it outperforms them, even when used jointly with a state-of-the-art projection-data **enhancement** approach using machine-learning. These results are obtained for 2D and 3D imaging, on large biomedical datasets, in both projection space and image space.  
## cs.LG
---
### Heterogeneous Domain Adaptation with Adversarial Neural Representation Learning: Experiments on E-Commerce and Cybersecurity. (arXiv:2205.07853v1 [cs.LG])
- Authors : Mohammadreza Ebrahimi, Yidong Chai, Hao Helen, Hsinchun Chen
- Link : [http://arxiv.org/abs/2205.07853](http://arxiv.org/abs/2205.07853)
> ABSTRACT  :  Learning predictive models in new domains with scarce training data is a growing challenge in modern supervised learning scenarios. This incentivizes developing domain adaptation methods that leverage the knowledge in known domains (source) and adapt to new domains (target) with a different probability distribution. This becomes more challenging when the source and target domains are in heterogeneous feature spaces, known as heterogeneous domain adaptation (HDA). While most HDA methods utilize mathematical optimization to map source and target data to a common space, they suffer from low transferability. Neural representations have proven to be more transferable; however, they are mainly designed for homogeneous environments. Drawing on the theory of domain adaptation, we propose a novel framework, Heterogeneous Adversarial Neural Domain Adaptation (HANDA), to effectively maximize the transferability in heterogeneous environments. HANDA conducts feature and distribution alignment in a unified neural network architecture and achieves domain invariance through adversarial kernel learning. Three experiments were conducted to evaluate the performance against the state-of-the-art HDA methods on major image and text e-commerce benchmarks. HANDA shows statistically significant improvement in predictive performance. The practical utility of HANDA was shown in real-world **dark** web online markets. HANDA is an important step towards successful domain adaptation in e-commerce applications.  
### Predicting tacrolimus **exposure** in kidney transplanted patients using machine learning. (arXiv:2205.07858v1 [cs.LG])
- Authors : Inga Str
- Link : [http://arxiv.org/abs/2205.07858](http://arxiv.org/abs/2205.07858)
> ABSTRACT  :  Tacrolimus is one of the cornerstone immunosuppressive drugs in most transplantation centers worldwide following solid organ transplantation. Therapeutic drug monitoring of tacrolimus is necessary in order to avoid rejection of the transplanted organ or severe side effects. However, finding the right dose for a given patient is challenging, even for experienced clinicians. Consequently, a tool that can accurately estimate the drug **exposure** for individual dose adaptions would be of high clinical value. In this work, we propose a new technique using machine learning to estimate the tacrolimus **exposure** in kidney transplant recipients. Our models achieve predictive errors that are at the same level as an established population pharmacokinetic model, but are faster to develop and require less knowledge about the pharmacokinetic properties of the drug.  
### Privacy **Enhancement** for Cloud-Based Few-Shot Learning. (arXiv:2205.07864v1 [cs.LG])
- Authors : Archit Parnami, Muhammad Usama, Liyue Fan, Minwoo Lee
- Link : [http://arxiv.org/abs/2205.07864](http://arxiv.org/abs/2205.07864)
> ABSTRACT  :  Requiring less data for accurate models, few-shot learning has shown robustness and generality in many application domains. However, deploying few-shot models in untrusted environments may inflict privacy concerns, e.g., attacks or adversaries that may breach the privacy of user-supplied data. This paper studies the privacy **enhancement** for the few-shot learning in an untrusted environment, e.g., the cloud, by establishing a novel privacy-preserved embedding space that preserves the privacy of data and maintains the accuracy of the model. We examine the impact of various image privacy methods such as blurring, pixelization, Gaussian noise, and differentially private pixelization (DP-Pix) on few-shot image classification and propose a method that learns privacy-preserved representation through the joint loss. The empirical results show how privacy-performance trade-off can be negotiated for privacy-enhanced few-shot learning.  
### Learning Car Speed Using Inertial Sensors. (arXiv:2205.07883v1 [cs.LG])
- Authors : Maxim Freydin, Barak Or
- Link : [http://arxiv.org/abs/2205.07883](http://arxiv.org/abs/2205.07883)
> ABSTRACT  :  A deep neural network (DNN) is trained to estimate the speed of a car driving in an urban area using as input a stream of measurements from a low-cost six-axis inertial measurement unit (IMU). Three hours of data was collected by driving through the city of Ashdod, Israel in a car equipped with a global navigation satellite system (GNSS) **real time** kinematic (RTK) positioning device and a synchronized IMU. Ground truth labels for the car speed were calculated using the position measurements obtained at the high rate of 50 [Hz]. A DNN architecture with long short-term memory layers is proposed to enable high-frequency speed estimation that accounts for previous inputs history and the nonlinear relation between speed, acceleration, and angular velocity. A simplified aided dead reckoning localization scheme is formulated to assess the trained model which provides the speed pseudo-measurement. The trained model is shown to substantially improve the position accuracy during a 4 minutes drive without the use of GNSS position updates.  
### Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography. (arXiv:2205.07888v1 [eess.IV])
- Authors : Emilien Valat, Katayoun Farrahi, Thomas Blumensath
- Link : [http://arxiv.org/abs/2205.07888](http://arxiv.org/abs/2205.07888)
> ABSTRACT  :  We address the problem of reconstructing X-Ray tomographic images from scarce measurements by interpolating missing acquisitions using a self-supervised approach. To do so, we train shallow neural networks to combine two neighbouring acquisitions into an estimated measurement at an intermediate angle. This procedure yields an enhanced sequence of measurements that can be reconstructed using standard methods, or further enhanced using regularisation approaches.    Unlike methods that improve the sequence of acquisitions using an initial deterministic interpolation followed by machine-learning **enhancement**, we focus on inferring one measurement at once. This allows the method to scale to 3D, the computation to be faster and crucially, the interpolation to be significantly better than the current methods, when they exist. We also establish that a sequence of measurements must be processed as such, rather than as an image or a volume. We do so by comparing interpolation and up-sampling methods, and find that the latter significantly under-perform.    We compare the performance of the proposed method against deterministic interpolation and up-sampling procedures and find that it outperforms them, even when used jointly with a state-of-the-art projection-data **enhancement** approach using machine-learning. These results are obtained for 2D and 3D imaging, on large biomedical datasets, in both projection space and image space.  
### On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (arXiv:2205.07890v1 [cs.LG])
- Authors : Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad, Jonas Guan, Nicolas Papernot
- Link : [http://arxiv.org/abs/2205.07890](http://arxiv.org/abs/2205.07890)
> ABSTRACT  :  Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their **exposure** over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.  
### Fast and realistic large-scale structure from machine-learning-augmented random field simulations. (arXiv:2205.07898v1 [astro-ph.CO])
- Authors : Davide Piras, Benjamin Joachimi, Francisco Villaescusa
- Link : [http://arxiv.org/abs/2205.07898](http://arxiv.org/abs/2205.07898)
> ABSTRACT  :  Producing thousands of simulations of the **dark** matter distribution in the Universe with increasing precision is a challenging but critical task to facilitate the exploitation of current and forthcoming cosmological surveys. Many inexpensive substitutes to full $N$-body simulations have been proposed, even though they often fail to reproduce the statistics of the smaller, non-linear scales. Among these alternatives, a common approximation is represented by the lognormal distribution, which comes with its own limitations as well, while being extremely fast to compute even for high-resolution density fields. In this work, we train a machine learning model to transform projected lognormal **dark** matter density fields to more realistic **dark** matter maps, as obtained from full $N$-body simulations. We detail the procedure that we follow to generate highly correlated pairs of lognormal and simulated maps, which we use as our training data, exploiting the information of the Fourier phases. We demonstrate the performance of our model comparing various statistical tests with different field resolutions, redshifts and cosmological parameters, proving its robustness and explaining its current limitations. The augmented lognormal random fields reproduce the power spectrum up to wavenumbers of $1 \ h \ \rm{Mpc}^{-1}$, the bispectrum and the peak counts within 10%, and always within the error bars, of the fiducial target simulations. Finally, we describe how we plan to integrate our proposed model with existing tools to yield more accurate spherical random fields for weak lensing analysis, going beyond the lognormal approximation.  
### $\mathscr{H}$-Consistency Estimation Error of Surrogate Loss Minimizers. (arXiv:2205.08017v1 [cs.LG])
- Authors : Pranjal Awasthi, Anqi Mao, Mehryar Mohri, Yutao Zhong
- Link : [http://arxiv.org/abs/2205.08017](http://arxiv.org/abs/2205.08017)
> ABSTRACT  :  We present a detailed study of estimation errors in terms of surrogate loss estimation errors. We refer to such guarantees as $\mathscr{H}$-consistency estimation error bounds, since they account for the hypothesis set $\mathscr{H}$ adopted. These guarantees are significantly stronger than $\mathscr{H}$-calibration or $\mathscr{H}$-consistency. They are also more informative than similar excess error bounds derived in the literature, when $\mathscr{H}$ is the family of all measurable functions. We prove general theorems providing such guarantees, for both the distribution-dependent and distribution-independent settings. We show that our bounds are tight, modulo a convexity assumption. We also show that previous excess error bounds can be recovered as special cases of our general results.    We then present a series of explicit bounds in the case of the zero-one loss, with multiple choices of the surrogate loss and for both the family of linear functions and neural networks with one hidden-layer. We further prove more favorable distribution-dependent guarantees in that case. We also present a series of explicit bounds in the case of the adversarial loss, with surrogate losses based on the supremum of the $\rho$-margin, hinge or sigmoid loss and for the same two general hypothesis sets. Here too, we prove several **enhancement**s of these guarantees under natural distributional assumptions. Finally, we report the results of simulations illustrating our bounds and their tightness.  
## cs.AI
---
### Heterogeneous Domain Adaptation with Adversarial Neural Representation Learning: Experiments on E-Commerce and Cybersecurity. (arXiv:2205.07853v1 [cs.LG])
- Authors : Mohammadreza Ebrahimi, Yidong Chai, Hao Helen, Hsinchun Chen
- Link : [http://arxiv.org/abs/2205.07853](http://arxiv.org/abs/2205.07853)
> ABSTRACT  :  Learning predictive models in new domains with scarce training data is a growing challenge in modern supervised learning scenarios. This incentivizes developing domain adaptation methods that leverage the knowledge in known domains (source) and adapt to new domains (target) with a different probability distribution. This becomes more challenging when the source and target domains are in heterogeneous feature spaces, known as heterogeneous domain adaptation (HDA). While most HDA methods utilize mathematical optimization to map source and target data to a common space, they suffer from low transferability. Neural representations have proven to be more transferable; however, they are mainly designed for homogeneous environments. Drawing on the theory of domain adaptation, we propose a novel framework, Heterogeneous Adversarial Neural Domain Adaptation (HANDA), to effectively maximize the transferability in heterogeneous environments. HANDA conducts feature and distribution alignment in a unified neural network architecture and achieves domain invariance through adversarial kernel learning. Three experiments were conducted to evaluate the performance against the state-of-the-art HDA methods on major image and text e-commerce benchmarks. HANDA shows statistically significant improvement in predictive performance. The practical utility of HANDA was shown in real-world **dark** web online markets. HANDA is an important step towards successful domain adaptation in e-commerce applications.  
### Predicting tacrolimus **exposure** in kidney transplanted patients using machine learning. (arXiv:2205.07858v1 [cs.LG])
- Authors : Inga Str
- Link : [http://arxiv.org/abs/2205.07858](http://arxiv.org/abs/2205.07858)
> ABSTRACT  :  Tacrolimus is one of the cornerstone immunosuppressive drugs in most transplantation centers worldwide following solid organ transplantation. Therapeutic drug monitoring of tacrolimus is necessary in order to avoid rejection of the transplanted organ or severe side effects. However, finding the right dose for a given patient is challenging, even for experienced clinicians. Consequently, a tool that can accurately estimate the drug **exposure** for individual dose adaptions would be of high clinical value. In this work, we propose a new technique using machine learning to estimate the tacrolimus **exposure** in kidney transplant recipients. Our models achieve predictive errors that are at the same level as an established population pharmacokinetic model, but are faster to develop and require less knowledge about the pharmacokinetic properties of the drug.  
### Learning Car Speed Using Inertial Sensors. (arXiv:2205.07883v1 [cs.LG])
- Authors : Maxim Freydin, Barak Or
- Link : [http://arxiv.org/abs/2205.07883](http://arxiv.org/abs/2205.07883)
> ABSTRACT  :  A deep neural network (DNN) is trained to estimate the speed of a car driving in an urban area using as input a stream of measurements from a low-cost six-axis inertial measurement unit (IMU). Three hours of data was collected by driving through the city of Ashdod, Israel in a car equipped with a global navigation satellite system (GNSS) **real time** kinematic (RTK) positioning device and a synchronized IMU. Ground truth labels for the car speed were calculated using the position measurements obtained at the high rate of 50 [Hz]. A DNN architecture with long short-term memory layers is proposed to enable high-frequency speed estimation that accounts for previous inputs history and the nonlinear relation between speed, acceleration, and angular velocity. A simplified aided dead reckoning localization scheme is formulated to assess the trained model which provides the speed pseudo-measurement. The trained model is shown to substantially improve the position accuracy during a 4 minutes drive without the use of GNSS position updates.  
### On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (arXiv:2205.07890v1 [cs.LG])
- Authors : Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad, Jonas Guan, Nicolas Papernot
- Link : [http://arxiv.org/abs/2205.07890](http://arxiv.org/abs/2205.07890)
> ABSTRACT  :  Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their **exposure** over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.  
# Paper List
---
## cs.CV
---
**80** new papers in cs.CV:-) 
1. Functional2Structural: Cross-Modality Brain Networks Representation Learning. (arXiv:2205.07854v1 [cs.LG])
2. Privacy **Enhancement** for Cloud-Based Few-Shot Learning. (arXiv:2205.07864v1 [cs.LG])
3. Primal-Dual UNet for Sparse View Cone Beam Computed Tomography Volume Reconstruction. (arXiv:2205.07866v1 [eess.IV])
4. Revisiting the Updates of a Pre-trained Model for Few-shot Learning. (arXiv:2205.07874v1 [cs.LG])
5. Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography. (arXiv:2205.07888v1 [eess.IV])
6. Deep Apprenticeship Learning for Playing Games. (arXiv:2205.07959v1 [cs.LG])
7. Sparse Visual Counterfactual Explanations in Image Space. (arXiv:2205.07972v1 [cs.CV])
8. TOCH: Spatio-Temporal Object Correspondence to Hand for Motion Refinement. (arXiv:2205.07982v1 [cs.CV])
9. Test-Time Adaptation with Shape Moments for Image Segmentation. (arXiv:2205.07983v1 [cs.CV])
10. Lost in Compression: the Impact of Lossy Image Compression on Variable Size Object Detection within Infrared Imagery. (arXiv:2205.08002v1 [cs.CV])
11. Continual learning on 3D point clouds with random compressed rehearsal. (arXiv:2205.08013v1 [cs.LG])
12. Detection and Physical Interaction with Deformable Linear Objects. (arXiv:2205.08041v1 [cs.RO])
13. Collaborative Attention Memory Network for Video Object Segmentation. (arXiv:2205.08075v1 [cs.CV])
14. Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers. (arXiv:2205.08078v1 [cs.LG])
15. Region-Aware Metric Learning for Open World Semantic Segmentation via Meta-Channel Aggregation. (arXiv:2205.08083v1 [cs.CV])
16. Efficient Stereo Depth Estimation for Pseudo LiDAR: A Self-Supervised Approach Based on Multi-Input ResNet Encoder. (arXiv:2205.08089v1 [cs.CV])
17. A Linear Comb Filter for Event Flicker Removal. (arXiv:2205.08090v1 [cs.CV])
18. MATrIX -- Modality-Aware Transformer for Information eXtraction. (arXiv:2205.08094v1 [cs.CV])
19. Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey. (arXiv:2205.08099v1 [cs.LG])
20. Computerized Tomography Pulmonary Angiography Image Simulation using Cycle Generative Adversarial Network from Chest CT imaging in Pulmonary Embolism Patients. (arXiv:2205.08106v1 [eess.IV])
21. Using artificial intelligence to detect chest X-rays with no significant findings in a primary health care setting in Oulu, Finland. (arXiv:2205.08123v1 [eess.IV])
22. Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v1 [cs.RO])
23. Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A Comparative Study with Doctors' Manual Segmentation. (arXiv:2205.08143v1 [eess.IV])
24. Pairwise Comparison Network for Remote Sensing Scene Classification. (arXiv:2205.08147v1 [cs.CV])
25. Gender and Racial Bias in Visual Question Answering Datasets. (arXiv:2205.08148v1 [cs.CV])
26. UnPWC-SVDLO: Multi-SVD on PointPWC for Unsupervised Lidar Odometry. (arXiv:2205.08150v1 [cs.CV])
27. Uncertainty-based Network for Few-shot Image Classification. (arXiv:2205.08157v1 [cs.CV])
28. CellTypeGraph: A New Geometric Computer Vision Benchmark. (arXiv:2205.08166v1 [cs.CV])
29. DynPL-SVO: A New Method Using Point and Line Features for Stereo Visual Odometry in Dynamic Scenes. (arXiv:2205.08207v1 [cs.CV])
30. blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v1 [cs.CV])
31. CAS-Net: Conditional Atlas Generation and Brain Segmentation for Fetal MRI. (arXiv:2205.08239v1 [eess.IV])
32. Learnable Optimal Sequential Grouping for Video Scene Detection. (arXiv:2205.08249v1 [cs.CV])
33. Detection Masking for Improved OCR on Noisy Documents. (arXiv:2205.08257v1 [cs.CV])
34. MulT: An End-to-End Multitask Learning Transformer. (arXiv:2205.08303v1 [cs.CV])
35. Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation. (arXiv:2205.08316v1 [cs.RO])
36. Unified Interactive Image Matting. (arXiv:2205.08324v1 [cs.CV])
37. GraphMapper: Efficient Visual Navigation by Scene Graph Generation. (arXiv:2205.08325v1 [cs.CV])
38. Deep Supervised Information Bottleneck Hashing for Cross-modal Retrieval based Computer-aided Diagnosis. (arXiv:2205.08365v1 [cs.LG])
39. Automatic Velocity Picking Using Unsupervised Ensemble Learning. (arXiv:2205.08372v1 [cs.LG])
40. Bias and Fairness on Multimodal Emotion Detection Algorithms. (arXiv:2205.08383v1 [cs.LG])
41. HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images. (arXiv:2205.08390v1 [eess.IV])
42. Semi-Supervised Building Footprint Generation with Feature and Output Consistency Training. (arXiv:2205.08416v1 [cs.CV])
43. Conditional Visual Servoing for Multi-Step Tasks. (arXiv:2205.08441v1 [cs.RO])
44. Application of Graph Based Features in Computer Aided Diagnosis for Histopathological Image Classification of Gastric Cancer. (arXiv:2205.08467v1 [eess.IV])
45. ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation. (arXiv:2205.08473v1 [cs.CV])
46. A CLIP-Hitchhiker's Guide to Long Video Retrieval. (arXiv:2205.08508v1 [cs.CV])
47. Unsupervised Segmentation in Real-World Images via Spelke Object Inference. (arXiv:2205.08515v1 [cs.CV])
48. Do Neural Networks Compress Manifolds Optimally?. (arXiv:2205.08518v1 [cs.IT])
49. Self-supervised Neural Articulated Shape and Appearance Models. (arXiv:2205.08525v1 [cs.CV])
50. Vision Transformer Adapter for Dense Predictions. (arXiv:2205.08534v1 [cs.CV])
51. AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. (arXiv:2205.08535v1 [cs.CV])
52. Disentangling Visual Embeddings for Attributes and Objects. (arXiv:2205.08536v1 [cs.CV])
53. Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v8 [eess.SP] UPDATED)
54. Sparsely Activated Networks. (arXiv:1907.06592v9 [cs.LG] UPDATED)
55. Occluded Video Instance Segmentation: A Benchmark. (arXiv:2102.01558v6 [cs.CV] UPDATED)
56. Projection: A Mechanism for Human-like Reasoning in Artificial Intelligence. (arXiv:2103.13512v2 [cs.AI] UPDATED)
57. Weakly-supervised 3D Human Pose Estimation with Cross-view U-shaped Graph Convolutional Network. (arXiv:2105.10882v2 [cs.CV] UPDATED)
58. Deep Learning for Face Anti-Spoofing: A Survey. (arXiv:2106.14948v2 [cs.CV] UPDATED)
59. Source-Free Domain Adaptation for Image Segmentation. (arXiv:2108.03152v2 [cs.CV] UPDATED)
60. Learning Meta Pattern for Face Anti-Spoofing. (arXiv:2110.06753v2 [cs.CV] UPDATED)
61. Finding Strong Gravitational Lenses Through Self-Attention. (arXiv:2110.09202v3 [cs.CV] UPDATED)
62. Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v3 [cs.CV] UPDATED)
63. An Attack on Facial Soft-biometric Privacy **Enhancement**. (arXiv:2111.12405v2 [cs.CV] UPDATED)
64. **Swin**BERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v3 [cs.CV] UPDATED)
65. Unsupervised Domain Generalization by Learning a Bridge Across Domains. (arXiv:2112.02300v2 [cs.CV] UPDATED)
66. Improving Unsupervised Stain-To-Stain Translation using Self-Supervision and Meta-Learning. (arXiv:2112.08837v2 [eess.IV] UPDATED)
67. V-LinkNet: Learning Contextual Inpainting Across Latent Space of Generative Adversarial Network. (arXiv:2201.00323v2 [cs.CV] UPDATED)
68. A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v3 [cs.CV] UPDATED)
69. TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images. (arXiv:2201.12785v3 [eess.IV] UPDATED)
70. A Developmentally-Inspired Examination of Shape versus Texture Bias in Machines. (arXiv:2202.08340v2 [cs.CV] UPDATED)
71. Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks. (arXiv:2202.11099v3 [physics.flu-dyn] UPDATED)
72. RoVISQ: Reduction of Video Service Quality via Adversarial Attacks on Deep Learning-based Video Compression. (arXiv:2203.10183v2 [cs.CV] UPDATED)
73. Modality-Balanced Embedding for Video Retrieval. (arXiv:2204.08182v2 [cs.CV] UPDATED)
74. Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v2 [cs.CV] UPDATED)
75. The 6th AI City Challenge. (arXiv:2204.10380v3 [cs.CV] UPDATED)
76. Sequencer: Deep LSTM for Image Classification. (arXiv:2205.01972v2 [cs.CV] UPDATED)
77. Accelerating the Training of Video Super-Resolution Models. (arXiv:2205.05069v2 [cs.CV] UPDATED)
78. Deep Depth Completion: A Survey. (arXiv:2205.05335v2 [cs.CV] UPDATED)
79. PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning. (arXiv:2205.06401v2 [cs.CR] UPDATED)
80. An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage Detection Competition. (arXiv:2205.07556v2 [cs.CV] UPDATED)
## eess.IV
---
**23** new papers in eess.IV:-) 
1. Functional2Structural: Cross-Modality Brain Networks Representation Learning. (arXiv:2205.07854v1 [cs.LG])
2. Primal-Dual UNet for Sparse View Cone Beam Computed Tomography Volume Reconstruction. (arXiv:2205.07866v1 [eess.IV])
3. Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography. (arXiv:2205.07888v1 [eess.IV])
4. Joint cardiac $T_1$ mapping and cardiac function estimation using a deep manifold framework. (arXiv:2205.07994v1 [eess.IV])
5. Perceptual Evaluation on Audio-visual Dataset of 360 Content. (arXiv:2205.08007v1 [cs.MM])
6. Detection and Physical Interaction with Deformable Linear Objects. (arXiv:2205.08041v1 [cs.RO])
7. Classification of anatomic structures in head and neck by CT-based radiomics. (arXiv:2205.08054v1 [q-bio.QM])
8. Computerized Tomography Pulmonary Angiography Image Simulation using Cycle Generative Adversarial Network from Chest CT imaging in Pulmonary Embolism Patients. (arXiv:2205.08106v1 [eess.IV])
9. Using artificial intelligence to detect chest X-rays with no significant findings in a primary health care setting in Oulu, Finland. (arXiv:2205.08123v1 [eess.IV])
10. Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A Comparative Study with Doctors' Manual Segmentation. (arXiv:2205.08143v1 [eess.IV])
11. blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v1 [cs.CV])
12. Initial non-invasive in vivo sensing of the lung using time domain diffuse optics. (arXiv:2205.08211v1 [physics.med-ph])
13. CAS-Net: Conditional Atlas Generation and Brain Segmentation for Fetal MRI. (arXiv:2205.08239v1 [eess.IV])
14. Multiscale reconstruction of porous media based on multiple dictionaries learning. (arXiv:2205.08278v1 [eess.IV])
15. Deep Supervised Information Bottleneck Hashing for Cross-modal Retrieval based Computer-aided Diagnosis. (arXiv:2205.08365v1 [cs.LG])
16. HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images. (arXiv:2205.08390v1 [eess.IV])
17. Application of Graph Based Features in Computer Aided Diagnosis for Histopathological Image Classification of Gastric Cancer. (arXiv:2205.08467v1 [eess.IV])
18. Assimilation of SAR-derived Flood Observations for Improving Fluvial Flood Forecast. (arXiv:2205.08471v1 [eess.IV])
19. Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v8 [eess.SP] UPDATED)
20. Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans. (arXiv:2009.06412v6 [eess.IV] UPDATED)
21. A speckle filter for Sentinel-1 SAR Ground Range Detected data based on Residual Convolutional Neural Networks. (arXiv:2104.09350v2 [cs.AI] UPDATED)
22. Improving Unsupervised Stain-To-Stain Translation using Self-Supervision and Meta-Learning. (arXiv:2112.08837v2 [eess.IV] UPDATED)
23. TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images. (arXiv:2201.12785v3 [eess.IV] UPDATED)
## cs.LG
---
**181** new papers in cs.LG:-) 
1. ST-ExpertNet: A Deep Expert Framework for Traffic Prediction. (arXiv:2205.07851v1 [cs.LG])
2. REMuS-GNN: A Rotation-Equivariant Model for Simulating Continuum Dynamics. (arXiv:2205.07852v1 [cs.LG])
3. Heterogeneous Domain Adaptation with Adversarial Neural Representation Learning: Experiments on E-Commerce and Cybersecurity. (arXiv:2205.07853v1 [cs.LG])
4. Functional2Structural: Cross-Modality Brain Networks Representation Learning. (arXiv:2205.07854v1 [cs.LG])
5. Decentral and Incentivized Federated Learning Frameworks: A Systematic Literature Review. (arXiv:2205.07855v1 [cs.LG])
6. Impact of Learning Rate on Noise Resistant Property of Deep Learning Models. (arXiv:2205.07856v1 [cs.LG])
7. Neural Program Synthesis with Query. (arXiv:2205.07857v1 [cs.LG])
8. Predicting tacrolimus **exposure** in kidney transplanted patients using machine learning. (arXiv:2205.07858v1 [cs.LG])
9. Btech thesis report on adversarial attack detection and purification of adverserially attacked images. (arXiv:2205.07859v1 [cs.LG])
10. AdaCap: Adaptive Capacity control for Feed-Forward Neural Networks. (arXiv:2205.07860v1 [cs.LG])
11. Depression Diagnosis and Forecast based on Mobile Phone Sensor Data. (arXiv:2205.07861v1 [cs.LG])
12. A Safety Assurable Human-Inspired Perception Architecture. (arXiv:2205.07862v1 [cs.LG])
13. Quality versus speed in energy demand prediction for district heating systems. (arXiv:2205.07863v1 [cs.LG])
14. Privacy **Enhancement** for Cloud-Based Few-Shot Learning. (arXiv:2205.07864v1 [cs.LG])
15. Simple Contrastive Graph Clustering. (arXiv:2205.07865v1 [cs.LG])
16. Primal-Dual UNet for Sparse View Cone Beam Computed Tomography Volume Reconstruction. (arXiv:2205.07866v1 [eess.IV])
17. Feature and Instance Joint Selection: A Reinforcement Learning Perspective. (arXiv:2205.07867v1 [cs.LG])
18. Minimal Neural Network Models for Permutation Invariant Agents. (arXiv:2205.07868v1 [cs.LG])
19. Near out-of-distribution detection for low-resolution radar micro-Doppler signatures. (arXiv:2205.07869v1 [eess.SP])
20. Unsupervised Driving Behavior Analysis using Representation Learning and Exploiting Group-based Training. (arXiv:2205.07870v1 [cs.LG])
21. Mondrian Forest for Data Stream Classification Under Memory Constraints. (arXiv:2205.07871v1 [cs.LG])
22. ScAN: Suicide Attempt and Ideation Events Dataset. (arXiv:2205.07872v1 [cs.LG])
23. Revisiting the Updates of a Pre-trained Model for Few-shot Learning. (arXiv:2205.07874v1 [cs.LG])
24. A Comprehensive Survey on Model Quantization for Deep Neural Networks. (arXiv:2205.07877v1 [cs.LG])
25. A Note on the Chernoff Bound for Random Variables in the Unit Interval. (arXiv:2205.07880v1 [stat.ML])
26. Developing patient-driven artificial intelligence based on personal rankings of care decision making steps. (arXiv:2205.07881v1 [cs.HC])
27. Learning Car Speed Using Inertial Sensors. (arXiv:2205.07883v1 [cs.LG])
28. Enforcing KL Regularization in General Tsallis Entropy Reinforcement Learning via Advantage Learning. (arXiv:2205.07885v1 [cs.LG])
29. An Empirical Investigation of Representation Learning for Imitation. (arXiv:2205.07886v1 [cs.LG])
30. Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography. (arXiv:2205.07888v1 [eess.IV])
31. On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (arXiv:2205.07890v1 [cs.LG])
32. Fast and realistic large-scale structure from machine-learning-augmented random field simulations. (arXiv:2205.07898v1 [astro-ph.CO])
33. Fat-Tailed Variational Inference with Anisotropic Tail Adaptive Flows. (arXiv:2205.07918v1 [stat.ML])
34. An Extension to Basis-Hypervectors for Learning from Circular Data in Hyperdimensional Computing. (arXiv:2205.07920v1 [cs.LG])
35. Distributed Feature Selection for High-dimensional Additive Models. (arXiv:2205.07932v1 [cs.LG])
36. Constructing Trajectory and Predicting Estimated Time of Arrival for Long Distance Travelling Vessels: A Probability Density-based Scanning Approach. (arXiv:2205.07945v1 [eess.SY])
37. Application of multilayer perceptron with data augmentation in nuclear physics. (arXiv:2205.07953v1 [cs.LG])
38. Deep Apprenticeship Learning for Playing Games. (arXiv:2205.07959v1 [cs.LG])
39. An Exponentially Increasing Step-size for Parameter Estimation in Statistical Models. (arXiv:2205.07999v1 [stat.ML])
40. CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction. (arXiv:2205.08012v1 [cs.CL])
41. Continual learning on 3D point clouds with random compressed rehearsal. (arXiv:2205.08013v1 [cs.LG])
42. $\mathscr{H}$-Consistency Estimation Error of Surrogate Loss Minimizers. (arXiv:2205.08017v1 [cs.LG])
43. Partial Product Aware Machine Learning on DNA-Encoded Libraries. (arXiv:2205.08020v1 [cs.LG])
44. Automatic Error Classification and Root Cause Determination while Replaying Recorded Workload Data at SAP HANA. (arXiv:2205.08029v1 [cs.SE])
45. On Algebraic Constructions of Neural Networks with Small Weights. (arXiv:2205.08032v1 [cs.CC])
46. Using Embeddings for Causal Estimation of Peer Influence in Social Networks. (arXiv:2205.08033v1 [cs.SI])
47. DeepSim: A Reinforcement Learning Environment Build Toolkit for ROS and Gazebo. (arXiv:2205.08034v1 [cs.LG])
48. Explainable and Optimally Configured Artificial Neural Networks for Attack Detection in Smart Homes. (arXiv:2205.08043v1 [cs.CR])
49. Shape complexity in cluster analysis. (arXiv:2205.08046v1 [cs.LG])
50. Perfect Spectral Clustering with Discrete Covariates. (arXiv:2205.08047v1 [stat.ML])
51. HelixADMET: a robust and endpoint extensible ADMET system incorporating self-supervised knowledge transfer. (arXiv:2205.08055v1 [q-bio.BM])
52. "What makes a question inquisitive?" A Study on Type-Controlled Inquisitive Question Generation. (arXiv:2205.08056v1 [cs.CL])
53. Robust Perception Architecture Design for Automotive Cyber-Physical Systems. (arXiv:2205.08067v1 [cs.LG])
54. A Framework for CSI-Based Indoor Localization with 1D Convolutional Neural Networks. (arXiv:2205.08068v1 [eess.SP])
55. Multi-Head Attention Neural Network for Smartphone Invariant Indoor Localization. (arXiv:2205.08069v1 [eess.SP])
56. A Survey on Machine Learning for Geo-Distributed Cloud Data Center Management. (arXiv:2205.08072v1 [cs.DC])
57. Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers. (arXiv:2205.08078v1 [cs.LG])
58. Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher. (arXiv:2205.08096v1 [cs.LG])
59. Can We Do Better Than Random Start? The Power of Data Outsourcing. (arXiv:2205.08098v1 [cs.LG])
60. Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey. (arXiv:2205.08099v1 [cs.LG])
61. Computerized Tomography Pulmonary Angiography Image Simulation using Cycle Generative Adversarial Network from Chest CT imaging in Pulmonary Embolism Patients. (arXiv:2205.08106v1 [eess.IV])
62. Forecasting Solar Power Generation on the basis of Predictive and Corrective Maintenance Activities. (arXiv:2205.08109v1 [cs.LG])
63. Fast and Provably Convergent Algorithms for Gromov-Wasserstein in Graph Learning. (arXiv:2205.08115v1 [cs.LG])
64. ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks. (arXiv:2205.08119v1 [cs.LG])
65. Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v1 [cs.RO])
66. Latent Variable Method Demonstrator -- Software for Understanding Multivariate Data Analytics Algorithms. (arXiv:2205.08132v1 [stat.ML])
67. Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A Comparative Study with Doctors' Manual Segmentation. (arXiv:2205.08143v1 [eess.IV])
68. Uncertainty-based Network for Few-shot Image Classification. (arXiv:2205.08157v1 [cs.CV])
69. CellTypeGraph: A New Geometric Computer Vision Benchmark. (arXiv:2205.08166v1 [cs.CV])
70. On the Convergence of Policy in Unregularized Generalized Policy Mirror Descent. (arXiv:2205.08176v1 [math.OC])
71. Active learning of causal probability trees. (arXiv:2205.08178v1 [cs.LG])
72. SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation. (arXiv:2205.08180v1 [cs.CL])
73. SKILL: Structured Knowledge Infusion for Large Language Models. (arXiv:2205.08184v1 [cs.CL])
74. Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility. (arXiv:2205.08187v1 [stat.ML])
75. Automatic Acquisition of a Repertoire of Diverse Grasping Trajectories through Behavior Shaping and Novelty Search. (arXiv:2205.08189v1 [cs.RO])
76. Moral reinforcement learning using actual causation. (arXiv:2205.08192v1 [cs.LG])
77. Sharp asymptotics on the compression of two-layer neural networks. (arXiv:2205.08199v1 [cs.IT])
78. An Application of Scenario Exploration to Find New Scenarios for the Development and Testing of Automated Driving Systems in Urban Scenarios. (arXiv:2205.08202v1 [cs.SE])
79. blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v1 [cs.CV])
80. ROP inception: signal estimation with quadratic random sketching. (arXiv:2205.08225v1 [eess.SP])
81. Hyper-Learning for Gradient-Based Batch Size Adaptation. (arXiv:2205.08231v1 [cs.LG])
82. Delaytron: Efficient Learning of Multiclass Classifiers with Delayed Bandit Feedbacks. (arXiv:2205.08234v1 [cs.LG])
83. IIsy: Practical In-Network Classification. (arXiv:2205.08243v1 [cs.NI])
84. Monotonicity Regularization: Improved Penalties and Novel Applications to Disentangled Representation Learning and Robust Classification. (arXiv:2205.08247v1 [cs.LG])
85. Adaptive Momentum-Based Policy Gradient with Second-Order Information. (arXiv:2205.08253v1 [cs.LG])
86. Multiscale reconstruction of porous media based on multiple dictionaries learning. (arXiv:2205.08278v1 [eess.IV])
87. KGNN: Distributed Framework for Graph Neural Knowledge Representation. (arXiv:2205.08285v1 [cs.LG])
88. Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers. (arXiv:2205.08288v1 [cs.CL])
89. Semi-Parametric Contextual Bandits with Graph-Laplacian Regularization. (arXiv:2205.08295v1 [stat.ML])
90. Bayesian Physics-Informed Neural Networks for real-world nonlinear dynamical systems. (arXiv:2205.08304v1 [cs.LG])
91. A Study of the Attention Abnormality in Trojaned BERTs. (arXiv:2205.08305v1 [cs.CR])
92. Accurate Machine Learned Quantum-Mechanical Force Fields for Biomolecular Simulations. (arXiv:2205.08306v1 [physics.chem-ph])
93. Finite Element Method-enhanced Neural Network for Forward and Inverse Problems. (arXiv:2205.08321v1 [cs.CE])
94. Scalable algorithms for physics-informed neural and graph networks. (arXiv:2205.08332v1 [cs.LG])
95. Explanation-Guided Fairness Testing through Genetic Algorithm. (arXiv:2205.08335v1 [cs.NE])
96. A unified framework for dataset shift diagnostics. (arXiv:2205.08340v1 [stat.ML])
97. Topological Signal Processing using the Weighted Ordinal Partition Network. (arXiv:2205.08349v1 [stat.ML])
98. Demystifying the Data Need of ML-surrogates for CFD Simulations. (arXiv:2205.08355v1 [cs.LG])
99. DouFu: A Double Fusion Joint Learning Method For Driving Trajectory Representation. (arXiv:2205.08356v1 [cs.LG])
100. Perturbation of Deep Autoencoder Weights for Model Compression and Classification of Tabular Data. (arXiv:2205.08358v1 [cs.LG])
101. LPC-AD: Fast and Accurate Multivariate Time Series Anomaly Detection via Latent Predictive Coding. (arXiv:2205.08362v1 [cs.LG])
102. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. (arXiv:2205.08363v1 [cs.LG])
103. Network Gradient Descent Algorithm for Decentralized Federated Learning. (arXiv:2205.08364v1 [cs.LG])
104. Deep Supervised Information Bottleneck Hashing for Cross-modal Retrieval based Computer-aided Diagnosis. (arXiv:2205.08365v1 [cs.LG])
105. Applications of Reinforcement Learning in Deregulated Power Market: A Comprehensive Review. (arXiv:2205.08369v1 [cs.LG])
106. Individualized Risk Assessment of Preoperative Opioid Use by Interpretable Neural Network Regression. (arXiv:2205.08370v1 [cs.LG])
107. Evaluation of a User Authentication Schema Using Behavioral Biometrics and Machine Learning. (arXiv:2205.08371v1 [cs.CR])
108. Automatic Velocity Picking Using Unsupervised Ensemble Learning. (arXiv:2205.08372v1 [cs.LG])
109. Should attention be all we need? The epistemic and ethical implications of unification in machine learning. (arXiv:2205.08377v1 [cs.LG])
110. Machine learning and atomic layer deposition: predicting saturation times from reactor growth profiles using artificial neural networks. (arXiv:2205.08378v1 [cs.LG])
111. Compatible deep neural network framework with financial time series data, including data preprocessor, neural network model and trading strategy. (arXiv:2205.08382v1 [cs.LG])
112. Bias and Fairness on Multimodal Emotion Detection Algorithms. (arXiv:2205.08383v1 [cs.LG])
113. Deep Learning of Chaotic Systems from Partially-Observed Data. (arXiv:2205.08384v1 [cs.LG])
114. Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs. (arXiv:2205.08385v1 [cs.LG])
115. How do Variational Autoencoders Learn? Insights from Representational Similarity. (arXiv:2205.08399v1 [cs.LG])
116. Automated Mobility Context Detection with Inertial Signals. (arXiv:2205.08409v1 [eess.SP])
117. Fault Detection for Non-Condensing Boilers using Simulated Building Automation System Sensor Data. (arXiv:2205.08418v1 [eess.SP])
118. Human Emotion Classification based on EEG Signals Using Recurrent Neural Network And KNN. (arXiv:2205.08419v1 [eess.SP])
119. JUNO: Jump-Start Reinforcement Learning-based Node Selection for UWB Indoor Localization. (arXiv:2205.08422v1 [eess.SP])
120. Can You Still See Me?: Reconstructing Robot Operations Over End-to-End Encrypted Channels. (arXiv:2205.08426v1 [cs.CR])
121. DNNR: Differential Nearest Neighbors Regression. (arXiv:2205.08434v1 [cs.LG])
122. Conditional Visual Servoing for Multi-Step Tasks. (arXiv:2205.08441v1 [cs.RO])
123. On the Privacy of Decentralized Machine Learning. (arXiv:2205.08443v1 [cs.CR])
124. A psychological theory of explainability. (arXiv:2205.08452v1 [cs.AI])
125. Utterance Weighted Multi-Dilation Temporal Convolutional Networks for Monaural Speech Dereverberation. (arXiv:2205.08455v1 [cs.SD])
126. Dynamic Recognition of Speakers for Consent Management by Contrastive Embedding Replay. (arXiv:2205.08459v1 [cs.SD])
127. Robust Losses for Learning Value Functions. (arXiv:2205.08464v1 [cs.LG])
128. Application of Graph Based Features in Computer Aided Diagnosis for Histopathological Image Classification of Gastric Cancer. (arXiv:2205.08467v1 [eess.IV])
129. An Evaluation Framework for Legal Document Summarization. (arXiv:2205.08478v1 [cs.CL])
130. Experimentally realized in situ backpropagation for deep learning in nanophotonic neural networks. (arXiv:2205.08501v1 [cs.ET])
131. Recovering Private Text in Federated Learning of Language Models. (arXiv:2205.08514v1 [cs.CL])
132. Do Neural Networks Compress Manifolds Optimally?. (arXiv:2205.08518v1 [cs.IT])
133. Supervised Learning for Coverage-Directed Test Selection in Simulation-Based Verification. (arXiv:2205.08524v1 [cs.AR])
134. High-dimensional additive Gaussian processes under monotonicity constraints. (arXiv:2205.08528v1 [stat.ML])
135. Disentangling Visual Embeddings for Attributes and Objects. (arXiv:2205.08536v1 [cs.CV])
136. Sparsely Activated Networks. (arXiv:1907.06592v9 [cs.LG] UPDATED)
137. Causal Feature Selection via Orthogonal Search. (arXiv:2007.02938v2 [stat.ML] UPDATED)
138. Applications of Deep Neural Networks with Keras. (arXiv:2009.05673v5 [cs.LG] UPDATED)
139. Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans. (arXiv:2009.06412v6 [eess.IV] UPDATED)
140. On the Landscape of One-hidden-layer Sparse Networks and Beyond. (arXiv:2009.07439v4 [cs.LG] UPDATED)
141. Reinforcement Learning of Causal Variables Using Mediation Analysis. (arXiv:2010.15745v2 [cs.LG] UPDATED)
142. FedADC: Accelerated Federated Learning with Drift Control. (arXiv:2012.09102v3 [cs.LG] UPDATED)
143. Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey. (arXiv:2012.09830v6 [cs.LG] UPDATED)
144. A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts. (arXiv:2105.07190v3 [cs.LG] UPDATED)
145. FedAdapt: Adaptive Offloading for IoT Devices in Federated Learning. (arXiv:2107.04271v4 [cs.DC] UPDATED)
146. DebiasedDTA: Improving the Generalizability of Drug-Target Affinity Prediction Models. (arXiv:2107.05556v4 [q-bio.QM] UPDATED)
147. Classification Auto-Encoder based Detector against Diverse Data Poisoning Attacks. (arXiv:2108.04206v2 [cs.LG] UPDATED)
148. Data Efficient Human Intention Prediction: Leveraging Neural Network Verification and Expert Guidance. (arXiv:2108.06871v2 [cs.LG] UPDATED)
149. Uncertainty Quantification and Experimental Design for Large-Scale Linear Inverse Problems under Gaussian Process Priors. (arXiv:2109.03457v3 [stat.ML] UPDATED)
150. Discretization-independent surrogate modeling over complex geometries using hypernetworks and implicit representations. (arXiv:2109.07018v4 [physics.comp-ph] UPDATED)
151. Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents. (arXiv:2109.10341v2 [cs.CL] UPDATED)
152. Extended dynamic mode decomposition with dictionary learning using neural ordinary differential equations. (arXiv:2110.01450v2 [cs.LG] UPDATED)
153. Random matrices in service of ML footprint: ternary random features with no performance loss. (arXiv:2110.01899v2 [stat.ML] UPDATED)
154. PriorVAE: Encoding spatial priors with VAEs for small-area estimation. (arXiv:2110.10422v3 [cs.LG] UPDATED)
155. Eluding Secure Aggregation in Federated Learning via Model Inconsistency. (arXiv:2111.07380v4 [cs.LG] UPDATED)
156. Invariance Through Latent Alignment. (arXiv:2112.08526v3 [cs.LG] UPDATED)
157. Automated Deep Learning: Neural Architecture Search Is Not the End. (arXiv:2112.09245v3 [cs.LG] UPDATED)
158. Attribute Inference Attack of Speech Emotion Recognition in Federated Learning Settings. (arXiv:2112.13416v2 [cs.CR] UPDATED)
159. Stochastic Weight Averaging Revisited. (arXiv:2201.00519v2 [cs.LG] UPDATED)
160. DNS: Determinantal Point Process Based Neural Network Sampler for Ensemble Reinforcement Learning. (arXiv:2201.13357v3 [cs.LG] UPDATED)
161. Using Partial Monotonicity in Submodular Maximization. (arXiv:2202.03051v2 [cs.LG] UPDATED)
162. A survey of unsupervised learning methods for high-dimensional uncertainty quantification in black-box-type problems. (arXiv:2202.04648v2 [cs.LG] UPDATED)
163. A Developmentally-Inspired Examination of Shape versus Texture Bias in Machines. (arXiv:2202.08340v2 [cs.CV] UPDATED)
164. Spatio-Temporal Latent Graph Structure Learning for Traffic Forecasting. (arXiv:2202.12586v2 [cs.LG] UPDATED)
165. Diagonal State Spaces are as Effective as Structured State Spaces. (arXiv:2203.14343v2 [cs.LG] UPDATED)
166. Certified machine learning: A posteriori error estimation for physics-informed neural networks. (arXiv:2203.17055v2 [cs.LG] UPDATED)
167. A Computational Theory of Learning Flexible Reward-Seeking Behavior with Place Cells. (arXiv:2204.11843v2 [q-bio.NC] UPDATED)
168. Domain Knowledge-Infused Deep Learning for Automated Analog/Radio-Frequency Circuit Parameter Optimization. (arXiv:2204.12948v2 [cs.LG] UPDATED)
169. Counterfactual harm. (arXiv:2204.12993v2 [cs.AI] UPDATED)
170. Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning. (arXiv:2204.13060v3 [cs.LG] UPDATED)
171. A Neural Network-enhanced Reproducing Kernel Particle Method for Modeling Strain Localization. (arXiv:2204.13821v2 [cs.CE] UPDATED)
172. Sequencer: Deep LSTM for Image Classification. (arXiv:2205.01972v2 [cs.CV] UPDATED)
173. Model-Contrastive Learning for Backdoor Defense. (arXiv:2205.04411v2 [cs.LG] UPDATED)
174. Integrating User and Item Reviews in Deep Cooperative Neural Networks for Movie Recommendation. (arXiv:2205.06296v2 [cs.IR] UPDATED)
175. PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning. (arXiv:2205.06401v2 [cs.CR] UPDATED)
176. Cliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments. (arXiv:2205.07015v2 [cs.LG] UPDATED)
177. Federated learning for LEO constellations via inter-HAP links. (arXiv:2205.07216v2 [cs.LG] UPDATED)
178. Discovering the Representation Bottleneck of Graph Neural Networks from Multi-order Interactions. (arXiv:2205.07266v2 [cs.LG] UPDATED)
179. Sobolev Acceleration and Statistical Optimality for Learning Elliptic Equations via Gradient Descent. (arXiv:2205.07331v2 [math.NA] UPDATED)
180. Federated Anomaly Detection over Distributed Data Streams. (arXiv:2205.07829v2 [cs.LG] UPDATED)
181. How to Minimize the Weighted Sum AoI in Two-Source Status Update Systems: OMA or NOMA?. (arXiv:2205.03143v1 [cs.IT] CROSS LISTED)
## cs.AI
---
**99** new papers in cs.AI:-) 
1. ST-ExpertNet: A Deep Expert Framework for Traffic Prediction. (arXiv:2205.07851v1 [cs.LG])
2. Heterogeneous Domain Adaptation with Adversarial Neural Representation Learning: Experiments on E-Commerce and Cybersecurity. (arXiv:2205.07853v1 [cs.LG])
3. Functional2Structural: Cross-Modality Brain Networks Representation Learning. (arXiv:2205.07854v1 [cs.LG])
4. Decentral and Incentivized Federated Learning Frameworks: A Systematic Literature Review. (arXiv:2205.07855v1 [cs.LG])
5. Impact of Learning Rate on Noise Resistant Property of Deep Learning Models. (arXiv:2205.07856v1 [cs.LG])
6. Neural Program Synthesis with Query. (arXiv:2205.07857v1 [cs.LG])
7. Predicting tacrolimus **exposure** in kidney transplanted patients using machine learning. (arXiv:2205.07858v1 [cs.LG])
8. Depression Diagnosis and Forecast based on Mobile Phone Sensor Data. (arXiv:2205.07861v1 [cs.LG])
9. A Safety Assurable Human-Inspired Perception Architecture. (arXiv:2205.07862v1 [cs.LG])
10. Quality versus speed in energy demand prediction for district heating systems. (arXiv:2205.07863v1 [cs.LG])
11. Simple Contrastive Graph Clustering. (arXiv:2205.07865v1 [cs.LG])
12. Feature and Instance Joint Selection: A Reinforcement Learning Perspective. (arXiv:2205.07867v1 [cs.LG])
13. Minimal Neural Network Models for Permutation Invariant Agents. (arXiv:2205.07868v1 [cs.LG])
14. Near out-of-distribution detection for low-resolution radar micro-Doppler signatures. (arXiv:2205.07869v1 [eess.SP])
15. Unsupervised Driving Behavior Analysis using Representation Learning and Exploiting Group-based Training. (arXiv:2205.07870v1 [cs.LG])
16. Mondrian Forest for Data Stream Classification Under Memory Constraints. (arXiv:2205.07871v1 [cs.LG])
17. ScAN: Suicide Attempt and Ideation Events Dataset. (arXiv:2205.07872v1 [cs.LG])
18. Aligning Robot Representations with Humans. (arXiv:2205.07882v1 [cs.HC])
19. Learning Car Speed Using Inertial Sensors. (arXiv:2205.07883v1 [cs.LG])
20. Enforcing KL Regularization in General Tsallis Entropy Reinforcement Learning via Advantage Learning. (arXiv:2205.07885v1 [cs.LG])
21. An Empirical Investigation of Representation Learning for Imitation. (arXiv:2205.07886v1 [cs.LG])
22. On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (arXiv:2205.07890v1 [cs.LG])
23. Constructing Trajectory and Predicting Estimated Time of Arrival for Long Distance Travelling Vessels: A Probability Density-based Scanning Approach. (arXiv:2205.07945v1 [eess.SY])
24. Deep Apprenticeship Learning for Playing Games. (arXiv:2205.07959v1 [cs.LG])
25. Many Field Packet Classification with Decomposition and Reinforcement Learning. (arXiv:2205.07973v1 [cs.NI])
26. Expert Systems with Logic#. A Novel Modeling Framework for Logic Programming in an Object-Oriented Context of C#. (arXiv:2205.07985v1 [cs.AI])
27. Lost in Compression: the Impact of Lossy Image Compression on Variable Size Object Detection within Infrared Imagery. (arXiv:2205.08002v1 [cs.CV])
28. CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction. (arXiv:2205.08012v1 [cs.CL])
29. A Survey on Semantics in Automated Data Science. (arXiv:2205.08018v1 [cs.AI])
30. DeepSim: A Reinforcement Learning Environment Build Toolkit for ROS and Gazebo. (arXiv:2205.08034v1 [cs.LG])
31. Explainable and Optimally Configured Artificial Neural Networks for Attack Detection in Smart Homes. (arXiv:2205.08043v1 [cs.CR])
32. Shape complexity in cluster analysis. (arXiv:2205.08046v1 [cs.LG])
33. HelixADMET: a robust and endpoint extensible ADMET system incorporating self-supervised knowledge transfer. (arXiv:2205.08055v1 [q-bio.BM])
34. "What makes a question inquisitive?" A Study on Type-Controlled Inquisitive Question Generation. (arXiv:2205.08056v1 [cs.CL])
35. Natural evolutionary strategies applied to quantum-classical hybrid neural networks. (arXiv:2205.08059v1 [quant-ph])
36. A Framework for CSI-Based Indoor Localization with 1D Convolutional Neural Networks. (arXiv:2205.08068v1 [eess.SP])
37. Multi-Head Attention Neural Network for Smartphone Invariant Indoor Localization. (arXiv:2205.08069v1 [eess.SP])
38. Unbiased Math Word Problems Benchmark for Mitigating Solving Bias. (arXiv:2205.08108v1 [cs.AI])
39. Forecasting Solar Power Generation on the basis of Predictive and Corrective Maintenance Activities. (arXiv:2205.08109v1 [cs.LG])
40. ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks. (arXiv:2205.08119v1 [cs.LG])
41. Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v1 [cs.RO])
42. Pairwise Comparison Network for Remote Sensing Scene Classification. (arXiv:2205.08147v1 [cs.CV])
43. SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection. (arXiv:2205.08159v1 [cs.CL])
44. SKILL: Structured Knowledge Infusion for Large Language Models. (arXiv:2205.08184v1 [cs.CL])
45. Moral reinforcement learning using actual causation. (arXiv:2205.08192v1 [cs.LG])
46. LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning. (arXiv:2205.08232v1 [cs.AI])
47. Delaytron: Efficient Learning of Multiclass Classifiers with Delayed Bandit Feedbacks. (arXiv:2205.08234v1 [cs.LG])
48. Monotonicity Regularization: Improved Penalties and Novel Applications to Disentangled Representation Learning and Robust Classification. (arXiv:2205.08247v1 [cs.LG])
49. Adaptive Momentum-Based Policy Gradient with Second-Order Information. (arXiv:2205.08253v1 [cs.LG])
50. A two-steps approach to improve the performance of Android malware detectors. (arXiv:2205.08265v1 [cs.CR])
51. Tackling Math Word Problems with Fine-to-Coarse Abstracting and Reasoning. (arXiv:2205.08274v1 [cs.CL])
52. KGNN: Distributed Framework for Graph Neural Knowledge Representation. (arXiv:2205.08285v1 [cs.LG])
53. Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers. (arXiv:2205.08288v1 [cs.CL])
54. Experiments on Generalizability of User-Oriented Fairness in Recommender Systems. (arXiv:2205.08289v1 [cs.IR])
55. A Study of the Attention Abnormality in Trojaned BERTs. (arXiv:2205.08305v1 [cs.CR])
56. Scalable algorithms for physics-informed neural and graph networks. (arXiv:2205.08332v1 [cs.LG])
57. A unified framework for dataset shift diagnostics. (arXiv:2205.08340v1 [stat.ML])
58. Landing AI on Networks: An equipment vendor viewpoint on Autonomous Driving Networks. (arXiv:2205.08347v1 [cs.NI])
59. RISCLESS: A Reinforcement Learning Strategy to Exploit Unused Cloud Resources. (arXiv:2205.08350v1 [cs.DC])
60. Demystifying the Data Need of ML-surrogates for CFD Simulations. (arXiv:2205.08355v1 [cs.LG])
61. DouFu: A Double Fusion Joint Learning Method For Driving Trajectory Representation. (arXiv:2205.08356v1 [cs.LG])
62. Perturbation of Deep Autoencoder Weights for Model Compression and Classification of Tabular Data. (arXiv:2205.08358v1 [cs.LG])
63. LPC-AD: Fast and Accurate Multivariate Time Series Anomaly Detection via Latent Predictive Coding. (arXiv:2205.08362v1 [cs.LG])
64. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. (arXiv:2205.08363v1 [cs.LG])
65. Deep Supervised Information Bottleneck Hashing for Cross-modal Retrieval based Computer-aided Diagnosis. (arXiv:2205.08365v1 [cs.LG])
66. Evaluation of a User Authentication Schema Using Behavioral Biometrics and Machine Learning. (arXiv:2205.08371v1 [cs.CR])
67. Compatible deep neural network framework with financial time series data, including data preprocessor, neural network model and trading strategy. (arXiv:2205.08382v1 [cs.LG])
68. Bias and Fairness on Multimodal Emotion Detection Algorithms. (arXiv:2205.08383v1 [cs.LG])
69. Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs. (arXiv:2205.08385v1 [cs.LG])
70. A Comprehensive Study on Artificial Intelligence Algorithms to Implement Safety Using Communication Technologies. (arXiv:2205.08404v1 [cs.AI])
71. Human Emotion Classification based on EEG Signals Using Recurrent Neural Network And KNN. (arXiv:2205.08419v1 [eess.SP])
72. An Application of a Multivariate Estimation of Distribution Algorithm to Cancer Chemotherapy. (arXiv:2205.08438v1 [cs.AI])
73. Connection-minimal Abduction in EL via Translation to FOL -- Technical Report. (arXiv:2205.08449v1 [cs.AI])
74. MAS2HP: A Multi Agent System to predict protein structure in 2D HP model. (arXiv:2205.08451v1 [q-bio.BM])
75. A psychological theory of explainability. (arXiv:2205.08452v1 [cs.AI])
76. Unsupervised Segmentation in Real-World Images via Spelke Object Inference. (arXiv:2205.08515v1 [cs.CV])
77. Measuring Plagiarism in Introductory Programming Course Assignments. (arXiv:2205.08520v1 [cs.CL])
78. Supervised Learning for Coverage-Directed Test Selection in Simulation-Based Verification. (arXiv:2205.08524v1 [cs.AR])
79. Applications of Deep Neural Networks with Keras. (arXiv:2009.05673v5 [cs.LG] UPDATED)
80. Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey. (arXiv:2012.09830v6 [cs.LG] UPDATED)
81. Projection: A Mechanism for Human-like Reasoning in Artificial Intelligence. (arXiv:2103.13512v2 [cs.AI] UPDATED)
82. A speckle filter for Sentinel-1 SAR Ground Range Detected data based on Residual Convolutional Neural Networks. (arXiv:2104.09350v2 [cs.AI] UPDATED)
83. Analyzing Semantics of Aggregate Answer Set Programming Using Approximation Fixpoint Theory. (arXiv:2104.14789v5 [cs.AI] UPDATED)
84. A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts. (arXiv:2105.07190v3 [cs.LG] UPDATED)
85. KGAP: Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v4 [cs.CL] UPDATED)
86. Role of Human-AI Interaction in Selective Prediction. (arXiv:2112.06751v2 [cs.AI] UPDATED)
87. Black-Box Tuning for Language-Model-as-a-Service. (arXiv:2201.03514v3 [cs.CL] UPDATED)
88. Online Auction-Based Incentive Mechanism Design for Horizontal Federated Learning with Budget Constraint. (arXiv:2201.09047v3 [cs.GT] UPDATED)
89. Recursive Binding for Similarity-Preserving Hypervector Representations of Sequences. (arXiv:2201.11691v2 [cs.AI] UPDATED)
90. An ASP approach for reasoning on neural networks under a finitely many-valued semantics for weighted conditional knowledge bases. (arXiv:2202.01123v3 [cs.AI] UPDATED)
91. Modality-Balanced Embedding for Video Retrieval. (arXiv:2204.08182v2 [cs.CV] UPDATED)
92. A Computational Theory of Learning Flexible Reward-Seeking Behavior with Place Cells. (arXiv:2204.11843v2 [q-bio.NC] UPDATED)
93. Counterfactual harm. (arXiv:2204.12993v2 [cs.AI] UPDATED)
94. Sequencer: Deep LSTM for Image Classification. (arXiv:2205.01972v2 [cs.CV] UPDATED)
95. Accelerating the Training of Video Super-Resolution Models. (arXiv:2205.05069v2 [cs.CV] UPDATED)
96. Personalized Adversarial Data Augmentation for Dysarthric and Elderly Speech Recognition. (arXiv:2205.06445v2 [eess.AS] UPDATED)
97. Cliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments. (arXiv:2205.07015v2 [cs.LG] UPDATED)
98. Federated learning for LEO constellations via inter-HAP links. (arXiv:2205.07216v2 [cs.LG] UPDATED)
99. Discovering the Representation Bottleneck of Graph Neural Networks from Multi-order Interactions. (arXiv:2205.07266v2 [cs.LG] UPDATED)

