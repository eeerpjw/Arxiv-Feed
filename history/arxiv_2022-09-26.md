# Your interest papers
---
## cs.CV
---
### **Swin**2SR: **Swin**V2 Transformer for Compressed Image Super-Resolution and **Restoration**. (arXiv:2209.11345v1 [cs.CV])
- Authors : Jin Choi, Maxime Burchi, Radu Timofte
- Link : [http://arxiv.org/abs/2209.11345](http://arxiv.org/abs/2209.11345)
> ABSTRACT  :  Compression plays an important role on the efficient transmission and storage of images and videos through band-limited systems such as streaming services, virtual reality or videogames. However, compression unavoidably leads to artifacts and the loss of the original information, which may severely degrade the visual quality. For these reasons, quality **enhancement** of compressed images has become a popular research topic. While most state-of-the-art image **restoration** methods are based on convolutional neural networks, other transformers-based methods such as **Swin**IR, show impressive performance on these tasks.    In this paper, we explore the novel **Swin** Transformer V2, to improve **Swin**IR for image super-resolution, and in particular, the compressed input scenario. Using this method we can tackle the major issues in training transformer vision models, such as training instability, resolution gaps between pre-training and fine-tuning, and hunger on data. We conduct experiments on three representative tasks: JPEG compression artifacts removal, image super-resolution (classical and lightweight), and compressed image super-resolution. Experimental results demonstrate that our method, **Swin**2SR, can improve the training convergence and performance of **Swin**IR, and is a top-5 solution at the "AIM 2022 Challenge on Super-Resolution of Compressed Image and Video".  
### Modular Degradation Simulation and **Restoration** for Under-Display Camera. (arXiv:2209.11455v1 [eess.IV])
- Authors : Yang Zhou, Yuda Song, Xin Du
- Link : [http://arxiv.org/abs/2209.11455](http://arxiv.org/abs/2209.11455)
> ABSTRACT  :  Under-display camera (UDC) provides an elegant solution for full-screen smartphones. However, UDC captured images suffer from severe degradation since sensors lie under the display. Although this issue can be tackled by image **restoration** networks, these networks require large-scale image pairs for training. To this end, we propose a modular network dubbed MPGNet trained using the generative adversarial network (GAN) framework for simulating UDC imaging. Specifically, we note that the UDC imaging degradation process contains brightness attenuation, blurring, and noise corruption. Thus we model each degradation with a characteristic-related modular network, and all modular networks are cascaded to form the generator. Together with a pixel-wise discriminator and supervised loss, we can train the generator to simulate the UDC imaging degradation process. Furthermore, we present a Transformer-style network named DWFormer for UDC image **restoration**. For practical purposes, we use depth-wise convolution instead of the multi-head self-attention to aggregate local spatial information. Moreover, we propose a novel channel attention module to aggregate global information, which is critical for brightness recovery. We conduct evaluations on the UDC benchmark, and our method surpasses the previous state-of-the-art models by 1.23 dB on the P-OLED track and 0.71 dB on the T-OLED track, respectively.  
### Unpaired Depth Super-Resolution in the Wild. (arXiv:2105.12038v4 [cs.CV] UPDATED)
- Authors : Aleksandr Safin, Maxim Kan, Nikita Drobyshev, Oleg Voynov, Alexey Artemov, Alexander Filippov, Denis Zorin, Evgeny Burnaev
- Link : [http://arxiv.org/abs/2105.12038](http://arxiv.org/abs/2105.12038)
> ABSTRACT  :  Depth maps captured with commodity sensors are often of low quality and resolution; these maps need to be enhanced to be used in many applications. State-of-the-art data-driven methods of depth map super-resolution rely on registered pairs of low- and high-resolution depth maps of the same scenes. Acquisition of real-world paired data requires specialized setups. Another alternative, generating low-resolution maps from high-resolution maps by subsampling, adding noise and other artificial degradation methods, does not fully capture the characteristics of real-world low-resolution images. As a consequence, supervised learning methods trained on such artificial paired data may not perform well on real-world low-resolution inputs. We consider an approach to depth super-resolution based on learning from unpaired data. While many techniques for unpaired image-to-image translation have been proposed, most fail to deliver effective hole-filling or reconstruct accurate surfaces using depth maps. We propose an unpaired learning method for depth super-resolution, which is based on a learnable degradation model, **enhancement** component and surface normal estimates as features to produce more accurate depth maps. We propose a benchmark for unpaired depth SR and demonstrate that our method outperforms existing unpaired methods and performs on par with paired.  
### LightFuse: Lightweight CNN based Dual-**exposure** Fusion. (arXiv:2107.02299v5 [cs.CV] UPDATED)
- Authors : Ziyi Liu, Jie Yang, Svetlana Yanushkevich, Orly Yadid
- Link : [http://arxiv.org/abs/2107.02299](http://arxiv.org/abs/2107.02299)
> ABSTRACT  :  Deep convolutional neural networks (DCNNs) have aided **high dynamic range** (**HDR**) imaging recently and have received a lot of attention. The quality of DCNN-generated **HDR** images has overperformed the traditional counterparts. However, DCNNs are prone to be computationally intensive and power-hungry, and hence cannot be implemented on various embedded computing platforms with limited power and hardware resources. Embedded systems have a huge market, and utilizing DCNNs' powerful functionality into them will further reduce human intervention. To address the challenge, we propose LightFuse, a lightweight CNN-based algorithm for extreme dual-**exposure** image fusion, which achieves better functionality than a conventional DCNN and can be deployed in embedded systems. Two sub-networks are utilized: a GlobalNet (G) and a DetailNet (D). The goal of G is to learn the global illumination information on the spatial dimension, whereas D aims to enhance local details on the channel dimension. Both G and D are based solely on depthwise convolution (D_Conv) and pointwise convolution (P_Conv) to reduce required parameters and computations. Experimental results show that this proposed technique could generate **HDR** images in extremely exposed regions with sufficient details to be legible. Our model outperforms other state-of-the-art approaches in peak signal-to-noise ratio (PSNR) score by 0.9 to 8.7 while achieving 16.7 to 306.2 times parameter reduction.  
### Depth-aware Neural Style Transfer using Instance Normalization. (arXiv:2203.09242v2 [cs.CV] UPDATED)
- Authors : Eleftherios Ioannou, Steve Maddock
- Link : [http://arxiv.org/abs/2203.09242](http://arxiv.org/abs/2203.09242)
> ABSTRACT  :  Neural Style Transfer (NST) is concerned with the artistic stylization of visual media. It can be described as the process of transferring the style of an artistic image onto an ordinary photograph. Recently, a number of studies have considered the **enhancement** of the depth-preserving capabilities of the NST algorithms to address the undesired effects that occur when the input content images include numerous objects at various depths. Our approach uses a deep residual convolutional network with instance normalization layers that utilizes an advanced depth prediction network to integrate depth preservation as an additional loss function to content and style. We demonstrate results that are effective in retaining the depth and global structure of content images. Three different evaluation processes show that our system is capable of preserving the structure of the stylized results while exhibiting style-capture capabilities and aesthetic qualities comparable or superior to state-of-the-art methods. Project page: https://ioannoue.github.io/depth-aware-nst-using-in.html.  
### Transformer Inertial Poser: **Real-time** Human Motion Reconstruction from Sparse IMUs with Simultaneous Terrain Generation. (arXiv:2203.15720v2 [cs.CV] UPDATED)
- Authors : Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Karen Liu
- Link : [http://arxiv.org/abs/2203.15720](http://arxiv.org/abs/2203.15720)
> ABSTRACT  :  **Real-time** human motion reconstruction from a sparse set of (e.g. six) wearable IMUs provides a non-intrusive and economic approach to motion capture. Without the ability to acquire position information directly from IMUs, recent works took data-driven approaches that utilize large human motion datasets to tackle this under-determined problem. Still, challenges remain such as temporal consistency, drifting of global and joint motions, and diverse coverage of motion types on various terrains. We propose a novel method to simultaneously estimate full-body motion and generate plausible visited terrain from only six IMU sensors in real-time. Our method incorporates 1. a conditional Transformer decoder model giving consistent predictions by explicitly reasoning prediction history, 2. a simple yet general learning target named "stationary body points" (SBPs) which can be stably predicted by the Transformer model and utilized by analytical routines to correct joint and global drifting, and 3. an algorithm to generate regularized terrain height maps from noisy SBP predictions which can in turn correct noisy global motion estimation. We evaluate our framework extensively on synthesized and real IMU data, and with real-time live demos, and show superior performance over strong baseline methods.  
## eess.IV
---
### **Swin**2SR: **Swin**V2 Transformer for Compressed Image Super-Resolution and **Restoration**. (arXiv:2209.11345v1 [cs.CV])
- Authors : Jin Choi, Maxime Burchi, Radu Timofte
- Link : [http://arxiv.org/abs/2209.11345](http://arxiv.org/abs/2209.11345)
> ABSTRACT  :  Compression plays an important role on the efficient transmission and storage of images and videos through band-limited systems such as streaming services, virtual reality or videogames. However, compression unavoidably leads to artifacts and the loss of the original information, which may severely degrade the visual quality. For these reasons, quality **enhancement** of compressed images has become a popular research topic. While most state-of-the-art image **restoration** methods are based on convolutional neural networks, other transformers-based methods such as **Swin**IR, show impressive performance on these tasks.    In this paper, we explore the novel **Swin** Transformer V2, to improve **Swin**IR for image super-resolution, and in particular, the compressed input scenario. Using this method we can tackle the major issues in training transformer vision models, such as training instability, resolution gaps between pre-training and fine-tuning, and hunger on data. We conduct experiments on three representative tasks: JPEG compression artifacts removal, image super-resolution (classical and lightweight), and compressed image super-resolution. Experimental results demonstrate that our method, **Swin**2SR, can improve the training convergence and performance of **Swin**IR, and is a top-5 solution at the "AIM 2022 Challenge on Super-Resolution of Compressed Image and Video".  
### Modular Degradation Simulation and **Restoration** for Under-Display Camera. (arXiv:2209.11455v1 [eess.IV])
- Authors : Yang Zhou, Yuda Song, Xin Du
- Link : [http://arxiv.org/abs/2209.11455](http://arxiv.org/abs/2209.11455)
> ABSTRACT  :  Under-display camera (UDC) provides an elegant solution for full-screen smartphones. However, UDC captured images suffer from severe degradation since sensors lie under the display. Although this issue can be tackled by image **restoration** networks, these networks require large-scale image pairs for training. To this end, we propose a modular network dubbed MPGNet trained using the generative adversarial network (GAN) framework for simulating UDC imaging. Specifically, we note that the UDC imaging degradation process contains brightness attenuation, blurring, and noise corruption. Thus we model each degradation with a characteristic-related modular network, and all modular networks are cascaded to form the generator. Together with a pixel-wise discriminator and supervised loss, we can train the generator to simulate the UDC imaging degradation process. Furthermore, we present a Transformer-style network named DWFormer for UDC image **restoration**. For practical purposes, we use depth-wise convolution instead of the multi-head self-attention to aggregate local spatial information. Moreover, we propose a novel channel attention module to aggregate global information, which is critical for brightness recovery. We conduct evaluations on the UDC benchmark, and our method surpasses the previous state-of-the-art models by 1.23 dB on the P-OLED track and 0.71 dB on the T-OLED track, respectively.  
### LightFuse: Lightweight CNN based Dual-**exposure** Fusion. (arXiv:2107.02299v5 [cs.CV] UPDATED)
- Authors : Ziyi Liu, Jie Yang, Svetlana Yanushkevich, Orly Yadid
- Link : [http://arxiv.org/abs/2107.02299](http://arxiv.org/abs/2107.02299)
> ABSTRACT  :  Deep convolutional neural networks (DCNNs) have aided **high dynamic range** (**HDR**) imaging recently and have received a lot of attention. The quality of DCNN-generated **HDR** images has overperformed the traditional counterparts. However, DCNNs are prone to be computationally intensive and power-hungry, and hence cannot be implemented on various embedded computing platforms with limited power and hardware resources. Embedded systems have a huge market, and utilizing DCNNs' powerful functionality into them will further reduce human intervention. To address the challenge, we propose LightFuse, a lightweight CNN-based algorithm for extreme dual-**exposure** image fusion, which achieves better functionality than a conventional DCNN and can be deployed in embedded systems. Two sub-networks are utilized: a GlobalNet (G) and a DetailNet (D). The goal of G is to learn the global illumination information on the spatial dimension, whereas D aims to enhance local details on the channel dimension. Both G and D are based solely on depthwise convolution (D_Conv) and pointwise convolution (P_Conv) to reduce required parameters and computations. Experimental results show that this proposed technique could generate **HDR** images in extremely exposed regions with sufficient details to be legible. Our model outperforms other state-of-the-art approaches in peak signal-to-noise ratio (PSNR) score by 0.9 to 8.7 while achieving 16.7 to 306.2 times parameter reduction.  
## cs.LG
---
### StyleTime: Style Transfer for Synthetic Time Series Generation. (arXiv:2209.11306v1 [cs.LG])
- Authors : Yousef El, Svitlana Vyetrenko
- Link : [http://arxiv.org/abs/2209.11306](http://arxiv.org/abs/2209.11306)
> ABSTRACT  :  Neural style transfer is a powerful computer vision technique that can incorporate the artistic "style" of one image to the "content" of another. The underlying theory behind the approach relies on the assumption that the style of an image is represented by the Gram matrix of its features, which is typically extracted from pre-trained convolutional neural networks (e.g., VGG-19). This idea does not straightforwardly extend to time series stylization since notions of style for two-dimensional images are not analogous to notions of style for one-dimensional time series. In this work, a novel formulation of time series style transfer is proposed for the purpose of synthetic data generation and **enhancement**. We introduce the concept of stylized features for time series, which is directly related to the time series realism properties, and propose a novel stylization algorithm, called StyleTime, that uses explicit feature extraction techniques to combine the underlying content (trend) of one time series with the style (distributional properties) of another. Further, we discuss evaluation metrics, and compare our work to existing state-of-the-art time series generation and augmentation schemes. To validate the effectiveness of our methods, we use stylized synthetic data as a means for data augmentation to improve the performance of recurrent neural network models on several forecasting tasks.  
### **Real-time** Adversarial Perturbations against Deep Reinforcement Learning Policies: Attacks and Defenses. (arXiv:2106.08746v4 [cs.LG] UPDATED)
- Authors : Shelly Wang, Samuel Marchal
- Link : [http://arxiv.org/abs/2106.08746](http://arxiv.org/abs/2106.08746)
> ABSTRACT  :  Deep reinforcement learning (DRL) is vulnerable to adversarial perturbations. Adversaries can mislead the policies of DRL agents by perturbing the state of the environment observed by the agents. Existing attacks are feasible in principle, but face challenges in practice, either by being too slow to fool DRL policies in **real time** or by modifying past observations stored in the agent's memory. We show that Universal Adversarial Perturbations (UAP), independent of the individual inputs to which they are applied, can fool DRL policies effectively and in **real time**. We introduce three attack variants leveraging UAP. Via an extensive evaluation using three Atari 2600 games, we show that our attacks are effective, as they fully degrade the performance of three different DRL agents (up to 100%, even when the $l_\infty$ bound on the perturbation is as small as 0.01). It is faster than the frame rate (60 Hz) of image capture and considerably faster than prior attacks ($\approx 1.8$ms). Our attack technique is also efficient, incurring an online computational cost of $\approx 0.027$ms. Using two tasks involving robotic movement, we confirm that our results generalize to complex DRL tasks. Furthermore, we demonstrate that the effectiveness of known defenses diminishes against universal perturbations. We introduce an effective technique that detects all known adversarial perturbations against DRL policies, including all universal perturbations presented in this paper.  
### CMGAN: Conformer-based Metric GAN for Speech **Enhancement**. (arXiv:2203.15149v3 [cs.SD] UPDATED)
- Authors : Ruizhe Cao, Sherif Abdulatif, Bin Yang
- Link : [http://arxiv.org/abs/2203.15149](http://arxiv.org/abs/2203.15149)
> ABSTRACT  :  Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech **enhancement** (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset indicates the capability of CMGAN in outperforming various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB.  
### One Positive Label is Sufficient: Single-Positive Multi-Label Learning with Label **Enhancement**. (arXiv:2206.00517v3 [cs.LG] UPDATED)
- Authors : Ning Xu, Congyu Qiao, Jiaqi Lv, Xin Geng, Ling Zhang
- Link : [http://arxiv.org/abs/2206.00517](http://arxiv.org/abs/2206.00517)
> ABSTRACT  :  Multi-label learning (MLL) learns from the examples each associated with multiple labels simultaneously, where the high cost of annotating all relevant labels for each training example is challenging for real-world applications. To cope with the challenge, we investigate single-positive multi-label learning (SPMLL) where each example is annotated with only one relevant label and show that one can successfully learn a theoretically grounded multi-label classifier for the problem. In this paper, a novel SPMLL method named {\proposed}, i.e., Single-positive MultI-label learning with Label **Enhancement**, is proposed. Specifically, an unbiased risk estimator is derived, which could be guaranteed to approximately converge to the optimal risk minimizer of fully supervised learning and shows that one positive label of each instance is sufficient to train the predictive model. Then, the corresponding empirical risk estimator is established via recovering the latent soft label as a label **enhancement** process, where the posterior density of the latent soft labels is approximate to the variational Beta density parameterized by an inference model. Experiments on benchmark datasets validate the effectiveness of the proposed method.  
### Approximating Discontinuous Nash Equilibrial Values of Two-Player General-Sum Differential Games. (arXiv:2207.01773v2 [cs.LG] UPDATED)
- Authors : **Lei Zhang**, Mukesh Ghimire, Wenlong Zhang, Zhe Xu, Yi Ren
- Link : [http://arxiv.org/abs/2207.01773](http://arxiv.org/abs/2207.01773)
> ABSTRACT  :  Finding Nash equilibrial policies for two-player differential games requires solving Hamilton-Jacobi-Isaacs (HJI) PDEs. Self-supervised learning has been used to approximate solutions of such PDEs while circumventing the well-known curse of dimensionality. However, this method fails to learn discontinuous PDE solutions due to its sampling nature, leading to poor safety performance of the resulting controllers in robotics applications when player rewards are discontinuous. This paper investigates two potential solutions to this problem: a hybrid method that leverages both supervised Nash equilibria and the HJI PDE, and a value-hardening method where a sequence of HJIs are solved with a gradually hardening reward. We compare these solutions using the resulting generalization and safety performance in two vehicle interaction case studies with 5D and 9D state spaces, respectively. Result shows that with informative supervision (e.g., collision and near-collision demonstrations) and the low cost of self-supervised learning, the hybrid method achieves better safety performance than the supervised, self-supervised, and value hardening approaches on equal computational budget. Value hardening fails to generalize in the higher-dimensional case without informative supervision. Lastly, we show that the neural activation function needs to be continuously differentiable for learning PDEs and its choice can be case dependent.  
### Deep Learning based pipeline for anomaly detection and quality **enhancement** in industrial binder jetting processes. (arXiv:2209.10178v2 [cs.LG] UPDATED)
- Authors : Alexander Zeiser, Bas van
- Link : [http://arxiv.org/abs/2209.10178](http://arxiv.org/abs/2209.10178)
> ABSTRACT  :  Anomaly detection describes methods of finding abnormal states, instances or data points that differ from a normal value space. Industrial processes are a domain where predicitve models are needed for finding anomalous data instances for quality **enhancement**. A main challenge, however, is absence of labels in this environment. This paper contributes to a data-centric way of approaching artificial intelligence in industrial production. With a use case from additive manufacturing for automotive components we present a deep-learning-based image processing pipeline. Additionally, we integrate the concept of domain randomisation and synthetic data in the loop that shows promising results for bridging advances in deep learning and its application to real-world, industrial production processes.  
### CMGAN: Conformer-Based Metric-GAN for Monaural Speech **Enhancement**. (arXiv:2209.11112v2 [cs.SD] UPDATED)
- Authors : Sherif Abdulatif, Ruizhe Cao, Bin Yang
- Link : [http://arxiv.org/abs/2209.11112](http://arxiv.org/abs/2209.11112)
> ABSTRACT  :  Convolution-augmented transformers (Conformers) are recently proposed in various speech-domain applications, such as automatic speech recognition (ASR) and speech separation, as they can capture both local and global dependencies. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for speech **enhancement** (SE) in the time-frequency (TF) domain. The generator encodes the magnitude and complex spectrogram information using two-stage conformer blocks to model both time and frequency dependencies. The decoder then decouples the estimation into a magnitude mask decoder branch to filter out unwanted distortions and a complex refinement branch to further improve the magnitude estimation and implicitly enhance the phase information. Additionally, we include a metric discriminator to alleviate metric mismatch by optimizing the generator with respect to a corresponding evaluation score. Objective and subjective evaluations illustrate that CMGAN is able to show superior performance compared to state-of-the-art methods in three speech **enhancement** tasks (denoising, dereverberation and super-resolution). For instance, quantitative denoising analysis on Voice Bank+DEMAND dataset indicates that CMGAN outperforms various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB.  
## cs.AI
---
### **Real-time** Adversarial Perturbations against Deep Reinforcement Learning Policies: Attacks and Defenses. (arXiv:2106.08746v4 [cs.LG] UPDATED)
- Authors : Shelly Wang, Samuel Marchal
- Link : [http://arxiv.org/abs/2106.08746](http://arxiv.org/abs/2106.08746)
> ABSTRACT  :  Deep reinforcement learning (DRL) is vulnerable to adversarial perturbations. Adversaries can mislead the policies of DRL agents by perturbing the state of the environment observed by the agents. Existing attacks are feasible in principle, but face challenges in practice, either by being too slow to fool DRL policies in **real time** or by modifying past observations stored in the agent's memory. We show that Universal Adversarial Perturbations (UAP), independent of the individual inputs to which they are applied, can fool DRL policies effectively and in **real time**. We introduce three attack variants leveraging UAP. Via an extensive evaluation using three Atari 2600 games, we show that our attacks are effective, as they fully degrade the performance of three different DRL agents (up to 100%, even when the $l_\infty$ bound on the perturbation is as small as 0.01). It is faster than the frame rate (60 Hz) of image capture and considerably faster than prior attacks ($\approx 1.8$ms). Our attack technique is also efficient, incurring an online computational cost of $\approx 0.027$ms. Using two tasks involving robotic movement, we confirm that our results generalize to complex DRL tasks. Furthermore, we demonstrate that the effectiveness of known defenses diminishes against universal perturbations. We introduce an effective technique that detects all known adversarial perturbations against DRL policies, including all universal perturbations presented in this paper.  
### CMGAN: Conformer-based Metric GAN for Speech **Enhancement**. (arXiv:2203.15149v3 [cs.SD] UPDATED)
- Authors : Ruizhe Cao, Sherif Abdulatif, Bin Yang
- Link : [http://arxiv.org/abs/2203.15149](http://arxiv.org/abs/2203.15149)
> ABSTRACT  :  Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech **enhancement** (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset indicates the capability of CMGAN in outperforming various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB.  
### CMGAN: Conformer-Based Metric-GAN for Monaural Speech **Enhancement**. (arXiv:2209.11112v2 [cs.SD] UPDATED)
- Authors : Sherif Abdulatif, Ruizhe Cao, Bin Yang
- Link : [http://arxiv.org/abs/2209.11112](http://arxiv.org/abs/2209.11112)
> ABSTRACT  :  Convolution-augmented transformers (Conformers) are recently proposed in various speech-domain applications, such as automatic speech recognition (ASR) and speech separation, as they can capture both local and global dependencies. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for speech **enhancement** (SE) in the time-frequency (TF) domain. The generator encodes the magnitude and complex spectrogram information using two-stage conformer blocks to model both time and frequency dependencies. The decoder then decouples the estimation into a magnitude mask decoder branch to filter out unwanted distortions and a complex refinement branch to further improve the magnitude estimation and implicitly enhance the phase information. Additionally, we include a metric discriminator to alleviate metric mismatch by optimizing the generator with respect to a corresponding evaluation score. Objective and subjective evaluations illustrate that CMGAN is able to show superior performance compared to state-of-the-art methods in three speech **enhancement** tasks (denoising, dereverberation and super-resolution). For instance, quantitative denoising analysis on Voice Bank+DEMAND dataset indicates that CMGAN outperforms various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB.  
# Paper List
---
## cs.CV
---
**97** new papers in cs.CV:-) 
1. A Trio-Method for Retinal Vessel Segmentation using Image Processing. (arXiv:2209.11230v1 [eess.IV])
2. Hierarchical Graph Convolutional Network Built by Multiscale Atlases for Brain Disorder Diagnosis Using Functional Connectivity. (arXiv:2209.11232v1 [eess.IV])
3. 3DPCT: 3D Point Cloud Transformer with Dual Self-attention. (arXiv:2209.11255v1 [cs.CV])
4. Recurrence-free Survival Prediction under the Guidance of Automatic Gross Tumor Volume Segmentation for Head and Neck Cancers. (arXiv:2209.11268v1 [cs.CV])
5. Optimization of FPGA-based CNN Accelerators Using Metaheuristics. (arXiv:2209.11272v1 [cs.NE])
6. Capsule Network based Contrastive Learning of Unsupervised Visual Representations. (arXiv:2209.11276v1 [cs.CV])
7. FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion. (arXiv:2209.11277v1 [cs.CV])
8. Automated detection of Alzheimer disease using MRI images and deep neural networks- A review. (arXiv:2209.11282v1 [eess.IV])
9. T2FPV: Constructing High-Fidelity First-Person View Datasets From Real-World Pedestrian Trajectories. (arXiv:2209.11294v1 [cs.CV])
10. Deep Domain Adaptation for Detecting Bomb Craters in Aerial Images. (arXiv:2209.11299v1 [cs.CV])
11. Colonoscopy Landmark Detection using Vision Transformers. (arXiv:2209.11304v1 [cs.CV])
12. FuTH-Net: Fusing Temporal Relations and Holistic Features for Aerial Video Classification. (arXiv:2209.11316v1 [cs.CV])
13. Privacy-Preserving Person Detection Using Low-Resolution Infrared Cameras. (arXiv:2209.11335v1 [cs.CV])
14. UNav: An Infrastructure-Independent Vision-Based Navigation System for People with Blindness and Low vision. (arXiv:2209.11336v1 [cs.CV])
15. A domain adaptive deep learning solution for scanpath prediction of paintings. (arXiv:2209.11338v1 [cs.CV])
16. Fast Disparity Estimation from a Single Compressed Light Field Measurement. (arXiv:2209.11342v1 [cs.CV])
17. **Swin**2SR: **Swin**V2 Transformer for Compressed Image Super-Resolution and **Restoration**. (arXiv:2209.11345v1 [cs.CV])
18. Oracle Analysis of Representations for Deep Open Set Detection. (arXiv:2209.11350v1 [cs.CV])
19. Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body. (arXiv:2209.11355v1 [cs.CV])
20. NasHD: Efficient ViT Architecture Performance Ranking using Hyperdimensional Computing. (arXiv:2209.11356v1 [cs.CV])
21. CUTS: A Fully Unsupervised Framework for Medical Image Segmentation. (arXiv:2209.11359v1 [cs.CV])
22. Tensor-Based Multi-Modality Feature Selection and Regression for Alzheimer's Disease Diagnosis. (arXiv:2209.11372v1 [cs.LG])
23. LGDN: Language-Guided Denoising Network for Video-Language Modeling. (arXiv:2209.11388v1 [cs.CV])
24. Towards Frame Rate Agnostic Multi-Object Tracking. (arXiv:2209.11404v1 [cs.CV])
25. Learning to screen Glaucoma like the ophthalmologists. (arXiv:2209.11431v1 [eess.IV])
26. Understanding Open-Set Recognition by Jacobian Norm of Representation. (arXiv:2209.11436v1 [cs.CV])
27. Rethinking Performance Gains in Image Dehazing Networks. (arXiv:2209.11448v1 [cs.CV])
28. Motion Guided Deep Dynamic 3D Garments. (arXiv:2209.11449v1 [cs.CV])
29. Modular Degradation Simulation and **Restoration** for Under-Display Camera. (arXiv:2209.11455v1 [eess.IV])
30. Segmentation-based Information Extraction and Amalgamation in Fundus Images for Glaucoma Detection. (arXiv:2209.11456v1 [eess.IV])
31. TeST: Test-time Self-Training under Distribution Shift. (arXiv:2209.11459v1 [cs.CV])
32. Unsupervised Hashing with Semantic Concept Mining. (arXiv:2209.11475v1 [cs.CV])
33. Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model. (arXiv:2209.11477v1 [cs.CV])
34. GIDP: Learning a Good Initialization and Inducing Descriptor Post-enhancing for Large-scale Place Recognition. (arXiv:2209.11488v1 [cs.CV])
35. Grouped Adaptive Loss Weighting for Person Search. (arXiv:2209.11492v1 [cs.CV])
36. Comparison of synthetic dataset generation methods for medical intervention rooms using medical clothing detection as an example. (arXiv:2209.11493v1 [cs.CV])
37. Marine Video Kit: A New Marine Video Dataset for Content-based Analysis and Retrieval. (arXiv:2209.11518v1 [cs.CV])
38. Vector Quantized Semantic Communication System. (arXiv:2209.11519v1 [cs.CV])
39. WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels. (arXiv:2209.11523v1 [cs.CV])
40. Statistical shape representations for temporal registration of plant components in 3D. (arXiv:2209.11526v1 [cs.CV])
41. Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy. (arXiv:2209.11531v1 [eess.IV])
42. MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v1 [cs.CV])
43. Query-based Hard-Image Retrieval for Object Detection at Test Time. (arXiv:2209.11559v1 [cs.CV])
44. Multi-Modal Cross-Domain Alignment Network for Video Moment Retrieval. (arXiv:2209.11572v1 [cs.CV])
45. Towards Complete-View and High-Level Pose-based Gait Recognition. (arXiv:2209.11577v1 [cs.CV])
46. Pose-Aided Video-based Person Re-Identification via Recurrent Graph Convolutional Network. (arXiv:2209.11582v1 [cs.CV])
47. Multi-Granularity Graph Pooling for Video-based Person Re-Identification. (arXiv:2209.11584v1 [cs.CV])
48. I-SPLIT: Deep Network Interpretability for Split Computing. (arXiv:2209.11607v1 [cs.CV])
49. View-Invariant Skeleton-based Action Recognition via Global-Local Contrastive Learning. (arXiv:2209.11634v1 [cs.CV])
50. Image-to-Image Translation for Autonomous Driving from Coarsely-Aligned Image Pairs. (arXiv:2209.11673v1 [cs.CV])
51. PNeRF: Probabilistic Neural Scene Representations for Uncertain 3D Visual Mapping. (arXiv:2209.11677v1 [cs.CV])
52. An Overview of Violence Detection Techniques: Current Challenges and Future Directions. (arXiv:2209.11680v1 [cs.CV])
53. Meteorological Satellite Images Prediction Based on Deep Multi-scales Extrapolation Fusion. (arXiv:2209.11682v1 [cs.CV])
54. T3VIP: Transformation-based 3D Video Prediction. (arXiv:2209.11693v1 [cs.CV])
55. Rate-Distortion in Image Coding for Machines. (arXiv:2209.11694v1 [cs.CV])
56. Dynamic camera alignment optimization problem based on Fractal Decomposition based Algorithm. (arXiv:2209.11695v1 [cs.CV])
57. Edge-oriented Implicit Neural Representation with Channel Tuning. (arXiv:2209.11697v1 [cs.CV])
58. Multivariate Wasserstein Functional Connectivity for Autism Screening. (arXiv:2209.11703v1 [cs.CV])
59. Multilevel Robustness for 2D Vector Field Feature Tracking, Selection, and Comparison. (arXiv:2209.11708v1 [cs.CV])
60. Best Prompts for Text-to-Image Models and How to Find Them. (arXiv:2209.11711v1 [cs.HC])
61. Boost CTR Prediction for New Advertisements via Modeling Visual Content. (arXiv:2209.11727v1 [cs.IR])
62. Dual-Cycle: Self-Supervised Dual-View Fluorescence Microscopy Image Reconstruction using CycleGAN. (arXiv:2209.11729v1 [eess.IV])
63. Semantic scene descriptions as an objective of human vision. (arXiv:2209.11737v1 [cs.CV])
64. Catoptric Light can be Dangerous: Effective Physical-World Attack by Natural Phenomenon. (arXiv:2209.11739v1 [cs.CV])
65. On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks. (arXiv:2209.11740v1 [cs.CV])
66. Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics. (arXiv:2209.11741v1 [cs.CV])
67. Lightweight Transformers for Human Activity Recognition on Mobile Devices. (arXiv:2209.11750v1 [cs.CV])
68. Facial Emotions Recognition using Convolutional Neural Net. (arXiv:2001.01456v2 [cs.CV] UPDATED)
69. Unpaired Depth Super-Resolution in the Wild. (arXiv:2105.12038v4 [cs.CV] UPDATED)
70. LightFuse: Lightweight CNN based Dual-**exposure** Fusion. (arXiv:2107.02299v5 [cs.CV] UPDATED)
71. Improving Video Instance Segmentation via Temporal Pyramid Routing. (arXiv:2107.13155v2 [cs.CV] UPDATED)
72. Self-supervised Point Cloud Representation Learning via Separating Mixed Shapes. (arXiv:2109.00452v3 [cs.CV] UPDATED)
73. Distribution-sensitive Information Retention for Accurate Binary Neural Network. (arXiv:2109.12338v2 [cs.CV] UPDATED)
74. WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. (arXiv:2111.02403v4 [eess.IV] UPDATED)
75. Dilated convolution with learnable spacings. (arXiv:2112.03740v2 [cs.CV] UPDATED)
76. Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v3 [eess.SP] UPDATED)
77. Field-of-View IoU for Object Detection in 360{\deg} Images. (arXiv:2202.03176v2 [cs.CV] UPDATED)
78. FedDrive: Generalizing Federated Learning to Semantic Segmentation in Autonomous Driving. (arXiv:2202.13670v2 [cs.CV] UPDATED)
79. Deep, Deep Learning with BART. (arXiv:2202.14005v2 [cs.CV] UPDATED)
80. F2DNet: Fast Focal Detection Network for Pedestrian Detection. (arXiv:2203.02331v2 [cs.CV] UPDATED)
81. Object-centric and memory-guided normality reconstruction for video anomaly detection. (arXiv:2203.03677v3 [cs.CV] UPDATED)
82. TTCDist: Fast Distance Estimation From an Active Monocular Camera Using Time-to-Contact. (arXiv:2203.07530v2 [cs.RO] UPDATED)
83. Depth-aware Neural Style Transfer using Instance Normalization. (arXiv:2203.09242v2 [cs.CV] UPDATED)
84. Importance Sampling CAMs for Weakly-Supervised Segmentation with Highly Accurate Contours. (arXiv:2203.12459v2 [cs.CV] UPDATED)
85. Transformer Inertial Poser: **Real-time** Human Motion Reconstruction from Sparse IMUs with Simultaneous Terrain Generation. (arXiv:2203.15720v2 [cs.CV] UPDATED)
86. Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution. (arXiv:2204.01188v4 [cs.CV] UPDATED)
87. Unsupervised Deep Unrolled Reconstruction Using Regularization by Denoising. (arXiv:2205.03519v2 [eess.IV] UPDATED)
88. EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model. (arXiv:2205.15278v3 [cs.CV] UPDATED)
89. EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications. (arXiv:2206.10589v2 [cs.CV] UPDATED)
90. Learning Binary and Sparse Permutation-Invariant Representations for Fast and Memory Efficient Whole Slide Image Search. (arXiv:2208.13653v2 [cs.CV] UPDATED)
91. Blurring Diffusion Models. (arXiv:2209.05557v2 [cs.LG] UPDATED)
92. APTx: better activation function than MISH, SWISH, and ReLU's variants used in deep learning. (arXiv:2209.06119v3 [cs.LG] UPDATED)
93. CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment. (arXiv:2209.06430v2 [cs.CV] UPDATED)
94. Data-Centric AI Paradigm Based on Application-Driven Fine-grained Dataset Design. (arXiv:2209.09449v2 [cs.CV] UPDATED)
95. MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation. (arXiv:2209.11047v2 [cs.CV] UPDATED)
96. VToonify: Controllable High-Resolution Portrait Video Style Transfer. (arXiv:2209.11224v2 [cs.CV] UPDATED)
97. A Low-Cost Multi-Agent System for Physical Security in Smart Buildings. (arXiv:2209.00741v1 [cs.CR] CROSS LISTED)
## eess.IV
---
**20** new papers in eess.IV:-) 
1. A Trio-Method for Retinal Vessel Segmentation using Image Processing. (arXiv:2209.11230v1 [eess.IV])
2. Hierarchical Graph Convolutional Network Built by Multiscale Atlases for Brain Disorder Diagnosis Using Functional Connectivity. (arXiv:2209.11232v1 [eess.IV])
3. Automated detection of Alzheimer disease using MRI images and deep neural networks- A review. (arXiv:2209.11282v1 [eess.IV])
4. Fast Disparity Estimation from a Single Compressed Light Field Measurement. (arXiv:2209.11342v1 [cs.CV])
5. **Swin**2SR: **Swin**V2 Transformer for Compressed Image Super-Resolution and **Restoration**. (arXiv:2209.11345v1 [cs.CV])
6. Learning to screen Glaucoma like the ophthalmologists. (arXiv:2209.11431v1 [eess.IV])
7. Modular Degradation Simulation and **Restoration** for Under-Display Camera. (arXiv:2209.11455v1 [eess.IV])
8. Segmentation-based Information Extraction and Amalgamation in Fundus Images for Glaucoma Detection. (arXiv:2209.11456v1 [eess.IV])
9. Image Classification using Sequence of Pixels. (arXiv:2209.11495v1 [eess.IV])
10. Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy. (arXiv:2209.11531v1 [eess.IV])
11. Meteorological Satellite Images Prediction Based on Deep Multi-scales Extrapolation Fusion. (arXiv:2209.11682v1 [cs.CV])
12. Rate-Distortion in Image Coding for Machines. (arXiv:2209.11694v1 [cs.CV])
13. Dynamic camera alignment optimization problem based on Fractal Decomposition based Algorithm. (arXiv:2209.11695v1 [cs.CV])
14. Dual-Cycle: Self-Supervised Dual-View Fluorescence Microscopy Image Reconstruction using CycleGAN. (arXiv:2209.11729v1 [eess.IV])
15. LightFuse: Lightweight CNN based Dual-**exposure** Fusion. (arXiv:2107.02299v5 [cs.CV] UPDATED)
16. WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. (arXiv:2111.02403v4 [eess.IV] UPDATED)
17. Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v3 [eess.SP] UPDATED)
18. Importance Sampling CAMs for Weakly-Supervised Segmentation with Highly Accurate Contours. (arXiv:2203.12459v2 [cs.CV] UPDATED)
19. Unsupervised Deep Unrolled Reconstruction Using Regularization by Denoising. (arXiv:2205.03519v2 [eess.IV] UPDATED)
20. Phase Aberration Correction for in vivo Ultrasound Localization Microscopy Using a Spatiotemporal Complex-Valued Neural Network. (arXiv:2209.10650v2 [eess.IV] UPDATED)
## cs.LG
---
**116** new papers in cs.LG:-) 
1. Assessing Robustness of EEG Representations under Data-shifts via Latent Space and Uncertainty Analysis. (arXiv:2209.11233v1 [eess.SP])
2. Artificial Intelligence in Material Engineering: A review on applications of AI in Material Engineering. (arXiv:2209.11234v1 [cs.LG])
3. Computational Discovery of Energy-Efficient Heat Treatment for Microstructure Design using Deep Reinforcement Learning. (arXiv:2209.11259v1 [cond-mat.mtrl-sci])
4. Recurrence-free Survival Prediction under the Guidance of Automatic Gross Tumor Volume Segmentation for Head and Neck Cancers. (arXiv:2209.11268v1 [cs.CV])
5. Minimizing Human Assistance: Augmenting a Single Demonstration for Deep Reinforcement Learning. (arXiv:2209.11275v1 [cs.LG])
6. FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion. (arXiv:2209.11277v1 [cs.CV])
7. Environment Optimization for Multi-Agent Navigation. (arXiv:2209.11279v1 [cs.RO])
8. Scalable Gaussian Process Hyperparameter Optimization via Coverage Regularization. (arXiv:2209.11280v1 [cs.LG])
9. Automated detection of Alzheimer disease using MRI images and deep neural networks- A review. (arXiv:2209.11282v1 [eess.IV])
10. ProgPrompt: Generating Situated Robot Task Plans using Large Language Models. (arXiv:2209.11302v1 [cs.RO])
11. An Investigation of the Bias-Variance Tradeoff in Meta-Gradients. (arXiv:2209.11303v1 [cs.LG])
12. Colonoscopy Landmark Detection using Vision Transformers. (arXiv:2209.11304v1 [cs.CV])
13. StyleTime: Style Transfer for Synthetic Time Series Generation. (arXiv:2209.11306v1 [cs.LG])
14. Convolutional Learning on Multigraphs. (arXiv:2209.11354v1 [cs.LG])
15. Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body. (arXiv:2209.11355v1 [cs.CV])
16. A Jensen-Shannon Divergence Based Loss Function for Bayesian Neural Networks. (arXiv:2209.11366v1 [cs.LG])
17. Tensor-Based Multi-Modality Feature Selection and Regression for Alzheimer's Disease Diagnosis. (arXiv:2209.11372v1 [cs.LG])
18. Do Current Multi-Task Optimization Methods in Deep Learning Even Help?. (arXiv:2209.11379v1 [cs.LG])
19. Achieve the Minimum Width of Neural Networks for Universal Approximation. (arXiv:2209.11395v1 [cs.LG])
20. Relation Embedding based Graph Neural Networks for Handling Heterogeneous Graph. (arXiv:2209.11414v1 [cs.LG])
21. LEADER: Learning Attention over Driving Behaviors for Planning under Uncertainty. (arXiv:2209.11422v1 [cs.LG])
22. A Robust and Explainable Data-Driven Anomaly Detection Approach For Power Electronics. (arXiv:2209.11427v1 [eess.SY])
23. A Preliminary Investigation of MLOps Practices in GitHub. (arXiv:2209.11453v1 [cs.SE])
24. TeST: Test-time Self-Training under Distribution Shift. (arXiv:2209.11459v1 [cs.CV])
25. Smart Active Sampling to enhance Quality Assurance Efficiency. (arXiv:2209.11464v1 [cs.LG])
26. Optimizing Class Distribution in Memory for Multi-Label Online Continual Learning. (arXiv:2209.11469v1 [cs.LG])
27. Active Few-Shot Classification: a New Paradigm for Data-Scarce Learning Settings. (arXiv:2209.11481v1 [cs.LG])
28. Image Classification using Sequence of Pixels. (arXiv:2209.11495v1 [eess.IV])
29. Sequential Causal Effect Variational Autoencoder: Time Series Causal Link Estimation under Hidden Confounding. (arXiv:2209.11497v1 [cs.LG])
30. The complexity of unsupervised learning of lexicographic preferences. (arXiv:2209.11505v1 [cs.AI])
31. Error Mitigation-Aided Optimization of Parameterized Quantum Circuits: Convergence Analysis. (arXiv:2209.11514v1 [quant-ph])
32. Power Management in Smart Residential Building with Deep Learning Model for Occupancy Detection by Usage Pattern of Electric Appliances. (arXiv:2209.11520v1 [eess.SP])
33. An artificial neural network-based system for detecting machine failures using tiny sound data: A case study. (arXiv:2209.11527v1 [cs.SD])
34. Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy. (arXiv:2209.11531v1 [eess.IV])
35. A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models. (arXiv:2209.11533v1 [cs.LG])
36. MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v1 [cs.CV])
37. On Efficient Reinforcement Learning for Full-length Game of StarCraft II. (arXiv:2209.11553v1 [cs.LG])
38. Applications of Machine Learning in Chemical and Biological Oceanography. (arXiv:2209.11557v1 [cs.LG])
39. Query-based Hard-Image Retrieval for Object Detection at Test Time. (arXiv:2209.11559v1 [cs.CV])
40. Learning Rigid Body Dynamics with Lagrangian Graph Neural Network. (arXiv:2209.11588v1 [cs.LG])
41. Differentially private partitioned variational inference. (arXiv:2209.11595v1 [cs.LG])
42. Quantification before Selection: Active Dynamics Preference for Robust Reinforcement Learning. (arXiv:2209.11596v1 [cs.LG])
43. Machine Learning and Analytical Power Consumption Models for 5G Base Stations. (arXiv:2209.11600v1 [cs.NI])
44. Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration. (arXiv:2209.11604v1 [cs.LG])
45. I-SPLIT: Deep Network Interpretability for Split Computing. (arXiv:2209.11607v1 [cs.CV])
46. Differentiable physics-enabled closure modeling for Burgers' turbulence. (arXiv:2209.11614v1 [physics.flu-dyn])
47. Robust Domain Adaptation for Machine Reading Comprehension. (arXiv:2209.11615v1 [cs.LG])
48. A Neural Model for Regular Grammar Induction. (arXiv:2209.11628v1 [cs.LG])
49. From Weakly Supervised Learning to Active Learning. (arXiv:2209.11629v1 [cs.LG])
50. Exact conservation laws for neural network integrators of dynamical systems. (arXiv:2209.11661v1 [math.DS])
51. Multidimensional Interactive Fixed-Effects. (arXiv:2209.11691v1 [econ.EM])
52. T3VIP: Transformation-based 3D Video Prediction. (arXiv:2209.11693v1 [cs.CV])
53. Semantic scene descriptions as an objective of human vision. (arXiv:2209.11737v1 [cs.CV])
54. Catoptric Light can be Dangerous: Effective Physical-World Attack by Natural Phenomenon. (arXiv:2209.11739v1 [cs.CV])
55. Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics. (arXiv:2209.11741v1 [cs.CV])
56. Unified Algorithms for RL with Decision-Estimation Coefficients: No-Regret, PAC, and Reward-Free Learning. (arXiv:2209.11745v1 [cs.LG])
57. GLSO: Grammar-guided Latent Space Optimization for Sample-efficient Robot Design Automation. (arXiv:2209.11748v1 [cs.RO])
58. Lightweight Transformers for Human Activity Recognition on Mobile Devices. (arXiv:2209.11750v1 [cs.CV])
59. Stochastic Inverse Reinforcement Learning. (arXiv:1905.08513v8 [cs.LG] UPDATED)
60. Neural Lyapunov Control. (arXiv:2005.00611v4 [cs.LG] UPDATED)
61. Adapting $k$-means algorithms for outliers. (arXiv:2007.01118v2 [cs.DS] UPDATED)
62. Combinatorial optimization and reasoning with graph neural networks. (arXiv:2102.09544v3 [cs.LG] UPDATED)
63. Holmes: An Efficient and Lightweight Semantic Based Anomalous Email Detector. (arXiv:2104.08044v12 [cs.CR] UPDATED)
64. TNet: A Model-Constrained Tikhonov Network Approach for Inverse Problems. (arXiv:2105.12033v3 [stat.ML] UPDATED)
65. **Real-time** Adversarial Perturbations against Deep Reinforcement Learning Policies: Attacks and Defenses. (arXiv:2106.08746v4 [cs.LG] UPDATED)
66. Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Optimization. (arXiv:2106.10022v2 [cs.LG] UPDATED)
67. Detecting Concept Drift With Neural Network Model Uncertainty. (arXiv:2107.01873v2 [cs.LG] UPDATED)
68. Model Free Barrier Functions via Implicit Evading Maneuvers. (arXiv:2107.12871v3 [cs.LG] UPDATED)
69. Privacy-preserving Federated Adversarial Domain Adaption over Feature Groups for Interpretability. (arXiv:2111.10934v2 [cs.LG] UPDATED)
70. Learning State Representations via Retracing in Reinforcement Learning. (arXiv:2111.12600v2 [cs.LG] UPDATED)
71. A singular Riemannian geometry approach to Deep Neural Networks II. Reconstruction of 1-D equivalence classes. (arXiv:2112.10583v2 [cs.LG] UPDATED)
72. Separation of Scales and a Thermodynamic Description of Feature Learning in Some CNNs. (arXiv:2112.15383v3 [stat.ML] UPDATED)
73. On the Robustness of Sparse Counterfactual Explanations to Adverse Perturbations. (arXiv:2201.09051v3 [cs.LG] UPDATED)
74. A singular Riemannian geometry approach to Deep Neural Networks I. Theoretical foundations. (arXiv:2201.09656v2 [cs.LG] UPDATED)
75. Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v3 [eess.SP] UPDATED)
76. Deadwooding: Robust Global Pruning for Deep Neural Networks. (arXiv:2202.05226v4 [cs.LG] UPDATED)
77. FinNet: Solving Time-Independent Differential Equations with Finite Difference Neural Network. (arXiv:2202.09282v2 [cs.LG] UPDATED)
78. STEADY: Simultaneous State Estimation and Dynamics Learning from Indirect Observations. (arXiv:2203.01299v3 [cs.RO] UPDATED)
79. Variational inference of fractional Brownian motion with linear computational complexity. (arXiv:2203.07961v4 [cs.LG] UPDATED)
80. Importance Sampling CAMs for Weakly-Supervised Segmentation with Highly Accurate Contours. (arXiv:2203.12459v2 [cs.CV] UPDATED)
81. Amortized Projection Optimization for Sliced Wasserstein Generative Models. (arXiv:2203.13417v4 [stat.ML] UPDATED)
82. CMGAN: Conformer-based Metric GAN for Speech **Enhancement**. (arXiv:2203.15149v3 [cs.SD] UPDATED)
83. Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution. (arXiv:2204.01188v4 [cs.CV] UPDATED)
84. Unsupervised Deep Unrolled Reconstruction Using Regularization by Denoising. (arXiv:2205.03519v2 [eess.IV] UPDATED)
85. CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction. (arXiv:2205.08012v2 [cs.CL] UPDATED)
86. FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning. (arXiv:2205.11039v2 [cs.AI] UPDATED)
87. Phased Progressive Learning with Coupling-Regulation-Imbalance Loss for Imbalanced Data Classification. (arXiv:2205.12117v2 [cs.LG] UPDATED)
88. Introducing Non-Linear Activations into Quantum Generative Models. (arXiv:2205.14506v3 [quant-ph] UPDATED)
89. One Positive Label is Sufficient: Single-Positive Multi-Label Learning with Label **Enhancement**. (arXiv:2206.00517v3 [cs.LG] UPDATED)
90. Stochastic Multiple Target Sampling Gradient Descent. (arXiv:2206.01934v3 [cs.LG] UPDATED)
91. Two-terminal source coding with common sum reconstruction. (arXiv:2206.06973v2 [cs.IT] UPDATED)
92. Identifying the Context Shift between Test Benchmarks and Production Data. (arXiv:2207.01059v2 [cs.LG] UPDATED)
93. Approximating Discontinuous Nash Equilibrial Values of Two-Player General-Sum Differential Games. (arXiv:2207.01773v2 [cs.LG] UPDATED)
94. An Additive Instance-Wise Approach to Multi-class Model Interpretation. (arXiv:2207.03113v2 [cs.LG] UPDATED)
95. MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks. (arXiv:2207.07941v2 [cs.LG] UPDATED)
96. Domain Adapting Deep Reinforcement Learning for Real-world Speech Emotion Recognition. (arXiv:2207.12248v2 [cs.SD] UPDATED)
97. Thermodynamics of learning physical phenomena. (arXiv:2207.12749v2 [cs.LG] UPDATED)
98. Reducing Exploitability with Population Based Training. (arXiv:2208.05083v2 [cs.LG] UPDATED)
99. Complex-Value Spatio-temporal Graph Convolutional Neural Networks and its Applications to Electric Power Systems AI. (arXiv:2208.08485v2 [cs.LG] UPDATED)
100. Bayesian Optimization-based Combinatorial Assignment. (arXiv:2208.14698v2 [cs.LG] UPDATED)
101. Distributional Drift Adaptation with Temporal Conditional Variational Autoencoder for Multivariate Time Series Forecasting. (arXiv:2209.00654v3 [cs.LG] UPDATED)
102. Optimizing the Performative Risk under Weak Convexity Assumptions. (arXiv:2209.00771v3 [cs.LG] UPDATED)
103. ApproxTrain: Fast Simulation of Approximate Multipliers for DNN Training and Inference. (arXiv:2209.04161v3 [cs.AR] UPDATED)
104. Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm. (arXiv:2209.04213v2 [eess.SP] UPDATED)
105. Blurring Diffusion Models. (arXiv:2209.05557v2 [cs.LG] UPDATED)
106. Deep Reinforcement Learning for Cryptocurrency Trading: Practical Approach to Address Backtest Overfitting. (arXiv:2209.05559v3 [q-fin.ST] UPDATED)
107. Class-Level Logit Perturbation. (arXiv:2209.05668v2 [cs.LG] UPDATED)
108. APTx: better activation function than MISH, SWISH, and ReLU's variants used in deep learning. (arXiv:2209.06119v3 [cs.LG] UPDATED)
109. GAGA: Deciphering Age-path of Generalized Self-paced Regularizer. (arXiv:2209.07063v2 [cs.LG] UPDATED)
110. Library transfer between distinct Laser-Induced Breakdown Spectroscopy systems with shared standards. (arXiv:2209.07637v2 [physics.data-an] UPDATED)
111. Deep Fusion of Multi-Object Densities Using Transformer. (arXiv:2209.08857v2 [cs.LG] UPDATED)
112. Deep Learning based pipeline for anomaly detection and quality **enhancement** in industrial binder jetting processes. (arXiv:2209.10178v2 [cs.LG] UPDATED)
113. Fast Few shot Self-attentive Semi-supervised Political Inclination Prediction. (arXiv:2209.10292v2 [cs.CY] UPDATED)
114. CMGAN: Conformer-Based Metric-GAN for Monaural Speech **Enhancement**. (arXiv:2209.11112v2 [cs.SD] UPDATED)
115. VToonify: Controllable High-Resolution Portrait Video Style Transfer. (arXiv:2209.11224v2 [cs.CV] UPDATED)
116. DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation. (arXiv:2209.10797v1 [eess.SY] CROSS LISTED)
## cs.AI
---
**50** new papers in cs.AI:-) 
1. Hierarchical Graph Convolutional Network Built by Multiscale Atlases for Brain Disorder Diagnosis Using Functional Connectivity. (arXiv:2209.11232v1 [eess.IV])
2. Capsule Network based Contrastive Learning of Unsupervised Visual Representations. (arXiv:2209.11276v1 [cs.CV])
3. FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion. (arXiv:2209.11277v1 [cs.CV])
4. ProgPrompt: Generating Situated Robot Task Plans using Large Language Models. (arXiv:2209.11302v1 [cs.RO])
5. Colonoscopy Landmark Detection using Vision Transformers. (arXiv:2209.11304v1 [cs.CV])
6. Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body. (arXiv:2209.11355v1 [cs.CV])
7. Do Current Multi-Task Optimization Methods in Deep Learning Even Help?. (arXiv:2209.11379v1 [cs.LG])
8. Improving Conversational Recommender System via Contextual and Time-Aware Modeling with Less Domain-Specific Knowledge. (arXiv:2209.11386v1 [cs.CL])
9. LGDN: Language-Guided Denoising Network for Video-Language Modeling. (arXiv:2209.11388v1 [cs.CV])
10. Conversational QA Dataset Generation with Answer Revision. (arXiv:2209.11396v1 [cs.CL])
11. IDEA: Interactive DoublE Attentions from Label Embedding for Text Classification. (arXiv:2209.11407v1 [cs.CL])
12. The complexity of unsupervised learning of lexicographic preferences. (arXiv:2209.11505v1 [cs.AI])
13. WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels. (arXiv:2209.11523v1 [cs.CV])
14. MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v1 [cs.CV])
15. On Efficient Reinforcement Learning for Full-length Game of StarCraft II. (arXiv:2209.11553v1 [cs.LG])
16. Multi-Modal Cross-Domain Alignment Network for Video Moment Retrieval. (arXiv:2209.11572v1 [cs.CV])
17. involve-MI: Informative Planning with High-Dimensional Non-Parametric Beliefs. (arXiv:2209.11591v1 [cs.AI])
18. The SpeakIn Speaker Verification System for Far-Field Speaker Verification Challenge 2022. (arXiv:2209.11625v1 [cs.SD])
19. Rethinking Missing Data: Aleatoric Uncertainty-Aware Recommendation. (arXiv:2209.11679v1 [cs.IR])
20. An Overview of Violence Detection Techniques: Current Challenges and Future Directions. (arXiv:2209.11680v1 [cs.CV])
21. T3VIP: Transformation-based 3D Video Prediction. (arXiv:2209.11693v1 [cs.CV])
22. Edge-oriented Implicit Neural Representation with Channel Tuning. (arXiv:2209.11697v1 [cs.CV])
23. The "Beatrix'' Resurrections: Robust Backdoor Detection via Gram Matrices. (arXiv:2209.11715v1 [cs.CR])
24. On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks. (arXiv:2209.11740v1 [cs.CV])
25. Unified Algorithms for RL with Decision-Estimation Coefficients: No-Regret, PAC, and Reward-Free Learning. (arXiv:2209.11745v1 [cs.LG])
26. Evaluating Agent Interactions Through Episodic Knowledge Graphs. (arXiv:2209.11746v1 [cs.AI])
27. Lightweight Transformers for Human Activity Recognition on Mobile Devices. (arXiv:2209.11750v1 [cs.CV])
28. Stochastic Inverse Reinforcement Learning. (arXiv:1905.08513v8 [cs.LG] UPDATED)
29. Explainable Goal-Driven Agents and Robots -- A Comprehensive Review. (arXiv:2004.09705v9 [cs.RO] UPDATED)
30. Emergence in artificial life. (arXiv:2105.03216v2 [physics.gen-ph] UPDATED)
31. **Real-time** Adversarial Perturbations against Deep Reinforcement Learning Policies: Attacks and Defenses. (arXiv:2106.08746v4 [cs.LG] UPDATED)
32. Detecting Concept Drift With Neural Network Model Uncertainty. (arXiv:2107.01873v2 [cs.LG] UPDATED)
33. ViF-SD2E: A Robust Weakly-Supervised Method for Neural Decoding. (arXiv:2112.01261v3 [cs.NE] UPDATED)
34. Dilated convolution with learnable spacings. (arXiv:2112.03740v2 [cs.CV] UPDATED)
35. Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue. (arXiv:2203.07657v3 [cs.CL] UPDATED)
36. CMGAN: Conformer-based Metric GAN for Speech **Enhancement**. (arXiv:2203.15149v3 [cs.SD] UPDATED)
37. CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction. (arXiv:2205.08012v2 [cs.CL] UPDATED)
38. FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning. (arXiv:2205.11039v2 [cs.AI] UPDATED)
39. Stochastic Multiple Target Sampling Gradient Descent. (arXiv:2206.01934v3 [cs.LG] UPDATED)
40. Identifying the Context Shift between Test Benchmarks and Production Data. (arXiv:2207.01059v2 [cs.LG] UPDATED)
41. An Additive Instance-Wise Approach to Multi-class Model Interpretation. (arXiv:2207.03113v2 [cs.LG] UPDATED)
42. Reducing Exploitability with Population Based Training. (arXiv:2208.05083v2 [cs.LG] UPDATED)
43. Representation Learning based and Interpretable Reactor System Diagnosis Using Denoising Padded Autoencoder. (arXiv:2208.14319v2 [eess.SP] UPDATED)
44. ApproxTrain: Fast Simulation of Approximate Multipliers for DNN Training and Inference. (arXiv:2209.04161v3 [cs.AR] UPDATED)
45. Deep Reinforcement Learning for Cryptocurrency Trading: Practical Approach to Address Backtest Overfitting. (arXiv:2209.05559v3 [q-fin.ST] UPDATED)
46. Class-Level Logit Perturbation. (arXiv:2209.05668v2 [cs.LG] UPDATED)
47. APTx: better activation function than MISH, SWISH, and ReLU's variants used in deep learning. (arXiv:2209.06119v3 [cs.LG] UPDATED)
48. Fast Few shot Self-attentive Semi-supervised Political Inclination Prediction. (arXiv:2209.10292v2 [cs.CY] UPDATED)
49. SR-GCL: Session-Based Recommendation with Global Context Enhanced Augmentation in Contrastive Learning. (arXiv:2209.10807v2 [cs.IR] UPDATED)
50. CMGAN: Conformer-Based Metric-GAN for Monaural Speech **Enhancement**. (arXiv:2209.11112v2 [cs.SD] UPDATED)

