# Your interest papers
---
## cs.CV
---
### Image Reconstruction of Multi Branch Feature Multiplexing Fusion Network with Mixed Multi-layer Attention. (arXiv:2205.13738v1 [cs.CV])
- Authors : Yuxi Cai, Huicheng Lai
- Link : [http://arxiv.org/abs/2205.13738](http://arxiv.org/abs/2205.13738)
> ABSTRACT  :  Image super-resolution reconstruction achieves better results than traditional methods with the help of the powerful nonlinear representation ability of convolution neural network. However, some existing algorithms also have some problems, such as insufficient utilization of phased features, ignoring the importance of early phased feature fusion to improve network performance, and the inability of the network to pay more attention to high-frequency information in the reconstruction process. To solve these problems, we propose a multi-branch feature multiplexing fusion network with mixed multi-layer attention (MBMFN), which realizes the multiple utilization of features and the multistage fusion of different levels of features. To further improve the networks performance, we propose a lightweight enhanced residual channel attention (LERCA), which can not only effectively avoid the loss of channel information but also make the network pay more attention to the key channel information and benefit from it. Finally, the attention mechanism is introduced into the reconstruction process to strengthen the **restoration** of edge texture and other details. A large number of experiments on several benchmark sets show that, compared with other advanced reconstruction algorithms, our algorithm produces highly competitive objective indicators and restores more image detail texture information.  
### Comparison of Deep Learning Segmentation and Multigrader-annotated Mandibular Canals of Multicenter CBCT scans. (arXiv:2205.13874v1 [cs.LG])
- Authors : Jaakko Sahlsten, Joel Jaskari, Kimmo Kaski, Helena Mehtonen, Ziyuan Lin, Ari Hietanen, Osku Sundqvist, Vesa Varjonen, Vesa Mattila, Sangsom Prapayasotok, Sakarat Nalampang
- Link : [http://arxiv.org/abs/2205.13874](http://arxiv.org/abs/2205.13874)
> ABSTRACT  :  Deep learning approach has been demonstrated to automatically segment the **bilateral** mandibular canals from CBCT scans, yet systematic studies of its clinical and technical validation are scarce. To validate the mandibular canal localization accuracy of a deep learning system (DLS) we trained it with 982 CBCT scans and evaluated using 150 scans of five scanners from clinical workflow patients of European and Southeast Asian Institutes, annotated by four radiologists. The interobserver variability was compared to the variability between the DLS and the radiologists. In addition, the generalization of DLS to CBCT scans from scanners not used in the training data was examined to evaluate the out-of-distribution generalization capability. The DLS had lower variability to the radiologists than the interobserver variability between them and it was able to generalize to three new devices. For the radiologists' consensus segmentation, used as gold standard, the DLS had a symmetric mean curve distance of 0.39 mm compared to those of the individual radiologists with 0.62 mm, 0.55 mm, 0.47 mm, and 0.42 mm. The DLS showed comparable or slightly better performance in the segmentation of the mandibular canal with the radiologists and generalization capability to new scanners.  
### Image Harmonization with Region-wise Contrastive Learning. (arXiv:2205.14058v1 [cs.CV])
- Authors : Jingtang Liang, Man Pun
- Link : [http://arxiv.org/abs/2205.14058](http://arxiv.org/abs/2205.14058)
> ABSTRACT  :  Image harmonization task aims at harmonizing different composite foreground regions according to specific background image. Previous methods would rather focus on improving the reconstruction ability of the generator by some internal **enhancement**s such as attention, adaptive normalization and light adjustment, $etc.$. However, they pay less attention to discriminating the foreground and background appearance features within a restricted generator, which becomes a new challenge in image harmonization task. In this paper, we propose a novel image harmonization framework with external style fusion and region-wise contrastive learning scheme. For the external style fusion, we leverage the external background appearance from the encoder as the style reference to generate harmonized foreground in the decoder. This approach enhances the harmonization ability of the decoder by external background guidance. Moreover, for the contrastive learning scheme, we design a region-wise contrastive loss function for image harmonization task. Specifically, we first introduce a straight-forward samples generation method that selects negative samples from the output harmonized foreground region and selects positive samples from the ground-truth background region. Our method attempts to bring together corresponding positive and negative samples by maximizing the mutual information between the foreground and background styles, which desirably makes our harmonization network more robust to discriminate the foreground and background style features when harmonizing composite images. Extensive experiments on the benchmark datasets show that our method can achieve a clear improvement in harmonization quality and demonstrate the good generalization capability in real-scenario applications.  
### Improving Road Segmentation in Challenging Domains Using Similar Place Priors. (arXiv:2205.14112v1 [cs.CV])
- Authors : Connor Malone, Sourav Garg, Ming Xu, Thierry Peynot, Michael Milford
- Link : [http://arxiv.org/abs/2205.14112](http://arxiv.org/abs/2205.14112)
> ABSTRACT  :  Road segmentation in challenging domains, such as **night**, snow or rain, is a difficult task. Most current approaches boost performance using fine-tuning, domain adaptation, style transfer, or by referencing previously acquired imagery. These approaches share one or more of three significant limitations: a reliance on large amounts of annotated training data that can be costly to obtain, both anticipation of and training data from the type of environmental conditions expected at inference time, and/or imagery captured from a previous visit to the location. In this research, we remove these restrictions by improving road segmentation based on similar places. We use Visual Place Recognition (VPR) to find similar but geographically distinct places, and fuse segmentations for query images and these similar place priors using a Bayesian approach and novel segmentation quality metric. Ablation studies show the need to re-evaluate notions of VPR utility for this task. We demonstrate the system achieving state-of-the-art road segmentation performance across multiple challenging condition scenarios including **night** time and snow, without requiring any prior training or previous access to the same geographical locations. Furthermore, we show that this method is network agnostic, improves multiple baseline techniques and is competitive against methods specialised for road prediction.  
### Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v1 [cs.CV])
- Authors : Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, Baining Guo
- Link : [http://arxiv.org/abs/2205.14141](http://arxiv.org/abs/2205.14141)
> ABSTRACT  :  Masked image modeling (MIM) learns representations with remarkably good fine-tuning performances, overshadowing previous prevalent pre-training approaches such as image classification, instance contrastive learning, and image-text alignment. In this paper, we show that the inferior fine-tuning performance of these pre-training approaches can be significantly improved by a simple post-processing in the form of feature distillation (FD). The feature distillation converts the old representations to new representations that have a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by a set of attention- and optimization-related diagnosis tools. With these properties, the new representations show strong fine-tuning performance. Specifically, the contrastive self-supervised learning methods are made as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is also significantly improved, with a CLIP ViT-L model reaching 89.0% top-1 accuracy on ImageNet-1K classification. More importantly, our work provides a way for the future research to focus more effort on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness since it can be enhanced rather easily. The code will be available at https://github.com/**Swin**Transformer/Feature-Distillation.  
### Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v3 [cs.CV] UPDATED)
- Authors : 
- Link : [http://arxiv.org/abs/2110.13285](http://arxiv.org/abs/2110.13285)
> ABSTRACT  :  Due to the success of generative flows to model data distributions, they have been explored in inverse problems. Given a pre-trained generative flow, previous work proposed to minimize the 2-norm of the latent variables as a regularization term. The intuition behind it was to ensure high likelihood latent variables that produce the closest **restoration**. However, high-likelihood latent variables may generate unrealistic samples as we show in our experiments. We therefore propose a solver to directly produce high-likelihood reconstructions. We hypothesize that our approach could make generative flows a general purpose solver for inverse problems. Furthermore, we propose 1 x 1 coupling functions to introduce permutations in a generative flow. It has the advantage that its inverse does not require to be calculated in the generation process. Finally, we evaluate our method for denoising, deblurring, inpainting, and colorization. We observe a compelling improvement of our method over prior works.  
### UV Volumes for **Real-time** Rendering of Editable Free-view Human Performance. (arXiv:2203.14402v2 [cs.CV] UPDATED)
- Authors : Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue Wang, Fei Wang
- Link : [http://arxiv.org/abs/2203.14402](http://arxiv.org/abs/2203.14402)
> ABSTRACT  :  Neural volume rendering enables photo-realistic renderings of a human performer in free-view, a critical task in immersive VR/AR applications. But the practice is severely limited by high computational costs in the rendering process. To solve this problem, we propose the UV Volumes, a new approach that can render an editable free-view video of a human performer in realtime. It separates the high-frequency (i.e., non-smooth) human appearance from the 3D volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV volumes allow much smaller and shallower neural networks to obtain densities and texture coordinates in 3D while capturing detailed appearance in 2D NTS. For editability, the mapping between the parameterized human model and the smooth texture coordinates allows us a better generalization on novel poses and shapes. Furthermore, the use of NTS enables interesting applications, e.g., retexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M datasets show that our model can render 960 * 540 images in 30FPS on average with comparable photo-realism to state-of-the-art methods. The project and supplementary materials are available at https://fanegg.github.io/UV-Volumes.  
### Fast Multi-grid Methods for Minimizing Curvature Energy. (arXiv:2204.07921v2 [eess.IV] UPDATED)
- Authors : Zhenwei Zhang, Ke Chen, Yuping Duan
- Link : [http://arxiv.org/abs/2204.07921](http://arxiv.org/abs/2204.07921)
> ABSTRACT  :  The geometric high-order regularization methods such as mean curvature and Gaussian curvature, have been intensively studied during the last decades due to their abilities in preserving geometric properties including image edges, corners, and image contrast. However, the dilemma between **restoration** quality and computational efficiency is an essential roadblock for high-order methods. In this paper, we propose fast multi-grid algorithms for minimizing both mean curvature and Gaussian curvature energy functionals without sacrificing the accuracy for efficiency. Unlike the existing approaches based on operator splitting and the Augmented Lagrangian method (ALM), no artificial parameters are introduced in our formulation, which guarantees the robustness of the proposed algorithm. Meanwhile, we adopt the domain decomposition method to promote parallel computing and use the fine-to-coarse structure to accelerate the convergence. Numerical experiments are presented on both image denoising and CT reconstruction problem to demonstrate the ability to recover image texture and the efficiency of the proposed method.  
### Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v2 [cs.CV] UPDATED)
- Authors : Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, Yue Cao
- Link : [http://arxiv.org/abs/2205.13543](http://arxiv.org/abs/2205.13543)
> ABSTRACT  :  Masked image modeling (MIM) as pre-training is shown to be effective for numerous vision downstream tasks, but how and where MIM works remain unclear. In this paper, we compare MIM with the long-dominant supervised pre-trained models from two perspectives, the visualizations and the experiments, to uncover their key representational differences. From the visualizations, we find that MIM brings locality inductive bias to all layers of the trained models, but supervised models tend to focus locally at lower layers but more globally at higher layers. That may be the reason why MIM helps Vision Transformers that have a very large receptive field to optimize. Using MIM, the model can maintain a large diversity on attention heads in all layers. But for supervised models, the diversity on attention heads almost disappears from the last three layers and less diversity harms the fine-tuning performance. From the experiments, we find that MIM models can perform significantly better on geometric and motion tasks with weak semantics or fine-grained classification tasks, than their supervised counterparts. Without bells and whistles, a standard MIM pre-trained **Swin**V2-L could achieve state-of-the-art performance on pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the categories are sufficiently covered by the supervised pre-training, MIM models can still achieve highly competitive transfer performance. With a deeper understanding of MIM, we hope that our work can inspire new and solid research in this direction.  
## eess.IV
---
### Image Reconstruction of Multi Branch Feature Multiplexing Fusion Network with Mixed Multi-layer Attention. (arXiv:2205.13738v1 [cs.CV])
- Authors : Yuxi Cai, Huicheng Lai
- Link : [http://arxiv.org/abs/2205.13738](http://arxiv.org/abs/2205.13738)
> ABSTRACT  :  Image super-resolution reconstruction achieves better results than traditional methods with the help of the powerful nonlinear representation ability of convolution neural network. However, some existing algorithms also have some problems, such as insufficient utilization of phased features, ignoring the importance of early phased feature fusion to improve network performance, and the inability of the network to pay more attention to high-frequency information in the reconstruction process. To solve these problems, we propose a multi-branch feature multiplexing fusion network with mixed multi-layer attention (MBMFN), which realizes the multiple utilization of features and the multistage fusion of different levels of features. To further improve the networks performance, we propose a lightweight enhanced residual channel attention (LERCA), which can not only effectively avoid the loss of channel information but also make the network pay more attention to the key channel information and benefit from it. Finally, the attention mechanism is introduced into the reconstruction process to strengthen the **restoration** of edge texture and other details. A large number of experiments on several benchmark sets show that, compared with other advanced reconstruction algorithms, our algorithm produces highly competitive objective indicators and restores more image detail texture information.  
### Fast Multi-grid Methods for Minimizing Curvature Energy. (arXiv:2204.07921v2 [eess.IV] UPDATED)
- Authors : Zhenwei Zhang, Ke Chen, Yuping Duan
- Link : [http://arxiv.org/abs/2204.07921](http://arxiv.org/abs/2204.07921)
> ABSTRACT  :  The geometric high-order regularization methods such as mean curvature and Gaussian curvature, have been intensively studied during the last decades due to their abilities in preserving geometric properties including image edges, corners, and image contrast. However, the dilemma between **restoration** quality and computational efficiency is an essential roadblock for high-order methods. In this paper, we propose fast multi-grid algorithms for minimizing both mean curvature and Gaussian curvature energy functionals without sacrificing the accuracy for efficiency. Unlike the existing approaches based on operator splitting and the Augmented Lagrangian method (ALM), no artificial parameters are introduced in our formulation, which guarantees the robustness of the proposed algorithm. Meanwhile, we adopt the domain decomposition method to promote parallel computing and use the fine-to-coarse structure to accelerate the convergence. Numerical experiments are presented on both image denoising and CT reconstruction problem to demonstrate the ability to recover image texture and the efficiency of the proposed method.  
## cs.LG
---
### MyoSuite -- A contact-rich simulation suite for musculoskeletal motor control. (arXiv:2205.13600v1 [cs.RO])
- Authors : Vittorio Caggiano, Huawei Wang, Guillaume Durandau, Massimo Sartori, Vikash Kumar
- Link : [http://arxiv.org/abs/2205.13600](http://arxiv.org/abs/2205.13600)
> ABSTRACT  :  Embodied agents in continuous control domains have had limited **exposure** to tasks allowing to explore musculoskeletal properties that enable agile and nimble behaviors in biological beings. The sophistication behind neuro-musculoskeletal control can pose new challenges for the motor learning community. At the same time, agents solving complex neural control problems allow impact in fields such as neuro-rehabilitation, as well as collaborative-robotics. Human biomechanics underlies complex multi-joint-multi-actuator musculoskeletal systems. The sensory-motor system relies on a range of sensory-contact rich and proprioceptive inputs that define and condition muscle actuation required to exhibit intelligent behaviors in the physical world. Current frameworks for musculoskeletal control do not support physiological sophistication of the musculoskeletal systems along with physical world interaction capabilities. In addition, they are neither embedded in complex and skillful motor tasks nor are computationally effective and scalable to study large-scale learning paradigms. Here, we present MyoSuite -- a suite of physiologically accurate biomechanical models of elbow, wrist, and hand, with physical contact capabilities, which allow learning of complex and skillful contact-rich real-world tasks. We provide diverse motor-control challenges: from simple postural control to skilled hand-object interactions such as turning a key, twirling a pen, rotating two balls in one hand, etc. By supporting physiological alterations in musculoskeletal geometry (tendon transfer), assistive devices (exoskeleton assistance), and muscle contraction dynamics (muscle fatigue, sarcopenia), we present real-life tasks with temporal changes, thereby exposing realistic non-stationary conditions in our tasks which most continuous control benchmarks lack.  
### Comparison of Deep Learning Segmentation and Multigrader-annotated Mandibular Canals of Multicenter CBCT scans. (arXiv:2205.13874v1 [cs.LG])
- Authors : Jaakko Sahlsten, Joel Jaskari, Kimmo Kaski, Helena Mehtonen, Ziyuan Lin, Ari Hietanen, Osku Sundqvist, Vesa Varjonen, Vesa Mattila, Sangsom Prapayasotok, Sakarat Nalampang
- Link : [http://arxiv.org/abs/2205.13874](http://arxiv.org/abs/2205.13874)
> ABSTRACT  :  Deep learning approach has been demonstrated to automatically segment the **bilateral** mandibular canals from CBCT scans, yet systematic studies of its clinical and technical validation are scarce. To validate the mandibular canal localization accuracy of a deep learning system (DLS) we trained it with 982 CBCT scans and evaluated using 150 scans of five scanners from clinical workflow patients of European and Southeast Asian Institutes, annotated by four radiologists. The interobserver variability was compared to the variability between the DLS and the radiologists. In addition, the generalization of DLS to CBCT scans from scanners not used in the training data was examined to evaluate the out-of-distribution generalization capability. The DLS had lower variability to the radiologists than the interobserver variability between them and it was able to generalize to three new devices. For the radiologists' consensus segmentation, used as gold standard, the DLS had a symmetric mean curve distance of 0.39 mm compared to those of the individual radiologists with 0.62 mm, 0.55 mm, 0.47 mm, and 0.42 mm. The DLS showed comparable or slightly better performance in the segmentation of the mandibular canal with the radiologists and generalization capability to new scanners.  
### Average Adjusted Association: Efficient Estimation with High Dimensional Confounders. (arXiv:2205.14048v1 [stat.ME])
- Authors : Sung Jae, Sokbae Lee
- Link : [http://arxiv.org/abs/2205.14048](http://arxiv.org/abs/2205.14048)
> ABSTRACT  :  The log odds ratio is a common parameter to measure association between (binary) outcome and **exposure** variables. Much attention has been paid to its parametric but robust estimation, or its nonparametric estimation as a function of confounders. However, discussion on how to use a summary statistic by averaging the log odds ratio function is surprisingly difficult to find despite the popularity and importance of averaging in other contexts such as estimating the average treatment effect. We propose a couple of efficient double/debiased machine learning (DML) estimators of the average log odds ratio, where the odds ratios are adjusted for observed (potentially high dimensional) confounders and are averaged over them. The estimators are built from two equivalent forms of the efficient influence function. The first estimator uses a prospective probability of the outcome conditional on the **exposure** and confounders; the second one employs a retrospective probability of the **exposure** conditional on the outcome and confounders. Our framework encompasses random sampling as well as outcome-based or **exposure**-based sampling. Finally, we illustrate how to apply the proposed estimators using real data.  
### Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v1 [cs.CV])
- Authors : Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, Baining Guo
- Link : [http://arxiv.org/abs/2205.14141](http://arxiv.org/abs/2205.14141)
> ABSTRACT  :  Masked image modeling (MIM) learns representations with remarkably good fine-tuning performances, overshadowing previous prevalent pre-training approaches such as image classification, instance contrastive learning, and image-text alignment. In this paper, we show that the inferior fine-tuning performance of these pre-training approaches can be significantly improved by a simple post-processing in the form of feature distillation (FD). The feature distillation converts the old representations to new representations that have a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by a set of attention- and optimization-related diagnosis tools. With these properties, the new representations show strong fine-tuning performance. Specifically, the contrastive self-supervised learning methods are made as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is also significantly improved, with a CLIP ViT-L model reaching 89.0% top-1 accuracy on ImageNet-1K classification. More importantly, our work provides a way for the future research to focus more effort on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness since it can be enhanced rather easily. The code will be available at https://github.com/**Swin**Transformer/Feature-Distillation.  
### Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v3 [cs.CV] UPDATED)
- Authors : 
- Link : [http://arxiv.org/abs/2110.13285](http://arxiv.org/abs/2110.13285)
> ABSTRACT  :  Due to the success of generative flows to model data distributions, they have been explored in inverse problems. Given a pre-trained generative flow, previous work proposed to minimize the 2-norm of the latent variables as a regularization term. The intuition behind it was to ensure high likelihood latent variables that produce the closest **restoration**. However, high-likelihood latent variables may generate unrealistic samples as we show in our experiments. We therefore propose a solver to directly produce high-likelihood reconstructions. We hypothesize that our approach could make generative flows a general purpose solver for inverse problems. Furthermore, we propose 1 x 1 coupling functions to introduce permutations in a generative flow. It has the advantage that its inverse does not require to be calculated in the generation process. Finally, we evaluate our method for denoising, deblurring, inpainting, and colorization. We observe a compelling improvement of our method over prior works.  
### Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v2 [cs.CV] UPDATED)
- Authors : Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, Yue Cao
- Link : [http://arxiv.org/abs/2205.13543](http://arxiv.org/abs/2205.13543)
> ABSTRACT  :  Masked image modeling (MIM) as pre-training is shown to be effective for numerous vision downstream tasks, but how and where MIM works remain unclear. In this paper, we compare MIM with the long-dominant supervised pre-trained models from two perspectives, the visualizations and the experiments, to uncover their key representational differences. From the visualizations, we find that MIM brings locality inductive bias to all layers of the trained models, but supervised models tend to focus locally at lower layers but more globally at higher layers. That may be the reason why MIM helps Vision Transformers that have a very large receptive field to optimize. Using MIM, the model can maintain a large diversity on attention heads in all layers. But for supervised models, the diversity on attention heads almost disappears from the last three layers and less diversity harms the fine-tuning performance. From the experiments, we find that MIM models can perform significantly better on geometric and motion tasks with weak semantics or fine-grained classification tasks, than their supervised counterparts. Without bells and whistles, a standard MIM pre-trained **Swin**V2-L could achieve state-of-the-art performance on pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the categories are sufficiently covered by the supervised pre-training, MIM models can still achieve highly competitive transfer performance. With a deeper understanding of MIM, we hope that our work can inspire new and solid research in this direction.  
## cs.AI
---
### MyoSuite -- A contact-rich simulation suite for musculoskeletal motor control. (arXiv:2205.13600v1 [cs.RO])
- Authors : Vittorio Caggiano, Huawei Wang, Guillaume Durandau, Massimo Sartori, Vikash Kumar
- Link : [http://arxiv.org/abs/2205.13600](http://arxiv.org/abs/2205.13600)
> ABSTRACT  :  Embodied agents in continuous control domains have had limited **exposure** to tasks allowing to explore musculoskeletal properties that enable agile and nimble behaviors in biological beings. The sophistication behind neuro-musculoskeletal control can pose new challenges for the motor learning community. At the same time, agents solving complex neural control problems allow impact in fields such as neuro-rehabilitation, as well as collaborative-robotics. Human biomechanics underlies complex multi-joint-multi-actuator musculoskeletal systems. The sensory-motor system relies on a range of sensory-contact rich and proprioceptive inputs that define and condition muscle actuation required to exhibit intelligent behaviors in the physical world. Current frameworks for musculoskeletal control do not support physiological sophistication of the musculoskeletal systems along with physical world interaction capabilities. In addition, they are neither embedded in complex and skillful motor tasks nor are computationally effective and scalable to study large-scale learning paradigms. Here, we present MyoSuite -- a suite of physiologically accurate biomechanical models of elbow, wrist, and hand, with physical contact capabilities, which allow learning of complex and skillful contact-rich real-world tasks. We provide diverse motor-control challenges: from simple postural control to skilled hand-object interactions such as turning a key, twirling a pen, rotating two balls in one hand, etc. By supporting physiological alterations in musculoskeletal geometry (tendon transfer), assistive devices (exoskeleton assistance), and muscle contraction dynamics (muscle fatigue, sarcopenia), we present real-life tasks with temporal changes, thereby exposing realistic non-stationary conditions in our tasks which most continuous control benchmarks lack.  
### Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v2 [cs.CV] UPDATED)
- Authors : Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, Yue Cao
- Link : [http://arxiv.org/abs/2205.13543](http://arxiv.org/abs/2205.13543)
> ABSTRACT  :  Masked image modeling (MIM) as pre-training is shown to be effective for numerous vision downstream tasks, but how and where MIM works remain unclear. In this paper, we compare MIM with the long-dominant supervised pre-trained models from two perspectives, the visualizations and the experiments, to uncover their key representational differences. From the visualizations, we find that MIM brings locality inductive bias to all layers of the trained models, but supervised models tend to focus locally at lower layers but more globally at higher layers. That may be the reason why MIM helps Vision Transformers that have a very large receptive field to optimize. Using MIM, the model can maintain a large diversity on attention heads in all layers. But for supervised models, the diversity on attention heads almost disappears from the last three layers and less diversity harms the fine-tuning performance. From the experiments, we find that MIM models can perform significantly better on geometric and motion tasks with weak semantics or fine-grained classification tasks, than their supervised counterparts. Without bells and whistles, a standard MIM pre-trained **Swin**V2-L could achieve state-of-the-art performance on pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the categories are sufficiently covered by the supervised pre-training, MIM models can still achieve highly competitive transfer performance. With a deeper understanding of MIM, we hope that our work can inspire new and solid research in this direction.  
# Paper List
---
## cs.CV
---
**89** new papers in cs.CV:-) 
1. CA-UDA: Class-Aware Unsupervised Domain Adaptation with Optimal Assignment and Pseudo-Label Refinement. (arXiv:2205.13579v1 [cs.CV])
2. Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis. (arXiv:2205.13583v1 [cs.AI])
3. VectorAdam for Rotation Equivariant Geometry Optimization. (arXiv:2205.13599v1 [cs.LG])
4. Circumventing Backdoor Defenses That Are Based on Latent Separability. (arXiv:2205.13613v1 [cs.LG])
5. Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations. (arXiv:2205.13616v1 [cs.LG])
6. Denial-of-Service Attack on Object Detection Model Using Universal Adversarial Perturbation. (arXiv:2205.13618v1 [cs.CV])
7. A Hybrid Neural Autoencoder for Sensory Neuroprostheses and Its Applications in Bionic Vision. (arXiv:2205.13623v1 [cs.LG])
8. Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation. (arXiv:2205.13629v1 [cs.CV])
9. Spatio-temporally separable non-linear latent factor learning: an application to somatomotor cortex fMRI data. (arXiv:2205.13640v1 [cs.CV])
10. Membership Inference Attack Using Self Influence Functions. (arXiv:2205.13680v1 [cs.LG])
11. ANISE: Assembly-based Neural Implicit Surface rEconstruction. (arXiv:2205.13682v1 [cs.CV])
12. PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences. (arXiv:2205.13713v1 [cs.CV])
13. Effective Abstract Reasoning with Dual-Contrast Network. (arXiv:2205.13720v1 [cs.CV])
14. DLTTA: Dynamic Learning Rate for Test-time Adaptation on Cross-domain Medical Images. (arXiv:2205.13723v1 [cs.CV])
15. V-Doc : Visual questions answers with Documents. (arXiv:2205.13724v1 [cs.AI])
16. Image Reconstruction of Multi Branch Feature Multiplexing Fusion Network with Mixed Multi-layer Attention. (arXiv:2205.13738v1 [cs.CV])
17. Learning Instance Representation Banks for Aerial Scene Classification. (arXiv:2205.13744v1 [cs.CV])
18. Attention Awareness Multiple Instance Neural Network. (arXiv:2205.13750v1 [cs.CV])
19. CIGMO: Categorical invariant representations in a deep generative framework. (arXiv:2205.13758v1 [cs.CV])
20. Fully Convolutional One-Stage 3D Object Detection on LiDAR Range Images. (arXiv:2205.13764v1 [cs.CV])
21. Semantic-aware Dense Representation Learning for Remote Sensing Image Change Detection. (arXiv:2205.13769v1 [cs.CV])
22. LEAF + AIO: Edge-Assisted Energy-Aware Object Detection for Mobile Augmented Reality. (arXiv:2205.13770v1 [cs.CV])
23. Classification of COVID-19 Patients with their Severity Level from Chest CT Scans using Transfer Learning. (arXiv:2205.13774v1 [eess.IV])
24. A Survey on Long-Tailed Visual Recognition. (arXiv:2205.13775v1 [cs.CV])
25. BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework. (arXiv:2205.13790v1 [cs.CV])
26. Face Morphing: Fooling a Face Recognition System Is Simple!. (arXiv:2205.13796v1 [cs.CV])
27. Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions. (arXiv:2205.13803v1 [cs.CV])
28. X-ViT: High Performance Linear Vision Transformer without Softmax. (arXiv:2205.13805v1 [cs.CV])
29. A Look at Improving Robustness in Visual-inertial SLAM by Moment Matching. (arXiv:2205.13821v1 [cs.RO])
30. Deep Learning Fetal Ultrasound Video Model Match Human Observers in Biometric Measurements. (arXiv:2205.13835v1 [eess.IV])
31. Textural-Structural Joint Learning for No-Reference Super-Resolution Image Quality Assessment. (arXiv:2205.13847v1 [eess.IV])
32. Finding Patterns in Visualized Data by Adding Redundant Visual Information. (arXiv:2205.13856v1 [stat.CO])
33. TrackNet: A Triplet metric-based method for Multi-Target Multi-Camera Vehicle Tracking. (arXiv:2205.13857v1 [cs.CV])
34. Comparison of Deep Learning Segmentation and Multigrader-annotated Mandibular Canals of Multicenter CBCT scans. (arXiv:2205.13874v1 [cs.LG])
35. TraClets: Harnessing the power of computer vision for trajectory classification. (arXiv:2205.13880v1 [cs.CV])
36. Dynamic Domain Generalization. (arXiv:2205.13913v1 [cs.LG])
37. 3DILG: Irregular Latent Grids for 3D Generative Modeling. (arXiv:2205.13914v1 [cs.CV])
38. CREAM: Weakly Supervised Object Localization via Class RE-Activation Mapping. (arXiv:2205.13922v1 [cs.CV])
39. Deep face recognition with clustering based domain adaptation. (arXiv:2205.13937v1 [cs.CV])
40. Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v1 [cs.CV])
41. Cycle Label-Consistent Networks for Unsupervised Domain Adaptation. (arXiv:2205.13957v1 [cs.CV])
42. Video2StyleGAN: Disentangling Local and Global Variations in a Video. (arXiv:2205.13996v1 [cs.CV])
43. Future Transformer for Long-term Action Anticipation. (arXiv:2205.14022v1 [cs.CV])
44. Lesion classification by model-based feature extraction: A differential affine invariant model of soft tissue elasticity. (arXiv:2205.14029v1 [eess.IV])
45. Reinforced Pedestrian Attribute Recognition with Group Optimization Reward. (arXiv:2205.14042v1 [cs.CV])
46. Fine-tuning deep learning models for stereo matching using results from semi-global matching. (arXiv:2205.14051v1 [cs.CV])
47. Image Harmonization with Region-wise Contrastive Learning. (arXiv:2205.14058v1 [cs.CV])
48. Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos. (arXiv:2205.14065v1 [cs.CV])
49. Sharpness-Aware Training for Free. (arXiv:2205.14083v1 [cs.LG])
50. OpenCalib: A multi-sensor calibration toolbox for autonomous driving. (arXiv:2205.14087v1 [cs.RO])
51. GIT: A Generative Image-to-text Transformer for Vision and Language. (arXiv:2205.14100v1 [cs.CV])
52. Scalable Interpretability via Polynomials. (arXiv:2205.14108v1 [cs.LG])
53. Improving Road Segmentation in Challenging Domains Using Similar Place Priors. (arXiv:2205.14112v1 [cs.CV])
54. Efficient textual explanations for complex road and traffic scenarios based on semantic segmentation. (arXiv:2205.14118v1 [cs.CV])
55. Neural Basis Models for Interpretability. (arXiv:2205.14120v1 [cs.LG])
56. Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v1 [cs.CV])
57. DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v5 [cs.CV] UPDATED)
58. 3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame. (arXiv:2105.06340v4 [cs.CV] UPDATED)
59. Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v3 [cs.CV] UPDATED)
60. Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts. (arXiv:2108.11926v4 [cs.CV] UPDATED)
61. InfAnFace: Bridging the infant-adult domain gap in facial landmark estimation in the wild. (arXiv:2110.08935v3 [cs.CV] UPDATED)
62. IMPROVE Visiolinguistic Performance with Re-Query. (arXiv:2110.10206v2 [cs.CV] UPDATED)
63. Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v3 [cs.CV] UPDATED)
64. VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v2 [cs.CV] UPDATED)
65. Semi-Supervised Domain Generalization in Real World: New Benchmark and Strong Baseline. (arXiv:2111.10221v2 [cs.CV] UPDATED)
66. Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks. (arXiv:2111.12965v2 [cs.CR] UPDATED)
67. Anonymization for Skeleton Action Recognition. (arXiv:2111.15129v2 [cs.CV] UPDATED)
68. Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks. (arXiv:2112.15139v4 [cs.CV] UPDATED)
69. Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis. (arXiv:2202.02444v2 [cs.CV] UPDATED)
70. Random Walks for Adversarial Meshes. (arXiv:2202.07453v2 [cs.CV] UPDATED)
71. A Simple and Universal Rotation Equivariant Point-cloud Network. (arXiv:2203.01216v3 [cs.LG] UPDATED)
72. Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v4 [cs.CV] UPDATED)
73. Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4. (arXiv:2203.06649v2 [q-bio.NC] UPDATED)
74. A Two-Stage Federated Transfer Learning Framework in Medical Images Classification on Limited Data: A COVID-19 Case Study. (arXiv:2203.12803v2 [eess.IV] UPDATED)
75. Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. (arXiv:2203.13457v2 [cs.LG] UPDATED)
76. FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable Model from a Hybrid Dataset. (arXiv:2203.14057v3 [cs.CV] UPDATED)
77. UV Volumes for **Real-time** Rendering of Editable Free-view Human Performance. (arXiv:2203.14402v2 [cs.CV] UPDATED)
78. Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v2 [cs.CV] UPDATED)
79. E^2TAD: An Energy-Efficient Tracking-based Action Detector. (arXiv:2204.04416v2 [cs.CV] UPDATED)
80. Fast Multi-grid Methods for Minimizing Curvature Energy. (arXiv:2204.07921v2 [eess.IV] UPDATED)
81. Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (arXiv:2204.14057v3 [cs.SD] UPDATED)
82. CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers. (arXiv:2204.14217v2 [cs.CV] UPDATED)
83. Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations. (arXiv:2205.05167v2 [cs.CV] UPDATED)
84. "Teaching Independent Parts Separately" (TIPSy-GAN) : Improving Accuracy and Stability in Unsupervised Adversarial 2D to 3D Pose Estimation. (arXiv:2205.05980v3 [cs.CV] UPDATED)
85. Structured Attention Composition for Temporal Action Localization. (arXiv:2205.09956v2 [cs.CV] UPDATED)
86. UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes. (arXiv:2205.10337v2 [cs.CV] UPDATED)
87. SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v2 [cs.CV] UPDATED)
88. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v2 [cs.CV] UPDATED)
89. Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v2 [cs.CV] UPDATED)
## eess.IV
---
**12** new papers in eess.IV:-) 
1. Effective drug combination for Caenorhabditis elegans nematodes discovered by output-driven feedback system control technique. (arXiv:2205.13544v1 [q-bio.QM])
2. Image Reconstruction of Multi Branch Feature Multiplexing Fusion Network with Mixed Multi-layer Attention. (arXiv:2205.13738v1 [cs.CV])
3. Classification of COVID-19 Patients with their Severity Level from Chest CT Scans using Transfer Learning. (arXiv:2205.13774v1 [eess.IV])
4. Deep Learning Fetal Ultrasound Video Model Match Human Observers in Biometric Measurements. (arXiv:2205.13835v1 [eess.IV])
5. Textural-Structural Joint Learning for No-Reference Super-Resolution Image Quality Assessment. (arXiv:2205.13847v1 [eess.IV])
6. Lesion classification by model-based feature extraction: A differential affine invariant model of soft tissue elasticity. (arXiv:2205.14029v1 [eess.IV])
7. Deep Coding Patterns Design for Compressive Near-Infrared Spectral Classification. (arXiv:2205.14069v1 [cs.LG])
8. Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts. (arXiv:2108.11926v4 [cs.CV] UPDATED)
9. Learning English with Peppa Pig. (arXiv:2202.12917v2 [cs.CL] UPDATED)
10. Hybrid training of optical neural networks. (arXiv:2203.11207v2 [cs.LG] UPDATED)
11. A Two-Stage Federated Transfer Learning Framework in Medical Images Classification on Limited Data: A COVID-19 Case Study. (arXiv:2203.12803v2 [eess.IV] UPDATED)
12. Fast Multi-grid Methods for Minimizing Curvature Energy. (arXiv:2204.07921v2 [eess.IV] UPDATED)
## cs.LG
---
**214** new papers in cs.LG:-) 
1. Learning black- and gray-box chemotactic PDEs/closures from agent based Monte Carlo simulation data. (arXiv:2205.13545v1 [q-bio.QM])
2. Training and Inference on Any-Order Autoregressive Models the Right Way. (arXiv:2205.13554v1 [cs.LG])
3. Unequal Covariance Awareness for Fisher Discriminant Analysis and Its Variants in Classification. (arXiv:2205.13565v1 [cs.LG])
4. Exploration, Exploitation, and Engagement in Multi-Armed Bandits with Abandonment. (arXiv:2205.13566v1 [cs.LG])
5. Learning Dialogue Representations from Consecutive Utterances. (arXiv:2205.13568v1 [cs.CL])
6. Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations. (arXiv:2205.13571v1 [cs.LG])
7. Efficient Approximation of Gromov-Wasserstein Distance using Importance Sparsification. (arXiv:2205.13573v1 [cs.LG])
8. Pruning has a disparate impact on model accuracy. (arXiv:2205.13574v1 [cs.LG])
9. Predictor-corrector algorithms for stochastic optimization under gradual distribution shift. (arXiv:2205.13575v1 [cs.LG])
10. Understanding new tasks through the lens of training data via exponential tilting. (arXiv:2205.13577v1 [cs.LG])
11. Dynamic Network Reconfiguration for Entropy Maximization using Deep Reinforcement Learning. (arXiv:2205.13578v1 [cs.LG])
12. Learning in Feedback-driven Recurrent Spiking Neural Networks using full-FORCE Training. (arXiv:2205.13585v1 [cs.AI])
13. Evolution of beliefs in social networks. (arXiv:2205.13587v1 [cs.LG])
14. Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes. (arXiv:2205.13589v1 [cs.LG])
15. DRLComplex: Reconstruction of protein quaternary structures using deep reinforcement learning. (arXiv:2205.13594v1 [cs.LG])
16. VectorAdam for Rotation Equivariant Geometry Optimization. (arXiv:2205.13599v1 [cs.LG])
17. MyoSuite -- A contact-rich simulation suite for musculoskeletal motor control. (arXiv:2205.13600v1 [cs.RO])
18. Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods. (arXiv:2205.13602v1 [stat.ME])
19. Tensor Program Optimization with Probabilistic Programs. (arXiv:2205.13603v1 [cs.LG])
20. Self-supervised Pretraining and Transfer Learning Enable Flu and COVID-19 Predictions in Small Mobile Sensing Datasets. (arXiv:2205.13607v1 [cs.LG])
21. Circumventing Backdoor Defenses That Are Based on Latent Separability. (arXiv:2205.13613v1 [cs.LG])
22. Emergent organization of receptive fields in networks of excitatory and inhibitory neurons. (arXiv:2205.13614v1 [q-bio.NC])
23. Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations. (arXiv:2205.13616v1 [cs.LG])
24. Approximate Q-learning and SARSA(0) under the $\epsilon$-greedy Policy: a Differential Inclusion Analysis. (arXiv:2205.13617v1 [cs.LG])
25. Denial-of-Service Attack on Object Detection Model Using Universal Adversarial Perturbation. (arXiv:2205.13618v1 [cs.CV])
26. Fairness in Recommendation: A Survey. (arXiv:2205.13619v1 [cs.IR])
27. Differentially Private Decoding in Large Language Models. (arXiv:2205.13621v1 [cs.CL])
28. A Hybrid Neural Autoencoder for Sensory Neuroprostheses and Its Applications in Bionic Vision. (arXiv:2205.13623v1 [cs.LG])
29. Faster Optimization on Sparse Graphs via Neural Reparametrization. (arXiv:2205.13624v1 [cs.LG])
30. BagFlip: A Certified Defense against Data Poisoning. (arXiv:2205.13634v1 [cs.LG])
31. RIGID: Robust Linear Regression with Missing Data. (arXiv:2205.13635v1 [cs.LG])
32. Quark: Controllable Text Generation with Reinforced Unlearning. (arXiv:2205.13636v1 [cs.CL])
33. A Model Predictive Control Functional Continuous Time Bayesian Network for Self-Management of Multiple Chronic Conditions. (arXiv:2205.13639v1 [cs.LG])
34. Learning to Reason with Neural Networks: Generalization, Unseen Data and Boolean Measures. (arXiv:2205.13647v1 [cs.LG])
35. A Unified Analysis of Federated Learning with Arbitrary Client Participation. (arXiv:2205.13648v1 [cs.LG])
36. Mixed Federated Learning: Joint Decentralized and Centralized Learning. (arXiv:2205.13655v1 [cs.LG])
37. Contextual Adapters for Personalized Speech Recognition in Neural Transducers. (arXiv:2205.13660v1 [cs.CL])
38. Explaining Preferences with Shapley Values. (arXiv:2205.13662v1 [stat.ML])
39. Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v1 [cs.LG])
40. Global Normalization for Streaming Speech Recognition in a Modular Framework. (arXiv:2205.13674v1 [cs.LG])
41. Reinforcement Learning Approach for Mapping Applications to Dataflow-Based Coarse-Grained Reconfigurable Array. (arXiv:2205.13675v1 [cs.AR])
42. Fast variable selection makes scalable Gaussian process BSS-ANOVA a speedy and accurate choice for tabular and time series regression. (arXiv:2205.13676v1 [cs.LG])
43. SeedGNN: Graph Neural Networks for Supervised Seeded Graph Matching. (arXiv:2205.13679v1 [cs.LG])
44. Membership Inference Attack Using Self Influence Functions. (arXiv:2205.13680v1 [cs.LG])
45. Learning with Stochastic Orders. (arXiv:2205.13684v1 [stat.ML])
46. Asymptotic Convergence Rate and Statistical Inference for Stochastic Sequential Quadratic Programming. (arXiv:2205.13687v1 [math.OC])
47. Safety Aware Changepoint Detection for Piecewise i.i.d. Bandits. (arXiv:2205.13689v1 [cs.LG])
48. FedAvg with Fine Tuning: Local Updates Lead to Representation Learning. (arXiv:2205.13692v1 [cs.LG])
49. FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v1 [cs.LG])
50. Maximum Likelihood Training of Implicit Nonlinear Diffusion Models. (arXiv:2205.13699v1 [cs.LG])
51. ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge Splitting. (arXiv:2205.13700v1 [cs.LG])
52. Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters. (arXiv:2205.13703v1 [cs.LG])
53. DP-PCA: Statistically Optimal and Differentially Private PCA. (arXiv:2205.13709v1 [cs.LG])
54. Privacy of Noisy Stochastic Gradient Descent: More Iterations without More Privacy Loss. (arXiv:2205.13710v1 [cs.LG])
55. Incorporating the Barzilai-Borwein Adaptive Step Size into Sugradient Methods for Deep Network Training. (arXiv:2205.13711v1 [cs.LG])
56. Hazard Gradient Penalty for Survival Analysis. (arXiv:2205.13717v1 [cs.LG])
57. Off-Beat Multi-Agent Reinforcement Learning. (arXiv:2205.13718v1 [cs.MA])
58. Effective Abstract Reasoning with Dual-Contrast Network. (arXiv:2205.13720v1 [cs.CV])
59. Can Foundation Models Help Us Achieve Perfect Secrecy?. (arXiv:2205.13722v1 [cs.LG])
60. On Consistency in Graph Neural Network Interpretation. (arXiv:2205.13733v1 [cs.LG])
61. Subverting machines, fluctuating identities: Re-learning human categorization. (arXiv:2205.13740v1 [cs.LG])
62. Group GAN. (arXiv:2205.13741v1 [cs.LG])
63. Generating personalized counterfactual interventions for algorithmic recourse by eliciting user preferences. (arXiv:2205.13743v1 [cs.LG])
64. Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games. (arXiv:2205.13746v1 [math.OC])
65. Auto-PINN: Understanding and Optimizing Physics-Informed Neural Architecture. (arXiv:2205.13748v1 [cs.LG])
66. HOUDINI: Escaping from Moderately Constrained Saddles. (arXiv:2205.13753v1 [cs.LG])
67. Representing Polymers as Periodic Graphs with Learned Descriptors for Accurate Polymer Property Predictions. (arXiv:2205.13757v1 [cond-mat.mtrl-sci])
68. CIGMO: Categorical invariant representations in a deep generative framework. (arXiv:2205.13758v1 [cs.CV])
69. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. (arXiv:2205.13760v1 [cs.LG])
70. Block-coordinate Frank-Wolfe algorithm and convergence analysis for semi-relaxed optimal transport problem. (arXiv:2205.13766v1 [cs.LG])
71. Text-Based Automatic Personality Prediction Using KGrAt-Net; A Knowledge Graph Attention Network Classifier. (arXiv:2205.13780v1 [cs.CL])
72. A Sea of Words: An In-Depth Analysis of Anchors for Text Data. (arXiv:2205.13789v1 [stat.ML])
73. AsyncFedED: Asynchronous Federated Learning with Euclidean Distance based Adaptive Weight Aggregation. (arXiv:2205.13797v1 [cs.LG])
74. Generalization Bounds for Gradient Methods via Discrete and Continuous Prior. (arXiv:2205.13799v1 [cs.LG])
75. Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions. (arXiv:2205.13803v1 [cs.CV])
76. End-to-End Learning of Hybrid Inverse Dynamics Models for Precise and Compliant Impedance Control. (arXiv:2205.13804v1 [cs.RO])
77. X-ViT: High Performance Linear Vision Transformer without Softmax. (arXiv:2205.13805v1 [cs.CV])
78. fakeWeather: Adversarial Attacks for Deep Neural Networks Emulating Weather Conditions on the Camera Lens of Autonomous Systems. (arXiv:2205.13807v1 [cs.LG])
79. Global Convergence of Over-parameterized Deep Equilibrium Models. (arXiv:2205.13814v1 [cs.LG])
80. Prune and distill: similar reformatting of image information along rat visual cortex and deep neural networks. (arXiv:2205.13816v1 [q-bio.NC])
81. Isolating and Leveraging Controllable and Noncontrollable Visual Dynamics in World Models. (arXiv:2205.13817v1 [cs.LG])
82. Multivariate Probabilistic Forecasting of Intraday Electricity Prices using Normalizing Flows. (arXiv:2205.13826v1 [cs.LG])
83. Error Bound of Empirical $\ell_2$ Risk Minimization for Noisy Standard and Generalized Phase Retrieval Problems. (arXiv:2205.13827v1 [stat.ML])
84. Counterfactual Analysis in Dynamic Models: Copulas and Bounds. (arXiv:2205.13832v1 [cs.LG])
85. Improving Bidding and Playing Strategies in the Trick-Taking game Wizard using Deep Q-Networks. (arXiv:2205.13834v1 [cs.LG])
86. Deep Learning Fetal Ultrasound Video Model Match Human Observers in Biometric Measurements. (arXiv:2205.13835v1 [eess.IV])
87. Feudal Multi-Agent Reinforcement Learning with Adaptive Network Partition for Traffic Signal Control. (arXiv:2205.13836v1 [cs.MA])
88. Adaptive Random Forests for Energy-Efficient Inference on Microcontrollers. (arXiv:2205.13838v1 [cs.LG])
89. Raising the Bar in Graph-level Anomaly Detection. (arXiv:2205.13845v1 [cs.LG])
90. On the Convergence of Semi-Relaxed Sinkhorn with Marginal Constraint and OT Distance Gaps. (arXiv:2205.13846v1 [cs.LG])
91. Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power. (arXiv:2205.13863v1 [cs.LG])
92. MissDAG: Causal Discovery in the Presence of Missing Data with Continuous Additive Noise Models. (arXiv:2205.13869v1 [cs.LG])
93. Probabilistic Systems with Hidden State and Unobservable Transitions. (arXiv:2205.13871v1 [cs.LG])
94. Comparison of Deep Learning Segmentation and Multigrader-annotated Mandibular Canals of Multicenter CBCT scans. (arXiv:2205.13874v1 [cs.LG])
95. MIMII DG: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection for Domain Generalization Task. (arXiv:2205.13879v1 [cs.SD])
96. TraClets: Harnessing the power of computer vision for trajectory classification. (arXiv:2205.13880v1 [cs.CV])
97. Automated Dynamic Algorithm Configuration. (arXiv:2205.13881v1 [cs.AI])
98. Transformers from an Optimization Perspective. (arXiv:2205.13891v1 [cs.LG])
99. EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks. (arXiv:2205.13892v1 [cs.LG])
100. How Tempering Fixes Data Augmentation in Bayesian Neural Networks. (arXiv:2205.13900v1 [cs.LG])
101. Bias Reduction via Cooperative Bargaining in Synthetic Graph Dataset Generation. (arXiv:2205.13901v1 [cs.LG])
102. Sample-Efficient Optimisation with Probabilistic Transformer Surrogates. (arXiv:2205.13902v1 [cs.LG])
103. (De-)Randomized Smoothing for Decision Stump Ensembles. (arXiv:2205.13909v1 [cs.LG])
104. Dynamic Domain Generalization. (arXiv:2205.13913v1 [cs.LG])
105. Fast Causal Orientation Learning in Directed Acyclic Graphs. (arXiv:2205.13919v1 [cs.LG])
106. ProtoFSSL: Federated Semi-Supervised Learning with Prototype-based Consistency Regularization. (arXiv:2205.13921v1 [cs.LG])
107. Lifting the Information Ratio: An Information-Theoretic Analysis of Thompson Sampling for Contextual Bandits. (arXiv:2205.13924v1 [cs.LG])
108. Client Selection in Nonconvex Federated Learning: Improved Convergence Analysis for Optimal Unbiased Sampling Strategy. (arXiv:2205.13925v1 [cs.LG])
109. Probabilistic Transformer: Modelling Ambiguities and Distributions for RNA Folding and Molecule Design. (arXiv:2205.13927v1 [cs.LG])
110. Fairness and Welfare Quantification for Regret in Multi-Armed Bandits. (arXiv:2205.13930v1 [cs.LG])
111. Standalone Neural ODEs with Sensitivity Analysis. (arXiv:2205.13933v1 [cs.LG])
112. Combining observational datasets from multiple environments to detect hidden confounding. (arXiv:2205.13935v1 [stat.ME])
113. Auditing Differential Privacy in High Dimensions with the Kernel Quantum R\'enyi Divergence. (arXiv:2205.13941v1 [cs.LG])
114. Deep Reinforcement Learning for Distributed and Uncoordinated Cognitive Radios Resource Allocation. (arXiv:2205.13944v1 [cs.LG])
115. Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge Transfer. (arXiv:2205.13947v1 [cs.LG])
116. Non-Markovian policies occupancy measures. (arXiv:2205.13950v1 [cs.LG])
117. Guided Exploration of Data Summaries. (arXiv:2205.13956v1 [cs.LG])
118. Exploring Techniques for the Analysis of Spontaneous Asynchronicity in MPI-Parallel Applications. (arXiv:2205.13963v1 [cs.DC])
119. Counterfactual Fairness with Partially Known Causal Graph. (arXiv:2205.13972v1 [cs.LG])
120. Deep Ensembles for Graphs with Higher-order Dependencies. (arXiv:2205.13988v1 [cs.LG])
121. Prototype Based Classification from Hierarchy to Fairness. (arXiv:2205.13997v1 [cs.LG])
122. RecipeRec: A Heterogeneous Graph Learning Model for Recipe Recommendation. (arXiv:2205.14005v1 [cs.IR])
123. What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v1 [cs.LG])
124. Inference and Sampling for Archimax Copulas. (arXiv:2205.14025v1 [stat.ME])
125. Learning Dynamical Systems via Koopman Operator Regression in Reproducing Kernel Hilbert Spaces. (arXiv:2205.14027v1 [cs.LG])
126. Learning to Control Linear Systems can be Hard. (arXiv:2205.14035v1 [cs.LG])
127. Group-invariant max filtering. (arXiv:2205.14039v1 [cs.IT])
128. Intelligent Transportation Systems' Orchestration: Lessons Learned & Potential Opportunities. (arXiv:2205.14040v1 [cs.NI])
129. Double Deep Q Networks for Sensor Management in Space Situational Awareness. (arXiv:2205.14041v1 [cs.LG])
130. Average Adjusted Association: Efficient Estimation with High Dimensional Confounders. (arXiv:2205.14048v1 [stat.ME])
131. Contrastive Siamese Network for Semi-supervised Speech Recognition. (arXiv:2205.14054v1 [cs.LG])
132. Benign Overparameterization in Membership Inference with Early Stopping. (arXiv:2205.14055v1 [cs.LG])
133. Dual Convexified Convolutional Neural Networks. (arXiv:2205.14056v1 [cs.LG])
134. Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos. (arXiv:2205.14065v1 [cs.CV])
135. Finite mixture of skewed sub-Gaussian stable distributions. (arXiv:2205.14067v1 [stat.ME])
136. Deep Coding Patterns Design for Compressive Near-Infrared Spectral Classification. (arXiv:2205.14069v1 [cs.LG])
137. AANG: Automating Auxiliary Learning. (arXiv:2205.14082v1 [cs.LG])
138. Sharpness-Aware Training for Free. (arXiv:2205.14083v1 [cs.LG])
139. Surrogate modeling for Bayesian optimization beyond a single Gaussian process. (arXiv:2205.14090v1 [stat.ML])
140. Capturing Graphs with Hypo-Elliptic Diffusions. (arXiv:2205.14092v1 [cs.LG])
141. Solving infinite-horizon POMDPs with memoryless stochastic policies in state-action space. (arXiv:2205.14098v1 [cs.LG])
142. Generalizing Brain Decoding Across Subjects with Deep Learning. (arXiv:2205.14102v1 [cs.LG])
143. Efficient Forecasting of Large Scale Hierarchical Time Series via Multilevel Clustering. (arXiv:2205.14104v1 [cs.LG])
144. Learning to Solve Combinatorial Graph Partitioning Problems via Efficient Exploration. (arXiv:2205.14105v1 [cs.LG])
145. Spartan: Differentiable Sparsity via Regularized Transportation. (arXiv:2205.14107v1 [cs.LG])
146. Scalable Interpretability via Polynomials. (arXiv:2205.14108v1 [cs.LG])
147. Bayesian Robust Graph Contrastive Learning. (arXiv:2205.14109v1 [cs.LG])
148. Robust Counterfactual Explanations for Random Forests. (arXiv:2205.14116v1 [cs.LG])
149. Neural Basis Models for Interpretability. (arXiv:2205.14120v1 [cs.LG])
150. Meta-Learning Adversarial Bandits. (arXiv:2205.14128v1 [cs.LG])
151. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. (arXiv:2205.14135v1 [cs.LG])
152. PSL is Dead. Long Live PSL. (arXiv:2205.14136v1 [cs.LG])
153. Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v1 [cs.CV])
154. A Multilabel Classification Framework for Approximate Nearest Neighbor Search. (arXiv:1910.08322v4 [cs.LG] UPDATED)
155. Waiting but not Aging: Optimizing Information Freshness Under the Pull Model. (arXiv:1912.08722v4 [cs.NI] UPDATED)
156. Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v3 [cs.CL] UPDATED)
157. Towards Handling Uncertainty-at-Source in AI -- A Review and Next Steps for Interval Regression. (arXiv:2104.07245v2 [cs.LG] UPDATED)
158. FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v4 [cs.CL] UPDATED)
159. Safe Value Functions. (arXiv:2105.12204v3 [eess.SY] UPDATED)
160. FairCanary: Rapid Continuous Explainable Fairness. (arXiv:2106.07057v2 [cs.LG] UPDATED)
161. Seeing Differently, Acting Similarly: Heterogeneously Observable Imitation Learning. (arXiv:2106.09256v3 [cs.LG] UPDATED)
162. Benchpress: A Scalable and Versatile Workflow for Benchmarking Structure Learning Algorithms. (arXiv:2107.03863v3 [stat.ML] UPDATED)
163. Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v3 [cs.CV] UPDATED)
164. Approximating the Manifold Structure of Attributed Incentive Salience from Large Scale Behavioural Data. A Representation Learning Approach Based on Artificial Neural Networks. (arXiv:2108.01724v2 [cs.LG] UPDATED)
165. Notes on Generalizing the Maximum Entropy Principle to Uncertain Data. (arXiv:2109.04530v2 [cs.IT] UPDATED)
166. Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks. (arXiv:2109.08844v2 [stat.ML] UPDATED)
167. Does Momentum Change the Implicit Regularization on Separable Data?. (arXiv:2110.03891v2 [cs.LG] UPDATED)
168. On the Sample Complexity of Decentralized Linear Quadratic Regulator with Partially Nested Information Structure. (arXiv:2110.07112v2 [math.OC] UPDATED)
169. Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v3 [cs.CV] UPDATED)
170. C$^2$SP-Net: Joint Compression and Classification Network for Epilepsy Seizure Prediction. (arXiv:2110.13674v2 [cs.LG] UPDATED)
171. VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v2 [cs.CV] UPDATED)
172. Composing Partial Differential Equations with Physics-Aware Neural Networks. (arXiv:2111.11798v2 [cs.LG] UPDATED)
173. A Bayesian Model for Online Activity Sample Sizes. (arXiv:2111.12157v3 [stat.ML] UPDATED)
174. Anonymization for Skeleton Action Recognition. (arXiv:2111.15129v2 [cs.CV] UPDATED)
175. Probabilistic Forecasting with Generative Networks via Scoring Rule Minimization. (arXiv:2112.08217v2 [stat.ML] UPDATED)
176. Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment. (arXiv:2112.11947v2 [cs.AI] UPDATED)
177. An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification. (arXiv:2112.13236v3 [cs.CR] UPDATED)
178. A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning. (arXiv:2112.15400v2 [cs.LG] UPDATED)
179. A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges. (arXiv:2201.00680v2 [cs.LG] UPDATED)
180. Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks. (arXiv:2201.02143v2 [cs.LG] UPDATED)
181. Bootstrapping Informative Graph Augmentation via A Meta Learning Approach. (arXiv:2201.03812v3 [cs.LG] UPDATED)
182. Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation. (arXiv:2202.01336v4 [cs.LG] UPDATED)
183. GALAXY: Graph-based Active Learning at the Extreme. (arXiv:2202.01402v2 [cs.LG] UPDATED)
184. Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis. (arXiv:2202.02444v2 [cs.CV] UPDATED)
185. Rethinking ValueDice: Does It Really Improve Performance?. (arXiv:2202.02468v2 [cs.LG] UPDATED)
186. TimeREISE: Time-series Randomized Evolving Input Sample Explanation. (arXiv:2202.07952v2 [cs.LG] UPDATED)
187. A Simple and Universal Rotation Equivariant Point-cloud Network. (arXiv:2203.01216v3 [cs.LG] UPDATED)
188. Distributionally Robust Bayesian Optimization with $\phi$-divergences. (arXiv:2203.02128v2 [cs.LG] UPDATED)
189. ReVar: Strengthening Policy Evaluation via Reduced Variance Sampling. (arXiv:2203.04510v2 [cs.LG] UPDATED)
190. Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4. (arXiv:2203.06649v2 [q-bio.NC] UPDATED)
191. Comparing two samples through stochastic dominance: a graphical approach. (arXiv:2203.07889v2 [stat.ML] UPDATED)
192. Hybrid training of optical neural networks. (arXiv:2203.11207v2 [cs.LG] UPDATED)
193. Minimax Regret for Cascading Bandits. (arXiv:2203.12577v2 [cs.LG] UPDATED)
194. A Two-Stage Federated Transfer Learning Framework in Medical Images Classification on Limited Data: A COVID-19 Case Study. (arXiv:2203.12803v2 [eess.IV] UPDATED)
195. Supervised Training of Siamese Spiking Neural Networks with Earth Mover's Distance. (arXiv:2203.13207v2 [cs.NE] UPDATED)
196. Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. (arXiv:2203.13457v2 [cs.LG] UPDATED)
197. Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v2 [cs.CV] UPDATED)
198. Characterizing Parametric and Convergence Stability in Nonconvex and Nonsmooth Optimizations: A Geometric Approach. (arXiv:2204.01643v2 [cs.GT] UPDATED)
199. Semantic Exploration from Language Abstractions and Pretrained Representations. (arXiv:2204.05080v2 [cs.LG] UPDATED)
200. Towards a Unified Framework for Uncertainty-aware Nonlinear Variable Selection with Theoretical Guarantees. (arXiv:2204.07293v2 [stat.ML] UPDATED)
201. Big-means: Less is More for K-means Clustering. (arXiv:2204.07485v2 [cs.LG] UPDATED)
202. TabNAS: Rejection Sampling for Neural Architecture Search on Tabular Datasets. (arXiv:2204.07615v2 [cs.LG] UPDATED)
203. U-NO: U-shaped Neural Operators. (arXiv:2204.11127v2 [cs.LG] UPDATED)
204. CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers. (arXiv:2204.14217v2 [cs.CV] UPDATED)
205. Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations. (arXiv:2205.05167v2 [cs.CV] UPDATED)
206. The Impact of COVID-19 Pandemic on LGBTQ Online Communities. (arXiv:2205.09511v2 [cs.SI] UPDATED)
207. Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. (arXiv:2205.09702v3 [cs.LG] UPDATED)
208. Let the Model Decide its Curriculum for Multitask Learning. (arXiv:2205.09898v2 [cs.LG] UPDATED)
209. Sparse Infinite Random Feature Latent Variable Modeling. (arXiv:2205.09909v2 [stat.ML] UPDATED)
210. Learning Dense Reward with Temporal Variant Self-Supervision. (arXiv:2205.10431v2 [cs.LG] UPDATED)
211. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v2 [cs.CV] UPDATED)
212. FedAug: Reducing the Local Learning Bias Improves Federated Learning on Heterogeneous Data. (arXiv:2205.13462v2 [cs.LG] UPDATED)
213. Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v2 [cs.CV] UPDATED)
214. QGNN: Value Function Factorisation with Graph Neural Networks. (arXiv:2205.13005v1 [cs.LG] CROSS LISTED)
## cs.AI
---
**90** new papers in cs.AI:-) 
1. Learning black- and gray-box chemotactic PDEs/closures from agent based Monte Carlo simulation data. (arXiv:2205.13545v1 [q-bio.QM])
2. Training and Inference on Any-Order Autoregressive Models the Right Way. (arXiv:2205.13554v1 [cs.LG])
3. Physics-Guided Hierarchical Reward Mechanism for LearningBased Multi-Finger Object Grasping. (arXiv:2205.13561v1 [cs.RO])
4. Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations. (arXiv:2205.13571v1 [cs.LG])
5. Clinical Dialogue Transcription Error Correction using Seq2Seq Models. (arXiv:2205.13572v1 [cs.CL])
6. Dynamic Network Reconfiguration for Entropy Maximization using Deep Reinforcement Learning. (arXiv:2205.13578v1 [cs.LG])
7. Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis. (arXiv:2205.13583v1 [cs.AI])
8. Learning in Feedback-driven Recurrent Spiking Neural Networks using full-FORCE Training. (arXiv:2205.13585v1 [cs.AI])
9. Comparing the Digital Annealer with Classical Evolutionary Algorithm. (arXiv:2205.13586v1 [cs.NE])
10. Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes. (arXiv:2205.13589v1 [cs.LG])
11. DRLComplex: Reconstruction of protein quaternary structures using deep reinforcement learning. (arXiv:2205.13594v1 [cs.LG])
12. MyoSuite -- A contact-rich simulation suite for musculoskeletal motor control. (arXiv:2205.13600v1 [cs.RO])
13. Fairness in Recommendation: A Survey. (arXiv:2205.13619v1 [cs.IR])
14. Experimental Design for Linear Functionals in Reproducing Kernel Hilbert Spaces. (arXiv:2205.13627v1 [cs.AI])
15. Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation. (arXiv:2205.13629v1 [cs.CV])
16. Machine and Deep Learning Applications to Mouse Dynamics for Continuous User Authentication. (arXiv:2205.13646v1 [cs.AI])
17. Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v1 [cs.LG])
18. Global Normalization for Streaming Speech Recognition in a Modular Framework. (arXiv:2205.13674v1 [cs.LG])
19. Reinforcement Learning Approach for Mapping Applications to Dataflow-Based Coarse-Grained Reconfigurable Array. (arXiv:2205.13675v1 [cs.AR])
20. Sequential Nature of Recommender Systems Disrupts the Evaluation Process. (arXiv:2205.13681v1 [cs.IR])
21. Safety Aware Changepoint Detection for Piecewise i.i.d. Bandits. (arXiv:2205.13689v1 [cs.LG])
22. FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v1 [cs.LG])
23. Off-Beat Multi-Agent Reinforcement Learning. (arXiv:2205.13718v1 [cs.MA])
24. Effective Abstract Reasoning with Dual-Contrast Network. (arXiv:2205.13720v1 [cs.CV])
25. V-Doc : Visual questions answers with Documents. (arXiv:2205.13724v1 [cs.AI])
26. GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic Synthesis. (arXiv:2205.13728v1 [cs.AI])
27. Understanding Long Programming Languages with Structure-Aware Sparse Attention. (arXiv:2205.13730v1 [cs.CL])
28. Subverting machines, fluctuating identities: Re-learning human categorization. (arXiv:2205.13740v1 [cs.LG])
29. Generating personalized counterfactual interventions for algorithmic recourse by eliciting user preferences. (arXiv:2205.13743v1 [cs.LG])
30. DeepSAT: An EDA-Driven Learning Framework for SAT. (arXiv:2205.13745v1 [cs.AI])
31. CIGMO: Categorical invariant representations in a deep generative framework. (arXiv:2205.13758v1 [cs.CV])
32. Tutorial on Course-of-Action (COA) Attack Search Methods in Computer Networks. (arXiv:2205.13763v1 [cs.AI])
33. Block-coordinate Frank-Wolfe algorithm and convergence analysis for semi-relaxed optimal transport problem. (arXiv:2205.13766v1 [cs.LG])
34. A Sea of Words: An In-Depth Analysis of Anchors for Text Data. (arXiv:2205.13789v1 [stat.ML])
35. Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions. (arXiv:2205.13803v1 [cs.CV])
36. Isolating and Leveraging Controllable and Noncontrollable Visual Dynamics in World Models. (arXiv:2205.13817v1 [cs.LG])
37. Deep Learning Fetal Ultrasound Video Model Match Human Observers in Biometric Measurements. (arXiv:2205.13835v1 [eess.IV])
38. Raising the Bar in Graph-level Anomaly Detection. (arXiv:2205.13845v1 [cs.LG])
39. On the Convergence of Semi-Relaxed Sinkhorn with Marginal Constraint and OT Distance Gaps. (arXiv:2205.13846v1 [cs.LG])
40. Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power. (arXiv:2205.13863v1 [cs.LG])
41. MIMII DG: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection for Domain Generalization Task. (arXiv:2205.13879v1 [cs.SD])
42. Automated Dynamic Algorithm Configuration. (arXiv:2205.13881v1 [cs.AI])
43. Learning to Automate Follow-up Question Generation using Process Knowledge for Depression Triage on Reddit Posts. (arXiv:2205.13884v1 [cs.AI])
44. EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks. (arXiv:2205.13892v1 [cs.LG])
45. (De-)Randomized Smoothing for Decision Stump Ensembles. (arXiv:2205.13909v1 [cs.LG])
46. Fast Causal Orientation Learning in Directed Acyclic Graphs. (arXiv:2205.13919v1 [cs.LG])
47. ProtoFSSL: Federated Semi-Supervised Learning with Prototype-based Consistency Regularization. (arXiv:2205.13921v1 [cs.LG])
48. CREAM: Weakly Supervised Object Localization via Class RE-Activation Mapping. (arXiv:2205.13922v1 [cs.CV])
49. Commonsense and Named Entity Aware Knowledge Grounded Dialogue Generation. (arXiv:2205.13928v1 [cs.CL])
50. Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v1 [cs.CV])
51. Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge Transfer. (arXiv:2205.13947v1 [cs.LG])
52. Geometer: Graph Few-Shot Class-Incremental Learning via Prototype Representation. (arXiv:2205.13954v1 [cs.AI])
53. Machine Learning-Based User Scheduling in Integrated Satellite-HAPS-Ground Networks. (arXiv:2205.13958v1 [cs.AI])
54. Prototype Based Classification from Hierarchy to Fairness. (arXiv:2205.13997v1 [cs.LG])
55. What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v1 [cs.LG])
56. Ontology Design Facilitating Wikibase Integration -- and a Worked Example for Historical Data. (arXiv:2205.14032v1 [cs.AI])
57. Multi-criteria Decision-making of Intelligent Vehicles under Fault Condition Enhancing Public-private Partnership. (arXiv:2205.14070v1 [eess.SY])
58. AANG: Automating Auxiliary Learning. (arXiv:2205.14082v1 [cs.LG])
59. Sharpness-Aware Training for Free. (arXiv:2205.14083v1 [cs.LG])
60. Failure Detection in Medical Image Classification: A Reality Check and Benchmarking Testbed. (arXiv:2205.14094v1 [cs.AI])
61. Learning to Solve Combinatorial Graph Partitioning Problems via Efficient Exploration. (arXiv:2205.14105v1 [cs.LG])
62. AI-aided multiscale modeling of physiologically-significant blood clots. (arXiv:2205.14121v1 [physics.med-ph])
63. Cycle Mutation: Evolving Permutations via Cycle Induction. (arXiv:2205.14125v1 [cs.NE])
64. Meta-Learning Adversarial Bandits. (arXiv:2205.14128v1 [cs.LG])
65. Local and Central Differential Privacy for Robustness and Privacy in Federated Learning. (arXiv:2009.03561v5 [cs.CR] UPDATED)
66. Siamese Labels Auxiliary Learning. (arXiv:2103.00200v3 [cs.AI] UPDATED)
67. Towards Handling Uncertainty-at-Source in AI -- A Review and Next Steps for Interval Regression. (arXiv:2104.07245v2 [cs.LG] UPDATED)
68. DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v5 [cs.CV] UPDATED)
69. Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v5 [cs.CL] UPDATED)
70. Seeing Differently, Acting Similarly: Heterogeneously Observable Imitation Learning. (arXiv:2106.09256v3 [cs.LG] UPDATED)
71. C$^2$SP-Net: Joint Compression and Classification Network for Epilepsy Seizure Prediction. (arXiv:2110.13674v2 [cs.LG] UPDATED)
72. Adversarial Deep Reinforcement Learning for Improving the Robustness of Multi-agent Autonomous Driving Policies. (arXiv:2112.11937v2 [cs.AI] UPDATED)
73. Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment. (arXiv:2112.11947v2 [cs.AI] UPDATED)
74. An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification. (arXiv:2112.13236v3 [cs.CR] UPDATED)
75. A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning. (arXiv:2112.15400v2 [cs.LG] UPDATED)
76. A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges. (arXiv:2201.00680v2 [cs.LG] UPDATED)
77. Bootstrapping Informative Graph Augmentation via A Meta Learning Approach. (arXiv:2201.03812v3 [cs.LG] UPDATED)
78. TimeREISE: Time-series Randomized Evolving Input Sample Explanation. (arXiv:2202.07952v2 [cs.LG] UPDATED)
79. Learning English with Peppa Pig. (arXiv:2202.12917v2 [cs.CL] UPDATED)
80. Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4. (arXiv:2203.06649v2 [q-bio.NC] UPDATED)
81. Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v2 [cs.CV] UPDATED)
82. Characterizing Parametric and Convergence Stability in Nonconvex and Nonsmooth Optimizations: A Geometric Approach. (arXiv:2204.01643v2 [cs.GT] UPDATED)
83. Semantic Exploration from Language Abstractions and Pretrained Representations. (arXiv:2204.05080v2 [cs.LG] UPDATED)
84. Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021. (arXiv:2205.02388v2 [cs.CL] UPDATED)
85. Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations. (arXiv:2205.05167v2 [cs.CV] UPDATED)
86. Sparse Infinite Random Feature Latent Variable Modeling. (arXiv:2205.09909v2 [stat.ML] UPDATED)
87. Learning Dense Reward with Temporal Variant Self-Supervision. (arXiv:2205.10431v2 [cs.LG] UPDATED)
88. Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v3 [cs.CL] UPDATED)
89. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v2 [cs.CV] UPDATED)
90. Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v2 [cs.CV] UPDATED)

