# Your interest papers
---
## cs.CV
---
### Flex**HDR**: Modelling Alignment and **Exposure** Uncertainties for Flexible **HDR** Imaging. (arXiv:2201.02625v1 [eess.IV])
- Authors : Sibi Catley, Thomas Tanay, Lucas Vandroux, Gregory Slabaugh
- Link : [http://arxiv.org/abs/2201.02625](http://arxiv.org/abs/2201.02625)
> ABSTRACT  :  **High dynamic range** (**HDR**) imaging is of fundamental importance in modern digital photography pipelines and used to produce a high-quality photograph with well exposed regions despite varying illumination across the image. This is typically achieved by merging multiple low dynamic range (LDR) images taken at different **exposure**s. However, over-exposed regions and misalignment errors due to poorly compensated motion result in artefacts such as ghosting. In this paper, we present a new **HDR** imaging technique that specifically models alignment and **exposure** uncertainties to produce high quality **HDR** results. We introduce a strategy that learns to jointly align and assess the alignment and **exposure** reliability using an **HDR**-aware, uncertainty-driven attention map that robustly merges the frames into a single high quality **HDR** image. Further, we introduce a progressive, multi-stage image fusion approach that can flexibly merge any number of LDR images in a permutation-invariant manner. Experimental results show our method can produce better quality **HDR** images with up to 0.8dB PSNR improvement to the state-of-the-art, and subjective improvements in terms of better detail, colours, and fewer artefacts.  
### Real-time Rail Recognition Based on 3D Point Clouds. (arXiv:2201.02726v1 [cs.CV])
- Authors : Xinyi Yu, Weiqi He, Xuecheng Qian, Yang Yang, Linlin Ou
- Link : [http://arxiv.org/abs/2201.02726](http://arxiv.org/abs/2201.02726)
> ABSTRACT  :  Accurate rail location is a crucial part in the railway support driving system for safety monitoring. LiDAR can obtain point clouds that carry 3D information for the railway environment, especially in **dark**ness and terrible weather conditions. In this paper, a real-time rail recognition method based on 3D point clouds is proposed to solve the challenges, such as disorderly, uneven density and large volume of the point clouds. A voxel down-sampling method is first presented for density balanced of railway point clouds, and pyramid partition is designed to divide the 3D scanning area into the voxels with different volumes. Then, a feature encoding module is developed to find the nearest neighbor points and to aggregate their local geometric features for the center point. Finally, a multi-scale neural network is proposed to generate the prediction results of each voxel and the rail location. The experiments are conducted under 9 sequences of 3D point cloud data for the railway. The results show that the method has good performance in detecting straight, curved and other complex topologies rails.  
### A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])
- Authors : Shuyue Guan, Murray Loew
- Link : [http://arxiv.org/abs/2201.02771](http://arxiv.org/abs/2201.02771)
> ABSTRACT  :  Instead of using current deep-learning segmentation models (like the UNet and variants), we approach the segmentation problem using trained Convolutional Neural Network (CNN) classifiers, which automatically extract important features from classified targets for image classification. Those extracted features can be visualized and formed heatmaps using Gradient-weighted Class Activation Mapping (Grad-CAM). This study tested whether the heatmaps could be used to segment the classified targets. We also proposed an evaluation method for the heatmaps; that is, to re-train the CNN classifier using images filtered by heatmaps and examine its performance. We used the mean-Dice coefficient to evaluate segmentation results. Results from our experiments show that heatmaps can locate and segment partial tumor areas. But only use of the heatmaps from CNN classifiers may not be an optimal approach for segmentation. In addition, we have verified that the predictions of CNN classifiers mainly depend on tumor areas, and **dark** regions in Grad-CAM's heatmaps also contribute to classification.  
### Counteracting **Dark** Web Text-Based CAPTCHA with Generative Adversarial Learning for Proactive Cyber Threat Intelligence. (arXiv:2201.02799v1 [cs.CV])
- Authors : Ning Zhang, Mohammadreza Ebrahimi, Weifeng Li, Hsinchun Chen
- Link : [http://arxiv.org/abs/2201.02799](http://arxiv.org/abs/2201.02799)
> ABSTRACT  :  Automated monitoring of **dark** web (DW) platforms on a large scale is the first step toward developing proactive Cyber Threat Intelligence (CTI). While there are efficient methods for collecting data from the surface web, large-scale **dark** web data collection is often hindered by anti-crawling measures. In particular, text-based CAPTCHA serves as the most prevalent and prohibiting type of these measures in the **dark** web. Text-based CAPTCHA identifies and blocks automated crawlers by forcing the user to enter a combination of hard-to-recognize alphanumeric characters. In the **dark** web, CAPTCHA images are meticulously designed with additional background noise and variable character length to prevent automated CAPTCHA breaking. Existing automated CAPTCHA breaking methods have difficulties in overcoming these **dark** web challenges. As such, solving **dark** web text-based CAPTCHA has been relying heavily on human involvement, which is labor-intensive and time-consuming. In this study, we propose a novel framework for automated breaking of **dark** web CAPTCHA to facilitate **dark** web data collection. This framework encompasses a novel generative method to recognize **dark** web text-based CAPTCHA with noisy background and variable character length. To eliminate the need for human involvement, the proposed framework utilizes Generative Adversarial Network (GAN) to counteract **dark** web background noise and leverages an enhanced character segmentation algorithm to handle CAPTCHA images with variable character length. Our proposed framework, DW-GAN, was systematically evaluated on multiple **dark** web CAPTCHA testbeds. DW-GAN significantly outperformed the state-of-the-art benchmark methods on all datasets, achieving over 94.4% success rate on a carefully collected real-world **dark** web dataset...  
### CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwnannoma and Cochlea Segmentation. (arXiv:2201.02831v1 [eess.IV])
- Authors : Reuben Dorent, Aaron Kujawa, Marina Ivory, Spyridon Bakas, Nicola Rieke, Samuel Joutard, Ben Glocker, Jorge Cardoso, Marc Modat, Kayhan Batmanghelich, Arseniy Belkov, Maria Baldeon, Jae Won, Hexin Dong, Sergio Escalera, Yubo Fan, Lasse Hansen, Smriti Joshi, Victoriya Kashtanova, Hyeon Gyu, Satoshi Kondo, Hao Li, Han Liu, Buntheng Ly, Ipek Oguz, Hyungseob Shin, Boris Shirokikh, Zixian Su, Guotai Wang, Jianghao Wu, Yanwu Xu, Kai Yao, Li Zhang, Sebastien Ourselin, Jonathan Shapey, Tom Vercauteren
- Link : [http://arxiv.org/abs/2201.02831](http://arxiv.org/abs/2201.02831)
> ABSTRACT  :  Domain Adaptation (DA) has recently raised strong interests in the medical imaging community. While a large variety of DA techniques has been proposed for image segmentation, most of these techniques have been validated either on private datasets or on small publicly available datasets. Moreover, these datasets mostly addressed single-class problems. To tackle these limitations, the Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in conjunction with the 24th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large and multi-class benchmark for unsupervised cross-modality DA. The challenge's goal is to segment two key brain structures involved in the follow-up and treatment planning of vestibular schwannoma (VS): the VS and the cochleas. Currently, the diagnosis and surveillance in patients with VS are performed using contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in using non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore, we created an unsupervised cross-modality segmentation benchmark. The training set provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105). The aim was to automatically perform unilateral VS and **bilateral** cochlea segmentation on hrT2 as provided in the testing set (N=137). A total of 16 teams submitted their algorithm for the evaluation phase. The level of performance reached by the top-performing teams is strikingly high (best median Dice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice - VS:92.5%; Cochleas:87.7%). All top-performing methods made use of an image-to-image translation approach to transform the source-domain images into pseudo-target-domain images. A segmentation network was then trained using these generated images and the manual annotations provided for the source image.  
### SGUIE-Net: Semantic Attention Guided Underwater Image **Enhancement** with Multi-Scale Perception. (arXiv:2201.02832v1 [eess.IV])
- Authors : Qi Qi, Kunqian Li, Haiyong Zheng, Xiang Gao, Guojia Hou, Kun Sun
- Link : [http://arxiv.org/abs/2201.02832](http://arxiv.org/abs/2201.02832)
> ABSTRACT  :  Due to the wavelength-dependent light attenuation, refraction and scattering, underwater images usually suffer from color distortion and blurred details. However, due to the limited number of paired underwater images with undistorted images as reference, training deep **enhancement** models for diverse degradation types is quite difficult. To boost the performance of data-driven approaches, it is essential to establish more effective learning mechanisms that mine richer supervised information from limited training sample resources. In this paper, we propose a novel underwater image **enhancement** network, called SGUIE-Net, in which we introduce semantic information as high-level guidance across different images that share common semantic regions. Accordingly, we propose semantic region-wise **enhancement** module to perceive the degradation of different semantic regions from multiple scales and feed it back to the global attention features extracted from its original scale. This strategy helps to achieve robust and visually pleasant **enhancement**s to different semantic objects, which should thanks to the guidance of semantic information for differentiated **enhancement**. More importantly, for those degradation types that are not common in the training sample distribution, the guidance connects them with the already well-learned types according to their semantic relevance. Extensive experiments on the publicly available datasets and our proposed dataset demonstrated the impressive performance of SGUIE-Net. The code and proposed dataset are available at: https://trentqq.github.io/SGUIE-Net.html  
### MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v1 [eess.IV])
- Authors : Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, **Peyman Milanfar**, Alan Bovik, Yinxiao Li
- Link : [http://arxiv.org/abs/2201.02973](http://arxiv.org/abs/2201.02973)
> ABSTRACT  :  Recent progress on Transformers and multi-layer perceptron (MLP) models provide new network architectural designs for computer vision tasks. Although these models proved to be effective in many vision tasks such as image recognition, there remain challenges in adapting them for low-level vision. The inflexibility to support high-resolution images and limitations of local attention are perhaps the main bottlenecks for using Transformers and MLPs in image **restoration**. In this work we present a multi-axis MLP based architecture, called MAXIM, that can serve as an efficient and flexible general-purpose vision backbone for image processing tasks. MAXIM uses a UNet-shaped hierarchical structure and supports long-range interactions enabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based building blocks: a multi-axis gated MLP that allows for efficient and scalable spatial mixing of local and global visual cues, and a cross-gating block, an alternative to cross-attention, which accounts for cross-feature mutual conditioning. Both these modules are exclusively based on MLPs, but also benefit from being both global and `fully-convolutional', two properties that are desirable for image processing. Our extensive experimental results show that the proposed MAXIM model achieves state-of-the-art performance on more than ten benchmarks across a range of image processing tasks, including denoising, deblurring, deraining, dehazing, and **enhancement** while requiring fewer or comparable numbers of parameters and FLOPs than competitive models.  
### Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v1 [eess.IV])
- Authors : Congpei An, Ning Wu, Xiaoming Yuan
- Link : [http://arxiv.org/abs/2201.02979](http://arxiv.org/abs/2201.02979)
> ABSTRACT  :  The total variation (TV) regularization has phenomenally boosted various variational models for image processing tasks. We propose combining the backward diffusion process in the earlier literature of image **enhancement** with the TV regularization and show that the resulting enhanced TV minimization model is particularly effective for reducing the loss of contrast, which is often encountered by models using the TV regularization. We establish stable reconstruction guarantees for the enhanced TV model from noisy subsampled measurements; non-adaptive linear measurements and variable-density sampled Fourier measurements are considered. In particular, under some weaker restricted isometry property conditions, the enhanced TV minimization model is shown to have tighter reconstruction error bounds than various TV-based models for the scenario where the level of noise is significant and the amount of measurements is limited. The advantages of the enhanced TV model are also numerically validated by preliminary experiments on the reconstruction of some synthetic, natural, and medical images.  
### Enhancing Low-Light Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v1 [eess.IV])
- Authors : Lanqing Guo, Renjie Wan, **Wenhan Yang**, Alex Kot, Bihan Wen
- Link : [http://arxiv.org/abs/2201.03145](http://arxiv.org/abs/2201.03145)
> ABSTRACT  :  Images captured in the **low-light** condition suffer from low visibility and various imaging artifacts, e.g., real noise. Existing supervised enlightening algorithms require a large set of pixel-aligned training image pairs, which are hard to prepare in practice. Though weakly-supervised or unsupervised methods can alleviate such challenges without using paired training images, some real-world artifacts inevitably get falsely amplified because of the lack of corresponded supervision. In this paper, instead of using perfectly aligned images for training, we creatively employ the misaligned real-world images as the guidance, which are considerably easier to collect. Specifically, we propose a Cross-Image Disentanglement Network (CIDN) to separately extract cross-image brightness and image-specific content features from low/normal-light images. Based on that, CIDN can simultaneously correct the brightness and suppress image artifacts in the feature domain, which largely increases the robustness to the pixel shifts. Furthermore, we collect a new **low-light** image **enhancement** dataset consisting of misaligned training images with real-world corruptions. Experimental results show that our model achieves state-of-the-art performances on both the newly proposed dataset and other popular **low-light** datasets.  
### **Swin** transformers make strong contextual encoders for VHR image road extraction. (arXiv:2201.03178v1 [cs.CV])
- Authors : Tao Chen, Daguang Jiang, Ruirui Li
- Link : [http://arxiv.org/abs/2201.03178](http://arxiv.org/abs/2201.03178)
> ABSTRACT  :  Significant progress has been made in automatic road extra-ction or segmentation based on deep learning, but there are still margins to improve in terms of the completeness and connectivity of the results. This is mainly due to the challenges of large intra-class variances, ambiguous inter-class distinctions, and occlusions from shadows, trees, and buildings. Therefore, being able to perceive global context and model geometric information is essential to further improve the accuracy of road segmentation. In this paper, we design a novel dual-branch encoding block Co**Swin** which exploits the capability of global context modeling of **Swin** Transformer and that of local feature extraction of ResNet. Furthermore, we also propose a context-guided filter block named CFilter, which can filter out context-independent noisy features for better reconstructing of the details. We use Co**Swin** and CFilter in a U-shaped network architecture. Experiments on Massachusetts and CHN6-CUG datasets show that the proposed method outperforms other state-of-the-art methods on the metrics of F1, IoU, and OA. Further analysis reveals that the improvement in accuracy comes from better integrity and connectivity of segmented roads.  
### **Swin** Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])
- Authors : Jiahao Huang, Yingying Fang, Yinzhe Wu, Huanjun Wu, Zhifan Gao, Yang Li, Javier Del, Jun Xia, Guang Yang
- Link : [http://arxiv.org/abs/2201.03230](http://arxiv.org/abs/2201.03230)
> ABSTRACT  :  Magnetic resonance imaging (MRI) is an important non-invasive clinical tool that can produce high-resolution and reproducible images. However, a long scanning time is required for high-quality MR images, which leads to exhaustion and discomfort of patients, inducing more artefacts due to voluntary movements of the patients and involuntary physiological movements. To accelerate the scanning process, methods by k-space undersampling and deep learning based reconstruction have been popularised. This work introduced **Swin**MR, a novel **Swin** transformer based method for fast MRI reconstruction. The whole network consisted of an input module (IM), a feature extraction module (FEM) and an output module (OM). The IM and OM were 2D convolutional layers and the FEM was composed of a cascaded of residual **Swin** transformer blocks (RSTBs) and 2D convolutional layers. The RSTB consisted of a series of **Swin** transformer layers (STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was performed in shifted windows rather than the multi-head self-attention (MSA) of the original transformer in the whole image space. A novel multi-channel loss was proposed by using the sensitivity maps, which was proved to reserve more textures and details. We performed a series of comparative studies and ablation studies in the Calgary-Campinas public brain MR dataset and conducted a downstream segmentation experiment in the Multi-modal Brain Tumour Segmentation Challenge 2017 dataset. The results demonstrate our **Swin**MR achieved high-quality reconstruction compared with other benchmark methods, and it shows great robustness with different undersampling masks, under noise interruption and on different datasets. The code is publicly available at https://github.com/ayanglab/**Swin**MR.  
### Vision in adverse weather: Augmentation using CycleGANs with various object detectors for robust perception in autonomous racing. (arXiv:2201.03246v1 [cs.CV])
- Authors : Izzeddin Teeti, Valentina Musat, Salman Khan, Alexander Rast, Fabio Cuzzolin, Andrew Bradley
- Link : [http://arxiv.org/abs/2201.03246](http://arxiv.org/abs/2201.03246)
> ABSTRACT  :  In an autonomous driving system, perception - identification of features and objects from the environment - is crucial. In autonomous racing, high speeds and small margins demand rapid and accurate detection systems. During the race, the weather can change abruptly, causing significant degradation in perception, resulting in ineffective manoeuvres. In order to improve detection in adverse weather, deep-learning-based models typically require extensive datasets captured in such conditions - the collection of which is a tedious, laborious, and costly process. However, recent developments in CycleGAN architectures allow the synthesis of highly realistic scenes in multiple weather conditions. To this end, we introduce an approach of using synthesised adverse condition datasets in autonomous racing (generated using CycleGAN) to improve the performance of four out of five state-of-the-art detectors by an average of 42.7 and 4.4 mAP percentage points in the presence of **night**-time conditions and droplets, respectively. Furthermore, we present a comparative analysis of five object detectors - identifying the optimal pairing of detector and training data for use during autonomous racing in challenging conditions.  
### BIGPrior: Towards Decoupling Learned Prior Hallucination and Data Fidelity in Image **Restoration**. (arXiv:2011.01406v3 [cs.CV] UPDATED)
- Authors : Majed El
- Link : [http://arxiv.org/abs/2011.01406](http://arxiv.org/abs/2011.01406)
> ABSTRACT  :  Classic image-**restoration** algorithms use a variety of priors, either implicitly or explicitly. Their priors are hand-designed and their corresponding weights are heuristically assigned. Hence, deep learning methods often produce superior image **restoration** quality. Deep networks are, however, capable of inducing strong and hardly predictable hallucinations. Networks implicitly learn to be jointly faithful to the observed data while learning an image prior; and the separation of original data and hallucinated data downstream is then not possible. This limits their wide-spread adoption in image **restoration**. Furthermore, it is often the hallucinated part that is victim to degradation-model overfitting.    We present an approach with decoupled network-prior based hallucination and data fidelity terms. We refer to our framework as the Bayesian Integration of a Generative Prior (BIGPrior). Our method is rooted in a Bayesian framework and tightly connected to classic **restoration** methods. In fact, it can be viewed as a generalization of a large family of classic **restoration** algorithms. We use network inversion to extract image prior information from a generative network. We show that, on image colorization, inpainting and denoising, our framework consistently improves the inversion results. Our method, though partly reliant on the quality of the generative network inversion, is competitive with state-of-the-art supervised and task-specific **restoration** methods. It also provides an additional metric that sets forth the degree of prior reliance per pixel relative to data fidelity.  
### Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v3 [cs.CV] UPDATED)
- Authors : Lei Li, Xiahai Zhuang
- Link : [http://arxiv.org/abs/2106.09862](http://arxiv.org/abs/2106.09862)
> ABSTRACT  :  Late gadolinium **enhancement** magnetic resonance imaging (LGE MRI) is commonly used to visualize and quantify left atrial (LA) scars. The position and extent of scars provide important information of the pathophysiology and progression of atrial fibrillation (AF). Hence, LA scar segmentation and quantification from LGE MRI can be useful in computer-assisted diagnosis and treatment stratification of AF patients. Since manual delineation can be time-consuming and subject to intra- and inter-expert variability, automating this computing is highly desired, which nevertheless is still challenging and under-researched.    This paper aims to provide a systematic review on computing methods for LA cavity, wall, scar and ablation gap segmentation and quantification from LGE MRI, and the related literature for AF studies. Specifically, we first summarize AF-related imaging techniques, particularly LGE MRI. Then, we review the methodologies of the four computing tasks in detail, and summarize the validation strategies applied in each task. Finally, the possible future developments are outlined, with a brief survey on the potential clinical applications of the aforementioned methods. The review shows that the research into this topic is still in early stages. Although several methods have been proposed, especially for LA segmentation, there is still large scope for further algorithmic developments due to performance issues related to the high variability of **enhancement** appearance and differences in image acquisition.  
### CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v3 [cs.CV] UPDATED)
- Authors : Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo
- Link : [http://arxiv.org/abs/2107.00652](http://arxiv.org/abs/2107.00652)
> ABSTRACT  :  We present CSWin Transformer, an efficient and effective Transformer-based backbone for general-purpose vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute whereas local self-attention often limits the field of interactions of each token. To address this issue, we develop the Cross-Shaped Window self-attention mechanism for computing self-attention in the horizontal and vertical stripes in parallel that form a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. We provide a mathematical analysis of the effect of the stripe width and vary the stripe width for different layers of the Transformer network which achieves strong modeling capability while limiting the computation cost. We also introduce Locally-enhanced Positional Encoding (LePE), which handles the local positional information better than existing encoding schemes. LePE naturally supports arbitrary input resolutions, and is thus especially effective and friendly for downstream tasks. Incorporated with these designs and a hierarchical structure, CSWin Transformer demonstrates competitive performance on common vision tasks. Specifically, it achieves 85.4\% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task, and 52.2 mIOU on the ADE20K semantic segmentation task, surpassing previous state-of-the-art **Swin** Transformer backbone by +1.2, +2.0, +1.4, and +2.0 respectively under the similar FLOPs setting. By further pretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. The code and models are available at https://github.com/microsoft/CSWin-Transformer.  
### Hierarchical Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v4 [cs.CV] UPDATED)
- Authors : Ruwen Bai, Min Li, Bo Meng, Fengfa Li, Miao Jiang, Junxing Ren, Degang Sun
- Link : [http://arxiv.org/abs/2109.02860](http://arxiv.org/abs/2109.02860)
> ABSTRACT  :  Graph convolutional networks (GCNs) have emerged as dominant methods for skeleton-based action recognition.    However, they still suffer from two problems, namely, neighborhood constraints and entangled spatiotemporal feature representations.    Most studies have focused on improving the design of graph topology to solve the first problem but they have yet to fully explore the latter.    In this work, we design a disentangled spatiotemporal transformer (DSTT) block to overcome the above limitations of GCNs in three steps: (i) feature disentanglement for spatiotemporal decomposition;(ii) global spatiotemporal attention for capturing correlations in the global context; and (iii) local information **enhancement** for utilizing more local information.    Thereon, we propose a novel architecture, named Hierarchical Graph Convolutional skeleton Transformer (HGCT), to employ the complementary advantages of GCN (i.e., local topology, temporal dynamics and hierarchy) and Transformer (i.e., global context and dynamic attention).    HGCT is lightweight and computationally efficient.    Quantitative analysis demonstrates the superiority and good interpretability of HGCT.  
## eess.IV
---
### Flex**HDR**: Modelling Alignment and **Exposure** Uncertainties for Flexible **HDR** Imaging. (arXiv:2201.02625v1 [eess.IV])
- Authors : Sibi Catley, Thomas Tanay, Lucas Vandroux, Gregory Slabaugh
- Link : [http://arxiv.org/abs/2201.02625](http://arxiv.org/abs/2201.02625)
> ABSTRACT  :  **High dynamic range** (**HDR**) imaging is of fundamental importance in modern digital photography pipelines and used to produce a high-quality photograph with well exposed regions despite varying illumination across the image. This is typically achieved by merging multiple low dynamic range (LDR) images taken at different **exposure**s. However, over-exposed regions and misalignment errors due to poorly compensated motion result in artefacts such as ghosting. In this paper, we present a new **HDR** imaging technique that specifically models alignment and **exposure** uncertainties to produce high quality **HDR** results. We introduce a strategy that learns to jointly align and assess the alignment and **exposure** reliability using an **HDR**-aware, uncertainty-driven attention map that robustly merges the frames into a single high quality **HDR** image. Further, we introduce a progressive, multi-stage image fusion approach that can flexibly merge any number of LDR images in a permutation-invariant manner. Experimental results show our method can produce better quality **HDR** images with up to 0.8dB PSNR improvement to the state-of-the-art, and subjective improvements in terms of better detail, colours, and fewer artefacts.  
### A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])
- Authors : Shuyue Guan, Murray Loew
- Link : [http://arxiv.org/abs/2201.02771](http://arxiv.org/abs/2201.02771)
> ABSTRACT  :  Instead of using current deep-learning segmentation models (like the UNet and variants), we approach the segmentation problem using trained Convolutional Neural Network (CNN) classifiers, which automatically extract important features from classified targets for image classification. Those extracted features can be visualized and formed heatmaps using Gradient-weighted Class Activation Mapping (Grad-CAM). This study tested whether the heatmaps could be used to segment the classified targets. We also proposed an evaluation method for the heatmaps; that is, to re-train the CNN classifier using images filtered by heatmaps and examine its performance. We used the mean-Dice coefficient to evaluate segmentation results. Results from our experiments show that heatmaps can locate and segment partial tumor areas. But only use of the heatmaps from CNN classifiers may not be an optimal approach for segmentation. In addition, we have verified that the predictions of CNN classifiers mainly depend on tumor areas, and **dark** regions in Grad-CAM's heatmaps also contribute to classification.  
### CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwnannoma and Cochlea Segmentation. (arXiv:2201.02831v1 [eess.IV])
- Authors : Reuben Dorent, Aaron Kujawa, Marina Ivory, Spyridon Bakas, Nicola Rieke, Samuel Joutard, Ben Glocker, Jorge Cardoso, Marc Modat, Kayhan Batmanghelich, Arseniy Belkov, Maria Baldeon, Jae Won, Hexin Dong, Sergio Escalera, Yubo Fan, Lasse Hansen, Smriti Joshi, Victoriya Kashtanova, Hyeon Gyu, Satoshi Kondo, Hao Li, Han Liu, Buntheng Ly, Ipek Oguz, Hyungseob Shin, Boris Shirokikh, Zixian Su, Guotai Wang, Jianghao Wu, Yanwu Xu, Kai Yao, Li Zhang, Sebastien Ourselin, Jonathan Shapey, Tom Vercauteren
- Link : [http://arxiv.org/abs/2201.02831](http://arxiv.org/abs/2201.02831)
> ABSTRACT  :  Domain Adaptation (DA) has recently raised strong interests in the medical imaging community. While a large variety of DA techniques has been proposed for image segmentation, most of these techniques have been validated either on private datasets or on small publicly available datasets. Moreover, these datasets mostly addressed single-class problems. To tackle these limitations, the Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in conjunction with the 24th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large and multi-class benchmark for unsupervised cross-modality DA. The challenge's goal is to segment two key brain structures involved in the follow-up and treatment planning of vestibular schwannoma (VS): the VS and the cochleas. Currently, the diagnosis and surveillance in patients with VS are performed using contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in using non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore, we created an unsupervised cross-modality segmentation benchmark. The training set provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105). The aim was to automatically perform unilateral VS and **bilateral** cochlea segmentation on hrT2 as provided in the testing set (N=137). A total of 16 teams submitted their algorithm for the evaluation phase. The level of performance reached by the top-performing teams is strikingly high (best median Dice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice - VS:92.5%; Cochleas:87.7%). All top-performing methods made use of an image-to-image translation approach to transform the source-domain images into pseudo-target-domain images. A segmentation network was then trained using these generated images and the manual annotations provided for the source image.  
### SGUIE-Net: Semantic Attention Guided Underwater Image **Enhancement** with Multi-Scale Perception. (arXiv:2201.02832v1 [eess.IV])
- Authors : Qi Qi, Kunqian Li, Haiyong Zheng, Xiang Gao, Guojia Hou, Kun Sun
- Link : [http://arxiv.org/abs/2201.02832](http://arxiv.org/abs/2201.02832)
> ABSTRACT  :  Due to the wavelength-dependent light attenuation, refraction and scattering, underwater images usually suffer from color distortion and blurred details. However, due to the limited number of paired underwater images with undistorted images as reference, training deep **enhancement** models for diverse degradation types is quite difficult. To boost the performance of data-driven approaches, it is essential to establish more effective learning mechanisms that mine richer supervised information from limited training sample resources. In this paper, we propose a novel underwater image **enhancement** network, called SGUIE-Net, in which we introduce semantic information as high-level guidance across different images that share common semantic regions. Accordingly, we propose semantic region-wise **enhancement** module to perceive the degradation of different semantic regions from multiple scales and feed it back to the global attention features extracted from its original scale. This strategy helps to achieve robust and visually pleasant **enhancement**s to different semantic objects, which should thanks to the guidance of semantic information for differentiated **enhancement**. More importantly, for those degradation types that are not common in the training sample distribution, the guidance connects them with the already well-learned types according to their semantic relevance. Extensive experiments on the publicly available datasets and our proposed dataset demonstrated the impressive performance of SGUIE-Net. The code and proposed dataset are available at: https://trentqq.github.io/SGUIE-Net.html  
### MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v1 [eess.IV])
- Authors : Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, **Peyman Milanfar**, Alan Bovik, Yinxiao Li
- Link : [http://arxiv.org/abs/2201.02973](http://arxiv.org/abs/2201.02973)
> ABSTRACT  :  Recent progress on Transformers and multi-layer perceptron (MLP) models provide new network architectural designs for computer vision tasks. Although these models proved to be effective in many vision tasks such as image recognition, there remain challenges in adapting them for low-level vision. The inflexibility to support high-resolution images and limitations of local attention are perhaps the main bottlenecks for using Transformers and MLPs in image **restoration**. In this work we present a multi-axis MLP based architecture, called MAXIM, that can serve as an efficient and flexible general-purpose vision backbone for image processing tasks. MAXIM uses a UNet-shaped hierarchical structure and supports long-range interactions enabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based building blocks: a multi-axis gated MLP that allows for efficient and scalable spatial mixing of local and global visual cues, and a cross-gating block, an alternative to cross-attention, which accounts for cross-feature mutual conditioning. Both these modules are exclusively based on MLPs, but also benefit from being both global and `fully-convolutional', two properties that are desirable for image processing. Our extensive experimental results show that the proposed MAXIM model achieves state-of-the-art performance on more than ten benchmarks across a range of image processing tasks, including denoising, deblurring, deraining, dehazing, and **enhancement** while requiring fewer or comparable numbers of parameters and FLOPs than competitive models.  
### Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v1 [eess.IV])
- Authors : Congpei An, Ning Wu, Xiaoming Yuan
- Link : [http://arxiv.org/abs/2201.02979](http://arxiv.org/abs/2201.02979)
> ABSTRACT  :  The total variation (TV) regularization has phenomenally boosted various variational models for image processing tasks. We propose combining the backward diffusion process in the earlier literature of image **enhancement** with the TV regularization and show that the resulting enhanced TV minimization model is particularly effective for reducing the loss of contrast, which is often encountered by models using the TV regularization. We establish stable reconstruction guarantees for the enhanced TV model from noisy subsampled measurements; non-adaptive linear measurements and variable-density sampled Fourier measurements are considered. In particular, under some weaker restricted isometry property conditions, the enhanced TV minimization model is shown to have tighter reconstruction error bounds than various TV-based models for the scenario where the level of noise is significant and the amount of measurements is limited. The advantages of the enhanced TV model are also numerically validated by preliminary experiments on the reconstruction of some synthetic, natural, and medical images.  
### Enhancing Low-Light Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v1 [eess.IV])
- Authors : Lanqing Guo, Renjie Wan, **Wenhan Yang**, Alex Kot, Bihan Wen
- Link : [http://arxiv.org/abs/2201.03145](http://arxiv.org/abs/2201.03145)
> ABSTRACT  :  Images captured in the **low-light** condition suffer from low visibility and various imaging artifacts, e.g., real noise. Existing supervised enlightening algorithms require a large set of pixel-aligned training image pairs, which are hard to prepare in practice. Though weakly-supervised or unsupervised methods can alleviate such challenges without using paired training images, some real-world artifacts inevitably get falsely amplified because of the lack of corresponded supervision. In this paper, instead of using perfectly aligned images for training, we creatively employ the misaligned real-world images as the guidance, which are considerably easier to collect. Specifically, we propose a Cross-Image Disentanglement Network (CIDN) to separately extract cross-image brightness and image-specific content features from low/normal-light images. Based on that, CIDN can simultaneously correct the brightness and suppress image artifacts in the feature domain, which largely increases the robustness to the pixel shifts. Furthermore, we collect a new **low-light** image **enhancement** dataset consisting of misaligned training images with real-world corruptions. Experimental results show that our model achieves state-of-the-art performances on both the newly proposed dataset and other popular **low-light** datasets.  
### **Swin** Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])
- Authors : Jiahao Huang, Yingying Fang, Yinzhe Wu, Huanjun Wu, Zhifan Gao, Yang Li, Javier Del, Jun Xia, Guang Yang
- Link : [http://arxiv.org/abs/2201.03230](http://arxiv.org/abs/2201.03230)
> ABSTRACT  :  Magnetic resonance imaging (MRI) is an important non-invasive clinical tool that can produce high-resolution and reproducible images. However, a long scanning time is required for high-quality MR images, which leads to exhaustion and discomfort of patients, inducing more artefacts due to voluntary movements of the patients and involuntary physiological movements. To accelerate the scanning process, methods by k-space undersampling and deep learning based reconstruction have been popularised. This work introduced **Swin**MR, a novel **Swin** transformer based method for fast MRI reconstruction. The whole network consisted of an input module (IM), a feature extraction module (FEM) and an output module (OM). The IM and OM were 2D convolutional layers and the FEM was composed of a cascaded of residual **Swin** transformer blocks (RSTBs) and 2D convolutional layers. The RSTB consisted of a series of **Swin** transformer layers (STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was performed in shifted windows rather than the multi-head self-attention (MSA) of the original transformer in the whole image space. A novel multi-channel loss was proposed by using the sensitivity maps, which was proved to reserve more textures and details. We performed a series of comparative studies and ablation studies in the Calgary-Campinas public brain MR dataset and conducted a downstream segmentation experiment in the Multi-modal Brain Tumour Segmentation Challenge 2017 dataset. The results demonstrate our **Swin**MR achieved high-quality reconstruction compared with other benchmark methods, and it shows great robustness with different undersampling masks, under noise interruption and on different datasets. The code is publicly available at https://github.com/ayanglab/**Swin**MR.  
## cs.LG
---
### A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])
- Authors : Shuyue Guan, Murray Loew
- Link : [http://arxiv.org/abs/2201.02771](http://arxiv.org/abs/2201.02771)
> ABSTRACT  :  Instead of using current deep-learning segmentation models (like the UNet and variants), we approach the segmentation problem using trained Convolutional Neural Network (CNN) classifiers, which automatically extract important features from classified targets for image classification. Those extracted features can be visualized and formed heatmaps using Gradient-weighted Class Activation Mapping (Grad-CAM). This study tested whether the heatmaps could be used to segment the classified targets. We also proposed an evaluation method for the heatmaps; that is, to re-train the CNN classifier using images filtered by heatmaps and examine its performance. We used the mean-Dice coefficient to evaluate segmentation results. Results from our experiments show that heatmaps can locate and segment partial tumor areas. But only use of the heatmaps from CNN classifiers may not be an optimal approach for segmentation. In addition, we have verified that the predictions of CNN classifiers mainly depend on tumor areas, and **dark** regions in Grad-CAM's heatmaps also contribute to classification.  
### **Swin** Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])
- Authors : Jiahao Huang, Yingying Fang, Yinzhe Wu, Huanjun Wu, Zhifan Gao, Yang Li, Javier Del, Jun Xia, Guang Yang
- Link : [http://arxiv.org/abs/2201.03230](http://arxiv.org/abs/2201.03230)
> ABSTRACT  :  Magnetic resonance imaging (MRI) is an important non-invasive clinical tool that can produce high-resolution and reproducible images. However, a long scanning time is required for high-quality MR images, which leads to exhaustion and discomfort of patients, inducing more artefacts due to voluntary movements of the patients and involuntary physiological movements. To accelerate the scanning process, methods by k-space undersampling and deep learning based reconstruction have been popularised. This work introduced **Swin**MR, a novel **Swin** transformer based method for fast MRI reconstruction. The whole network consisted of an input module (IM), a feature extraction module (FEM) and an output module (OM). The IM and OM were 2D convolutional layers and the FEM was composed of a cascaded of residual **Swin** transformer blocks (RSTBs) and 2D convolutional layers. The RSTB consisted of a series of **Swin** transformer layers (STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was performed in shifted windows rather than the multi-head self-attention (MSA) of the original transformer in the whole image space. A novel multi-channel loss was proposed by using the sensitivity maps, which was proved to reserve more textures and details. We performed a series of comparative studies and ablation studies in the Calgary-Campinas public brain MR dataset and conducted a downstream segmentation experiment in the Multi-modal Brain Tumour Segmentation Challenge 2017 dataset. The results demonstrate our **Swin**MR achieved high-quality reconstruction compared with other benchmark methods, and it shows great robustness with different undersampling masks, under noise interruption and on different datasets. The code is publicly available at https://github.com/ayanglab/**Swin**MR.  
### BIGPrior: Towards Decoupling Learned Prior Hallucination and Data Fidelity in Image **Restoration**. (arXiv:2011.01406v3 [cs.CV] UPDATED)
- Authors : Majed El
- Link : [http://arxiv.org/abs/2011.01406](http://arxiv.org/abs/2011.01406)
> ABSTRACT  :  Classic image-**restoration** algorithms use a variety of priors, either implicitly or explicitly. Their priors are hand-designed and their corresponding weights are heuristically assigned. Hence, deep learning methods often produce superior image **restoration** quality. Deep networks are, however, capable of inducing strong and hardly predictable hallucinations. Networks implicitly learn to be jointly faithful to the observed data while learning an image prior; and the separation of original data and hallucinated data downstream is then not possible. This limits their wide-spread adoption in image **restoration**. Furthermore, it is often the hallucinated part that is victim to degradation-model overfitting.    We present an approach with decoupled network-prior based hallucination and data fidelity terms. We refer to our framework as the Bayesian Integration of a Generative Prior (BIGPrior). Our method is rooted in a Bayesian framework and tightly connected to classic **restoration** methods. In fact, it can be viewed as a generalization of a large family of classic **restoration** algorithms. We use network inversion to extract image prior information from a generative network. We show that, on image colorization, inpainting and denoising, our framework consistently improves the inversion results. Our method, though partly reliant on the quality of the generative network inversion, is competitive with state-of-the-art supervised and task-specific **restoration** methods. It also provides an additional metric that sets forth the degree of prior reliance per pixel relative to data fidelity.  
### CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v3 [cs.CV] UPDATED)
- Authors : Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo
- Link : [http://arxiv.org/abs/2107.00652](http://arxiv.org/abs/2107.00652)
> ABSTRACT  :  We present CSWin Transformer, an efficient and effective Transformer-based backbone for general-purpose vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute whereas local self-attention often limits the field of interactions of each token. To address this issue, we develop the Cross-Shaped Window self-attention mechanism for computing self-attention in the horizontal and vertical stripes in parallel that form a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. We provide a mathematical analysis of the effect of the stripe width and vary the stripe width for different layers of the Transformer network which achieves strong modeling capability while limiting the computation cost. We also introduce Locally-enhanced Positional Encoding (LePE), which handles the local positional information better than existing encoding schemes. LePE naturally supports arbitrary input resolutions, and is thus especially effective and friendly for downstream tasks. Incorporated with these designs and a hierarchical structure, CSWin Transformer demonstrates competitive performance on common vision tasks. Specifically, it achieves 85.4\% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task, and 52.2 mIOU on the ADE20K semantic segmentation task, surpassing previous state-of-the-art **Swin** Transformer backbone by +1.2, +2.0, +1.4, and +2.0 respectively under the similar FLOPs setting. By further pretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. The code and models are available at https://github.com/microsoft/CSWin-Transformer.  
### Discretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem. (arXiv:2112.09382v2 [cs.SD] UPDATED)
- Authors : Jing Shi, Xuankai Chang, Tomoki Hayashi, Ju Lu, Shinji Watanabe, Bo Xu
- Link : [http://arxiv.org/abs/2112.09382](http://arxiv.org/abs/2112.09382)
> ABSTRACT  :  Deep learning based models have significantly improved the performance of speech separation with input mixtures like the cocktail party. Prominent methods (e.g., frequency-domain and time-domain speech separation) usually build regression models to predict the ground-truth speech from the mixture, using the masking-based design and the signal-level loss criterion (e.g., MSE or SI-SNR). This study demonstrates, for the first time, that the synthesis-based approach can also perform well on this problem, with great flexibility and strong potential. Specifically, we propose a novel speech separation/**enhancement** model based on the recognition of discrete symbols, and convert the paradigm of the speech separation/**enhancement** related tasks from regression to classification. By utilizing the synthesis model with the input of discrete symbols, after the prediction of discrete symbol sequence, each target speech could be re-synthesized. Evaluation results based on the WSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed method can steadily synthesize the separated speech with high speech quality and without any interference, which is difficult to avoid in regression-based methods. In addition, with negligible loss of listening quality, the speaker conversion of enhanced/separated speech could be easily realized through our method.  
## cs.AI
---
### A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])
- Authors : Shuyue Guan, Murray Loew
- Link : [http://arxiv.org/abs/2201.02771](http://arxiv.org/abs/2201.02771)
> ABSTRACT  :  Instead of using current deep-learning segmentation models (like the UNet and variants), we approach the segmentation problem using trained Convolutional Neural Network (CNN) classifiers, which automatically extract important features from classified targets for image classification. Those extracted features can be visualized and formed heatmaps using Gradient-weighted Class Activation Mapping (Grad-CAM). This study tested whether the heatmaps could be used to segment the classified targets. We also proposed an evaluation method for the heatmaps; that is, to re-train the CNN classifier using images filtered by heatmaps and examine its performance. We used the mean-Dice coefficient to evaluate segmentation results. Results from our experiments show that heatmaps can locate and segment partial tumor areas. But only use of the heatmaps from CNN classifiers may not be an optimal approach for segmentation. In addition, we have verified that the predictions of CNN classifiers mainly depend on tumor areas, and **dark** regions in Grad-CAM's heatmaps also contribute to classification.  
### **Swin** Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])
- Authors : Jiahao Huang, Yingying Fang, Yinzhe Wu, Huanjun Wu, Zhifan Gao, Yang Li, Javier Del, Jun Xia, Guang Yang
- Link : [http://arxiv.org/abs/2201.03230](http://arxiv.org/abs/2201.03230)
> ABSTRACT  :  Magnetic resonance imaging (MRI) is an important non-invasive clinical tool that can produce high-resolution and reproducible images. However, a long scanning time is required for high-quality MR images, which leads to exhaustion and discomfort of patients, inducing more artefacts due to voluntary movements of the patients and involuntary physiological movements. To accelerate the scanning process, methods by k-space undersampling and deep learning based reconstruction have been popularised. This work introduced **Swin**MR, a novel **Swin** transformer based method for fast MRI reconstruction. The whole network consisted of an input module (IM), a feature extraction module (FEM) and an output module (OM). The IM and OM were 2D convolutional layers and the FEM was composed of a cascaded of residual **Swin** transformer blocks (RSTBs) and 2D convolutional layers. The RSTB consisted of a series of **Swin** transformer layers (STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was performed in shifted windows rather than the multi-head self-attention (MSA) of the original transformer in the whole image space. A novel multi-channel loss was proposed by using the sensitivity maps, which was proved to reserve more textures and details. We performed a series of comparative studies and ablation studies in the Calgary-Campinas public brain MR dataset and conducted a downstream segmentation experiment in the Multi-modal Brain Tumour Segmentation Challenge 2017 dataset. The results demonstrate our **Swin**MR achieved high-quality reconstruction compared with other benchmark methods, and it shows great robustness with different undersampling masks, under noise interruption and on different datasets. The code is publicly available at https://github.com/ayanglab/**Swin**MR.  
### Hierarchical Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v4 [cs.CV] UPDATED)
- Authors : Ruwen Bai, Min Li, Bo Meng, Fengfa Li, Miao Jiang, Junxing Ren, Degang Sun
- Link : [http://arxiv.org/abs/2109.02860](http://arxiv.org/abs/2109.02860)
> ABSTRACT  :  Graph convolutional networks (GCNs) have emerged as dominant methods for skeleton-based action recognition.    However, they still suffer from two problems, namely, neighborhood constraints and entangled spatiotemporal feature representations.    Most studies have focused on improving the design of graph topology to solve the first problem but they have yet to fully explore the latter.    In this work, we design a disentangled spatiotemporal transformer (DSTT) block to overcome the above limitations of GCNs in three steps: (i) feature disentanglement for spatiotemporal decomposition;(ii) global spatiotemporal attention for capturing correlations in the global context; and (iii) local information **enhancement** for utilizing more local information.    Thereon, we propose a novel architecture, named Hierarchical Graph Convolutional skeleton Transformer (HGCT), to employ the complementary advantages of GCN (i.e., local topology, temporal dynamics and hierarchy) and Transformer (i.e., global context and dynamic attention).    HGCT is lightweight and computationally efficient.    Quantitative analysis demonstrates the superiority and good interpretability of HGCT.  
### Data-Driven AI Model Signal-Awareness **Enhancement** and Introspection. (arXiv:2111.05827v2 [cs.SE] UPDATED)
- Authors : Sahil Suneja, Yufan Zhuang, Yunhui Zheng, Jim Laredo, Alessandro Morari
- Link : [http://arxiv.org/abs/2111.05827](http://arxiv.org/abs/2111.05827)
> ABSTRACT  :  AI modeling for source code understanding tasks has been making significant progress, and is being adopted in production development pipelines. However, reliability concerns, especially whether the models are actually learning task-related aspects of source code, are being raised. While recent model-probing approaches have observed a lack of signal awareness in many AI-for-code models, i.e. models not capturing task-relevant signals, they do not offer solutions to rectify this problem. In this paper, we explore data-driven approaches to enhance models' signal-awareness: 1) we combine the SE concept of code complexity with the AI technique of curriculum learning; 2) we incorporate SE assistance into AI models by customizing Delta Debugging to generate simplified signal-preserving programs, augmenting them to the training dataset. With our techniques, we achieve up to 4.8x improvement in model signal awareness. Using the notion of code complexity, we further present a novel model learning introspection approach from the perspective of the dataset.  
# Paper List
---
## cs.CV
---
**137** new papers in cs.CV:-) 
1. Compressing Models with Few Samples: Mimicking then Replacing. (arXiv:2201.02620v1 [cs.LG])
2. Microdosing: Knowledge Distillation for GAN based Compression. (arXiv:2201.02624v1 [eess.IV])
3. Flex**HDR**: Modelling Alignment and **Exposure** Uncertainties for Flexible **HDR** Imaging. (arXiv:2201.02625v1 [eess.IV])
4. Learning with less labels in Digital Pathology via Scribble Supervision from natural images. (arXiv:2201.02627v1 [eess.IV])
5. United adversarial learning for liver tumor segmentation and detection of multi-modality non-contrast MRI. (arXiv:2201.02629v1 [eess.IV])
6. MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v1 [cs.CV])
7. GPU-Net: Lightweight U-Net with more diverse features. (arXiv:2201.02656v1 [eess.IV])
8. Video Coding for Machines: Partial transmission of SIFT features. (arXiv:2201.02689v1 [eess.IV])
9. BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing. (arXiv:2201.02693v1 [cs.LG])
10. Development of Automatic Tree Counting Software from UAV Based Aerial Images With Machine Learning. (arXiv:2201.02698v1 [cs.CV])
11. Block Walsh-Hadamard Transform Based Binary Layers in Deep Neural Networks. (arXiv:2201.02711v1 [cs.LG])
12. Pseudo-labelling and Meta Reweighting Learning for Image Aesthetic Quality Assessment. (arXiv:2201.02714v1 [cs.CV])
13. Real-time Rail Recognition Based on 3D Point Clouds. (arXiv:2201.02726v1 [cs.CV])
14. Expert Knowledge-guided Geometric Representation Learning for Magnetic Resonance Imaging-based Glioma Grading. (arXiv:2201.02746v1 [eess.IV])
15. QuadTree Attention for Vision Transformers. (arXiv:2201.02767v1 [cs.CV])
16. A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])
17. A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval. (arXiv:2201.02772v1 [cs.CV])
18. A Baseline Statistical Method For Robust User-Assisted Multiple Segmentation. (arXiv:2201.02779v1 [cs.CV])
19. Relieving Long-tailed Instance Segmentation via Pairwise Class Balance. (arXiv:2201.02784v1 [cs.CV])
20. RARA: Zero-shot Sim2Real Visual Navigation with Following Foreground Cues. (arXiv:2201.02798v1 [cs.CV])
21. Counteracting **Dark** Web Text-Based CAPTCHA with Generative Adversarial Learning for Proactive Cyber Threat Intelligence. (arXiv:2201.02799v1 [cs.CV])
22. Hyperspectral Image Denoising Using Non-convex Local Low-rank and Sparse Separation with Spatial-Spectral Total Variation Regularization. (arXiv:2201.02812v1 [eess.IV])
23. Classification of Hyperspectral Images by Using Spectral Data and Fully Connected Neural Network. (arXiv:2201.02821v1 [eess.IV])
24. CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwnannoma and Cochlea Segmentation. (arXiv:2201.02831v1 [eess.IV])
25. SGUIE-Net: Semantic Attention Guided Underwater Image **Enhancement** with Multi-Scale Perception. (arXiv:2201.02832v1 [eess.IV])
26. Weighted Encoding Optimization for Dynamic Single-pixel Imaging and Sensing. (arXiv:2201.02833v1 [eess.IV])
27. Self-aligned Spatial Feature Extraction Network for UAV Vehicle Re-identification. (arXiv:2201.02836v1 [cs.CV])
28. Mushrooms Detection, Localization and 3D Pose Estimation using RGB-D Sensor for Robotic-picking Applications. (arXiv:2201.02837v1 [cs.CV])
29. Learning Sample Importance for Cross-Scenario Video Temporal Grounding. (arXiv:2201.02848v1 [cs.CV])
30. Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition. (arXiv:2201.02849v1 [cs.CV])
31. Image-based Automatic Dial Meter Reading in Unconstrained Scenarios. (arXiv:2201.02850v1 [cs.CV])
32. Fake Hilsa Fish Detection Using Machine Vision. (arXiv:2201.02853v1 [cs.CV])
33. Decoupling Makes Weakly Supervised Local Feature Better. (arXiv:2201.02861v1 [cs.CV])
34. Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscop. (arXiv:2201.02867v1 [eess.IV])
35. Defocus Deblur Microscopy via feature interactive coarse-to-fine network. (arXiv:2201.02876v1 [eess.IV])
36. Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision. (arXiv:2201.02885v1 [cs.CV])
37. Resolving Camera Position for a Practical Application of Gaze Estimation on Edge Devices. (arXiv:2201.02946v1 [cs.CV])
38. Box2Seg: Learning Semantics of 3D Point Clouds with Box-Level Supervision. (arXiv:2201.02963v1 [cs.CV])
39. MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v1 [eess.IV])
40. Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v1 [eess.IV])
41. Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v1 [cs.CV])
42. A Survey on Face Recognition Systems. (arXiv:2201.02991v1 [cs.CV])
43. MaskMTL: Attribute prediction in masked facial images with deep multitask learning. (arXiv:2201.03002v1 [cs.CV])
44. ThreshNet: An Efficient DenseNet using Threshold Mechanism to Reduce Connections. (arXiv:2201.03013v1 [cs.CV])
45. Glance and Focus Networks for Dynamic Visual Recognition. (arXiv:2201.03014v1 [cs.CV])
46. Learning class prototypes from Synthetic InSAR with Vision Transformers. (arXiv:2201.03016v1 [eess.IV])
47. Self-Supervised Feature Learning from Partial Point Clouds via Pose Disentanglement. (arXiv:2201.03018v1 [cs.CV])
48. Semantics-driven Attentive Few-shot Learning over Clean and Noisy Samples. (arXiv:2201.03043v1 [cs.CV])
49. Applying Artificial Intelligence for Age Estimation in Digital Forensic Investigations. (arXiv:2201.03045v1 [cs.CV])
50. Lung infection and normal region segmentation from CT volumes of COVID-19 cases. (arXiv:2201.03050v1 [eess.IV])
51. COVID-19 Infection Segmentation from Chest CT Images Based on Scale Uncertainty. (arXiv:2201.03053v1 [eess.IV])
52. The State of Aerial Surveillance: A Survey. (arXiv:2201.03080v1 [cs.CV])
53. ImageSubject: A Large-scale Dataset for Subject Detection. (arXiv:2201.03101v1 [cs.CV])
54. Preserving Domain Private Representation via Mutual Information Maximization. (arXiv:2201.03102v1 [cs.LG])
55. Signal Reconstruction from Quantized Noisy Samples of the Discrete Fourier Transform. (arXiv:2201.03114v1 [eess.SP])
56. Systematic biases when using deep neural networks for annotating large catalogs of astronomical images. (arXiv:2201.03131v1 [astro-ph.GA])
57. Multi-Level Attention for Unsupervised Person Re-Identification. (arXiv:2201.03141v1 [cs.CV])
58. Enhancing Low-Light Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v1 [eess.IV])
59. TFS Recognition: Investigating MPH]{Thai Finger Spelling Recognition: Investigating MediaPipe Hands Potentials. (arXiv:2201.03170v1 [cs.CV])
60. Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond. (arXiv:2201.03176v1 [cs.CV])
61. **Swin** transformers make strong contextual encoders for VHR image road extraction. (arXiv:2201.03178v1 [cs.CV])
62. Transfer Learning for Scene Text Recognition in Indian Languages. (arXiv:2201.03180v1 [cs.CV])
63. Towards Boosting the Accuracy of Non-Latin Scene Text Recognition. (arXiv:2201.03185v1 [cs.CV])
64. MyoPS: A Benchmark of Myocardial Pathology Segmentation Combining Three-Sequence Cardiac Magnetic Resonance Images. (arXiv:2201.03186v1 [eess.IV])
65. Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification. (arXiv:2201.03194v1 [cs.CV])
66. End-to-end lossless compression of high precision depth maps guided by pseudo-residual. (arXiv:2201.03195v1 [eess.IV])
67. Model-Based Image Signal Processors via Learnable Dictionaries. (arXiv:2201.03210v1 [eess.IV])
68. Why-So-Deep: Towards Boosting Previously Trained Models for Visual Place Recognition. (arXiv:2201.03212v1 [cs.CV])
69. Fully automatic scoring of handwritten descriptive answers in Japanese language tests. (arXiv:2201.03215v1 [cs.LG])
70. **Swin** Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])
71. Small Object Detection using Deep Learning. (arXiv:2201.03243v1 [cs.CV])
72. Vision in adverse weather: Augmentation using CycleGANs with various object detectors for robust perception in autonomous racing. (arXiv:2201.03246v1 [cs.CV])
73. A statistical shape model for radiation-free assessment and classification of craniosynostosis. (arXiv:2201.03288v1 [eess.IV])
74. GhostNets on Heterogeneous Devices via Cheap Operations. (arXiv:2201.03297v1 [cs.CV])
75. Avoiding Overfitting: A Survey on Regularization Methods for Convolutional Neural Networks. (arXiv:2201.03299v1 [cs.CV])
76. Comparison of Representation Learning Techniques for Tracking in time resolved 3D Ultrasound. (arXiv:2201.03319v1 [eess.IV])
77. Gait Recognition Based on Deep Learning: A Survey. (arXiv:2201.03323v1 [cs.CV])
78. COIN: Counterfactual Image Generation for VQA Interpretation. (arXiv:2201.03342v1 [cs.CV])
79. GMFIM: A Generative Mask-guided Facial Image Manipulation Model for Privacy Preservation. (arXiv:2201.03353v1 [cs.CV])
80. High-resolution Ecosystem Mapping in Repetitive Environments Using Dual Camera SLAM. (arXiv:2201.03364v1 [cs.RO])
81. General Deformations of Point Configurations Viewed By a Pinhole Model Camera. (arXiv:1505.08070v2 [cs.CV] UPDATED)
82. NeoNav: Improving the Generalization of Visual Navigation via Generating Next Expected Observations. (arXiv:1906.07207v4 [cs.RO] UPDATED)
83. A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D images. (arXiv:1912.10230v4 [cs.CV] UPDATED)
84. 1D Probabilistic Undersampling Pattern Optimization for MR Image Reconstruction. (arXiv:2003.03797v3 [eess.IV] UPDATED)
85. Teaching CNNs to mimic Human Visual Cognitive Process & regularise Texture-Shape bias. (arXiv:2006.14722v2 [cs.CV] UPDATED)
86. Comparative study of deep learning methods for the automatic segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients. (arXiv:2007.15546v4 [eess.IV] UPDATED)
87. Improving concave point detection to better segment overlapped objects in images. (arXiv:2008.00997v3 [cs.CV] UPDATED)
88. A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images. (arXiv:2010.10563v2 [cs.CV] UPDATED)
89. BIGPrior: Towards Decoupling Learned Prior Hallucination and Data Fidelity in Image **Restoration**. (arXiv:2011.01406v3 [cs.CV] UPDATED)
90. A Three-Stage Self-Training Framework for Semi-Supervised Semantic Segmentation. (arXiv:2012.00827v2 [cs.CV] UPDATED)
91. ACE-Net: Fine-Level Face Alignment through Anchors and Contours Estimation. (arXiv:2012.01461v2 [cs.CV] UPDATED)
92. Morphology on categorical distributions. (arXiv:2012.07315v2 [cs.CV] UPDATED)
93. AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles. (arXiv:2101.06549v3 [cs.RO] UPDATED)
94. Deep Learning for Instance Retrieval: A Survey. (arXiv:2101.11282v3 [cs.CV] UPDATED)
95. Tiny Adversarial Mulit-Objective Oneshot Neural Architecture Search. (arXiv:2103.00363v2 [cs.LG] UPDATED)
96. Feedback Refined Local-Global Network for Super-Resolution of Hyperspectral Imagery. (arXiv:2103.04354v2 [eess.IV] UPDATED)
97. Reframing Neural Networks: Deep Structure in Overcomplete Representations. (arXiv:2103.05804v2 [cs.LG] UPDATED)
98. Video-Specific Autoencoders for Exploring, Editing and Transmitting Videos. (arXiv:2103.17261v2 [cs.CV] UPDATED)
99. Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network. (arXiv:2105.09378v2 [eess.IV] UPDATED)
100. Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v3 [cs.CV] UPDATED)
101. SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis. (arXiv:2106.01444v2 [cs.CL] UPDATED)
102. A Multi-Implicit Neural Representation for Fonts. (arXiv:2106.06866v2 [cs.CV] UPDATED)
103. Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v2 [cs.CV] UPDATED)
104. SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v8 [math.OC] UPDATED)
105. Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v3 [cs.CV] UPDATED)
106. Action Transformer: A Self-Attention Model for Short-Time Pose-Based Human Action Recognition. (arXiv:2107.00606v6 [cs.CV] UPDATED)
107. CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v3 [cs.CV] UPDATED)
108. Visual Parser: Representing Part-whole Hierarchies with Transformers. (arXiv:2107.05790v2 [cs.CV] UPDATED)
109. Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents. (arXiv:2108.00273v2 [cs.CV] UPDATED)
110. O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v2 [cs.CL] UPDATED)
111. PVT: Point-Voxel Transformer for Point Cloud Learning. (arXiv:2108.06076v3 [cs.CV] UPDATED)
112. Specificity-preserving RGB-D Saliency Detection. (arXiv:2108.08162v2 [cs.CV] UPDATED)
113. GlassNet: Label Decoupling-based Three-stream Neural Network for Robust Image Glass Detection. (arXiv:2108.11117v2 [cs.CV] UPDATED)
114. Hierarchical Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v4 [cs.CV] UPDATED)
115. PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds. (arXiv:2109.05566v2 [cs.CV] UPDATED)
116. Unsupervised Image Decomposition with Phase-Correlation Networks. (arXiv:2110.03473v3 [cs.CV] UPDATED)
117. Distinguishing Natural and Computer-Generated Images using Multi-Colorspace fused EfficientNet. (arXiv:2110.09428v2 [cs.CV] UPDATED)
118. Video-based fully automatic assessment of open surgery suturing skills. (arXiv:2110.13972v2 [cs.CV] UPDATED)
119. FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy. (arXiv:2111.05623v2 [cs.RO] UPDATED)
120. An Analysis of the Influence of Transfer Learning When Measuring the Tortuosity of Blood Vessels. (arXiv:2111.10255v2 [eess.IV] UPDATED)
121. Hierarchical Graph-Convolutional Variational AutoEncoding for Generative Modelling of Human Motion. (arXiv:2111.12602v3 [cs.CV] UPDATED)
122. PGGANet: Pose Guided Graph Attention Network for Person Re-identification. (arXiv:2111.14411v2 [cs.CV] UPDATED)
123. DiffSDFSim: Differentiable Rigid-Body Dynamics With Implicit Shapes. (arXiv:2111.15318v2 [cs.CV] UPDATED)
124. Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data. (arXiv:2112.00054v2 [cs.CV] UPDATED)
125. Self-supervised Spatiotemporal Representation Learning by Exploiting Video Continuity. (arXiv:2112.05883v2 [cs.CV] UPDATED)
126. SAC-GAN: Structure-Aware Image-to-Image Composition for Self-Driving. (arXiv:2112.06596v3 [cs.CV] UPDATED)
127. Reconstructing Compact Building Models from Point Clouds Using Deep Implicit Fields. (arXiv:2112.13142v2 [cs.CV] UPDATED)
128. D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation. (arXiv:2201.00462v2 [cs.CV] UPDATED)
129. GAT-CADNet: Graph Attention Network for Panoptic Symbol Spotting in CAD Drawings. (arXiv:2201.00625v2 [cs.CV] UPDATED)
130. HWRCNet: Handwritten Word Recognition in JPEG Compressed Domain using CNN-BiLSTM Network. (arXiv:2201.00947v2 [cs.CV] UPDATED)
131. A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v2 [cs.CV] UPDATED)
132. Linear Variational State Space Filtering. (arXiv:2201.01353v2 [cs.LG] UPDATED)
133. Aerial Scene Parsing: From Tile-level Scene Classification to Pixel-wise Semantic Labeling. (arXiv:2201.01953v2 [cs.CV] UPDATED)
134. Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v2 [eess.IV] UPDATED)
135. Video Summarization Based on Video-text Modelling. (arXiv:2201.02494v2 [cs.CV] UPDATED)
136. A Novel Incremental Learning Driven Instance Segmentation Framework to Recognize Highly Cluttered Instances of the Contraband Items. (arXiv:2201.02560v2 [cs.CV] UPDATED)
137. Detecting Twenty-thousand Classes using Image-level Supervision. (arXiv:2201.02605v2 [cs.CV] UPDATED)
## eess.IV
---
**44** new papers in eess.IV:-) 
1. High-contrast, speckle-free, true 3D holography via binary CGH optimization. (arXiv:2201.02619v1 [eess.IV])
2. Microdosing: Knowledge Distillation for GAN based Compression. (arXiv:2201.02624v1 [eess.IV])
3. Flex**HDR**: Modelling Alignment and **Exposure** Uncertainties for Flexible **HDR** Imaging. (arXiv:2201.02625v1 [eess.IV])
4. Learning with less labels in Digital Pathology via Scribble Supervision from natural images. (arXiv:2201.02627v1 [eess.IV])
5. United adversarial learning for liver tumor segmentation and detection of multi-modality non-contrast MRI. (arXiv:2201.02629v1 [eess.IV])
6. GPU-Net: Lightweight U-Net with more diverse features. (arXiv:2201.02656v1 [eess.IV])
7. Video Coding for Machines: Partial transmission of SIFT features. (arXiv:2201.02689v1 [eess.IV])
8. BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing. (arXiv:2201.02693v1 [cs.LG])
9. Block Walsh-Hadamard Transform Based Binary Layers in Deep Neural Networks. (arXiv:2201.02711v1 [cs.LG])
10. Expert Knowledge-guided Geometric Representation Learning for Magnetic Resonance Imaging-based Glioma Grading. (arXiv:2201.02746v1 [eess.IV])
11. A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])
12. Hyperspectral Image Denoising Using Non-convex Local Low-rank and Sparse Separation with Spatial-Spectral Total Variation Regularization. (arXiv:2201.02812v1 [eess.IV])
13. Classification of Hyperspectral Images by Using Spectral Data and Fully Connected Neural Network. (arXiv:2201.02821v1 [eess.IV])
14. CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwnannoma and Cochlea Segmentation. (arXiv:2201.02831v1 [eess.IV])
15. SGUIE-Net: Semantic Attention Guided Underwater Image **Enhancement** with Multi-Scale Perception. (arXiv:2201.02832v1 [eess.IV])
16. Weighted Encoding Optimization for Dynamic Single-pixel Imaging and Sensing. (arXiv:2201.02833v1 [eess.IV])
17. Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscop. (arXiv:2201.02867v1 [eess.IV])
18. Defocus Deblur Microscopy via feature interactive coarse-to-fine network. (arXiv:2201.02876v1 [eess.IV])
19. MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v1 [eess.IV])
20. Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v1 [eess.IV])
21. Learning class prototypes from Synthetic InSAR with Vision Transformers. (arXiv:2201.03016v1 [eess.IV])
22. Higher Order Dynamic Mode Decomposition: from Fluid Dynamics to Heart Disease Analysis. (arXiv:2201.03030v1 [eess.IV])
23. Lung infection and normal region segmentation from CT volumes of COVID-19 cases. (arXiv:2201.03050v1 [eess.IV])
24. COVID-19 Infection Segmentation from Chest CT Images Based on Scale Uncertainty. (arXiv:2201.03053v1 [eess.IV])
25. Signal Reconstruction from Quantized Noisy Samples of the Discrete Fourier Transform. (arXiv:2201.03114v1 [eess.SP])
26. Enhancing Low-Light Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v1 [eess.IV])
27. MyoPS: A Benchmark of Myocardial Pathology Segmentation Combining Three-Sequence Cardiac Magnetic Resonance Images. (arXiv:2201.03186v1 [eess.IV])
28. End-to-end lossless compression of high precision depth maps guided by pseudo-residual. (arXiv:2201.03195v1 [eess.IV])
29. Model-Based Image Signal Processors via Learnable Dictionaries. (arXiv:2201.03210v1 [eess.IV])
30. **Swin** Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])
31. A statistical shape model for radiation-free assessment and classification of craniosynostosis. (arXiv:2201.03288v1 [eess.IV])
32. Comparison of Representation Learning Techniques for Tracking in time resolved 3D Ultrasound. (arXiv:2201.03319v1 [eess.IV])
33. Two Methods for Iso-Surface Extraction from Volumetric Data and Their Comparison. (arXiv:2201.03446v1 [cs.GR])
34. Learning Population-level Shape Statistics and Anatomy Segmentation From Images: A Joint Deep Learning Model. (arXiv:2201.03481v1 [eess.IV])
35. Data Processing of Functional Optical Microscopy for Neuroscience. (arXiv:2201.03537v1 [eess.IV])
36. 1D Probabilistic Undersampling Pattern Optimization for MR Image Reconstruction. (arXiv:2003.03797v3 [eess.IV] UPDATED)
37. Teaching CNNs to mimic Human Visual Cognitive Process & regularise Texture-Shape bias. (arXiv:2006.14722v2 [cs.CV] UPDATED)
38. Comparative study of deep learning methods for the automatic segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients. (arXiv:2007.15546v4 [eess.IV] UPDATED)
39. Improving concave point detection to better segment overlapped objects in images. (arXiv:2008.00997v3 [cs.CV] UPDATED)
40. Feedback Refined Local-Global Network for Super-Resolution of Hyperspectral Imagery. (arXiv:2103.04354v2 [eess.IV] UPDATED)
41. Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network. (arXiv:2105.09378v2 [eess.IV] UPDATED)
42. An Analysis of the Influence of Transfer Learning When Measuring the Tortuosity of Blood Vessels. (arXiv:2111.10255v2 [eess.IV] UPDATED)
43. HWRCNet: Handwritten Word Recognition in JPEG Compressed Domain using CNN-BiLSTM Network. (arXiv:2201.00947v2 [cs.CV] UPDATED)
44. Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v2 [eess.IV] UPDATED)
## cs.LG
---
**209** new papers in cs.LG:-) 
1. Posture Prediction for Healthy Sitting using a Smart Chair. (arXiv:2201.02615v1 [cs.LG])
2. Compressing Models with Few Samples: Mimicking then Replacing. (arXiv:2201.02620v1 [cs.LG])
3. Spatio-Temporal Graph Representation Learning for Fraudster Group Detection. (arXiv:2201.02621v1 [cs.LG])
4. Neighbor2vec: an efficient and effective method for Graph Embedding. (arXiv:2201.02626v1 [cs.SI])
5. Learning with less labels in Digital Pathology via Scribble Supervision from natural images. (arXiv:2201.02627v1 [eess.IV])
6. Attention Option-Critic. (arXiv:2201.02628v1 [cs.LG])
7. MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v1 [cs.CV])
8. Data-Efficient Information Extraction from Form-Like Documents. (arXiv:2201.02647v1 [cs.LG])
9. Fair and efficient contribution valuation for vertical federated learning. (arXiv:2201.02658v1 [cs.LG])
10. Stay Positive: Knowledge Graph Embedding Without Negative Sampling. (arXiv:2201.02661v1 [cs.LG])
11. Optimizing the Communication-Accuracy Trade-off in Federated Learning with Rate-Distortion Theory. (arXiv:2201.02664v1 [cs.LG])
12. Detecting CAN Masquerade Attacks with Signal Clustering Similarity. (arXiv:2201.02665v1 [cs.CR])
13. Improved Input Reprogramming for GAN Conditioning. (arXiv:2201.02692v1 [cs.LG])
14. BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing. (arXiv:2201.02693v1 [cs.LG])
15. Unsupervised Machine Learning for Exploratory Data Analysis of Exoplanet Transmission Spectra. (arXiv:2201.02696v1 [astro-ph.EP])
16. An Improved Mathematical Model of Sepsis: Modeling, Bifurcation Analysis, and Optimal Control Study for Complex Nonlinear Infectious Disease System. (arXiv:2201.02702v1 [math.DS])
17. Block Walsh-Hadamard Transform Based Binary Layers in Deep Neural Networks. (arXiv:2201.02711v1 [cs.LG])
18. Bitcoin Price Predictive Modeling Using Expert Correction. (arXiv:2201.02729v1 [q-fin.ST])
19. Testing the Robustness of a BiLSTM-based Structural Story Classifier. (arXiv:2201.02733v1 [cs.CL])
20. A Deep Learning Approach to Integrate Human-Level Understanding in a Chatbot. (arXiv:2201.02735v1 [cs.CL])
21. Cognitive Computing to Optimize IT Services. (arXiv:2201.02737v1 [cs.CL])
22. Provable Clustering of a Union of Linear Manifolds Using Optimal Directions. (arXiv:2201.02745v1 [cs.LG])
23. Conditional Approximate Normalizing Flows for Joint Multi-Step Probabilistic Electricity Demand Forecasting. (arXiv:2201.02753v1 [cs.LG])
24. Machine Learning-Based Disease Diagnosis:A Bibliometric Analysis. (arXiv:2201.02755v1 [cs.LG])
25. DeHIN: A Decentralized Framework for Embedding Large-scale Heterogeneous Information Networks. (arXiv:2201.02757v1 [cs.LG])
26. Global Convergence Analysis of Deep Linear Networks with A One-neuron Layer. (arXiv:2201.02761v1 [cs.LG])
27. A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])
28. Attacking Vertical Collaborative Learning System Using Adversarial Dominating Inputs. (arXiv:2201.02775v1 [cs.CR])
29. A Fair and Efficient Hybrid Federated Learning Framework based on XGBoost for Distributed Power Prediction. (arXiv:2201.02783v1 [cs.LG])
30. Scaling Knowledge Graph Embedding Models. (arXiv:2201.02791v1 [cs.LG])
31. RARA: Zero-shot Sim2Real Visual Navigation with Following Foreground Cues. (arXiv:2201.02798v1 [cs.CV])
32. A fall alert system with prior-fall activity identification. (arXiv:2201.02803v1 [cs.LG])
33. Hyperspectral Image Denoising Using Non-convex Local Low-rank and Sparse Separation with Spatial-Spectral Total Variation Regularization. (arXiv:2201.02812v1 [eess.IV])
34. Clustering Text Using Attention. (arXiv:2201.02816v1 [cs.CL])
35. Optimal 1-Wasserstein Distance for WGANs. (arXiv:2201.02824v1 [stat.ML])
36. Reconfigurable Intelligent Surface Enabled Spatial Multiplexing with Fully Convolutional Network. (arXiv:2201.02834v1 [eess.SP])
37. PocketNN: Integer-only Training and Inference of Neural Networks via Direct Feedback Alignment and Pocket Activations in Pure C++. (arXiv:2201.02863v1 [cs.LG])
38. Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscop. (arXiv:2201.02867v1 [eess.IV])
39. LoMar: A Local Defense Against Poisoning Attack on Federated Learning. (arXiv:2201.02873v1 [cs.LG])
40. Assessing Policy, Loss and Planning Combinations in Reinforcement Learning using a New Modular Architecture. (arXiv:2201.02874v1 [cs.LG])
41. Attention-based Random Forest and Contamination Model. (arXiv:2201.02880v1 [cs.LG])
42. Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision. (arXiv:2201.02885v1 [cs.CV])
43. Lazy Lagrangians with Predictions for Online Learning. (arXiv:2201.02890v1 [cs.LG])
44. Extraction of Product Specifications from the Web -- Going Beyond Tables and Lists. (arXiv:2201.02896v1 [cs.IR])
45. {\lambda}-Scaled-Attention: A Novel Fast Attention Mechanism for Efficient Modeling of Protein Sequences. (arXiv:2201.02912v1 [cs.LG])
46. Open-Set Recognition of Breast Cancer Treatments. (arXiv:2201.02923v1 [cs.LG])
47. A Multi-agent Reinforcement Learning Approach for Efficient Client Selection in Federated Learning. (arXiv:2201.02932v1 [cs.LG])
48. Causal Discovery from Sparse Time-Series Data Using Echo State Network. (arXiv:2201.02933v1 [cs.LG])
49. Weak Supervision for Affordable Modeling of Electrocardiogram Data. (arXiv:2201.02936v1 [eess.SP])
50. TPAD: Identifying Effective Trajectory Predictions Under the Guidance of Trajectory Anomaly Detection Model. (arXiv:2201.02941v1 [cs.LG])
51. Fast solver for J2-perturbed Lambert problem using deep neural network. (arXiv:2201.02942v1 [math.NA])
52. Robust classification with flexible discriminant analysis in heterogeneous data. (arXiv:2201.02967v1 [stat.ML])
53. An Adaptive Device-Edge Co-Inference Framework Based on Soft Actor-Critic. (arXiv:2201.02968v1 [cs.LG])
54. Auto-Encoder based Co-Training Multi-View Representation Learning. (arXiv:2201.02978v1 [cs.LG])
55. Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v1 [cs.CV])
56. A Survey on Face Recognition Systems. (arXiv:2201.02991v1 [cs.CV])
57. Rethink Stealthy Backdoor Attacks in Natural Language Processing. (arXiv:2201.02993v1 [cs.CL])
58. Privacy-aware Early Detection of COVID-19 through Adversarial Training. (arXiv:2201.03004v1 [cs.LG])
59. Glance and Focus Networks for Dynamic Visual Recognition. (arXiv:2201.03014v1 [cs.CV])
60. Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay. (arXiv:2201.03019v1 [cs.LG])
61. Meta-Generalization for Multiparty Privacy Learning to Identify Anomaly Multimedia Traffic in Graynet. (arXiv:2201.03027v1 [cs.CR])
62. Development of a hybrid machine-learning and optimization tool for performance-based solar shading design. (arXiv:2201.03028v1 [cs.LG])
63. Discriminant Analysis in Contrasting Dimensions for Polycystic Ovary Syndrome Prognostication. (arXiv:2201.03029v1 [cs.LG])
64. Medication Error Detection Using Contextual Language Models. (arXiv:2201.03035v1 [cs.CL])
65. Applying Artificial Intelligence for Age Estimation in Digital Forensic Investigations. (arXiv:2201.03045v1 [cs.CV])
66. Lung infection and normal region segmentation from CT volumes of COVID-19 cases. (arXiv:2201.03050v1 [eess.IV])
67. COVID-19 Infection Segmentation from Chest CT Images Based on Scale Uncertainty. (arXiv:2201.03053v1 [eess.IV])
68. Stability Based Generalization Bounds for Exponential Family Langevin Dynamics. (arXiv:2201.03064v1 [cs.LG])
69. The State of Aerial Surveillance: A Survey. (arXiv:2201.03080v1 [cs.CV])
70. Uncovering the Source of Machine Bias. (arXiv:2201.03092v1 [cs.LG])
71. Preserving Domain Private Representation via Mutual Information Maximization. (arXiv:2201.03102v1 [cs.LG])
72. Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning. (arXiv:2201.03110v1 [cs.CL])
73. Opportunities of Hybrid Model-based Reinforcement Learning for Cell Therapy Manufacturing Process Development and Control. (arXiv:2201.03116v1 [eess.SY])
74. Information-Theoretic Bias Reduction via Causal View of Spurious Correlation. (arXiv:2201.03121v1 [cs.LG])
75. Loss-calibrated expectation propagation for approximate Bayesian decision-making. (arXiv:2201.03128v1 [stat.ML])
76. Systematic biases when using deep neural networks for annotating large catalogs of astronomical images. (arXiv:2201.03131v1 [astro-ph.GA])
77. An Interpretable Federated Learning-based Network Intrusion Detection Framework. (arXiv:2201.03134v1 [cs.CR])
78. Differentially Private Generative Adversarial Networks with Model Inversion. (arXiv:2201.03139v1 [cs.LG])
79. $m^\ast$ of two-dimensional electron gas: a neural canonical transformation study. (arXiv:2201.03156v1 [cond-mat.stat-mech])
80. Collaborative Reflection-Augmented Autoencoder Network for Recommender Systems. (arXiv:2201.03158v1 [cs.IR])
81. FedDTG:Federated Data-Free Knowledge Distillation via Three-Player Generative Adversarial Networks. (arXiv:2201.03169v1 [cs.LG])
82. Communication-Efficient Federated Learning with Acceleration of Global Momentum. (arXiv:2201.03172v1 [cs.LG])
83. Non-Asymptotic Guarantees for Robust Statistical Learning under $(1+\varepsilon)$-th Moment Assumption. (arXiv:2201.03182v1 [stat.ML])
84. An Adaptive Neuro-Fuzzy System with Integrated Feature Selection and Rule Extraction for High-Dimensional Classification Problems. (arXiv:2201.03187v1 [cs.LG])
85. Predictions of Reynolds and Nusselt numbers in turbulent convection using machine-learning models. (arXiv:2201.03200v1 [physics.flu-dyn])
86. Differentiable and Scalable Generative Adversarial Models for Data Imputation. (arXiv:2201.03202v1 [cs.LG])
87. Differentially Private $\ell_1$-norm Linear Regression with Heavy-tailed Data. (arXiv:2201.03204v1 [cs.LG])
88. Noisy Neonatal Chest Sound Separation for High-Quality Heart and Lung Sounds. (arXiv:2201.03211v1 [eess.AS])
89. Fully automatic scoring of handwritten descriptive answers in Japanese language tests. (arXiv:2201.03215v1 [cs.LG])
90. Local Information Assisted Attention-free Decoder for Audio Captioning. (arXiv:2201.03217v1 [cs.SD])
91. Integration of Explainable Artificial Intelligence to Identify Significant Landslide Causal Factors for Extreme Gradient Boosting based Landslide Susceptibility Mapping with Improved Feature Selection. (arXiv:2201.03225v1 [cs.LG])
92. Wind Park Power Prediction: Attention-Based Graph Networks and Deep Learning to Capture Wake Losses. (arXiv:2201.03229v1 [cs.LG])
93. **Swin** Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])
94. Small Object Detection using Deep Learning. (arXiv:2201.03243v1 [cs.CV])
95. GridTuner: Reinvestigate Grid Size Selection for Spatiotemporal Prediction Models [Technical Report]. (arXiv:2201.03244v1 [cs.DB])
96. A Study on Mitigating Hard Boundaries of Decision-Tree-based Uncertainty Estimates for AI Models. (arXiv:2201.03263v1 [cs.LG])
97. IoTGAN: GAN Powered Camouflage Against Machine Learning Based IoT Device Identification. (arXiv:2201.03281v1 [cs.CR])
98. A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study. (arXiv:2201.03291v1 [cs.LG])
99. Avoiding Overfitting: A Survey on Regularization Methods for Convolutional Neural Networks. (arXiv:2201.03299v1 [cs.CV])
100. Comparison of Representation Learning Techniques for Tracking in time resolved 3D Ultrasound. (arXiv:2201.03319v1 [eess.IV])
101. Gait Recognition Based on Deep Learning: A Survey. (arXiv:2201.03323v1 [cs.CV])
102. Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach. (arXiv:2201.03326v1 [cs.LG])
103. Cross-view Self-Supervised Learning on Heterogeneous Graph Neural Network via Bootstrapping. (arXiv:2201.03340v1 [cs.LG])
104. COIN: Counterfactual Image Generation for VQA Interpretation. (arXiv:2201.03342v1 [cs.CV])
105. GBRS: An Unified Model of Pawlak Rough Set and Neighborhood Rough Set. (arXiv:2201.03349v1 [cs.AI])
106. Continual Learning of Long Topic Sequences in Neural Information Retrieval. (arXiv:2201.03356v1 [cs.IR])
107. Morphological Analysis of Japanese Hiragana Sentences using the BI-LSTM CRF Model. (arXiv:2201.03366v1 [cs.CL])
108. Introduction to Multi-Armed Bandits. (arXiv:1904.07272v7 [cs.LG] UPDATED)
109. NeoNav: Improving the Generalization of Visual Navigation via Generating Next Expected Observations. (arXiv:1906.07207v4 [cs.RO] UPDATED)
110. Masked Gradient-Based Causal Structure Learning. (arXiv:1910.08527v3 [cs.LG] UPDATED)
111. Modeling Historical AIS Data For Vessel Path Prediction: A Comprehensive Treatment. (arXiv:2001.01592v3 [cs.AI] UPDATED)
112. A Tutorial on Learning With Bayesian Networks. (arXiv:2002.00269v3 [cs.LG] UPDATED)
113. Surrogate-assisted performance prediction for data-driven knowledge discovery algorithms: application to evolutionary modeling of clinical pathways. (arXiv:2004.01123v2 [cs.LG] UPDATED)
114. Trade-offs between membership privacy & adversarially robust learning. (arXiv:2006.04622v2 [cs.LG] UPDATED)
115. Teaching CNNs to mimic Human Visual Cognitive Process & regularise Texture-Shape bias. (arXiv:2006.14722v2 [cs.CV] UPDATED)
116. Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks. (arXiv:2007.02901v2 [cs.LG] UPDATED)
117. Trade-off on Sim2Real Learning: Real-world Learning Faster than Simulations. (arXiv:2007.10675v4 [cs.RO] UPDATED)
118. Individual Privacy Accounting via a Renyi Filter. (arXiv:2008.11193v4 [cs.CR] UPDATED)
119. GTAdam: Gradient Tracking with Adaptive Momentum for Distributed Online Optimization. (arXiv:2009.01745v2 [math.OC] UPDATED)
120. Neural-PDE: A RNN based neural network for solving time dependent PDEs. (arXiv:2009.03892v3 [math.NA] UPDATED)
121. Applications of Deep Neural Networks. (arXiv:2009.05673v4 [cs.LG] UPDATED)
122. GECKO: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep Learning. (arXiv:2010.00912v3 [cs.CR] UPDATED)
123. Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation. (arXiv:2010.01184v5 [stat.ML] UPDATED)
124. Enhancing Haptic Distinguishability of Surface Materials with Boosting Technique. (arXiv:2010.02002v4 [cs.LG] UPDATED)
125. Astraea: Grammar-based Fairness Testing. (arXiv:2010.02542v5 [cs.SE] UPDATED)
126. Near Optimality of Finite Memory Feedback Policies in Partially Observed Markov Decision Processes. (arXiv:2010.07452v2 [math.OC] UPDATED)
127. A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images. (arXiv:2010.10563v2 [cs.CV] UPDATED)
128. BIGPrior: Towards Decoupling Learned Prior Hallucination and Data Fidelity in Image **Restoration**. (arXiv:2011.01406v3 [cs.CV] UPDATED)
129. Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v3 [cs.CL] UPDATED)
130. Addressing the Lack of Comparability & Testing in CAN Intrusion Detection Research: A Comprehensive Guide to CAN IDS Data & Introduction of the ROAD Dataset. (arXiv:2012.14600v2 [cs.CR] UPDATED)
131. AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles. (arXiv:2101.06549v3 [cs.RO] UPDATED)
132. Structure-preserving Gaussian Process Dynamics. (arXiv:2102.01606v3 [cs.LG] UPDATED)
133. Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement. (arXiv:2102.05185v3 [cs.LG] UPDATED)
134. Online Deterministic Annealing for Classification and Clustering. (arXiv:2102.05836v4 [cs.LG] UPDATED)
135. HAWKS: Evolving Challenging Benchmark Sets for Cluster Analysis. (arXiv:2102.06940v2 [cs.NE] UPDATED)
136. MARINA: Faster Non-Convex Distributed Learning with Compression. (arXiv:2102.07845v3 [cs.LG] UPDATED)
137. Grounded Relational Inference: Domain Knowledge Driven Explainable Autonomous Driving. (arXiv:2102.11905v2 [cs.AI] UPDATED)
138. Tiny Adversarial Mulit-Objective Oneshot Neural Architecture Search. (arXiv:2103.00363v2 [cs.LG] UPDATED)
139. Reframing Neural Networks: Deep Structure in Overcomplete Representations. (arXiv:2103.05804v2 [cs.LG] UPDATED)
140. DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation. (arXiv:2103.11109v5 [cs.LG] UPDATED)
141. Video-Specific Autoencoders for Exploring, Editing and Transmitting Videos. (arXiv:2103.17261v2 [cs.CV] UPDATED)
142. The Logic of Graph Neural Networks. (arXiv:2104.14624v2 [cs.LG] UPDATED)
143. A semigroup method for high dimensional elliptic PDEs and eigenvalue problems based on neural networks. (arXiv:2105.03480v3 [math.NA] UPDATED)
144. Continual Learning via Bit-Level Information Preserving. (arXiv:2105.04444v2 [cs.LG] UPDATED)
145. Understanding occupants' behaviour, engagement, emotion, and comfort indoors with heterogeneous sensors and wearables. (arXiv:2105.06637v2 [cs.HC] UPDATED)
146. Optimal radial basis for density-based atomic representations. (arXiv:2105.08717v2 [stat.ML] UPDATED)
147. A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow for Commercial-Scale Geologic Carbon Storage. (arXiv:2105.09468v2 [physics.geo-ph] UPDATED)
148. Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v3 [cs.CV] UPDATED)
149. Autonomous Kinetic Modeling of Biomass Pyrolysis using Chemical Reaction Neural Networks. (arXiv:2105.11397v2 [physics.chem-ph] UPDATED)
150. Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Law. (arXiv:2105.11627v3 [math.NA] UPDATED)
151. GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v3 [cs.LG] UPDATED)
152. Enhanced Doubly Robust Learning for Debiasing Post-click Conversion Rate Estimation. (arXiv:2105.13623v3 [cs.LG] UPDATED)
153. GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v6 [cs.LG] UPDATED)
154. SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v8 [math.OC] UPDATED)
155. Lifted Model Checking for Relational MDPs. (arXiv:2106.11735v2 [cs.LG] UPDATED)
156. Action Transformer: A Self-Attention Model for Short-Time Pose-Based Human Action Recognition. (arXiv:2107.00606v6 [cs.CV] UPDATED)
157. CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v3 [cs.CV] UPDATED)
158. A Survey of Uncertainty in Deep Neural Networks. (arXiv:2107.03342v2 [cs.LG] UPDATED)
159. Cluster Regularization via a Hierarchical Feature Regression. (arXiv:2107.04831v2 [stat.ML] UPDATED)
160. Project Achoo: A Practical Model and Application for COVID-19 Detection from Recordings of Breath, Voice, and Cough. (arXiv:2107.10716v2 [eess.SP] UPDATED)
161. Communication-Efficient Federated Learning via Predictive Coding. (arXiv:2108.00918v2 [cs.DC] UPDATED)
162. Retiring Adult: New Datasets for Fair Machine Learning. (arXiv:2108.04884v3 [cs.LG] UPDATED)
163. Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning. (arXiv:2108.12988v2 [cs.LG] UPDATED)
164. Topographic VAEs learn Equivariant Capsules. (arXiv:2109.01394v2 [cs.LG] UPDATED)
165. Refinement of Hottopixx Method for Nonnegative Matrix Factorization Under Noisy Separability. (arXiv:2109.02863v2 [cs.LG] UPDATED)
166. Improving Fairness for Data Valuation in Federated Learning. (arXiv:2109.09046v2 [cs.LG] UPDATED)
167. Toward a Fairness-Aware Scoring System for Algorithmic Decision-Making. (arXiv:2109.10053v3 [cs.LG] UPDATED)
168. Comparing Sequential Forecasters. (arXiv:2110.00115v3 [stat.ME] UPDATED)
169. Attention is All You Need? Good Embeddings with Statistics are enough:Large Scale Audio Understanding without Transformers/ Convolutions/ BERTs/ Mixers/ Attention/ RNNs or ..... (arXiv:2110.03183v3 [cs.SD] UPDATED)
170. Unsupervised Image Decomposition with Phase-Correlation Networks. (arXiv:2110.03473v3 [cs.CV] UPDATED)
171. On the Double Descent of Random Features Models Trained with SGD. (arXiv:2110.06910v4 [stat.ML] UPDATED)
172. Modeling and Analysis of Intermittent Federated Learning Over Cellular-Connected UAV Networks. (arXiv:2110.07077v2 [cs.LG] UPDATED)
173. Stability Analysis of Unfolded WMMSE for Power Allocation. (arXiv:2110.07471v2 [eess.SP] UPDATED)
174. Federated learning and next generation wireless communications: A survey on bidirectional relationship. (arXiv:2110.07649v2 [eess.SP] UPDATED)
175. Distinguishing Natural and Computer-Generated Images using Multi-Colorspace fused EfficientNet. (arXiv:2110.09428v2 [cs.CV] UPDATED)
176. AutoDEUQ: Automated Deep Ensemble with Uncertainty Quantification. (arXiv:2110.13511v2 [cs.LG] UPDATED)
177. Bridging the gap to real-world for network intrusion detection systems with data-centric approach. (arXiv:2110.13655v2 [cs.CR] UPDATED)
178. Learning to Simulate Self-Driven Particles System with Coordinated Policy Optimization. (arXiv:2110.13827v2 [cs.LG] UPDATED)
179. Video-based fully automatic assessment of open surgery suturing skills. (arXiv:2110.13972v2 [cs.CV] UPDATED)
180. Sustainable AI: Environmental Implications, Challenges and Opportunities. (arXiv:2111.00364v2 [cs.LG] UPDATED)
181. Logically Sound Arguments for the Effectiveness of ML Safety Measures. (arXiv:2111.02649v2 [cs.LO] UPDATED)
182. Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models. (arXiv:2111.02840v2 [cs.CL] UPDATED)
183. Understanding Layer-wise Contributions in Deep Neural Networks through Spectral Analysis. (arXiv:2111.03972v3 [cs.LG] UPDATED)
184. Reducing the Long Tail Losses in Scientific Emulations with Active Learning. (arXiv:2111.08498v2 [cs.LG] UPDATED)
185. Traversing the Local Polytopes of ReLU Neural Networks: A Unified Approach for Network Verification. (arXiv:2111.08922v2 [cs.LG] UPDATED)
186. Hierarchical Graph-Convolutional Variational AutoEncoding for Generative Modelling of Human Motion. (arXiv:2111.12602v3 [cs.CV] UPDATED)
187. L\'evy Induced Stochastic Differential Equation Equipped with Neural Network for Time Series Forecasting. (arXiv:2111.13164v2 [cs.LG] UPDATED)
188. Federated Deep Learning in Electricity Forecasting: An MCDM Approach. (arXiv:2111.13834v3 [math.OC] UPDATED)
189. Enabling Fast Deep Learning on Tiny Energy-Harvesting IoT Devices. (arXiv:2111.14051v2 [cs.LG] UPDATED)
190. DiffSDFSim: Differentiable Rigid-Body Dynamics With Implicit Shapes. (arXiv:2111.15318v2 [cs.CV] UPDATED)
191. Studying Hadronization by Machine Learning Techniques. (arXiv:2111.15655v2 [hep-ph] UPDATED)
192. Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data. (arXiv:2112.00054v2 [cs.CV] UPDATED)
193. Towards Intrinsic Interactive Reinforcement Learning. (arXiv:2112.01575v2 [cs.AI] UPDATED)
194. Specializing Versatile Skill Libraries using Local Mixture of Experts. (arXiv:2112.04216v2 [cs.LG] UPDATED)
195. Self-supervised Spatiotemporal Representation Learning by Exploiting Video Continuity. (arXiv:2112.05883v2 [cs.CV] UPDATED)
196. AGMI: Attention-Guided Multi-omics Integration for Drug Response Prediction with Graph Neural Networks. (arXiv:2112.08366v2 [q-bio.GN] UPDATED)
197. Discretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem. (arXiv:2112.09382v2 [cs.SD] UPDATED)
198. AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms. (arXiv:2112.13366v2 [eess.AS] UPDATED)
199. A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v2 [cs.LG] UPDATED)
200. ViNMT: Neural Machine Translation Toolkit. (arXiv:2112.15272v2 [cs.CL] UPDATED)
201. Distributed Random Reshuffling over Networks. (arXiv:2112.15287v2 [math.OC] UPDATED)
202. Improving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI. (arXiv:2201.00009v2 [cs.LG] UPDATED)
203. Self-directed Machine Learning. (arXiv:2201.01289v2 [cs.LG] UPDATED)
204. Learning Differentiable Safety-Critical Control using Control Barrier Functions for Generalization to Novel Environments. (arXiv:2201.01347v2 [eess.SY] UPDATED)
205. Linear Variational State Space Filtering. (arXiv:2201.01353v2 [cs.LG] UPDATED)
206. Understanding Entropy Coding With Asymmetric Numeral Systems (ANS): a Statistician's Perspective. (arXiv:2201.01741v2 [stat.ML] UPDATED)
207. SABLAS: Learning Safe Control for Black-box Dynamical Systems. (arXiv:2201.01918v2 [cs.LG] UPDATED)
208. Time Series Forecasting Using Fuzzy Cognitive Maps: A Survey. (arXiv:2201.02297v2 [cs.AI] UPDATED)
209. Visual Attention Prediction Improves Performance of Autonomous Drone Racing Agents. (arXiv:2201.02569v2 [cs.RO] UPDATED)
## cs.AI
---
**101** new papers in cs.AI:-) 
1. Posture Prediction for Healthy Sitting using a Smart Chair. (arXiv:2201.02615v1 [cs.LG])
2. Spatio-Temporal Graph Representation Learning for Fraudster Group Detection. (arXiv:2201.02621v1 [cs.LG])
3. Attention Option-Critic. (arXiv:2201.02628v1 [cs.LG])
4. United adversarial learning for liver tumor segmentation and detection of multi-modality non-contrast MRI. (arXiv:2201.02629v1 [eess.IV])
5. Stay Positive: Knowledge Graph Embedding Without Negative Sampling. (arXiv:2201.02661v1 [cs.LG])
6. Computational Lens on Cognition: Study Of Autobiographical Versus Imagined Stories With Large-Scale Language Models. (arXiv:2201.02662v1 [cs.CL])
7. Improved Input Reprogramming for GAN Conditioning. (arXiv:2201.02692v1 [cs.LG])
8. Tisane: Authoring Statistical Models via Formal Reasoning from Conceptual and Data Relationships. (arXiv:2201.02705v1 [cs.AI])
9. C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v1 [cs.CL])
10. Building Human-like Communicative Intelligence: A Grounded Perspective. (arXiv:2201.02734v1 [cs.CL])
11. Best of Both Worlds: A Hybrid Approach for Multi-Hop Explanation with Declarative Facts. (arXiv:2201.02740v1 [cs.CL])
12. DeHIN: A Decentralized Framework for Embedding Large-scale Heterogeneous Information Networks. (arXiv:2201.02757v1 [cs.LG])
13. Modeling Human-AI Team Decision Making. (arXiv:2201.02759v1 [cs.HC])
14. Global Convergence Analysis of Deep Linear Networks with A One-neuron Layer. (arXiv:2201.02761v1 [cs.LG])
15. A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])
16. A Fair and Efficient Hybrid Federated Learning Framework based on XGBoost for Distributed Power Prediction. (arXiv:2201.02783v1 [cs.LG])
17. Scaling Knowledge Graph Embedding Models. (arXiv:2201.02791v1 [cs.LG])
18. Clustering Text Using Attention. (arXiv:2201.02816v1 [cs.CL])
19. AnomMAN: Detect Anomaly on Multi-view Attributed Networks. (arXiv:2201.02822v1 [cs.SI])
20. Self-aligned Spatial Feature Extraction Network for UAV Vehicle Re-identification. (arXiv:2201.02836v1 [cs.CV])
21. Fake Hilsa Fish Detection Using Machine Vision. (arXiv:2201.02853v1 [cs.CV])
22. PocketNN: Integer-only Training and Inference of Neural Networks via Direct Feedback Alignment and Pocket Activations in Pure C++. (arXiv:2201.02863v1 [cs.LG])
23. LoMar: A Local Defense Against Poisoning Attack on Federated Learning. (arXiv:2201.02873v1 [cs.LG])
24. Assessing Policy, Loss and Planning Combinations in Reinforcement Learning using a New Modular Architecture. (arXiv:2201.02874v1 [cs.LG])
25. Attention-based Random Forest and Contamination Model. (arXiv:2201.02880v1 [cs.LG])
26. A Multi-agent Reinforcement Learning Approach for Efficient Client Selection in Federated Learning. (arXiv:2201.02932v1 [cs.LG])
27. Weak Supervision for Affordable Modeling of Electrocardiogram Data. (arXiv:2201.02936v1 [eess.SP])
28. Fast solver for J2-perturbed Lambert problem using deep neural network. (arXiv:2201.02942v1 [math.NA])
29. Resolving Camera Position for a Practical Application of Gaze Estimation on Edge Devices. (arXiv:2201.02946v1 [cs.CV])
30. Arguments about Highly Reliable Agent Designs as a Useful Path to Artificial Intelligence Safety. (arXiv:2201.02950v1 [cs.AI])
31. An Adaptive Device-Edge Co-Inference Framework Based on Soft Actor-Critic. (arXiv:2201.02968v1 [cs.LG])
32. Privacy-aware Early Detection of COVID-19 through Adversarial Training. (arXiv:2201.03004v1 [cs.LG])
33. Glance and Focus Networks for Dynamic Visual Recognition. (arXiv:2201.03014v1 [cs.CV])
34. Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay. (arXiv:2201.03019v1 [cs.LG])
35. Meta-Generalization for Multiparty Privacy Learning to Identify Anomaly Multimedia Traffic in Graynet. (arXiv:2201.03027v1 [cs.CR])
36. The State of Aerial Surveillance: A Survey. (arXiv:2201.03080v1 [cs.CV])
37. Semantic and sentiment analysis of selected Bhagavad Gita translations using BERT-based language framework. (arXiv:2201.03115v1 [cs.CL])
38. Information-Theoretic Bias Reduction via Causal View of Spurious Correlation. (arXiv:2201.03121v1 [cs.LG])
39. Supervised Contrastive Learning for Recommendation. (arXiv:2201.03144v1 [cs.IR])
40. Collaborative Reflection-Augmented Autoencoder Network for Recommender Systems. (arXiv:2201.03158v1 [cs.IR])
41. Communication-Efficient Federated Learning with Acceleration of Global Momentum. (arXiv:2201.03172v1 [cs.LG])
42. A Simulation Platform for Multi-tenant Machine Learning Services on Thousands of GPUs. (arXiv:2201.03175v1 [cs.DC])
43. An Adaptive Neuro-Fuzzy System with Integrated Feature Selection and Rule Extraction for High-Dimensional Classification Problems. (arXiv:2201.03187v1 [cs.LG])
44. Task planning and explanation with virtual actions. (arXiv:2201.03199v1 [cs.RO])
45. Wind Park Power Prediction: Attention-Based Graph Networks and Deep Learning to Capture Wake Losses. (arXiv:2201.03229v1 [cs.LG])
46. **Swin** Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])
47. A Study on Mitigating Hard Boundaries of Decision-Tree-based Uncertainty Estimates for AI Models. (arXiv:2201.03263v1 [cs.LG])
48. An application of the splitting-up method for the computation of a neural network representation for the solution for the filtering equations. (arXiv:2201.03283v1 [math.PR])
49. Cross-Modal ASR Post-Processing System for Error Correction and Utterance Rejection. (arXiv:2201.03313v1 [eess.AS])
50. Cross-view Self-Supervised Learning on Heterogeneous Graph Neural Network via Bootstrapping. (arXiv:2201.03340v1 [cs.LG])
51. GBRS: An Unified Model of Pawlak Rough Set and Neighborhood Rough Set. (arXiv:2201.03349v1 [cs.AI])
52. Introduction to Multi-Armed Bandits. (arXiv:1904.07272v7 [cs.LG] UPDATED)
53. Modeling Historical AIS Data For Vessel Path Prediction: A Comprehensive Treatment. (arXiv:2001.01592v3 [cs.AI] UPDATED)
54. A Tutorial on Learning With Bayesian Networks. (arXiv:2002.00269v3 [cs.LG] UPDATED)
55. Categorical Stochastic Processes and Likelihood. (arXiv:2005.04735v5 [cs.AI] UPDATED)
56. Teaching CNNs to mimic Human Visual Cognitive Process & regularise Texture-Shape bias. (arXiv:2006.14722v2 [cs.CV] UPDATED)
57. Trade-off on Sim2Real Learning: Real-world Learning Faster than Simulations. (arXiv:2007.10675v4 [cs.RO] UPDATED)
58. Applications of Deep Neural Networks. (arXiv:2009.05673v4 [cs.LG] UPDATED)
59. Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation. (arXiv:2010.01184v5 [stat.ML] UPDATED)
60. Astraea: Grammar-based Fairness Testing. (arXiv:2010.02542v5 [cs.SE] UPDATED)
61. A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images. (arXiv:2010.10563v2 [cs.CV] UPDATED)
62. AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles. (arXiv:2101.06549v3 [cs.RO] UPDATED)
63. Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement. (arXiv:2102.05185v3 [cs.LG] UPDATED)
64. Grounded Relational Inference: Domain Knowledge Driven Explainable Autonomous Driving. (arXiv:2102.11905v2 [cs.AI] UPDATED)
65. Tiny Adversarial Mulit-Objective Oneshot Neural Architecture Search. (arXiv:2103.00363v2 [cs.LG] UPDATED)
66. A Whole Brain Probabilistic Generative Model: Toward Realizing Cognitive Architectures for Developmental Robots. (arXiv:2103.08183v2 [cs.AI] UPDATED)
67. Human Schema Curation via Causal Association Rule Mining. (arXiv:2104.08811v2 [cs.CL] UPDATED)
68. The Logic of Graph Neural Networks. (arXiv:2104.14624v2 [cs.LG] UPDATED)
69. Genetic Algorithms For Extractive Summarization. (arXiv:2105.02365v2 [cs.CL] UPDATED)
70. Transitioning to human interaction with AI systems: New challenges and opportunities for HCI professionals to enable human-centered AI. (arXiv:2105.05424v3 [cs.HC] UPDATED)
71. Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v3 [cs.CV] UPDATED)
72. GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v3 [cs.LG] UPDATED)
73. Emergent Communication of Generalizations. (arXiv:2106.02668v2 [cs.CL] UPDATED)
74. GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v6 [cs.LG] UPDATED)
75. Lifted Model Checking for Relational MDPs. (arXiv:2106.11735v2 [cs.LG] UPDATED)
76. Plinko: Eliciting beliefs to build better models of statistical learning and mental model updating. (arXiv:2107.11477v2 [q-bio.NC] UPDATED)
77. Communication-Efficient Federated Learning via Predictive Coding. (arXiv:2108.00918v2 [cs.DC] UPDATED)
78. PVT: Point-Voxel Transformer for Point Cloud Learning. (arXiv:2108.06076v3 [cs.CV] UPDATED)
79. mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v4 [cs.CL] UPDATED)
80. Topographic VAEs learn Equivariant Capsules. (arXiv:2109.01394v2 [cs.LG] UPDATED)
81. Hierarchical Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v4 [cs.CV] UPDATED)
82. Attention is All You Need? Good Embeddings with Statistics are enough:Large Scale Audio Understanding without Transformers/ Convolutions/ BERTs/ Mixers/ Attention/ RNNs or ..... (arXiv:2110.03183v3 [cs.SD] UPDATED)
83. Rebuilding Trust: Queer in AI Approach to Artificial Intelligence Risk Management. (arXiv:2110.09271v2 [cs.CY] UPDATED)
84. Bridging the gap to real-world for network intrusion detection systems with data-centric approach. (arXiv:2110.13655v2 [cs.CR] UPDATED)
85. Sustainable AI: Environmental Implications, Challenges and Opportunities. (arXiv:2111.00364v2 [cs.LG] UPDATED)
86. User Centered Design (VI): Human Factors Approaches for Intelligent Human-Computer Interaction. (arXiv:2111.04880v2 [cs.HC] UPDATED)
87. Data-Driven AI Model Signal-Awareness **Enhancement** and Introspection. (arXiv:2111.05827v2 [cs.SE] UPDATED)
88. Hierarchical Graph-Convolutional Variational AutoEncoding for Generative Modelling of Human Motion. (arXiv:2111.12602v3 [cs.CV] UPDATED)
89. Machines and Influence. (arXiv:2111.13365v3 [cs.CY] UPDATED)
90. Federated Deep Learning in Electricity Forecasting: An MCDM Approach. (arXiv:2111.13834v3 [math.OC] UPDATED)
91. Variational Autoencoders for Precoding Matrices with High Spectral Efficiency. (arXiv:2111.15626v3 [eess.SP] UPDATED)
92. Towards Intrinsic Interactive Reinforcement Learning. (arXiv:2112.01575v2 [cs.AI] UPDATED)
93. Semantic Answer Type and Relation Prediction Task (SMART 2021). (arXiv:2112.07606v2 [cs.CL] UPDATED)
94. AGMI: Attention-Guided Multi-omics Integration for Drug Response Prediction with Graph Neural Networks. (arXiv:2112.08366v2 [q-bio.GN] UPDATED)
95. From Procedures, Objects, Actors, Components, Services, to Agents -- A Comparative Analysis of the History and Evolution of Programming Abstractions. (arXiv:2112.12508v3 [cs.SE] UPDATED)
96. Improving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI. (arXiv:2201.00009v2 [cs.LG] UPDATED)
97. D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation. (arXiv:2201.00462v2 [cs.CV] UPDATED)
98. Self-directed Machine Learning. (arXiv:2201.01289v2 [cs.LG] UPDATED)
99. SABLAS: Learning Safe Control for Black-box Dynamical Systems. (arXiv:2201.01918v2 [cs.LG] UPDATED)
100. Time Series Forecasting Using Fuzzy Cognitive Maps: A Survey. (arXiv:2201.02297v2 [cs.AI] UPDATED)
101. Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v2 [eess.IV] UPDATED)
