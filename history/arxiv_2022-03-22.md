# Your interest papers
---
## cs.CV
---
### Adversarial Attacks on Deep Learning-based Video Compression and Classification Systems. (arXiv:2203.10183v1 [cs.CV])
- Authors : Woo Chang, Mojan Javaheripi, Seira Hidano, Farinaz Koushanfar
- Link : [http://arxiv.org/abs/2203.10183](http://arxiv.org/abs/2203.10183)
> ABSTRACT  :  Video compression plays a crucial role in enabling video streaming and classification systems and maximizing the end-user quality of experience (QoE) at a given bandwidth budget. In this paper, we conduct the first systematic study for adversarial attacks on deep learning based video compression and downstream classification systems. We propose an adaptive adversarial attack that can manipulate the Rate-Distortion (R-D) relationship of a video compression model to achieve two adversarial goals: (1) increasing the network bandwidth or (2) degrading the video quality for end-users. We further devise novel objectives for targeted and untargeted attacks to a downstream video classification service. Finally, we design an input-invariant perturbation that universally disrupts video compression and classification systems in **real time**. Unlike previously proposed attacks on video classification, our adversarial perturbations are the first to withstand compression. We empirically show the resilience of our attacks against various defenses, i.e., adversarial training, video denoising, and JPEG compression. Our extensive experimental results on various video datasets demonstrate the effectiveness of our attacks. Our video quality and bandwidth attacks deteriorate peak signal-to-noise ratio by up to 5.4dB and the bit-rate by up to 2.4 times on the standard video compression datasets while achieving over 90% attack success rate on a downstream classifier.  
### **Swin**TextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition. (arXiv:2203.10209v1 [cs.CV])
- Authors : Mingxin Huang, Yuliang Liu, Zhenghao Peng, Chongyu Liu, Dahua Lin, Shenggao Zhu, Nicholas Yuan, Kai Ding, Lianwen Jin
- Link : [http://arxiv.org/abs/2203.10209](http://arxiv.org/abs/2203.10209)
> ABSTRACT  :  End-to-end scene text spotting has attracted great attention in recent years due to the success of excavating the intrinsic synergy of the scene text detection and recognition. However, recent state-of-the-art methods usually incorporate detection and recognition simply by sharing the backbone, which does not directly take advantage of the feature interaction between the two tasks. In this paper, we propose a new end-to-end scene text spotting framework termed **Swin**TextSpotter. Using a transformer encoder with dynamic head as the detector, we unify the two tasks with a novel Recognition Conversion mechanism to explicitly guide text localization through recognition loss. The straightforward design results in a concise framework that requires neither additional rectification module nor character-level annotation for the arbitrarily-shaped text. Qualitative and quantitative experiments on multi-oriented datasets RoIC13 and ICDAR 2015, arbitrarily-shaped datasets Total-Text and CTW1500, and multi-lingual datasets ReCTS (Chinese) and VinText (Vietnamese) demonstrate **Swin**TextSpotter significantly outperforms existing methods. Code is available at https://github.com/mxin262/**Swin**TextSpotter.  
### Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds. (arXiv:2203.10314v1 [cs.CV])
- Authors : Chenhang He, Ruihuang Li, Shuai Li, **Lei Zhang**
- Link : [http://arxiv.org/abs/2203.10314](http://arxiv.org/abs/2203.10314)
> ABSTRACT  :  Transformer has demonstrated promising performance in many 2D vision tasks. However, it is cumbersome to compute the self-attention on large-scale point cloud data because point cloud is a long sequence and unevenly distributed in 3D space. To solve this issue, existing methods usually compute self-attention locally by grouping the points into clusters of the same size, or perform convolutional self-attention on a discretized representation. However, the former results in stochastic point dropout, while the latter typically has narrow attention fields. In this paper, we propose a novel voxel-based architecture, namely Voxel Set Transformer (VoxSeT), to detect 3D objects from point clouds by means of set-to-set translation. VoxSeT is built upon a voxel-based set attention (VSA) module, which reduces the self-attention in each voxel by two cross-attentions and models features in a hidden space induced by a group of latent codes. With the VSA module, VoxSeT can manage voxelized point clusters with arbitrary size in a wide range, and process them in parallel with linear complexity. The proposed VoxSeT integrates the high performance of transformer with the efficiency of voxel-based model, which can be used as a good alternative to the convolutional and point-based backbones. VoxSeT reports competitive results on the KITTI and Waymo detection benchmarks. The source codes can be found at \url{https://github.com/skyhehe123/VoxSeT}.  
### simCrossTrans: A Simple Cross-Modality Transfer Learning for Object Detection with ConvNets or Vision Transformers. (arXiv:2203.10456v1 [cs.CV])
- Authors : Xiaoke Shen, Ioannis Stamos
- Link : [http://arxiv.org/abs/2203.10456](http://arxiv.org/abs/2203.10456)
> ABSTRACT  :  Transfer learning is widely used in computer vision (CV), natural language processing (NLP) and achieves great success. Most transfer learning systems are based on the same modality (e.g. RGB image in CV and text in NLP). However, the cross-modality transfer learning (CMTL) systems are scarce. In this work, we study CMTL from 2D to 3D sensor to explore the upper bound performance of 3D sensor only systems, which play critical roles in robotic navigation and perform well in **low light** scenarios. While most CMTL pipelines from 2D to 3D vision are complicated and based on Convolutional Neural Networks (ConvNets), ours is easy to implement, expand and based on both ConvNets and Vision transformers(ViTs): 1) By converting point clouds to pseudo-images, we can use an almost identical network from pre-trained models based on 2D images. This makes our system easy to implement and expand. 2) Recently ViTs have been showing good performance and robustness to occlusions, one of the key reasons for poor performance of 3D vision systems. We explored both ViT and ConvNet with similar model sizes to investigate the performance difference. We name our approach simCrossTrans: simple cross-modality transfer learning with ConvNets or ViTs. Experiments on SUN RGB-D dataset show: with simCrossTrans we achieve $13.2\%$ and $16.1\%$ absolute performance gain based on ConvNets and ViTs separately. We also observed the ViTs based performs $9.7\%$ better than the ConvNets one, showing the power of simCrossTrans with ViT. simCrossTrans with ViTs surpasses the previous state-of-the-art (SOTA) by a large margin of $+15.4\%$ mAP50. Compared with the previous 2D detection SOTA based RGB images, our depth image only system only has a $1\%$ gap. The code, training/inference logs and models are publicly available at https://github.com/liketheflower/simCrossTrans  
### Unsupervised Domain Adaptation for **Night**time Aerial Tracking. (arXiv:2203.10541v1 [cs.CV])
- Authors : Junjie Ye, Changhong Fu, Guangze Zheng, Danda Pani, Guang Chen
- Link : [http://arxiv.org/abs/2203.10541](http://arxiv.org/abs/2203.10541)
> ABSTRACT  :  Previous advances in object tracking mostly reported on favorable illumination circumstances while neglecting performance at **night**time, which significantly impeded the development of related aerial robot applications. This work instead develops a novel unsupervised domain adaptation framework for **night**time aerial tracking (named UDAT). Specifically, a unique object discovery approach is provided to generate training patches from raw **night**time tracking videos. To tackle the domain discrepancy, we employ a Transformer-based bridging layer post to the feature extractor to align image features from both domains. With a Transformer day/**night** feature discriminator, the daytime tracking model is adversarially trained to track at **night**. Moreover, we construct a pioneering benchmark namely NAT2021 for unsupervised domain adaptive **night**time tracking, which comprises a test set of 180 manually annotated tracking sequences and a train set of over 276k unlabelled **night**time tracking frames. Exhaustive experiments demonstrate the robustness and domain adaptability of the proposed framework in **night**time aerial tracking. The code and benchmark are available at https://github.com/vision4robotics/UDAT.  
### CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v1 [cs.CV])
- Authors : Matheus Souza, Wolfgang Heidrich
- Link : [http://arxiv.org/abs/2203.10562](http://arxiv.org/abs/2203.10562)
> ABSTRACT  :  Image signal processors (ISPs) are historically grown legacy software systems for reconstructing color images from noisy raw sensor measurements. They are usually composited of many heuristic blocks for denoising, demosaicking, and color **restoration**. Color reproduction in this context is of particular importance, since the raw colors are often severely distorted, and each smart phone manufacturer has developed their own characteristic heuristics for improving the color rendition, for example of skin tones and other visually important colors.    In recent years there has been strong interest in replacing the historically grown ISP systems with deep learned pipelines. Much progress has been made in approximating legacy ISPs with such learned models. However, so far the focus of these efforts has been on reproducing the structural features of the images, with less attention paid to color rendition.    Here we present CRISPnet, the first learned ISP model to specifically target color rendition accuracy relative to a complex, legacy smart phone ISP. We achieve this by utilizing both image metadata (like a legacy ISP would), as well as by learning simple global semantics based on image classification -- similar to what a legacy ISP does to determine the scene type. We also contribute a new ISP image dataset consisting of both **high dynamic range** monitor data, as well as real-world data, both captured with an actual cell phone ISP pipeline under a variety of lighting conditions, **exposure** times, and gain settings.  
### **Real-time** Semantic Segmentation via Spatial-detail Guided Context Propagation. (arXiv:2005.11034v5 [cs.CV] UPDATED)
- Authors : Shijie Hao, Yuan Zhou, Yanrong Guo, Richang Hong, Jun Cheng, Meng Wang
- Link : [http://arxiv.org/abs/2005.11034](http://arxiv.org/abs/2005.11034)
> ABSTRACT  :  Nowadays, vision-based computing tasks play an important role in various real-world applications. However, many vision computing tasks, e.g. semantic segmentation, are usually computationally expensive, posing a challenge to the computing systems that are resource-constrained but require fast response speed. Therefore, it is valuable to develop accurate and real-time vision processing models that only require limited computational resources. To this end, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet) for achieving real-time semantic segmentation. In SGCPNet, we propose the strategy of spatial-detail guided context propagation. It uses the spatial details of shallow layers to guide the propagation of the low-resolution global contexts, in which the lost spatial information can be effectively reconstructed. In this way, the need for maintaining high-resolution features along the network is freed, therefore largely improving the model efficiency. On the other hand, due to the effective reconstruction of spatial details, the segmentation accuracy can be still preserved. In the experiments, we validate the effectiveness and efficiency of the proposed SGCPNet model. On the Citysacpes dataset, for example, our SGCPNet achieves 69.5% mIoU segmentation accuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX 1080 Ti GPU card. In addition, SGCPNet is very lightweight and only contains 0.61 M parameters.  
### SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v4 [cs.CV] UPDATED)
- Authors : Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence Staib
- Link : [http://arxiv.org/abs/2108.06227](http://arxiv.org/abs/2108.06227)
> ABSTRACT  :  Automated segmentation in medical image analysis is a challenging task that requires a large amount of manually labeled data. However, most existing learning-based approaches usually suffer from limited manually annotated medical data, which poses a major practical problem for accurate and robust medical image segmentation. In addition, most existing semi-supervised approaches are usually not robust compared with the supervised counterparts, and also lack explicit modeling of geometric structure and semantic information, both of which limit the segmentation accuracy. In this work, we present SimCVD, a simple contrastive distillation framework that significantly advances state-of-the-art voxel-wise representation learning. We first describe an unsupervised training strategy, which takes two views of an input volume and predicts their signed distance maps of object boundaries in a contrastive objective, with only two independent dropout as mask. This simple approach works surprisingly well, performing on the same level as previous fully supervised methods with much less labeled data. We hypothesize that dropout can be viewed as a minimal form of data augmentation and makes the network robust to representation collapse. Then, we propose to perform structural distillation by distilling pair-wise similarities. We evaluate SimCVD on two popular datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT dataset. The results on the LA dataset demonstrate that, in two types of labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of 90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to previous best results. Our method can be trained in an end-to-end fashion, showing the promise of utilizing SimCVD as a general framework for downstream tasks, such as medical image synthesis, **enhancement**, and registration.  
### Color Mapping Functions For **HDR** Panorama Imaging: Weighted Histogram Averaging. (arXiv:2111.07283v2 [cs.CV] UPDATED)
- Authors : Yilun Xu, Zhengguo Li, Weihai Chen, Changyun Wen
- Link : [http://arxiv.org/abs/2111.07283](http://arxiv.org/abs/2111.07283)
> ABSTRACT  :  It is challenging to stitch multiple images with different **exposure**s due to possible color distortion and loss of details in the brightest and **dark**est regions of input images. In this paper, a novel color mapping algorithm is first proposed by introducing a new concept of weighted histogram averaging (WHA). The proposed WHA algorithm leverages the correspondence between the histogram bins of two images which are built up by using the non-decreasing property of the color mapping functions (CMFs). The WHA algorithm is then adopted to synthesize a set of differently exposed panorama images. The intermediate panorama images are finally fused via a state-of-the-art multi-scale **exposure** fusion (MEF) algorithm to produce the final panorama image. Extensive experiments indicate that the proposed WHA algorithm significantly surpasses the related state-of-the-art color mapping methods. The proposed **high dynamic range** (**HDR**) stitching algorithm based on MEF also preserves details in the brightest and **dark**est regions of the input images well. The related materials will be publicly accessible at https://github.com/yilun-xu/WHA for reproducible research.  
### Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction. (arXiv:2111.07910v2 [eess.IV] UPDATED)
- Authors : Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2111.07910](http://arxiv.org/abs/2111.07910)
> ABSTRACT  :  Hyperspectral image (HSI) reconstruction aims to recover the 3D spatial-spectral signal from a 2D measurement in the coded aperture snapshot spectral imaging (CASSI) system. The HSI representations are highly similar and correlated across the spectral dimension. Modeling the inter-spectra interactions is beneficial for HSI reconstruction. However, existing CNN-based methods show limitations in capturing spectral-wise similarity and long-range dependencies. Besides, the HSI information is modulated by a coded aperture (physical mask) in CASSI. Nonetheless, current algorithms have not fully explored the guidance effect of the mask for HSI **restoration**. In this paper, we propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI reconstruction. Specifically, we present a Spectral-wise Multi-head Self-Attention (S-MSA) that treats each spectral feature as a token and calculates self-attention along the spectral dimension. In addition, we customize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to spatial regions with high-fidelity spectral representations. Extensive experiments show that our MST significantly outperforms state-of-the-art (SOTA) methods on simulation and real HSI datasets while requiring dramatically cheaper computational and memory costs. Code and pre-trained models are available at https://github.com/caiyuanhao1998/MST/  
### Confidence Propagation Cluster: Unleash Full Potential of Object Detectors. (arXiv:2112.00342v4 [cs.CV] UPDATED)
- Authors : Yichun Shen, Wanli Jiang, Zhen Xu, Rundong Li, Junghyun Kwon, Siyi Li
- Link : [http://arxiv.org/abs/2112.00342](http://arxiv.org/abs/2112.00342)
> ABSTRACT  :  It has been a long history that most object detection methods obtain objects by using the non-maximum suppression (NMS) and its improved versions like Soft-NMS to remove redundant bounding boxes. We challenge those NMS-based methods from three aspects: 1) The bounding box with highest confidence value may not be the true positive having the biggest overlap with the ground-truth box. 2) Not only suppression is required for redundant boxes, but also confidence **enhancement** is needed for those true positives. 3) Sorting candidate boxes by confidence values is not necessary so that full parallelism is achievable.    In this paper, inspired by belief propagation (BP), we propose the Confidence Propagation Cluster (CP-Cluster) to replace NMS-based methods, which is fully parallelizable as well as better in accuracy. In CP-Cluster, we borrow the message passing mechanism from BP to penalize redundant boxes and enhance true positives simultaneously in an iterative way until convergence. We verified the effectiveness of CP-Cluster by applying it to various mainstream detectors such as FasterRCNN, SSD, FCOS, YOLOv3, YOLOv5, Centernet etc. Experiments on MS COCO show that our plug and play method, without retraining detectors, is able to steadily improve average mAP of all those state-of-the-art models with a clear margin from 0.3 to 1.9 respectively when compared with NMS-based methods.  
### Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v2 [cs.CV] UPDATED)
- Authors : Zipeng Qin, Jianbo Liu, Xiaolin Zhang, Maoqing Tian, Aojun Zhou, Shuai Yi, Hongsheng Li
- Link : [http://arxiv.org/abs/2201.04019](http://arxiv.org/abs/2201.04019)
> ABSTRACT  :  The recently proposed MaskFormer gives a refreshed perspective on the task of semantic segmentation: it shifts from the popular pixel-level classification paradigm to a mask-level classification method. In essence, it generates paired probabilities and masks corresponding to category segments and combines them during inference for the segmentation maps. In our study, we find that per-mask classification decoder on top of a single-scale feature is not effective enough to extract reliable probability or mask. To mine for rich semantic information across the feature pyramid, we propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask approach semantic segmentation with multi-scale features. The proposed transformer decoder performs cross-attention between the learnable queries and each spatial feature from the feature pyramid in parallel and uses cross-scale inter-query attention to exchange complimentary information. We achieve competitive performance on three widely used semantic segmentation datasets. In particular, on ADE20K validation set, our result with **Swin**-B backbone surpasses that of MaskFormer's with a much larger **Swin**-L backbone in both single-scale and multi-scale inference, achieving 54.1 mIoU and 55.7 mIoU respectively. Using a **Swin**-L backbone, we achieve single-scale 56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on the dataset. Extensive experiments on three widely used semantic segmentation datasets verify the effectiveness of our proposed method.  
### Few-shot Object Counting with Similarity-Aware Feature **Enhancement**. (arXiv:2201.08959v4 [cs.CV] UPDATED)
- Authors : Zhiyuan You, Yujun Shen, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le
- Link : [http://arxiv.org/abs/2201.08959](http://arxiv.org/abs/2201.08959)
> ABSTRACT  :  This work studies the problem of few-shot object counting, which counts the number of exemplar objects (i.e., described by one or several support images) occurring in the query image. The major challenge lies in that the target objects can be densely packed in the query image, making it hard to recognize every single one. To tackle the obstacle, we propose a novel learning block, equipped with a similarity comparison module (SCM) and a feature **enhancement** module (FEM). Concretely, given a support image and a query image, we first derive a score map by comparing their projected features at every spatial position. The score maps regarding all support images are collected together and normalized across both the exemplar dimension and the spatial dimensions, producing a reliable similarity map. We then enhance the query feature with the support features by employing the developed point-wise similarities as the weighting coefficients. Such a design encourages the model to inspect the query image by focusing more on the regions akin to the support images, leading to much clearer boundaries between different objects. Extensive experiments on various benchmarks and training setups suggest that our method surpasses the state-of-the-art approaches by a sufficiently large margin. For instance, on the very recent large-scale FSC-147 dataset, we beat the second competitor by improving the mean absolute counting error from 22.08 to 14.32 (35% $\uparrow$).  
### Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy. (arXiv:2203.07772v2 [eess.IV] UPDATED)
- Authors : phane Cuenat, Louis Andr, Patrick Sandoz, Maxime Jacquot
- Link : [http://arxiv.org/abs/2203.07772](http://arxiv.org/abs/2203.07772)
> ABSTRACT  :  The numerical wavefront backpropagation principle of digital holography confers unique extended focus capabilities, without mechanical displacements along z-axis. However, the determination of the correct focusing distance is a non-trivial and time consuming issue. A deep learning (DL) solution is proposed to cast the autofocusing as a regression problem and tested over both experimental and simulated holograms. Single wavelength digital holograms were recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$ microscope objective from a patterned target moving in 3D over an axial range of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision Transformer (TViT), tiny VGG16 (TVGG) and a tiny **Swin**-Transfomer (T**Swin**T). The experiments show that the predicted focusing distance $Z_R^{\mathrm{Pred}}$ is accurately inferred with an accuracy of 1.2 $\mu$m in average in comparison with the DHM depth of field of 15 $\mu$m. Numerical simulations show that all tiny models give the $Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such a prospect would significantly improve the current capabilities of computer vision position sensing in applications such as 3D microscopy for life sciences or micro-robotics. Moreover, all models reach state of the art inference time on CPU, less than 25 ms per inference.  
### Semantic-aligned Fusion Transformer for One-shot Object Detection. (arXiv:2203.09093v2 [cs.CV] UPDATED)
- Authors : Yizhou Zhao, Xun Guo, Yan Lu
- Link : [http://arxiv.org/abs/2203.09093](http://arxiv.org/abs/2203.09093)
> ABSTRACT  :  One-shot object detection aims at detecting novel objects according to merely one given instance. With extreme data scarcity, current approaches explore various feature fusions to obtain directly transferable meta-knowledge. Yet, their performances are often unsatisfactory. In this paper, we attribute this to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structures and scale variances. Upon analysis, we leverage the attention mechanism and propose a simple but effective architecture named Semantic-aligned Fusion Transformer (SaFT) to resolve these issues. Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale semantic **enhancement** and a horizontal fusion module (HFM) for cross-sample feature fusion. Together, they broaden the vision for each feature point from the support to a whole augmented feature pyramid from the query, facilitating semantic-aligned associations. Extensive experiments on multiple benchmarks demonstrate the superiority of our framework. Without fine-tuning on novel classes, it brings significant performance gains to one-stage baselines, lifting state-of-the-art results to a higher level.  
## eess.IV
---
### Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction. (arXiv:2111.07910v2 [eess.IV] UPDATED)
- Authors : Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2111.07910](http://arxiv.org/abs/2111.07910)
> ABSTRACT  :  Hyperspectral image (HSI) reconstruction aims to recover the 3D spatial-spectral signal from a 2D measurement in the coded aperture snapshot spectral imaging (CASSI) system. The HSI representations are highly similar and correlated across the spectral dimension. Modeling the inter-spectra interactions is beneficial for HSI reconstruction. However, existing CNN-based methods show limitations in capturing spectral-wise similarity and long-range dependencies. Besides, the HSI information is modulated by a coded aperture (physical mask) in CASSI. Nonetheless, current algorithms have not fully explored the guidance effect of the mask for HSI **restoration**. In this paper, we propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI reconstruction. Specifically, we present a Spectral-wise Multi-head Self-Attention (S-MSA) that treats each spectral feature as a token and calculates self-attention along the spectral dimension. In addition, we customize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to spatial regions with high-fidelity spectral representations. Extensive experiments show that our MST significantly outperforms state-of-the-art (SOTA) methods on simulation and real HSI datasets while requiring dramatically cheaper computational and memory costs. Code and pre-trained models are available at https://github.com/caiyuanhao1998/MST/  
### Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy. (arXiv:2203.07772v2 [eess.IV] UPDATED)
- Authors : phane Cuenat, Louis Andr, Patrick Sandoz, Maxime Jacquot
- Link : [http://arxiv.org/abs/2203.07772](http://arxiv.org/abs/2203.07772)
> ABSTRACT  :  The numerical wavefront backpropagation principle of digital holography confers unique extended focus capabilities, without mechanical displacements along z-axis. However, the determination of the correct focusing distance is a non-trivial and time consuming issue. A deep learning (DL) solution is proposed to cast the autofocusing as a regression problem and tested over both experimental and simulated holograms. Single wavelength digital holograms were recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$ microscope objective from a patterned target moving in 3D over an axial range of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision Transformer (TViT), tiny VGG16 (TVGG) and a tiny **Swin**-Transfomer (T**Swin**T). The experiments show that the predicted focusing distance $Z_R^{\mathrm{Pred}}$ is accurately inferred with an accuracy of 1.2 $\mu$m in average in comparison with the DHM depth of field of 15 $\mu$m. Numerical simulations show that all tiny models give the $Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such a prospect would significantly improve the current capabilities of computer vision position sensing in applications such as 3D microscopy for life sciences or micro-robotics. Moreover, all models reach state of the art inference time on CPU, less than 25 ms per inference.  
## cs.LG
---
### The Sandbox Environment for Generalizable Agent Research (SEGAR). (arXiv:2203.10351v1 [cs.LG])
- Authors : Devon Hjelm, Bogdan Mazoure, Florian Golemo, Felipe Frujeri, Mihai Jalobeanu, Andrey Kolobov
- Link : [http://arxiv.org/abs/2203.10351](http://arxiv.org/abs/2203.10351)
> ABSTRACT  :  A broad challenge of research on generalization for sequential decision-making tasks in interactive environments is designing benchmarks that clearly landmark progress. While there has been notable headway, current benchmarks either do not provide suitable **exposure** nor intuitive control of the underlying factors, are not easy-to-implement, customizable, or extensible, or are computationally expensive to run. We built the Sandbox Environment for Generalizable Agent Research (SEGAR) with all of these things in mind. SEGAR improves the ease and accountability of generalization research in RL, as generalization objectives can be easy designed by specifying task distributions, which in turns allows the researcher to measure the nature of the generalization objective. We present an overview of SEGAR and how it contributes to these goals, as well as experiments that demonstrate a few types of research questions SEGAR can help answer.  
### On Robust Prefix-Tuning for Text Classification. (arXiv:2203.10378v1 [cs.CL])
- Authors : Zonghan Yang, Yang Liu
- Link : [http://arxiv.org/abs/2203.10378](http://arxiv.org/abs/2203.10378)
> ABSTRACT  :  Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness **enhancement**. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research.  
### The **Dark** Side: Security Concerns in Machine Learning for EDA. (arXiv:2203.10597v1 [cs.CR])
- Authors : Zhiyao Xie, Jingyu Pan, Chia Chang, Yiran Chen
- Link : [http://arxiv.org/abs/2203.10597](http://arxiv.org/abs/2203.10597)
> ABSTRACT  :  The growing IC complexity has led to a compelling need for design efficiency improvement through new electronic design automation (EDA) methodologies. In recent years, many unprecedented efficient EDA methods have been enabled by machine learning (ML) techniques. While ML demonstrates its great potential in circuit design, however, the **dark** side about security problems, is seldomly discussed. This paper gives a comprehensive and impartial summary of all security concerns we have observed in ML for EDA. Many of them are hidden or neglected by practitioners in this field. In this paper, we first provide our taxonomy to define four major types of security concerns, then we analyze different application scenarios and special properties in ML for EDA. After that, we present our detailed analysis of each security concern with experiments.  
### SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v4 [cs.CV] UPDATED)
- Authors : Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence Staib
- Link : [http://arxiv.org/abs/2108.06227](http://arxiv.org/abs/2108.06227)
> ABSTRACT  :  Automated segmentation in medical image analysis is a challenging task that requires a large amount of manually labeled data. However, most existing learning-based approaches usually suffer from limited manually annotated medical data, which poses a major practical problem for accurate and robust medical image segmentation. In addition, most existing semi-supervised approaches are usually not robust compared with the supervised counterparts, and also lack explicit modeling of geometric structure and semantic information, both of which limit the segmentation accuracy. In this work, we present SimCVD, a simple contrastive distillation framework that significantly advances state-of-the-art voxel-wise representation learning. We first describe an unsupervised training strategy, which takes two views of an input volume and predicts their signed distance maps of object boundaries in a contrastive objective, with only two independent dropout as mask. This simple approach works surprisingly well, performing on the same level as previous fully supervised methods with much less labeled data. We hypothesize that dropout can be viewed as a minimal form of data augmentation and makes the network robust to representation collapse. Then, we propose to perform structural distillation by distilling pair-wise similarities. We evaluate SimCVD on two popular datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT dataset. The results on the LA dataset demonstrate that, in two types of labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of 90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to previous best results. Our method can be trained in an end-to-end fashion, showing the promise of utilizing SimCVD as a general framework for downstream tasks, such as medical image synthesis, **enhancement**, and registration.  
## cs.AI
---
### On Robust Prefix-Tuning for Text Classification. (arXiv:2203.10378v1 [cs.CL])
- Authors : Zonghan Yang, Yang Liu
- Link : [http://arxiv.org/abs/2203.10378](http://arxiv.org/abs/2203.10378)
> ABSTRACT  :  Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness **enhancement**. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research.  
### SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v4 [cs.CV] UPDATED)
- Authors : Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence Staib
- Link : [http://arxiv.org/abs/2108.06227](http://arxiv.org/abs/2108.06227)
> ABSTRACT  :  Automated segmentation in medical image analysis is a challenging task that requires a large amount of manually labeled data. However, most existing learning-based approaches usually suffer from limited manually annotated medical data, which poses a major practical problem for accurate and robust medical image segmentation. In addition, most existing semi-supervised approaches are usually not robust compared with the supervised counterparts, and also lack explicit modeling of geometric structure and semantic information, both of which limit the segmentation accuracy. In this work, we present SimCVD, a simple contrastive distillation framework that significantly advances state-of-the-art voxel-wise representation learning. We first describe an unsupervised training strategy, which takes two views of an input volume and predicts their signed distance maps of object boundaries in a contrastive objective, with only two independent dropout as mask. This simple approach works surprisingly well, performing on the same level as previous fully supervised methods with much less labeled data. We hypothesize that dropout can be viewed as a minimal form of data augmentation and makes the network robust to representation collapse. Then, we propose to perform structural distillation by distilling pair-wise similarities. We evaluate SimCVD on two popular datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT dataset. The results on the LA dataset demonstrate that, in two types of labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of 90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to previous best results. Our method can be trained in an end-to-end fashion, showing the promise of utilizing SimCVD as a general framework for downstream tasks, such as medical image synthesis, **enhancement**, and registration.  
### Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v2 [cs.CV] UPDATED)
- Authors : Zipeng Qin, Jianbo Liu, Xiaolin Zhang, Maoqing Tian, Aojun Zhou, Shuai Yi, Hongsheng Li
- Link : [http://arxiv.org/abs/2201.04019](http://arxiv.org/abs/2201.04019)
> ABSTRACT  :  The recently proposed MaskFormer gives a refreshed perspective on the task of semantic segmentation: it shifts from the popular pixel-level classification paradigm to a mask-level classification method. In essence, it generates paired probabilities and masks corresponding to category segments and combines them during inference for the segmentation maps. In our study, we find that per-mask classification decoder on top of a single-scale feature is not effective enough to extract reliable probability or mask. To mine for rich semantic information across the feature pyramid, we propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask approach semantic segmentation with multi-scale features. The proposed transformer decoder performs cross-attention between the learnable queries and each spatial feature from the feature pyramid in parallel and uses cross-scale inter-query attention to exchange complimentary information. We achieve competitive performance on three widely used semantic segmentation datasets. In particular, on ADE20K validation set, our result with **Swin**-B backbone surpasses that of MaskFormer's with a much larger **Swin**-L backbone in both single-scale and multi-scale inference, achieving 54.1 mIoU and 55.7 mIoU respectively. Using a **Swin**-L backbone, we achieve single-scale 56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on the dataset. Extensive experiments on three widely used semantic segmentation datasets verify the effectiveness of our proposed method.  
# Paper List
---
## cs.CV
---
**163** new papers in cs.CV:-) 
1. FaceMap: Towards Unsupervised Face Clustering via Map Equation. (arXiv:2203.10090v1 [cs.CV])
2. Label conditioned segmentation. (arXiv:2203.10091v1 [eess.IV])
3. Selection of entropy based features for the analysis of the Archimedes' spiral applied to essential tremor. (arXiv:2203.10094v1 [q-bio.QM])
4. AlignTransformer: Hierarchical Alignment of Visual Regions and Disease Tags for Medical Report Generation. (arXiv:2203.10095v1 [eess.IV])
5. Towards a Perceptual Model for Estimating the Quality of Visual Speech. (arXiv:2203.10117v1 [cs.SD])
6. AI system for fetal ultrasound in low-resource settings. (arXiv:2203.10139v1 [cs.LG])
7. Closing the Generalization Gap of Cross-silo Federated Medical Image Segmentation. (arXiv:2203.10144v1 [cs.CV])
8. ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v1 [cs.CV])
9. Discovering Objects that Can Move. (arXiv:2203.10159v1 [cs.CV])
10. A Closer Look at Knowledge Distillation with Features, Logits, and Gradients. (arXiv:2203.10163v1 [cs.LG])
11. Concept-based Adversarial Attacks: Tricking Humans and Classifiers Alike. (arXiv:2203.10166v1 [cs.LG])
12. Evaluation of April Tag and WhyCode Fiducial Systems for Autonomous Precision Drone Landing with a Gimbal-Mounted Camera. (arXiv:2203.10180v1 [cs.CV])
13. Adversarial Attacks on Deep Learning-based Video Compression and Classification Systems. (arXiv:2203.10183v1 [cs.CV])
14. Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty Quantification. (arXiv:2203.10192v1 [cs.CV])
15. Analysis and Adaptation of YOLOv4 for Object Detection in Aerial Images. (arXiv:2203.10194v1 [cs.CV])
16. Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v1 [cs.CV])
17. Relationformer: A Unified Framework for Image-to-Graph Generation. (arXiv:2203.10202v1 [cs.CV])
18. Inferring topological transitions in pattern-forming processes with self-supervised learning. (arXiv:2203.10204v1 [cond-mat.mtrl-sci])
19. **Swin**TextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition. (arXiv:2203.10209v1 [cs.CV])
20. Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction. (arXiv:2203.10212v1 [cs.CV])
21. Volkit: A Performance-Portable Computer Vision Library for 3D Volumetric Data. (arXiv:2203.10213v1 [cs.CV])
22. DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition. (arXiv:2203.10233v1 [cs.CV])
23. HIPA: Hierarchical Patch Transformer for Single Image Super Resolution. (arXiv:2203.10247v1 [cs.CV])
24. Representation-Agnostic Shape Fields. (arXiv:2203.10259v1 [cs.CV])
25. Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation. (arXiv:2203.10278v1 [cs.CV])
26. Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation. (arXiv:2203.10291v1 [cs.CV])
27. Incremental Few-Shot Learning via Implanting and Compressing. (arXiv:2203.10297v1 [cs.CV])
28. Modelling nonlinear dependencies in the latent space of inverse scattering. (arXiv:2203.10307v1 [cs.CV])
29. Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds. (arXiv:2203.10314v1 [cs.CV])
30. Domain Adaptation Meets Zero-Shot Learning: An Annotation-Efficient Approach to Multi-Modality Medical Image Segmentation. (arXiv:2203.10332v1 [cs.CV])
31. TO-FLOW: Efficient Continuous Normalizing Flows with Temporal Optimization adjoint with Moving Speed. (arXiv:2203.10335v1 [cs.CV])
32. Occlusion-Aware Self-Supervised Monocular 6D Object Pose Estimation. (arXiv:2203.10339v1 [cs.CV])
33. No Shifted Augmentations (NSA): compact distributions for robust self-supervised Anomaly Detection. (arXiv:2203.10344v1 [cs.CV])
34. Font Generation with Missing Impression Labels. (arXiv:2203.10348v1 [cs.CV])
35. CLRNet: Cross Layer Refinement Network for Lane Detection. (arXiv:2203.10350v1 [cs.CV])
36. Multi-Domain Multi-Definition Landmark Localization for Small Datasets. (arXiv:2203.10358v1 [cs.CV])
37. ALAP-AE: As-Lite-as-Possible Auto-Encoder. (arXiv:2203.10363v1 [cs.CV])
38. A naive method to discover directions in the StyleGAN2 latent space. (arXiv:2203.10373v1 [cs.CV])
39. PressureVision: Estimating Hand Pressure from a Single RGB Image. (arXiv:2203.10385v1 [cs.CV])
40. Towards Robust Semantic Segmentation of Accident Scenes via Multi-Source Mixed Sampling and Meta-Learning. (arXiv:2203.10395v1 [cs.CV])
41. Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v1 [cs.CV])
42. CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration. (arXiv:2203.10421v1 [cs.CV])
43. End-to-End Human-Gaze-Target Detection with Transformers. (arXiv:2203.10433v1 [cs.CV])
44. Vision Transformer with Convolutions Architecture Search. (arXiv:2203.10435v1 [cs.CV])
45. VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning. (arXiv:2203.10444v1 [cs.CV])
46. Partitioning Image Representation in Contrastive Learning. (arXiv:2203.10454v1 [cs.CV])
47. Adversarial Mutual Leakage Network for Cell Image Segmentation. (arXiv:2203.10455v1 [cs.CV])
48. simCrossTrans: A Simple Cross-Modality Transfer Learning for Object Detection with ConvNets or Vision Transformers. (arXiv:2203.10456v1 [cs.CV])
49. Optical Flow for Video Super-Resolution: A Survey. (arXiv:2203.10462v1 [cs.CV])
50. {Unidirectional Thin Adapter for Efficient Adaptation of Deep Neural Networks. (arXiv:2203.10463v1 [cs.CV])
51. Portrait Eyeglasses and Shadow Removal by Leveraging 3D Synthetic Data. (arXiv:2203.10474v1 [cs.CV])
52. Optimizing Camera Placements for Overlapped Coverage with 3D Camera Projections. (arXiv:2203.10479v1 [cs.CV])
53. Inferring Articulated Rigid Body Dynamics from RGBD Video. (arXiv:2203.10488v1 [cs.RO])
54. TVConv: Efficient Translation Variant Convolution for Layout-aware Visual Processing. (arXiv:2203.10489v1 [cs.CV])
55. SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization. (arXiv:2203.10492v1 [cs.CV])
56. Depth Estimation by Combining Binocular Stereo and Monocular Structured-Light. (arXiv:2203.10493v1 [cs.CV])
57. Single-image Human-body Reshaping with Deep Neural Networks. (arXiv:2203.10496v1 [cs.CV])
58. Soft-CP: A Credible and Effective Data Augmentation for Semantic Segmentation of Medical Lesions. (arXiv:2203.10507v1 [cs.CV])
59. Learning Whole Heart Mesh Generation From Patient Images For Computational Simulations. (arXiv:2203.10517v1 [eess.IV])
60. Stochastic Video Prediction with Structure and Motion. (arXiv:2203.10528v1 [cs.CV])
61. Iwin: Human-Object Interaction Detection via Transformer with Irregular Windows. (arXiv:2203.10537v1 [cs.CV])
62. End-to-End Video Text Spotting with Transformer. (arXiv:2203.10539v1 [cs.CV])
63. Unsupervised Domain Adaptation for **Night**time Aerial Tracking. (arXiv:2203.10541v1 [cs.CV])
64. Document Dewarping with Control Points. (arXiv:2203.10543v1 [cs.CV])
65. Towards 3D Scene Understanding by Referring Synthetic Models. (arXiv:2203.10546v1 [cs.CV])
66. 3D Human Pose Estimation Using M\"obius Graph Convolutional Networks. (arXiv:2203.10554v1 [cs.CV])
67. CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v1 [cs.CV])
68. Accelerating Integrated Task and Motion Planning with Neural Feasibility Checking. (arXiv:2203.10568v1 [cs.RO])
69. Self-supervised Point Cloud Completion on Real Traffic Scenes via Scene-concerned Bottom-up Mechanism. (arXiv:2203.10569v1 [cs.CV])
70. Point3D: tracking actions as moving points with 3D CNNs. (arXiv:2203.10584v1 [cs.CV])
71. Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation. (arXiv:2203.10593v1 [cs.CV])
72. Towards Clinical Practice: Design and Implementation of Convolutional Neural Network-Based Assistive Diagnosis System for COVID-19 Case Detection from Chest X-Ray Images. (arXiv:2203.10596v1 [eess.IV])
73. Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks. (arXiv:1804.06039v3 [cs.CV] UPDATED)
74. **Real-time** Semantic Segmentation via Spatial-detail Guided Context Propagation. (arXiv:2005.11034v5 [cs.CV] UPDATED)
75. Fast Few-Shot Classification by Few-Iteration Meta-Learning. (arXiv:2010.00511v3 [cs.CV] UPDATED)
76. Curriculum Learning: A Survey. (arXiv:2101.10382v2 [cs.LG] UPDATED)
77. DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents. (arXiv:2101.11796v4 [cs.CV] UPDATED)
78. Atlas Generative Models and Geodesic Interpolation. (arXiv:2102.00264v2 [cs.LG] UPDATED)
79. Selfie Periocular Verification using an Efficient Super-Resolution Approach. (arXiv:2102.08449v2 [cs.CV] UPDATED)
80. Discriminative Semantic Transitive Consistency for Cross-Modal Learning. (arXiv:2103.14103v2 [cs.CV] UPDATED)
81. Text to Image Generation with Semantic-Spatial Aware GAN. (arXiv:2104.00567v5 [cs.CV] UPDATED)
82. M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers. (arXiv:2104.01122v2 [cs.CV] UPDATED)
83. How Well Does Self-Supervised Pre-Training Perform with Streaming Data?. (arXiv:2104.12081v2 [cs.LG] UPDATED)
84. Moving Towards Centers: Re-ranking with Attention and Memory for Re-identification. (arXiv:2105.01447v2 [cs.CV] UPDATED)
85. One Shot Face Swapping on Megapixels. (arXiv:2105.04932v2 [cs.CV] UPDATED)
86. Language-Driven Image Style Transfer. (arXiv:2106.00178v2 [cs.CV] UPDATED)
87. More Than Meets the Eye: Self-Supervised Depth Reconstruction from Brain Activity. (arXiv:2106.05113v2 [cs.CV] UPDATED)
88. Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v2 [cs.LG] UPDATED)
89. Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v7 [cs.CV] UPDATED)
90. SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v4 [cs.CV] UPDATED)
91. CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation. (arXiv:2109.06165v4 [cs.CV] UPDATED)
92. ElasticFace: Elastic Margin Loss for Deep Face Recognition. (arXiv:2109.09416v4 [cs.CV] UPDATED)
93. Improving Autoencoder Training Performance for Hyperspectral Unmixing with Network Reinitialisation. (arXiv:2109.13748v2 [eess.IV] UPDATED)
94. Generative Memory-Guided Semantic Reasoning Model for Image Inpainting. (arXiv:2110.00261v2 [cs.CV] UPDATED)
95. Optical-Flow-Reuse-Based Bidirectional Recurrent Network for Space-Time Video Super-Resolution. (arXiv:2110.06786v2 [cs.CV] UPDATED)
96. Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference. (arXiv:2110.10031v2 [cs.LG] UPDATED)
97. ConAM: Confidence Attention Module for Convolutional Neural Networks. (arXiv:2110.14369v3 [cs.CV] UPDATED)
98. Unsupervised Part Discovery from Contrastive Reconstruction. (arXiv:2111.06349v2 [cs.CV] UPDATED)
99. Color Mapping Functions For **HDR** Panorama Imaging: Weighted Histogram Averaging. (arXiv:2111.07283v2 [cs.CV] UPDATED)
100. Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction. (arXiv:2111.07910v2 [eess.IV] UPDATED)
101. Exploiting Segment-level Semantics for Online Phase Recognition from Surgical Videos. (arXiv:2111.11044v2 [cs.CV] UPDATED)
102. GeoNeRF: Generalizing NeRF with Geometry Priors. (arXiv:2111.13539v2 [cs.CV] UPDATED)
103. Robust and Accurate Superquadric Recovery: a Probabilistic Approach. (arXiv:2111.14517v2 [cs.CV] UPDATED)
104. FENeRF: Face Editing in Neural Radiance Fields. (arXiv:2111.15490v2 [cs.CV] UPDATED)
105. Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources. (arXiv:2112.00061v3 [cs.CV] UPDATED)
106. AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions. (arXiv:2112.00246v2 [cs.CV] UPDATED)
107. Confidence Propagation Cluster: Unleash Full Potential of Object Detectors. (arXiv:2112.00342v4 [cs.CV] UPDATED)
108. CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v3 [cs.CV] UPDATED)
109. Self-supervised Video Transformer. (arXiv:2112.01514v2 [cs.CV] UPDATED)
110. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting. (arXiv:2112.01518v2 [cs.CV] UPDATED)
111. Input-level Inductive Biases for 3D Reconstruction. (arXiv:2112.03243v2 [cs.CV] UPDATED)
112. A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion. (arXiv:2112.03530v4 [cs.CV] UPDATED)
113. GaTector: A Unified Framework for Gaze Object Prediction. (arXiv:2112.03549v2 [cs.CV] UPDATED)
114. Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction. (arXiv:2112.05146v2 [eess.IV] UPDATED)
115. COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality. (arXiv:2112.05892v2 [cs.CV] UPDATED)
116. Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v3 [cs.CV] UPDATED)
117. Towards General and Efficient Active Learning. (arXiv:2112.07963v2 [cs.CV] UPDATED)
118. CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v3 [cs.CV] UPDATED)
119. Efficient Strong Scaling Through Burst Parallel Training. (arXiv:2112.10065v2 [cs.DC] UPDATED)
120. On Efficient Transformer-Based Image Pre-training for Low-Level Vision. (arXiv:2112.10175v2 [cs.CV] UPDATED)
121. Projected Sliced Wasserstein Autoencoder-based Hyperspectral Images Anomaly Detection. (arXiv:2112.11243v3 [cs.CV] UPDATED)
122. FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition. (arXiv:2112.12496v3 [cs.CV] UPDATED)
123. Linear Variational State-Space Filtering. (arXiv:2201.01353v3 [cs.LG] UPDATED)
124. Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v2 [cs.CV] UPDATED)
125. Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip Connections and Hybrid Learning. (arXiv:2201.05585v2 [cs.CV] UPDATED)
126. Variable Augmented Network for Invertible MR Coil Compression. (arXiv:2201.07428v2 [cs.CV] UPDATED)
127. Virtual Coil Augmentation Technology for MR Coil Extrapolation via Deep Learning. (arXiv:2201.07540v2 [cs.CV] UPDATED)
128. Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v5 [cs.CV] UPDATED)
129. Few-shot Object Counting with Similarity-Aware Feature **Enhancement**. (arXiv:2201.08959v4 [cs.CV] UPDATED)
130. Infrastructure-Based Object Detection and Tracking for Cooperative Driving Automation: A Survey. (arXiv:2201.11871v2 [cs.CV] UPDATED)
131. Extracting Built Environment Features for Planning Research with Computer Vision: A Review and Discussion of State-of-the-Art Approaches. (arXiv:2201.12693v2 [cs.CV] UPDATED)
132. Learning Features with Parameter-Free Layers. (arXiv:2202.02777v2 [cs.CV] UPDATED)
133. HDAM: Heuristic Difference Attention Module for Convolutional Neural Networks. (arXiv:2202.09556v3 [cs.CV] UPDATED)
134. Tripartite: Tackle Noisy Labels by a More Precise Partition. (arXiv:2202.09579v2 [cs.CV] UPDATED)
135. A Smoothing and Thresholding Image Segmentation Framework with Weighted Anisotropic-Isotropic Total Variation. (arXiv:2202.10115v2 [cs.CV] UPDATED)
136. RIConv++: Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. (arXiv:2202.13094v2 [cs.CV] UPDATED)
137. Towards Class-agnostic Tracking Using Feature Decorrelation in Point Clouds. (arXiv:2202.13524v2 [cs.CV] UPDATED)
138. A Standardized Pipeline for Colon Nuclei Identification and Counting Challenge. (arXiv:2203.00171v2 [eess.IV] UPDATED)
139. DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with guttae. (arXiv:2203.01882v3 [eess.IV] UPDATED)
140. ACVNet: Attention Concatenation Volume for Accurate and Efficient Stereo Matching. (arXiv:2203.02146v2 [cs.CV] UPDATED)
141. BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation. (arXiv:2203.02533v2 [eess.IV] UPDATED)
142. UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation. (arXiv:2203.02557v2 [cs.CV] UPDATED)
143. Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer. (arXiv:2203.02794v2 [cs.LG] UPDATED)
144. Differentially Private Federated Learning with Local Regularization and Sparsification. (arXiv:2203.03106v3 [cs.LG] UPDATED)
145. Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v3 [cs.CV] UPDATED)
146. Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v2 [cs.CV] UPDATED)
147. Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classification. (arXiv:2203.04771v4 [cs.CV] UPDATED)
148. Cross-modal Map Learning for Vision and Language Navigation. (arXiv:2203.05137v3 [cs.CV] UPDATED)
149. Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v3 [cs.CV] UPDATED)
150. PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR. (arXiv:2203.06319v3 [cs.CV] UPDATED)
151. Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v3 [cs.CV] UPDATED)
152. Attention based Memory video portrait matting. (arXiv:2203.06890v2 [cs.CV] UPDATED)
153. Hierarchical Memory Learning for Fine-Grained Scene Graph Generation. (arXiv:2203.06907v2 [cs.CV] UPDATED)
154. Progressive End-to-End Object Detection in Crowded Scenes. (arXiv:2203.07669v2 [cs.CV] UPDATED)
155. Revitalize Region Feature for Democratizing Video-Language Pre-training. (arXiv:2203.07720v2 [cs.CV] UPDATED)
156. Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy. (arXiv:2203.07772v2 [eess.IV] UPDATED)
157. Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels. (arXiv:2203.07788v2 [cs.LG] UPDATED)
158. Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training. (arXiv:2203.08959v2 [cs.LG] UPDATED)
159. Semantic-aligned Fusion Transformer for One-shot Object Detection. (arXiv:2203.09093v2 [cs.CV] UPDATED)
160. TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes. (arXiv:2203.09440v2 [cs.CV] UPDATED)
161. FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos. (arXiv:2203.09463v2 [cs.CV] UPDATED)
162. Towards Data-Efficient Detection Transformers. (arXiv:2203.09507v2 [cs.CV] UPDATED)
163. HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v1 [cs.CV] CROSS LISTED)
## eess.IV
---
**30** new papers in eess.IV:-) 
1. Label conditioned segmentation. (arXiv:2203.10091v1 [eess.IV])
2. Selection of entropy based features for the analysis of the Archimedes' spiral applied to essential tremor. (arXiv:2203.10094v1 [q-bio.QM])
3. AlignTransformer: Hierarchical Alignment of Visual Regions and Disease Tags for Medical Report Generation. (arXiv:2203.10095v1 [eess.IV])
4. AI system for fetal ultrasound in low-resource settings. (arXiv:2203.10139v1 [cs.LG])
5. Towards Robust Semantic Segmentation of Accident Scenes via Multi-Source Mixed Sampling and Meta-Learning. (arXiv:2203.10395v1 [cs.CV])
6. Learning Whole Heart Mesh Generation From Patient Images For Computational Simulations. (arXiv:2203.10517v1 [eess.IV])
7. Towards Clinical Practice: Design and Implementation of Convolutional Neural Network-Based Assistive Diagnosis System for COVID-19 Case Detection from Chest X-Ray Images. (arXiv:2203.10596v1 [eess.IV])
8. Multi-Modal Learning Using Physicians Diagnostics for Optical Coherence Tomography Classification. (arXiv:2203.10622v1 [eess.IV])
9. Multimodal learning-based inversion models for the space-time reconstruction of satellite-derived geophysical fields. (arXiv:2203.10640v1 [cs.CV])
10. A direct geometry processing cartilage generation method using segmented bone models from datasets with poor cartilage visibility. (arXiv:2203.10667v1 [eess.IV])
11. SweiNet: Deep Learning Based Uncertainty Quantification for Ultrasound Shear Wave Elasticity Imaging. (arXiv:2203.10678v1 [eess.IV])
12. Classifications of Skull Fractures using CT Scan Images via CNN with Lazy Learning Approach. (arXiv:2203.10786v1 [eess.IV])
13. Longitudinal Self-Supervision for COVID-19 Pathology Quantification. (arXiv:2203.10804v1 [eess.IV])
14. Controllable energy angular spectrum method. (arXiv:2203.10966v1 [eess.IV])
15. SOLIS: Autonomous Solubility Screening using Deep Neural Networks. (arXiv:2203.10970v1 [cs.CV])
16. Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v1 [cs.CV])
17. VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations. (arXiv:2012.15029v3 [eess.IV] UPDATED)
18. Selfie Periocular Verification using an Efficient Super-Resolution Approach. (arXiv:2102.08449v2 [cs.CV] UPDATED)
19. Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v2 [cs.LG] UPDATED)
20. Improving Autoencoder Training Performance for Hyperspectral Unmixing with Network Reinitialisation. (arXiv:2109.13748v2 [eess.IV] UPDATED)
21. Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction. (arXiv:2111.07910v2 [eess.IV] UPDATED)
22. CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v3 [cs.CV] UPDATED)
23. Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction. (arXiv:2112.05146v2 [eess.IV] UPDATED)
24. A Standardized Pipeline for Colon Nuclei Identification and Counting Challenge. (arXiv:2203.00171v2 [eess.IV] UPDATED)
25. DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with guttae. (arXiv:2203.01882v3 [eess.IV] UPDATED)
26. BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation. (arXiv:2203.02533v2 [eess.IV] UPDATED)
27. UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation. (arXiv:2203.02557v2 [cs.CV] UPDATED)
28. Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer. (arXiv:2203.02794v2 [cs.LG] UPDATED)
29. Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy. (arXiv:2203.07772v2 [eess.IV] UPDATED)
30. DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications. (arXiv:2203.09096v2 [cs.LG] UPDATED)
## cs.LG
---
**230** new papers in cs.LG:-) 
1. FaceMap: Towards Unsupervised Face Clustering via Map Equation. (arXiv:2203.10090v1 [cs.CV])
2. Deep Reinforcement Learning Guided Graph Neural Networks for Brain Network Analysis. (arXiv:2203.10093v1 [cs.LG])
3. SiMCa: Sinkhorn Matrix Factorization with Capacity Constraints. (arXiv:2203.10107v1 [cs.LG])
4. Seamless lightning nowcasting with recurrent-convolutional deep learning. (arXiv:2203.10114v1 [physics.ao-ph])
5. Approximate Function Evaluation via Multi-Armed Bandits. (arXiv:2203.10124v1 [cs.LG])
6. Half-Inverse Gradients for Physical Deep Learning. (arXiv:2203.10131v1 [cs.LG])
7. Learning Compressed Embeddings for On-Device Inference. (arXiv:2203.10135v1 [cs.LG])
8. AI system for fetal ultrasound in low-resource settings. (arXiv:2203.10139v1 [cs.LG])
9. Infinite-Horizon Reach-Avoid Zero-Sum Games via Deep Reinforcement Learning. (arXiv:2203.10142v1 [eess.SY])
10. ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v1 [cs.CV])
11. A Closer Look at Knowledge Distillation with Features, Logits, and Gradients. (arXiv:2203.10163v1 [cs.LG])
12. Privacy-Preserving Reinforcement Learning Beyond Expectation. (arXiv:2203.10165v1 [cs.LG])
13. Concept-based Adversarial Attacks: Tricking Humans and Classifiers Alike. (arXiv:2203.10166v1 [cs.LG])
14. Equitable Ability Estimation in Neurodivergent Student Populations with Zero-Inflated Learner Models. (arXiv:2203.10170v1 [cs.CY])
15. Importance Sampling Placement in Off-Policy Temporal-Difference Methods. (arXiv:2203.10172v1 [cs.LG])
16. Active learning in open experimental environments: selecting the right information channel(s) based on predictability in deep kernel learning. (arXiv:2203.10181v1 [cs.LG])
17. Negative Inner-Loop Learning Rates Learn Universal Features. (arXiv:2203.10185v1 [cs.LG])
18. A Class of Two-Timescale Stochastic EM Algorithms for Nonconvex Latent Variable Models. (arXiv:2203.10186v1 [stat.ML])
19. Provably Fair Federated Learning via Bounded Group Loss. (arXiv:2203.10190v1 [cs.LG])
20. Analysis and Adaptation of YOLOv4 for Object Detection in Aerial Images. (arXiv:2203.10194v1 [cs.CV])
21. Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v1 [cs.CV])
22. Emulating Quantum Dynamics with Neural Networks via Knowledge Distillation. (arXiv:2203.10200v1 [quant-ph])
23. Inferring topological transitions in pattern-forming processes with self-supervised learning. (arXiv:2203.10204v1 [cond-mat.mtrl-sci])
24. Conjugate Gradient Adaptive Learning with Tukey's Biweight M-Estimate. (arXiv:2203.10205v1 [cs.LG])
25. Thompson Sampling on Asymmetric $\alpha$-Stable Bandits. (arXiv:2203.10214v1 [stat.ML])
26. FaiRR: Faithful and Robust Deductive Reasoning over Natural Language. (arXiv:2203.10261v1 [cs.CL])
27. Assessing Gender Bias in Predictive Algorithms using eXplainable AI. (arXiv:2203.10264v1 [cs.HC])
28. Meta-Weight Graph Neural Network: Push the Limits Beyond Global Homophily. (arXiv:2203.10280v1 [cs.LG])
29. Multi-channel CNN to classify nepali covid-19 related tweets using hybrid features. (arXiv:2203.10286v1 [cs.CL])
30. Adversarial Defense via Image Denoising with Chaotic Encryption. (arXiv:2203.10290v1 [cs.LG])
31. Exploring the impact of spatiotemporal granularity on the demand prediction of dynamic ride-hailing. (arXiv:2203.10301v1 [cs.LG])
32. PACE: A Parallelizable Computation Encoder for Directed Acyclic Graphs. (arXiv:2203.10304v1 [cs.LG])
33. Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction. (arXiv:2203.10316v1 [cs.CL])
34. Practical Recommendations for Replay-based Continual Learning Methods. (arXiv:2203.10317v1 [cs.LG])
35. Sequence-to-Sequence Knowledge Graph Completion and Question Answering. (arXiv:2203.10321v1 [cs.CL])
36. Implicit Parameter-free Online Learning with Truncated Linear Models. (arXiv:2203.10327v1 [cs.LG])
37. Desirable Companion for Vertical Federated Learning: New Zeroth-Order Gradient Based Algorithm. (arXiv:2203.10329v1 [cs.LG])
38. Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense. (arXiv:2203.10346v1 [cs.LG])
39. The Sandbox Environment for Generalizable Agent Research (SEGAR). (arXiv:2203.10351v1 [cs.LG])
40. Deep Learning Generalization, Extrapolation, and Over-parameterization. (arXiv:2203.10366v1 [cs.LG])
41. On Robust Prefix-Tuning for Text Classification. (arXiv:2203.10378v1 [cs.CL])
42. Anomaly Detection in Emails using Machine Learning and Header Information. (arXiv:2203.10408v1 [cs.CR])
43. Language-Preserving Reduction Rules for Block-Structured Workflow Nets. (arXiv:2203.10410v1 [cs.LG])
44. CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration. (arXiv:2203.10421v1 [cs.CV])
45. Subspace Modeling for Fast Out-Of-Distribution and Anomaly Detection. (arXiv:2203.10422v1 [cs.LG])
46. A Study on Robustness to Perturbations for Representations of Environmental Sound. (arXiv:2203.10425v1 [cs.SD])
47. PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication. (arXiv:2203.10428v1 [cs.LG])
48. Towards Structuring Real-World Data at Scale: Deep Learning for Extracting Key Oncology Information from Clinical Text with Patient-Level Supervision. (arXiv:2203.10442v1 [cs.CL])
49. Quantum Multi-Agent Reinforcement Learning via Variational Quantum Circuit Design. (arXiv:2203.10443v1 [quant-ph])
50. Smoothing Advantage Learning. (arXiv:2203.10445v1 [cs.LG])
51. A 3D Molecule Generative Model for Structure-Based Drug Design. (arXiv:2203.10446v1 [q-bio.BM])
52. Over-parameterization: A Necessary Condition for Models that Extrapolate. (arXiv:2203.10447v1 [cs.LG])
53. On the Computation of Necessary and Sufficient Explanations. (arXiv:2203.10451v1 [cs.AI])
54. CrossBeam: Learning to Search in Bottom-Up Program Synthesis. (arXiv:2203.10452v1 [cs.LG])
55. Fine-Tuning Graph Neural Networks via Graph Topology induced Optimal Transport. (arXiv:2203.10453v1 [cs.LG])
56. Inspection-L: Practical GNN-Based Money Laundering Detection System for Bitcoin. (arXiv:2203.10465v1 [cs.CR])
57. Federated Spatial Reuse Optimization in Next-Generation Decentralized IEEE 802.11 WLANs. (arXiv:2203.10472v1 [cs.NI])
58. ECAPA-TDNN for Multi-speaker Text-to-speech Synthesis. (arXiv:2203.10473v1 [cs.SD])
59. Learnable Encoder-Decoder Architecture for Dynamic Graph: A Survey. (arXiv:2203.10480v1 [cs.LG])
60. MicroRacer: a didactic environment for Deep Reinforcement Learning. (arXiv:2203.10494v1 [cs.LG])
61. Adversarial Parameter Attack on Deep Neural Networks. (arXiv:2203.10502v1 [cs.LG])
62. Soft-CP: A Credible and Effective Data Augmentation for Semantic Segmentation of Medical Lesions. (arXiv:2203.10507v1 [cs.CV])
63. An Adaptive and Scalable ANN-based Model-Order-Reduction Method for Large-Scale TO Designs. (arXiv:2203.10515v1 [cs.LG])
64. Learning on the Job: Long-Term Behavioural Adaptation in Human-Robot Interactions. (arXiv:2203.10518v1 [cs.RO])
65. Reinforcement learning reward function in unmanned aerial vehicle control tasks. (arXiv:2203.10519v1 [cs.RO])
66. Stochastic Video Prediction with Structure and Motion. (arXiv:2203.10528v1 [cs.CV])
67. A Learning Convolutional Neural Network Approach for Network Robustness Prediction. (arXiv:2203.10552v1 [eess.SY])
68. LEReg: Empower Graph Neural Networks with Local Energy Regularization. (arXiv:2203.10565v1 [cs.LG])
69. Multi-view Multi-behavior Contrastive Learning in Recommendation. (arXiv:2203.10576v1 [cs.IR])
70. Small Batch Sizes Improve Training of Low-Resource Neural MT. (arXiv:2203.10579v1 [cs.CL])
71. Cluster & Tune: Boost Cold Start Performance in Text Classification. (arXiv:2203.10581v1 [cs.CL])
72. Neuro-physical dynamic load modeling using differentiable parametric optimization. (arXiv:2203.10582v1 [eess.SY])
73. Variational Quantum Policy Gradients with an Application to Quantum Control. (arXiv:2203.10591v1 [quant-ph])
74. Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents. (arXiv:2203.10592v1 [stat.ML])
75. Towards Clinical Practice: Design and Implementation of Convolutional Neural Network-Based Assistive Diagnosis System for COVID-19 Case Detection from Chest X-Ray Images. (arXiv:2203.10596v1 [eess.IV])
76. The **Dark** Side: Security Concerns in Machine Learning for EDA. (arXiv:2203.10597v1 [cs.CR])
77. Efficient Linear Bandits through Matrix Sketching. (arXiv:1809.11033v3 [cs.LG] UPDATED)
78. SHARP: An Adaptable, Energy-Efficient Accelerator for Recurrent Neural Network. (arXiv:1911.01258v2 [cs.LG] UPDATED)
79. Methods for Stabilizing Models across Large Samples of Projects (with case studies on Predicting Defect and Project Health). (arXiv:1911.04250v4 [cs.SE] UPDATED)
80. A Modern Introduction to Online Learning. (arXiv:1912.13213v5 [cs.LG] UPDATED)
81. Regret analysis of the Piyavskii-Shubert algorithm for global Lipschitz optimization. (arXiv:2002.02390v3 [cs.LG] UPDATED)
82. Task-adaptive Asymmetric Deep Cross-modal Hashing. (arXiv:2004.00197v2 [cs.IR] UPDATED)
83. Riemannian Stochastic Proximal Gradient Methods for Nonsmooth Optimization over the Stiefel Manifold. (arXiv:2005.01209v2 [math.OC] UPDATED)
84. Adaptive Double-Exploration Tradeoff for Outlier Detection. (arXiv:2005.06092v2 [cs.LG] UPDATED)
85. Instability, Computational Efficiency and Statistical Accuracy. (arXiv:2005.11411v2 [cs.LG] UPDATED)
86. Learning the Travelling Salesperson Problem Requires Rethinking Generalization. (arXiv:2006.07054v5 [cs.LG] UPDATED)
87. Mat\'ern Gaussian processes on Riemannian manifolds. (arXiv:2006.10160v5 [stat.ML] UPDATED)
88. Confident Off-Policy Evaluation and Selection through Self-Normalized Importance Weighting. (arXiv:2006.10460v3 [cs.LG] UPDATED)
89. IMU Preintegrated Features for Efficient Deep Inertial Odometry. (arXiv:2007.02929v2 [cs.LG] UPDATED)
90. Macroscopic Traffic Flow Modeling with Physics Regularized Gaussian Process: Generalized Formulations. (arXiv:2007.07762v2 [stat.ML] UPDATED)
91. Heterogeneous Federated Learning. (arXiv:2008.06767v2 [cs.LG] UPDATED)
92. Survey: Geometric Foundations of Data Reduction. (arXiv:2008.06853v2 [cs.LG] UPDATED)
93. Learning Models for Actionable Recourse. (arXiv:2011.06146v3 [cs.LG] UPDATED)
94. Deep Depression Prediction on Longitudinal Data via Joint Anomaly Ranking and Classification. (arXiv:2012.02950v2 [cs.LG] UPDATED)
95. Optimal Thompson Sampling strategies for support-aware CVaR bandits. (arXiv:2012.05754v3 [cs.LG] UPDATED)
96. DuelGAN: A Duel Between Two Discriminators Stabilizes the GAN Training. (arXiv:2101.07524v3 [cs.LG] UPDATED)
97. Curriculum Learning: A Survey. (arXiv:2101.10382v2 [cs.LG] UPDATED)
98. Development of a Vertex Finding Algorithm using Recurrent Neural Network. (arXiv:2101.11906v3 [physics.data-an] UPDATED)
99. Atlas Generative Models and Geodesic Interpolation. (arXiv:2102.00264v2 [cs.LG] UPDATED)
100. Functional optimal transport: map estimation and domain adaptation for functional data. (arXiv:2102.03895v4 [stat.ML] UPDATED)
101. Cross-Domain Multi-Task Learning for Sequential Sentence Classification in Research Papers. (arXiv:2102.06008v2 [cs.CL] UPDATED)
102. IntSGD: Adaptive Floatless Compression of Stochastic Gradients. (arXiv:2102.08374v2 [cs.LG] UPDATED)
103. Approximation and Learning with Deep Convolutional Models: a Kernel Perspective. (arXiv:2102.10032v3 [stat.ML] UPDATED)
104. Bayesian Deep Learning for Segmentation for Autonomous Safe Planetary Landing. (arXiv:2102.10545v2 [cs.RO] UPDATED)
105. Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization. (arXiv:2103.01499v3 [cs.LG] UPDATED)
106. Are deep learning models superior for missing data imputation in large surveys? Evidence from an empirical comparison. (arXiv:2103.09316v3 [cs.LG] UPDATED)
107. Discriminative Semantic Transitive Consistency for Cross-Modal Learning. (arXiv:2103.14103v2 [cs.CV] UPDATED)
108. Text to Image Generation with Semantic-Spatial Aware GAN. (arXiv:2104.00567v5 [cs.CV] UPDATED)
109. How Well Does Self-Supervised Pre-Training Perform with Streaming Data?. (arXiv:2104.12081v2 [cs.LG] UPDATED)
110. On the Consistency of Max-Margin Losses. (arXiv:2105.15069v3 [cs.LG] UPDATED)
111. More Than Meets the Eye: Self-Supervised Depth Reconstruction from Brain Activity. (arXiv:2106.05113v2 [cs.CV] UPDATED)
112. Towards Understanding Generalization via Decomposing Excess Risk Dynamics. (arXiv:2106.06153v3 [cs.LG] UPDATED)
113. Invariant Information Bottleneck for Domain Generalization. (arXiv:2106.06333v6 [cs.LG] UPDATED)
114. Online Continual Adaptation with Active Self-Training. (arXiv:2106.06526v2 [cs.LG] UPDATED)
115. The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v2 [cs.LG] UPDATED)
116. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v4 [cs.LG] UPDATED)
117. From Coarse to Fine: Global Guided Patch-based Robust Tensor Completion for Visual Data. (arXiv:2106.10422v2 [cs.LG] UPDATED)
118. Fundamental limits for rank-one matrix estimation with groupwise heteroskedasticity. (arXiv:2106.11950v2 [stat.ML] UPDATED)
119. Sampling with Mirrored Stein Operators. (arXiv:2106.12506v2 [stat.ML] UPDATED)
120. UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach. (arXiv:2106.15734v4 [cs.LG] UPDATED)
121. Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v2 [cs.LG] UPDATED)
122. Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v2 [cs.CL] UPDATED)
123. Wide and Deep Graph Neural Network with Distributed Online Learning. (arXiv:2107.09203v2 [cs.LG] UPDATED)
124. Learning Altruistic Behaviours in Reinforcement Learning without External Rewards. (arXiv:2107.09598v4 [cs.AI] UPDATED)
125. Rethinking Hard-Parameter Sharing in Multi-Domain Learning. (arXiv:2107.11359v3 [cs.LG] UPDATED)
126. Connections between Numerical Algorithms for PDEs and Neural Networks. (arXiv:2107.14742v2 [math.NA] UPDATED)
127. Learning to Control DC Motor for Micromobility in Real Time with Reinforcement Learning. (arXiv:2108.00138v3 [cs.LG] UPDATED)
128. SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v4 [cs.CV] UPDATED)
129. Semi-supervised Network Embedding with Differentiable Deep Quantisation. (arXiv:2108.09128v2 [cs.LG] UPDATED)
130. Provable Tensor-Train Format Tensor Completion by Riemannian Optimization. (arXiv:2108.12163v2 [cs.LG] UPDATED)
131. CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems. (arXiv:2109.04645v4 [cs.CL] UPDATED)
132. Enhancing Unsupervised Anomaly Detection with Score-Guided Network. (arXiv:2109.04684v2 [cs.LG] UPDATED)
133. A Neural Tangent Kernel Perspective of Infinite Tree Ensembles. (arXiv:2109.04983v2 [cs.LG] UPDATED)
134. Estimation of Local Average Treatment Effect by Data Combination. (arXiv:2109.05175v2 [stat.ML] UPDATED)
135. DynSTGAT: Dynamic Spatial-Temporal Graph Attention Network for Traffic Signal Control. (arXiv:2109.05491v2 [cs.LG] UPDATED)
136. CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation. (arXiv:2109.06165v4 [cs.CV] UPDATED)
137. Improving Autoencoder Training Performance for Hyperspectral Unmixing with Network Reinitialisation. (arXiv:2109.13748v2 [eess.IV] UPDATED)
138. Learning Material Parameters and Hydrodynamics of Soft Robotic Fish via Differentiable Simulation. (arXiv:2109.14855v2 [cs.RO] UPDATED)
139. Is Policy Learning Overrated?: Width-Based Planning and Active Learning for Atari. (arXiv:2109.15310v2 [cs.AI] UPDATED)
140. Co-training an Unsupervised Constituency Parser with Weak Supervision. (arXiv:2110.02283v2 [cs.CL] UPDATED)
141. On the Generalization of Models Trained with SGD: Information-Theoretic Bounds and Implications. (arXiv:2110.03128v2 [cs.LG] UPDATED)
142. EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits. (arXiv:2110.03177v6 [cs.LG] UPDATED)
143. Towards Learning (Dis)-Similarity of Source Code from Program Contrasts. (arXiv:2110.03868v2 [cs.PL] UPDATED)
144. SCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition. (arXiv:2110.04187v2 [eess.AS] UPDATED)
145. The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v3 [cs.CL] UPDATED)
146. Phase Collapse in Neural Networks. (arXiv:2110.05283v2 [cs.LG] UPDATED)
147. Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v4 [cs.CL] UPDATED)
148. Dict-BERT: Enhancing Language Model Pre-training with Dictionary. (arXiv:2110.06490v2 [cs.CL] UPDATED)
149. Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and Partial Differential Equation in a Loop. (arXiv:2110.07584v2 [cs.LG] UPDATED)
150. Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference. (arXiv:2110.10031v2 [cs.LG] UPDATED)
151. Progressive Learning for Stabilizing Label Selection in Speech Separation with Mapping-based Method. (arXiv:2110.10593v2 [cs.SD] UPDATED)
152. Stochastic Learning Rate Optimization in the Stochastic Approximation and Online Learning Settings. (arXiv:2110.10710v2 [math.OC] UPDATED)
153. Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness. (arXiv:2110.10942v2 [cs.LG] UPDATED)
154. Fair Enough: Searching for Sufficient Measures of Fairness. (arXiv:2110.13029v2 [cs.LG] UPDATED)
155. Which Model to Trust: Assessing the Influence of Models on the Performance of Reinforcement Learning Algorithms for Continuous Control Tasks. (arXiv:2110.13079v2 [cs.LG] UPDATED)
156. Learning-Augmented $k$-means Clustering. (arXiv:2110.14094v2 [cs.LG] UPDATED)
157. How Well Does Kohn-Sham Regularizer Work for Weakly Correlated Systems?. (arXiv:2110.14846v4 [physics.chem-ph] UPDATED)
158. A Novel Sleep Stage Classification Using CNN Generated by an Efficient Neural Architecture Search with a New Data Processing Trick. (arXiv:2110.15277v3 [eess.SP] UPDATED)
159. Bayesian Sequential Optimal Experimental Design for Nonlinear Models Using Policy Gradient Reinforcement Learning. (arXiv:2110.15335v2 [cs.LG] UPDATED)
160. Exponential escape efficiency of SGD from sharp minima in non-stationary regime. (arXiv:2111.04004v2 [cs.LG] UPDATED)
161. Plumber: Diagnosing and Removing Performance Bottlenecks in Machine Learning Data Pipelines. (arXiv:2111.04131v2 [cs.LG] UPDATED)
162. Unsupervised Part Discovery from Contrastive Reconstruction. (arXiv:2111.06349v2 [cs.CV] UPDATED)
163. Neural Capacity Estimators: How Reliable Are They?. (arXiv:2111.07401v4 [cs.IT] UPDATED)
164. Data-driven discoveries of B\"acklund transforms and soliton evolution equations via deep neural network learning schemes. (arXiv:2111.09489v2 [cs.LG] UPDATED)
165. Implicit Acoustic Echo Cancellation for Keyword Spotting and Device-Directed Speech Detection. (arXiv:2111.10639v3 [cs.SD] UPDATED)
166. Benchmarking emergency department triage prediction models with machine learning and large public electronic health records. (arXiv:2111.11017v2 [cs.LG] UPDATED)
167. Physics Informed Neural Networks for Control Oriented Thermal Modeling of Buildings. (arXiv:2111.12066v2 [eess.SP] UPDATED)
168. SARS-CoV-2 Dissemination using a Network of the United States Counties. (arXiv:2111.13723v2 [cs.SI] UPDATED)
169. Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources. (arXiv:2112.00061v3 [cs.CV] UPDATED)
170. Training Experimentally Robust and Interpretable Binarized Regression Models Using Mixed-Integer Programming. (arXiv:2112.00434v2 [cs.LG] UPDATED)
171. Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent Flows. (arXiv:2112.01363v2 [math.OC] UPDATED)
172. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting. (arXiv:2112.01518v2 [cs.CV] UPDATED)
173. Probing Linguistic Information For Logical Inference In Pre-trained Language Models. (arXiv:2112.01753v2 [cs.CL] UPDATED)
174. DANets: Deep Abstract Networks for Tabular Data Classification and Regression. (arXiv:2112.02962v3 [cs.LG] UPDATED)
175. Graph Neural Networks Accelerated Molecular Dynamics. (arXiv:2112.03383v2 [cs.LG] UPDATED)
176. A systematic approach to random data augmentation on graph neural networks. (arXiv:2112.04314v2 [cs.LG] UPDATED)
177. Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction. (arXiv:2112.05146v2 [eess.IV] UPDATED)
178. Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v3 [cs.CV] UPDATED)
179. Towards General and Efficient Active Learning. (arXiv:2112.07963v2 [cs.CV] UPDATED)
180. Efficient Strong Scaling Through Burst Parallel Training. (arXiv:2112.10065v2 [cs.DC] UPDATED)
181. Learning Positional Embeddings for Coordinate-MLPs. (arXiv:2112.11577v2 [cs.LG] UPDATED)
182. Transformer Embeddings of Irregularly Spaced Events and Their Participants. (arXiv:2201.00044v2 [cs.LG] UPDATED)
183. Linear Variational State-Space Filtering. (arXiv:2201.01353v3 [cs.LG] UPDATED)
184. Mirror Learning: A Unifying Framework of Policy Optimisation. (arXiv:2201.02373v9 [cs.LG] UPDATED)
185. Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay. (arXiv:2201.03019v2 [cs.LG] UPDATED)
186. A Unified Granular-ball Learning Model of Pawlak Rough Set and Neighborhood Rough Set. (arXiv:2201.03349v3 [cs.AI] UPDATED)
187. Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip Connections and Hybrid Learning. (arXiv:2201.05585v2 [cs.CV] UPDATED)
188. Differentially Private Reinforcement Learning with Linear Function Approximation. (arXiv:2201.07052v2 [cs.LG] UPDATED)
189. Real-Time Seizure Detection using EEG: A Comprehensive Comparison of Recent Approaches under a Realistic Setting. (arXiv:2201.08780v2 [cs.LG] UPDATED)
190. Learning Neural Contextual Bandits Through Perturbed Rewards. (arXiv:2201.09910v2 [cs.LG] UPDATED)
191. Optimal estimation of Gaussian DAG models. (arXiv:2201.10548v2 [math.ST] UPDATED)
192. Adaptive Resonance Theory-based Topological Clustering with a Divisive Hierarchical Structure Capable of Continual Learning. (arXiv:2201.10713v3 [cs.LG] UPDATED)
193. Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers. (arXiv:2201.11870v2 [cs.CL] UPDATED)
194. Rapid determination of protein resonance assignments and three-dimensional structures from raw NMR spectra. (arXiv:2201.12041v3 [q-bio.BM] UPDATED)
195. Automated Feature Extraction on AsMap for Emotion Classification using EEG. (arXiv:2201.12055v2 [cs.LG] UPDATED)
196. Explaining Reinforcement Learning Policies through Counterfactual Trajectories. (arXiv:2201.12462v2 [cs.LG] UPDATED)
197. Directed Weight Neural Networks for Protein Structure Representation Learning. (arXiv:2201.13299v2 [q-bio.BM] UPDATED)
198. Learning Features with Parameter-Free Layers. (arXiv:2202.02777v2 [cs.CV] UPDATED)
199. Locating and Editing Factual Knowledge in GPT. (arXiv:2202.05262v2 [cs.CL] UPDATED)
200. A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications. (arXiv:2202.07893v2 [cs.LG] UPDATED)
201. Automated Data Augmentations for Graph Classification. (arXiv:2202.13248v2 [cs.LG] UPDATED)
202. Federated Online Sparse Decision Making. (arXiv:2202.13448v2 [stat.ML] UPDATED)
203. LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation. (arXiv:2202.13590v2 [cs.CL] UPDATED)
204. TRILLsson: Distilled Universal Paralinguistic Speech Representations. (arXiv:2203.00236v2 [eess.AS] UPDATED)
205. DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with guttae. (arXiv:2203.01882v3 [eess.IV] UPDATED)
206. BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation. (arXiv:2203.02533v2 [eess.IV] UPDATED)
207. Scaling R-GCN Training with Graph Summarization. (arXiv:2203.02622v2 [cs.LG] UPDATED)
208. Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer. (arXiv:2203.02794v2 [cs.LG] UPDATED)
209. Differentially Private Federated Learning with Local Regularization and Sparsification. (arXiv:2203.03106v3 [cs.LG] UPDATED)
210. Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v2 [cs.CV] UPDATED)
211. Leveraging Smooth Attention Prior for Multi-Agent Trajectory Prediction. (arXiv:2203.04421v2 [cs.LG] UPDATED)
212. Dimensionality Reduction and Prioritized Exploration for Policy Search. (arXiv:2203.04791v2 [cs.LG] UPDATED)
213. No Free Lunch Theorem for Security and Utility in Federated Learning. (arXiv:2203.05816v2 [cs.LG] UPDATED)
214. Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice. (arXiv:2203.06462v2 [cs.LG] UPDATED)
215. Attention based Memory video portrait matting. (arXiv:2203.06890v2 [cs.CV] UPDATED)
216. Distributed-Memory Sparse Kernels for Machine Learning. (arXiv:2203.07673v2 [cs.DC] UPDATED)
217. Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels. (arXiv:2203.07788v2 [cs.LG] UPDATED)
218. Amortised inference of fractional Brownian motion with linear computational complexity. (arXiv:2203.07961v2 [cs.LG] UPDATED)
219. Surrogate Gap Minimization Improves Sharpness-Aware Training. (arXiv:2203.08065v2 [cs.LG] UPDATED)
220. On the Usefulness of the Fit-on-the-Test View on Evaluating Calibration of Classifiers. (arXiv:2203.08958v2 [cs.LG] UPDATED)
221. Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training. (arXiv:2203.08959v2 [cs.LG] UPDATED)
222. AI Autonomy: Self-Initiation, Adaptation and Continual Learning. (arXiv:2203.08994v2 [cs.AI] UPDATED)
223. DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications. (arXiv:2203.09096v2 [cs.LG] UPDATED)
224. Context-Dependent Anomaly Detection with Knowledge Graph Embedding Models. (arXiv:2203.09354v2 [cs.LG] UPDATED)
225. Semi-Markov Offline Reinforcement Learning for Healthcare. (arXiv:2203.09365v2 [cs.LG] UPDATED)
226. A Stochastic Halpern Iteration with Variance Reduction for Stochastic Monotone Inclusion Problems. (arXiv:2203.09436v2 [math.OC] UPDATED)
227. Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v2 [cs.CL] UPDATED)
228. Multi-Modal Causal Inference with Deep Structural Equation Models. (arXiv:2203.09672v2 [cs.LG] UPDATED)
229. Recent Advances in Heterogeneous Relation Learning for Recommendation. (arXiv:2110.03455v1 [cs.IR] CROSS LISTED)
230. Sentiment Analysis of Microblogging dataset on Coronavirus Pandemic. (arXiv:2111.09275v1 [cs.SI] CROSS LISTED)
## cs.AI
---
**111** new papers in cs.AI:-) 
1. Deep Reinforcement Learning Guided Graph Neural Networks for Brain Network Analysis. (arXiv:2203.10093v1 [cs.LG])
2. Probing Factually Grounded Content Transfer with Factual Ablation. (arXiv:2203.10133v1 [cs.CL])
3. Learning Compressed Embeddings for On-Device Inference. (arXiv:2203.10135v1 [cs.LG])
4. AI system for fetal ultrasound in low-resource settings. (arXiv:2203.10139v1 [cs.LG])
5. Infinite-Horizon Reach-Avoid Zero-Sum Games via Deep Reinforcement Learning. (arXiv:2203.10142v1 [eess.SY])
6. Improving Heuristic-based Process Discovery Methods by Detecting Optimal Dependency Graphs. (arXiv:2203.10145v1 [cs.AI])
7. A Closer Look at Knowledge Distillation with Features, Logits, and Gradients. (arXiv:2203.10163v1 [cs.LG])
8. Privacy-Preserving Reinforcement Learning Beyond Expectation. (arXiv:2203.10165v1 [cs.LG])
9. Analysis and Adaptation of YOLOv4 for Object Detection in Aerial Images. (arXiv:2203.10194v1 [cs.CV])
10. Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v1 [cs.CV])
11. Representation-Agnostic Shape Fields. (arXiv:2203.10259v1 [cs.CV])
12. FaiRR: Faithful and Robust Deductive Reasoning over Natural Language. (arXiv:2203.10261v1 [cs.CL])
13. DiSECt: A Differentiable Simulator for Parameter Inference and Control in Robotic Cutting. (arXiv:2203.10263v1 [cs.RO])
14. Exploiting Cross Domain Acoustic-to-articulatory Inverted Features For Disordered Speech Recognition. (arXiv:2203.10274v1 [eess.AS])
15. Neural Machine Translation with Phrase-Level Universal Visual Representations. (arXiv:2203.10299v1 [cs.CL])
16. Understanding COVID-19 News Coverage using Medical NLP. (arXiv:2203.10338v1 [cs.CL])
17. Automatic Detection of Entity-Manipulated Text using Factual Knowledge. (arXiv:2203.10343v1 [cs.CL])
18. Meta-Learning for Online Update of Recommender Systems. (arXiv:2203.10354v1 [cs.IR])
19. On Robust Prefix-Tuning for Text Classification. (arXiv:2203.10378v1 [cs.CL])
20. Lazy Rearrangement Planning in Confined Spaces. (arXiv:2203.10379v1 [cs.RO])
21. Data Smells: Categories, Causes and Consequences, and Detection of Suspicious Data in AI-based Systems. (arXiv:2203.10384v1 [cs.SE])
22. Anomaly Detection in Emails using Machine Learning and Header Information. (arXiv:2203.10408v1 [cs.CR])
23. Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v1 [cs.CV])
24. A Study on Robustness to Perturbations for Representations of Environmental Sound. (arXiv:2203.10425v1 [cs.SD])
25. STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation. (arXiv:2203.10426v1 [cs.CL])
26. PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication. (arXiv:2203.10428v1 [cs.LG])
27. Interpretability of Fine-grained Classification of Sadness and Depression. (arXiv:2203.10432v1 [cs.CL])
28. Quantum Multi-Agent Reinforcement Learning via Variational Quantum Circuit Design. (arXiv:2203.10443v1 [quant-ph])
29. On the Computation of Necessary and Sufficient Explanations. (arXiv:2203.10451v1 [cs.AI])
30. Partitioning Image Representation in Contrastive Learning. (arXiv:2203.10454v1 [cs.CV])
31. {Unidirectional Thin Adapter for Efficient Adaptation of Deep Neural Networks. (arXiv:2203.10463v1 [cs.CV])
32. Federated Spatial Reuse Optimization in Next-Generation Decentralized IEEE 802.11 WLANs. (arXiv:2203.10472v1 [cs.NI])
33. Optimizing Camera Placements for Overlapped Coverage with 3D Camera Projections. (arXiv:2203.10479v1 [cs.CV])
34. Hierarchical Inductive Transfer for Continual Dialogue Learning. (arXiv:2203.10484v1 [cs.CL])
35. Inferring Articulated Rigid Body Dynamics from RGBD Video. (arXiv:2203.10488v1 [cs.RO])
36. Attention Aided CSI Wireless Localization. (arXiv:2203.10506v1 [eess.SP])
37. Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI. (arXiv:2203.10525v1 [cs.AI])
38. End-to-End Video Text Spotting with Transformer. (arXiv:2203.10539v1 [cs.CV])
39. Multi-Agent Terraforming: Efficient Multi-Agent Path Finding via Environment Manipulation. (arXiv:2203.10540v1 [cs.AI])
40. A Learning Convolutional Neural Network Approach for Network Robustness Prediction. (arXiv:2203.10552v1 [eess.SY])
41. Accelerating Integrated Task and Motion Planning with Neural Feasibility Checking. (arXiv:2203.10568v1 [cs.RO])
42. Multi-view Multi-behavior Contrastive Learning in Recommendation. (arXiv:2203.10576v1 [cs.IR])
43. Small Batch Sizes Improve Training of Low-Resource Neural MT. (arXiv:2203.10579v1 [cs.CL])
44. Neuro-physical dynamic load modeling using differentiable parametric optimization. (arXiv:2203.10582v1 [eess.SY])
45. Qualia as physical measurements: a mathematical model of qualia and pure concepts. (arXiv:2203.10602v1 [physics.hist-ph])
46. Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations. (arXiv:2002.01080v4 [cs.AI] UPDATED)
47. Learning Models for Actionable Recourse. (arXiv:2011.06146v3 [cs.LG] UPDATED)
48. Deep Depression Prediction on Longitudinal Data via Joint Anomaly Ranking and Classification. (arXiv:2012.02950v2 [cs.LG] UPDATED)
49. Bayesian Deep Learning for Segmentation for Autonomous Safe Planetary Landing. (arXiv:2102.10545v2 [cs.RO] UPDATED)
50. Hippocampal formation-inspired probabilistic generative model. (arXiv:2103.07356v3 [cs.AI] UPDATED)
51. Learning Accurate Business Process Simulation Models from Event Logs via Automated Process Discovery and Deep Learning. (arXiv:2103.11944v2 [cs.AI] UPDATED)
52. Towards Understanding Generalization via Decomposing Excess Risk Dynamics. (arXiv:2106.06153v3 [cs.LG] UPDATED)
53. Online Continual Adaptation with Active Self-Training. (arXiv:2106.06526v2 [cs.LG] UPDATED)
54. The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v2 [cs.LG] UPDATED)
55. Ethics Sheets for AI Tasks. (arXiv:2107.01183v4 [cs.AI] UPDATED)
56. Learning Altruistic Behaviours in Reinforcement Learning without External Rewards. (arXiv:2107.09598v4 [cs.AI] UPDATED)
57. Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v7 [cs.CV] UPDATED)
58. Memristive Charge-Flux Interaction Still Makes It Possible To Find An Ideal Memristor. (arXiv:2108.05708v2 [cond-mat.mes-hall] UPDATED)
59. SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v4 [cs.CV] UPDATED)
60. Convolutional Neural Networks Demystified: A Matched Filtering Perspective Based Tutorial. (arXiv:2108.11663v2 [cs.IT] UPDATED)
61. It is AI's Turn to Ask Humans a Question: Question-Answer Pair Generation for Children's Story Books. (arXiv:2109.03423v3 [cs.CL] UPDATED)
62. Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation. (arXiv:2109.04200v2 [cs.IR] UPDATED)
63. Enhancing Unsupervised Anomaly Detection with Score-Guided Network. (arXiv:2109.04684v2 [cs.LG] UPDATED)
64. Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v3 [cs.CL] UPDATED)
65. Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v3 [cs.CL] UPDATED)
66. Abstraction, Reasoning and Deep Learning: A Study of the "Look and Say" Sequence. (arXiv:2109.12755v2 [cs.AI] UPDATED)
67. Is Policy Learning Overrated?: Width-Based Planning and Active Learning for Atari. (arXiv:2109.15310v2 [cs.AI] UPDATED)
68. Co-training an Unsupervised Constituency Parser with Weak Supervision. (arXiv:2110.02283v2 [cs.CL] UPDATED)
69. Towards Learning (Dis)-Similarity of Source Code from Program Contrasts. (arXiv:2110.03868v2 [cs.PL] UPDATED)
70. Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v4 [cs.CL] UPDATED)
71. Dict-BERT: Enhancing Language Model Pre-training with Dictionary. (arXiv:2110.06490v2 [cs.CL] UPDATED)
72. Open Domain Question Answering with A Unified Knowledge Interface. (arXiv:2110.08417v2 [cs.CL] UPDATED)
73. RefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses. (arXiv:2111.00962v3 [cs.SD] UPDATED)
74. RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented Structural Neural Encoders. (arXiv:2111.10545v2 [cs.CL] UPDATED)
75. Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent Flows. (arXiv:2112.01363v2 [math.OC] UPDATED)
76. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting. (arXiv:2112.01518v2 [cs.CV] UPDATED)
77. Probing Linguistic Information For Logical Inference In Pre-trained Language Models. (arXiv:2112.01753v2 [cs.CL] UPDATED)
78. DANets: Deep Abstract Networks for Tabular Data Classification and Regression. (arXiv:2112.02962v3 [cs.LG] UPDATED)
79. A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion. (arXiv:2112.03530v4 [cs.CV] UPDATED)
80. A systematic approach to random data augmentation on graph neural networks. (arXiv:2112.04314v2 [cs.LG] UPDATED)
81. Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v3 [cs.CV] UPDATED)
82. Bridging between Cognitive Processing Signals and Linguistic Features via a Unified Attentional Network. (arXiv:2112.08831v2 [cs.CL] UPDATED)
83. CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v3 [cs.CV] UPDATED)
84. FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition. (arXiv:2112.12496v3 [cs.CV] UPDATED)
85. Transformer Embeddings of Irregularly Spaced Events and Their Participants. (arXiv:2201.00044v2 [cs.LG] UPDATED)
86. Mirror Learning: A Unifying Framework of Policy Optimisation. (arXiv:2201.02373v9 [cs.LG] UPDATED)
87. Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay. (arXiv:2201.03019v2 [cs.LG] UPDATED)
88. A Unified Granular-ball Learning Model of Pawlak Rough Set and Neighborhood Rough Set. (arXiv:2201.03349v3 [cs.AI] UPDATED)
89. Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v2 [cs.CV] UPDATED)
90. Optimal estimation of Gaussian DAG models. (arXiv:2201.10548v2 [math.ST] UPDATED)
91. Explaining Reinforcement Learning Policies through Counterfactual Trajectories. (arXiv:2201.12462v2 [cs.LG] UPDATED)
92. Computational Complexity of Segmentation. (arXiv:2201.13106v3 [cs.AI] UPDATED)
93. Directed Weight Neural Networks for Protein Structure Representation Learning. (arXiv:2201.13299v2 [q-bio.BM] UPDATED)
94. Enhanced Multi-Objective A* Using Balanced Binary Search Trees. (arXiv:2202.08992v2 [cs.AI] UPDATED)
95. RIConv++: Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. (arXiv:2202.13094v2 [cs.CV] UPDATED)
96. Automated Data Augmentations for Graph Classification. (arXiv:2202.13248v2 [cs.LG] UPDATED)
97. Rethinking and Refining the Distinct Metric. (arXiv:2202.13587v2 [cs.CL] UPDATED)
98. A Conformer Based Acoustic Model for Robust Automatic Speech Recognition. (arXiv:2203.00725v2 [cs.SD] UPDATED)
99. BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation. (arXiv:2203.02533v2 [eess.IV] UPDATED)
100. Scaling R-GCN Training with Graph Summarization. (arXiv:2203.02622v2 [cs.LG] UPDATED)
101. Just Rank: Rethinking Evaluation with Word and Sentence Similarities. (arXiv:2203.02679v2 [cs.CL] UPDATED)
102. A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v3 [cs.CL] UPDATED)
103. PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR. (arXiv:2203.06319v3 [cs.CV] UPDATED)
104. Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v3 [cs.CV] UPDATED)
105. Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning. (arXiv:2203.07782v2 [cs.AI] UPDATED)
106. Surrogate Gap Minimization Improves Sharpness-Aware Training. (arXiv:2203.08065v2 [cs.LG] UPDATED)
107. On the Usefulness of the Fit-on-the-Test View on Evaluating Calibration of Classifiers. (arXiv:2203.08958v2 [cs.LG] UPDATED)
108. AI Autonomy: Self-Initiation, Adaptation and Continual Learning. (arXiv:2203.08994v2 [cs.AI] UPDATED)
109. Context-Dependent Anomaly Detection with Knowledge Graph Embedding Models. (arXiv:2203.09354v2 [cs.LG] UPDATED)
110. Towards Data-Efficient Detection Transformers. (arXiv:2203.09507v2 [cs.CV] UPDATED)
111. HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v1 [cs.CV] CROSS LISTED)

