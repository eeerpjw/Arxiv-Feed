# Your interest papers
---
## cs.CV
---
### Designing an Efficient End-to-end Machine Learning Pipeline for **Real-time** Empty-shelf Detection. (arXiv:2205.13060v1 [cs.LG])
- Authors : Dipendra Jha, Ata Mahjoubfar, Anupama Joshi
- Link : [http://arxiv.org/abs/2205.13060](http://arxiv.org/abs/2205.13060)
> ABSTRACT  :  On-Shelf Availability (OSA) of products in retail stores is a critical business criterion in the fast moving consumer goods and retails sector. When a product is out-of-stock (OOS) and a customer cannot find it on its designed shelf, this causes a negative impact on the customer's behaviors and future demands. Several methods are being adopted by retailers today to detect empty shelves and ensure high OSA of products; however, such methods are generally ineffective and infeasible since they are either manual, expensive or less accurate. Recently machine learning based solutions have been proposed, but they suffer from high computation cost and low accuracy problem due to lack of large annotated datasets of on-shelf products. Here, we present an elegant approach for designing an end-to-end machine learning (ML) pipeline for real-time empty shelf detection. Considering the strong dependency between the quality of ML models and the quality of data, we focus on the importance of proper data collection, cleaning and correct data annotation before delving into modeling. Since an empty-shelf detection solution should be computationally-efficient for real-time predictions, we explore different run-time optimizations to improve the model performance. Our dataset contains 1000 images, collected and annotated by following well-defined guidelines. Our low-latency model achieves a mean average F1-score of 68.5%, and can process up to 67 images/s on Intel Xeon Gold and up to 860 images/s on an A100 GPU. Our annotated dataset is publicly available along with our optimized models.  
### **Swin**VRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation. (arXiv:2205.13158v1 [cs.CV])
- Authors : Yuan Hu, Lei Chen, Zhibin Wang, Hao Li
- Link : [http://arxiv.org/abs/2205.13158](http://arxiv.org/abs/2205.13158)
> ABSTRACT  :  Data-driven approaches for medium-range weather forecasting are recently shown extraordinarily promising for ensemble forecasting for their fast inference speed compared to traditional numerical weather prediction (NWP) models, but their forecast accuracy can hardly match the state-of-the-art operational ECMWF Integrated Forecasting System (IFS) model. Previous data-driven attempts achieve ensemble forecast using some simple perturbation methods, like initial condition perturbation and Monte Carlo dropout. However, they mostly suffer unsatisfactory ensemble performance, which is arguably attributed to the sub-optimal ways of applying perturbation. We propose a **Swin** Transformer-based Variational Recurrent Neural Network (**Swin**VRNN), which is a stochastic weather forecasting model combining a **Swin**RNN predictor with a perturbation module. **Swin**RNN is designed as a **Swin** Transformer-based recurrent neural network, which predicts future states deterministically. Furthermore, to model the stochasticity in prediction, we design a perturbation module following the Variational Auto-Encoder paradigm to learn multivariate Gaussian distributions of a time-variant stochastic latent variable from data. Ensemble forecasting can be easily achieved by perturbing the model features leveraging noise sampled from the learned distribution. We also compare four categories of perturbation methods for ensemble forecasting, i.e. fixed distribution perturbation, learned distribution perturbation, MC dropout, and multi model ensemble. Comparisons on WeatherBench dataset show the learned distribution perturbation method using our **Swin**VRNN model achieves superior forecast accuracy and reasonable ensemble spread due to joint optimization of the two targets. More notably, **Swin**VRNN surpasses operational IFS on surface variables of 2-m temperature and 6-hourly total precipitation at all lead times up to five days.  
### Light Field Raindrop Removal via 4D Re-sampling. (arXiv:2205.13165v1 [cs.CV])
- Authors : Dong Jing, Shuo Zhang, Song Chang, Youfang Lin
- Link : [http://arxiv.org/abs/2205.13165](http://arxiv.org/abs/2205.13165)
> ABSTRACT  :  The Light Field Raindrop Removal (LFRR) aims to restore the background areas obscured by raindrops in the Light Field (LF). Compared with single image, the LF provides more abundant information by regularly and densely sampling the scene. Since raindrops have larger disparities than the background in the LF, the majority of texture details occluded by raindrops are visible in other views. In this paper, we propose a novel LFRR network by directly utilizing the complementary pixel information of raindrop-free areas in the input raindrop LF, which consists of the re-sampling module and the refinement module. Specifically, the re-sampling module generates a new LF which is less polluted by raindrops through re-sampling position predictions and the proposed 4D interpolation. The refinement module improves the **restoration** of the completely occluded background areas and corrects the pixel error caused by 4D interpolation. Furthermore, we carefully build the first real scene LFRR dataset for model training and validation. Experiments demonstrate that the proposed method can effectively remove raindrops and achieves state-of-the-art performance in both background **restoration** and view consistency maintenance.  
### Measuring Perceptual Color Differences of Smartphone Photography. (arXiv:2205.13489v1 [cs.CV])
- Authors : Zhihua Wang, Keshuo Xu, Yang Yang, Jianlei Dong, Shuhang Gu, Lihao Xu, Yuming Fang, **Kede Ma**
- Link : [http://arxiv.org/abs/2205.13489](http://arxiv.org/abs/2205.13489)
> ABSTRACT  :  Measuring perceptual color differences (CDs) is of great importance in modern smartphone photography. Despite the long history, most CD measures have been constrained by psychophysical data of homogeneous color patches or a limited number of simplistic natural images. It is thus questionable whether existing CD measures generalize in the age of smartphone photography characterized by greater content complexities and learning-based image signal processors. In this paper, we put together so far the largest image dataset for perceptual CD assessment, in which the natural images are 1) captured by six flagship smartphones, 2) altered by Photoshop, 3) post-processed by built-in filters of the smartphones, and 4) reproduced with incorrect color profiles. We then conduct a large-scale psychophysical experiment to gather perceptual CDs of 30,000 image pairs in a carefully controlled laboratory environment. Based on the newly established dataset, we make one of the first attempts to construct an end-to-end learnable CD formula based on a lightweight neural network, as a generalization of several previous metrics. Extensive experiments demonstrate that the optimized formula outperforms 28 existing CD measures by a large margin, offers reasonable local CD maps without the use of dense supervision, generalizes well to color patch data, and empirically behaves as a proper metric in the mathematical sense.  
### Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v1 [cs.CV])
- Authors : Lang Huang, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Toshihiko Yamasaki
- Link : [http://arxiv.org/abs/2205.13515](http://arxiv.org/abs/2205.13515)
> ABSTRACT  :  We present an efficient approach for Masked Image Modeling (MIM) with hierarchical Vision Transformers (ViTs), e.g., **Swin** Transformer, allowing the hierarchical ViTs to discard masked patches and operate only on the visible ones. Our approach consists of two key components. First, for the window attention, we design a Group Window Attention scheme following the Divide-and-Conquer strategy. To mitigate the quadratic complexity of the self-attention w.r.t. the number of patches, group attention encourages a uniform partition that visible patches within each local window of arbitrary size can be grouped with equal size, where masked self-attention is then performed within each group. Second, we further improve the grouping strategy via the Dynamic Programming algorithm to minimize the overall computation cost of the attention on the grouped patches. As a result, MIM now can work on hierarchical ViTs in a green and efficient way. For example, we can train the hierarchical ViTs about 2.7$\times$ faster and reduce the GPU memory usage by 70%, while still enjoying competitive performance on ImageNet classification and the superiority on downstream COCO object detection benchmarks. Code and pre-trained models have been made publicly available at https://github.com/LayneH/GreenMIM.  
### Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v1 [cs.CV])
- Authors : Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, Yue Cao
- Link : [http://arxiv.org/abs/2205.13543](http://arxiv.org/abs/2205.13543)
> ABSTRACT  :  Masked image modeling (MIM) as pre-training is shown to be effective for numerous vision downstream tasks, but how and where MIM works remain unclear. In this paper, we compare MIM with the long-dominant supervised pre-trained models from two perspectives, the visualizations and the experiments, to uncover their key representational differences. From the visualizations, we find that MIM brings locality inductive bias to all layers of the trained models, but supervised models tend to focus locally at lower layers but more globally at higher layers. That may be the reason why MIM helps Vision Transformers that have a very large receptive field to optimize. Using MIM, the model can maintain a large diversity on attention heads in all layers. But for supervised models, the diversity on attention heads almost disappears from the last three layers and less diversity harms the fine-tuning performance. From the experiments, we find that MIM models can perform significantly better on geometric and motion tasks with weak semantics or fine-grained classification tasks, than their supervised counterparts. Without bells and whistles, a standard MIM pre-trained **Swin**V2-L could achieve state-of-the-art performance on pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the categories are sufficiently covered by the supervised pre-training, MIM models can still achieve highly competitive transfer performance. With a deeper understanding of MIM, we hope that our work can inspire new and solid research in this direction.  
### Continual Learning for Blind Image Quality Assessment. (arXiv:2102.09717v2 [cs.CV] UPDATED)
- Authors : Weixia Zhang, Dingquan Li, Chao Ma, Guangtao Zhai, Xiaokang Yang, **Kede Ma**
- Link : [http://arxiv.org/abs/2102.09717](http://arxiv.org/abs/2102.09717)
> ABSTRACT  :  The explosive growth of image data facilitates the fast development of image processing and computer vision methods for emerging visual applications, meanwhile introducing novel distortions to the processed images. This poses a grand challenge to existing blind image quality assessment (BIQA) models, failing to continually adapt to such subpopulation shift. Recent work suggests training BIQA methods on the combination of all available human-rated IQA datasets. However, this type of approach is not scalable to a large number of datasets, and is cumbersome to incorporate a newly created dataset as well. In this paper, we formulate continual learning for BIQA, where a model learns continually from a stream of IQA datasets, building on what was learned from previously seen data. We first identify five desiderata in the new setting with a measure to quantify the plasticity-stability trade-off. We then propose a simple yet effective method for learning BIQA models continually. Specifically, based on a shared backbone network, we add a prediction head for a new dataset, and enforce a regularizer to allow all prediction heads to evolve with new data while being resistant to catastrophic forgetting of old data. We compute the quality score by an adaptive weighted summation of estimates from all prediction heads. Extensive experiments demonstrate the promise of the proposed continual learning method in comparison to standard training techniques for BIQA. We made the code publicly available at https://github.com/zwx8981/BIQA_CL.  
### What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions. (arXiv:2204.00746v2 [cs.CV] UPDATED)
- Authors : Hao Chen, Kaustav Kundu, Xinyu Li, Joseph Tighe, Davide Modolo
- Link : [http://arxiv.org/abs/2204.00746](http://arxiv.org/abs/2204.00746)
> ABSTRACT  :  We propose a novel one-stage Transformer-based semantic and spatial refined transformer (SSRT) to solve the Human-Object Interaction detection task, which requires to localize humans and objects, and predicts their interactions. Differently from previous Transformer-based HOI approaches, which mostly focus at improving the design of the decoder outputs for the final detection, SSRT introduces two new modules to help select the most relevant object-action pairs within an image and refine the queries' representation using rich semantic and spatial features. These **enhancement**s lead to state-of-the-art results on the two most popular HOI benchmarks: V-COCO and HICO-DET.  
### Inception Transformer. (arXiv:2205.12956v2 [cs.CV] UPDATED)
- Authors : Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, Shuicheng Yan
- Link : [http://arxiv.org/abs/2205.12956](http://arxiv.org/abs/2205.12956)
> ABSTRACT  :  Recent studies show that Transformer has strong capability of building long-range dependencies, yet is incompetent in capturing high frequencies that predominantly convey local information. To tackle this issue, we present a novel and general-purpose Inception Transformer, or iFormer for short, that effectively learns comprehensive features with both high- and low-frequency information in visual data. Specifically, we design an Inception mixer to explicitly graft the advantages of convolution and max-pooling for capturing the high-frequency information to Transformers. Different from recent hybrid frameworks, the Inception mixer brings greater efficiency through a channel splitting mechanism to adopt parallel convolution/max-pooling path and self-attention path as high- and low-frequency mixers, while having the flexibility to model discriminative information scattered within a wide frequency range. Considering that bottom layers play more roles in capturing high-frequency details while top layers more in modeling low-frequency global information, we further introduce a frequency ramp structure, i.e. gradually decreasing the dimensions fed to the high-frequency mixer and increasing those to the low-frequency mixer, which can effectively trade-off high- and low-frequency components across different layers. We benchmark the iFormer on a series of vision tasks, and showcase that it achieves impressive performance on image classification, COCO detection and ADE20K segmentation. For example, our iFormer-S hits the top-1 accuracy of 83.4% on ImageNet-1K, much higher than DeiT-S by 3.6%, and even slightly better than much bigger model **Swin**-B (83.3%) with only 1/4 parameters and 1/3 FLOPs. Code and models will be released at https://github.com/sail-sg/iFormer.  
## eess.IV
---
### Light Field Raindrop Removal via 4D Re-sampling. (arXiv:2205.13165v1 [cs.CV])
- Authors : Dong Jing, Shuo Zhang, Song Chang, Youfang Lin
- Link : [http://arxiv.org/abs/2205.13165](http://arxiv.org/abs/2205.13165)
> ABSTRACT  :  The Light Field Raindrop Removal (LFRR) aims to restore the background areas obscured by raindrops in the Light Field (LF). Compared with single image, the LF provides more abundant information by regularly and densely sampling the scene. Since raindrops have larger disparities than the background in the LF, the majority of texture details occluded by raindrops are visible in other views. In this paper, we propose a novel LFRR network by directly utilizing the complementary pixel information of raindrop-free areas in the input raindrop LF, which consists of the re-sampling module and the refinement module. Specifically, the re-sampling module generates a new LF which is less polluted by raindrops through re-sampling position predictions and the proposed 4D interpolation. The refinement module improves the **restoration** of the completely occluded background areas and corrects the pixel error caused by 4D interpolation. Furthermore, we carefully build the first real scene LFRR dataset for model training and validation. Experiments demonstrate that the proposed method can effectively remove raindrops and achieves state-of-the-art performance in both background **restoration** and view consistency maintenance.  
### Measuring Perceptual Color Differences of Smartphone Photography. (arXiv:2205.13489v1 [cs.CV])
- Authors : Zhihua Wang, Keshuo Xu, Yang Yang, Jianlei Dong, Shuhang Gu, Lihao Xu, Yuming Fang, **Kede Ma**
- Link : [http://arxiv.org/abs/2205.13489](http://arxiv.org/abs/2205.13489)
> ABSTRACT  :  Measuring perceptual color differences (CDs) is of great importance in modern smartphone photography. Despite the long history, most CD measures have been constrained by psychophysical data of homogeneous color patches or a limited number of simplistic natural images. It is thus questionable whether existing CD measures generalize in the age of smartphone photography characterized by greater content complexities and learning-based image signal processors. In this paper, we put together so far the largest image dataset for perceptual CD assessment, in which the natural images are 1) captured by six flagship smartphones, 2) altered by Photoshop, 3) post-processed by built-in filters of the smartphones, and 4) reproduced with incorrect color profiles. We then conduct a large-scale psychophysical experiment to gather perceptual CDs of 30,000 image pairs in a carefully controlled laboratory environment. Based on the newly established dataset, we make one of the first attempts to construct an end-to-end learnable CD formula based on a lightweight neural network, as a generalization of several previous metrics. Extensive experiments demonstrate that the optimized formula outperforms 28 existing CD measures by a large margin, offers reasonable local CD maps without the use of dense supervision, generalizes well to color patch data, and empirically behaves as a proper metric in the mathematical sense.  
### Continual Learning for Blind Image Quality Assessment. (arXiv:2102.09717v2 [cs.CV] UPDATED)
- Authors : Weixia Zhang, Dingquan Li, Chao Ma, Guangtao Zhai, Xiaokang Yang, **Kede Ma**
- Link : [http://arxiv.org/abs/2102.09717](http://arxiv.org/abs/2102.09717)
> ABSTRACT  :  The explosive growth of image data facilitates the fast development of image processing and computer vision methods for emerging visual applications, meanwhile introducing novel distortions to the processed images. This poses a grand challenge to existing blind image quality assessment (BIQA) models, failing to continually adapt to such subpopulation shift. Recent work suggests training BIQA methods on the combination of all available human-rated IQA datasets. However, this type of approach is not scalable to a large number of datasets, and is cumbersome to incorporate a newly created dataset as well. In this paper, we formulate continual learning for BIQA, where a model learns continually from a stream of IQA datasets, building on what was learned from previously seen data. We first identify five desiderata in the new setting with a measure to quantify the plasticity-stability trade-off. We then propose a simple yet effective method for learning BIQA models continually. Specifically, based on a shared backbone network, we add a prediction head for a new dataset, and enforce a regularizer to allow all prediction heads to evolve with new data while being resistant to catastrophic forgetting of old data. We compute the quality score by an adaptive weighted summation of estimates from all prediction heads. Extensive experiments demonstrate the promise of the proposed continual learning method in comparison to standard training techniques for BIQA. We made the code publicly available at https://github.com/zwx8981/BIQA_CL.  
## cs.LG
---
### Designing an Efficient End-to-end Machine Learning Pipeline for **Real-time** Empty-shelf Detection. (arXiv:2205.13060v1 [cs.LG])
- Authors : Dipendra Jha, Ata Mahjoubfar, Anupama Joshi
- Link : [http://arxiv.org/abs/2205.13060](http://arxiv.org/abs/2205.13060)
> ABSTRACT  :  On-Shelf Availability (OSA) of products in retail stores is a critical business criterion in the fast moving consumer goods and retails sector. When a product is out-of-stock (OOS) and a customer cannot find it on its designed shelf, this causes a negative impact on the customer's behaviors and future demands. Several methods are being adopted by retailers today to detect empty shelves and ensure high OSA of products; however, such methods are generally ineffective and infeasible since they are either manual, expensive or less accurate. Recently machine learning based solutions have been proposed, but they suffer from high computation cost and low accuracy problem due to lack of large annotated datasets of on-shelf products. Here, we present an elegant approach for designing an end-to-end machine learning (ML) pipeline for real-time empty shelf detection. Considering the strong dependency between the quality of ML models and the quality of data, we focus on the importance of proper data collection, cleaning and correct data annotation before delving into modeling. Since an empty-shelf detection solution should be computationally-efficient for real-time predictions, we explore different run-time optimizations to improve the model performance. Our dataset contains 1000 images, collected and annotated by following well-defined guidelines. Our low-latency model achieves a mean average F1-score of 68.5%, and can process up to 67 images/s on Intel Xeon Gold and up to 860 images/s on an A100 GPU. Our annotated dataset is publicly available along with our optimized models.  
### Semi-supervised Drifted Stream Learning with Short Lookback. (arXiv:2205.13066v1 [cs.LG])
- Authors : Weijieying Ren, Pengyang Wang, Xiaolin Li, Yanjie Fu
- Link : [http://arxiv.org/abs/2205.13066](http://arxiv.org/abs/2205.13066)
> ABSTRACT  :  In many scenarios, 1) data streams are generated in **real time**; 2) labeled data are expensive and only limited labels are available in the beginning; 3) real-world data is not always i.i.d. and data drift over time gradually; 4) the storage of historical streams is limited and model updating can only be achieved based on a very short lookback window. This learning setting limits the applicability and availability of many Machine Learning (ML) algorithms. We generalize the learning task under such setting as a semi-supervised drifted stream learning with short lookback problem (SDSL). SDSL imposes two under-addressed challenges on existing methods in semi-supervised learning, continuous learning, and domain adaptation: 1) robust pseudo-labeling under gradual shifts and 2) anti-forgetting adaptation with short lookback. To tackle these challenges, we propose a principled and generic generation-replay framework to solve SDSL. The framework is able to accomplish: 1) robust pseudo-labeling in the generation step; 2) anti-forgetting adaption in the replay step. To achieve robust pseudo-labeling, we develop a novel pseudo-label classification model to leverage supervised knowledge of previously labeled data, unsupervised knowledge of new data, and, structure knowledge of invariant label semantics. To achieve adaptive anti-forgetting model replay, we propose to view the anti-forgetting adaptation task as a flat region search problem. We propose a novel minimax game-based replay objective function to solve the flat region search problem and develop an effective optimization solver. Finally, we present extensive experiments to demonstrate our framework can effectively address the task of anti-forgetting learning in drifted streams with short lookback.  
### BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection. (arXiv:2205.13084v1 [cs.LG])
- Authors : Mingxuan Lu, Zhichao Han, Susie Xi, Zitao Zhang, Yang Zhao, Yinan Shan, Ramesh Raghunathan, Ce Zhang, Jiawei Jiang
- Link : [http://arxiv.org/abs/2205.13084](http://arxiv.org/abs/2205.13084)
> ABSTRACT  :  Detecting fraudulent transactions is an essential component to control risk in e-commerce marketplaces. Apart from rule-based and machine learning filters that are already deployed in production, we want to enable efficient real-time inference with graph neural networks (GNNs), which is useful to catch multihop risk propagation in a transaction graph. However, two challenges arise in the implementation of GNNs in production. First, future information in a dynamic graph should not be considered in message passing to predict the past. Second, the latency of graph query and GNN model inference is usually up to hundreds of milliseconds, which is costly for some critical online services. To tackle these challenges, we propose a Batch and **Real-time** Inception GrapH Topology (BRIGHT) framework to conduct an end-to-end GNN learning that allows efficient online real-time inference. BRIGHT framework consists of a graph transformation module (Two-Stage Directed Graph) and a corresponding GNN architecture (Lambda Neural Network). The Two-Stage Directed Graph guarantees that the information passed through neighbors is only from the historical payment transactions. It consists of two subgraphs representing historical relationships and real-time links, respectively. The Lambda Neural Network decouples inference into two stages: batch inference of entity embeddings and real-time inference of transaction prediction. Our experiments show that BRIGHT outperforms the baseline models by &gt;2\% in average w.r.t.~precision. Furthermore, BRIGHT is computationally efficient for real-time fraud detection. Regarding end-to-end performance (including neighbor query and inference), BRIGHT can reduce the P99 latency by &gt;75\%. For the inference stage, our speedup is on average 7.8$\times$ compared to the traditional GNN.  
### Are Transformers Effective for Time Series Forecasting?. (arXiv:2205.13504v1 [cs.AI])
- Authors : Ailing Zeng, Muxi Chen, **Lei Zhang**, Qiang Xu
- Link : [http://arxiv.org/abs/2205.13504](http://arxiv.org/abs/2205.13504)
> ABSTRACT  :  Recently, there has been a surge of Transformer-based solutions for the time series forecasting (TSF) task, especially for the challenging long-term TSF problem. Transformer architecture relies on self-attention mechanisms to effectively extract the semantic correlations between paired elements in a long sequence, which is permutation-invariant and anti-ordering to some extent. However, in time series modeling, we are to extract the temporal relations among an ordering set of continuous points. Consequently, whether Transformer-based techniques are the right solutions for long-term time series forecasting is an interesting problem to investigate, despite the performance improvements shown in these studies. In this work, we question the validity of Transformer-based TSF solutions. In their experiments, the compared (non-Transformer) baselines are mainly autoregressive forecasting solutions, which usually have a poor long-term prediction capability due to inevitable error accumulation effects. In contrast, we use an embarrassingly simple architecture named DLinear that conducts direct multi-step (DMS) forecasting for comparison. DLinear decomposes the time series into a trend and a remainder series and employs two one-layer linear networks to model these two series for the forecasting task. Surprisingly, it outperforms existing complex Transformer-based models in most cases by a large margin. Therefore, we conclude that the relatively higher long-term forecasting accuracy of Transformer-based TSF solutions shown in existing works has little to do with the temporal relation extraction capabilities of the Transformer architecture. Instead, it is mainly due to the non-autoregressive DMS forecasting strategy used in them. We hope this study also advocates revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.  
### Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v1 [cs.CV])
- Authors : Lang Huang, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Toshihiko Yamasaki
- Link : [http://arxiv.org/abs/2205.13515](http://arxiv.org/abs/2205.13515)
> ABSTRACT  :  We present an efficient approach for Masked Image Modeling (MIM) with hierarchical Vision Transformers (ViTs), e.g., **Swin** Transformer, allowing the hierarchical ViTs to discard masked patches and operate only on the visible ones. Our approach consists of two key components. First, for the window attention, we design a Group Window Attention scheme following the Divide-and-Conquer strategy. To mitigate the quadratic complexity of the self-attention w.r.t. the number of patches, group attention encourages a uniform partition that visible patches within each local window of arbitrary size can be grouped with equal size, where masked self-attention is then performed within each group. Second, we further improve the grouping strategy via the Dynamic Programming algorithm to minimize the overall computation cost of the attention on the grouped patches. As a result, MIM now can work on hierarchical ViTs in a green and efficient way. For example, we can train the hierarchical ViTs about 2.7$\times$ faster and reduce the GPU memory usage by 70%, while still enjoying competitive performance on ImageNet classification and the superiority on downstream COCO object detection benchmarks. Code and pre-trained models have been made publicly available at https://github.com/LayneH/GreenMIM.  
### Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v1 [cs.CV])
- Authors : Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, Yue Cao
- Link : [http://arxiv.org/abs/2205.13543](http://arxiv.org/abs/2205.13543)
> ABSTRACT  :  Masked image modeling (MIM) as pre-training is shown to be effective for numerous vision downstream tasks, but how and where MIM works remain unclear. In this paper, we compare MIM with the long-dominant supervised pre-trained models from two perspectives, the visualizations and the experiments, to uncover their key representational differences. From the visualizations, we find that MIM brings locality inductive bias to all layers of the trained models, but supervised models tend to focus locally at lower layers but more globally at higher layers. That may be the reason why MIM helps Vision Transformers that have a very large receptive field to optimize. Using MIM, the model can maintain a large diversity on attention heads in all layers. But for supervised models, the diversity on attention heads almost disappears from the last three layers and less diversity harms the fine-tuning performance. From the experiments, we find that MIM models can perform significantly better on geometric and motion tasks with weak semantics or fine-grained classification tasks, than their supervised counterparts. Without bells and whistles, a standard MIM pre-trained **Swin**V2-L could achieve state-of-the-art performance on pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the categories are sufficiently covered by the supervised pre-training, MIM models can still achieve highly competitive transfer performance. With a deeper understanding of MIM, we hope that our work can inspire new and solid research in this direction.  
### Domain-informed neural networks for interaction localization within astroparticle experiments. (arXiv:2112.07995v2 [hep-ex] UPDATED)
- Authors : Shixiao Liang, Aaron Higuera, Christina Peters, Venkat Roy, Hagit Shatkay
- Link : [http://arxiv.org/abs/2112.07995](http://arxiv.org/abs/2112.07995)
> ABSTRACT  :  This work proposes a domain-informed neural network architecture for experimental particle physics, using particle interaction localization with the time-projection chamber (TPC) technology for **dark** matter research as an example application. A key feature of the signals generated within the TPC is that they allow localization of particle interactions through a process called reconstruction. While multilayer perceptrons (MLPs) have emerged as a leading contender for reconstruction in TPCs, such a black-box approach does not reflect prior knowledge of the underlying scientific processes. This paper looks anew at neural network-based interaction localization and encodes prior detector knowledge, in terms of both signal characteristics and detector geometry, into the feature encoding and the output layers of a multilayer neural network. The resulting Domain-informed Neural Network (DiNN) limits the receptive fields of the neurons in the initial feature encoding layers in order to account for the spatially localized nature of the signals produced within the TPC. This aspect of the DiNN, which has similarities with the emerging area of graph neural networks in that the neurons in the initial layers only connect to a handful of neurons in their succeeding layer, significantly reduces the number of parameters in the network in comparison to an MLP. In addition, in order to account for the detector geometry, the output layers of the network are modified using two geometric transformations to ensure the DiNN produces localizations within the interior of the detector. The end result is a neural network architecture that has 60% fewer parameters than an MLP, but that still achieves similar localization performance and provides a path to future architectural developments with improved performance because of their ability to encode additional domain knowledge into the architecture.  
### Evaluating Generalization in Classical and Quantum Generative Models. (arXiv:2201.08770v2 [cs.LG] UPDATED)
- Authors : Kaitlin Gili, Marta Mauri, Alejandro Perdomo
- Link : [http://arxiv.org/abs/2201.08770](http://arxiv.org/abs/2201.08770)
> ABSTRACT  :  Defining and accurately measuring generalization in generative models remains an ongoing challenge and a topic of active research within the machine learning community. This is in contrast to discriminative models, where there is a clear definition of generalization, i.e., the model's classification accuracy when faced with unseen data. In this work, we construct a simple and unambiguous approach to evaluate the generalization capabilities of generative models. Using the sample-based generalization metrics proposed here, any generative model, from state-of-the-art classical generative models such as GANs to quantum models such as Quantum Circuit Born Machines, can be evaluated on the same ground on a concrete well-defined framework. In contrast to other sample-based metrics for probing generalization, we leverage constrained optimization problems (e.g., cardinality constrained problems) and use these discrete datasets to define specific metrics capable of unambiguously measuring the quality of the samples and the model's generalization capabilities for generating data beyond the training set but still within the valid solution space. Additionally, our metrics can diagnose trainability issues such as mode collapse and overfitting, as we illustrate when comparing GANs to quantum-inspired models built out of tensor networks. Our simulation results show that our quantum-inspired models have up to a $68 \times$ **enhancement** in generating unseen unique and valid samples compared to GANs, and a ratio of 61:2 for generating samples with better quality than those observed in the training set. We foresee these metrics as valuable tools for rigorously defining practical quantum advantage in the domain of generative modeling.  
### Inception Transformer. (arXiv:2205.12956v2 [cs.CV] UPDATED)
- Authors : Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, Shuicheng Yan
- Link : [http://arxiv.org/abs/2205.12956](http://arxiv.org/abs/2205.12956)
> ABSTRACT  :  Recent studies show that Transformer has strong capability of building long-range dependencies, yet is incompetent in capturing high frequencies that predominantly convey local information. To tackle this issue, we present a novel and general-purpose Inception Transformer, or iFormer for short, that effectively learns comprehensive features with both high- and low-frequency information in visual data. Specifically, we design an Inception mixer to explicitly graft the advantages of convolution and max-pooling for capturing the high-frequency information to Transformers. Different from recent hybrid frameworks, the Inception mixer brings greater efficiency through a channel splitting mechanism to adopt parallel convolution/max-pooling path and self-attention path as high- and low-frequency mixers, while having the flexibility to model discriminative information scattered within a wide frequency range. Considering that bottom layers play more roles in capturing high-frequency details while top layers more in modeling low-frequency global information, we further introduce a frequency ramp structure, i.e. gradually decreasing the dimensions fed to the high-frequency mixer and increasing those to the low-frequency mixer, which can effectively trade-off high- and low-frequency components across different layers. We benchmark the iFormer on a series of vision tasks, and showcase that it achieves impressive performance on image classification, COCO detection and ADE20K segmentation. For example, our iFormer-S hits the top-1 accuracy of 83.4% on ImageNet-1K, much higher than DeiT-S by 3.6%, and even slightly better than much bigger model **Swin**-B (83.3%) with only 1/4 parameters and 1/3 FLOPs. Code and models will be released at https://github.com/sail-sg/iFormer.  
## cs.AI
---
### BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection. (arXiv:2205.13084v1 [cs.LG])
- Authors : Mingxuan Lu, Zhichao Han, Susie Xi, Zitao Zhang, Yang Zhao, Yinan Shan, Ramesh Raghunathan, Ce Zhang, Jiawei Jiang
- Link : [http://arxiv.org/abs/2205.13084](http://arxiv.org/abs/2205.13084)
> ABSTRACT  :  Detecting fraudulent transactions is an essential component to control risk in e-commerce marketplaces. Apart from rule-based and machine learning filters that are already deployed in production, we want to enable efficient real-time inference with graph neural networks (GNNs), which is useful to catch multihop risk propagation in a transaction graph. However, two challenges arise in the implementation of GNNs in production. First, future information in a dynamic graph should not be considered in message passing to predict the past. Second, the latency of graph query and GNN model inference is usually up to hundreds of milliseconds, which is costly for some critical online services. To tackle these challenges, we propose a Batch and **Real-time** Inception GrapH Topology (BRIGHT) framework to conduct an end-to-end GNN learning that allows efficient online real-time inference. BRIGHT framework consists of a graph transformation module (Two-Stage Directed Graph) and a corresponding GNN architecture (Lambda Neural Network). The Two-Stage Directed Graph guarantees that the information passed through neighbors is only from the historical payment transactions. It consists of two subgraphs representing historical relationships and real-time links, respectively. The Lambda Neural Network decouples inference into two stages: batch inference of entity embeddings and real-time inference of transaction prediction. Our experiments show that BRIGHT outperforms the baseline models by &gt;2\% in average w.r.t.~precision. Furthermore, BRIGHT is computationally efficient for real-time fraud detection. Regarding end-to-end performance (including neighbor query and inference), BRIGHT can reduce the P99 latency by &gt;75\%. For the inference stage, our speedup is on average 7.8$\times$ compared to the traditional GNN.  
### Are Transformers Effective for Time Series Forecasting?. (arXiv:2205.13504v1 [cs.AI])
- Authors : Ailing Zeng, Muxi Chen, **Lei Zhang**, Qiang Xu
- Link : [http://arxiv.org/abs/2205.13504](http://arxiv.org/abs/2205.13504)
> ABSTRACT  :  Recently, there has been a surge of Transformer-based solutions for the time series forecasting (TSF) task, especially for the challenging long-term TSF problem. Transformer architecture relies on self-attention mechanisms to effectively extract the semantic correlations between paired elements in a long sequence, which is permutation-invariant and anti-ordering to some extent. However, in time series modeling, we are to extract the temporal relations among an ordering set of continuous points. Consequently, whether Transformer-based techniques are the right solutions for long-term time series forecasting is an interesting problem to investigate, despite the performance improvements shown in these studies. In this work, we question the validity of Transformer-based TSF solutions. In their experiments, the compared (non-Transformer) baselines are mainly autoregressive forecasting solutions, which usually have a poor long-term prediction capability due to inevitable error accumulation effects. In contrast, we use an embarrassingly simple architecture named DLinear that conducts direct multi-step (DMS) forecasting for comparison. DLinear decomposes the time series into a trend and a remainder series and employs two one-layer linear networks to model these two series for the forecasting task. Surprisingly, it outperforms existing complex Transformer-based models in most cases by a large margin. Therefore, we conclude that the relatively higher long-term forecasting accuracy of Transformer-based TSF solutions shown in existing works has little to do with the temporal relation extraction capabilities of the Transformer architecture. Instead, it is mainly due to the non-autoregressive DMS forecasting strategy used in them. We hope this study also advocates revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.  
### Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v1 [cs.CV])
- Authors : Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, Yue Cao
- Link : [http://arxiv.org/abs/2205.13543](http://arxiv.org/abs/2205.13543)
> ABSTRACT  :  Masked image modeling (MIM) as pre-training is shown to be effective for numerous vision downstream tasks, but how and where MIM works remain unclear. In this paper, we compare MIM with the long-dominant supervised pre-trained models from two perspectives, the visualizations and the experiments, to uncover their key representational differences. From the visualizations, we find that MIM brings locality inductive bias to all layers of the trained models, but supervised models tend to focus locally at lower layers but more globally at higher layers. That may be the reason why MIM helps Vision Transformers that have a very large receptive field to optimize. Using MIM, the model can maintain a large diversity on attention heads in all layers. But for supervised models, the diversity on attention heads almost disappears from the last three layers and less diversity harms the fine-tuning performance. From the experiments, we find that MIM models can perform significantly better on geometric and motion tasks with weak semantics or fine-grained classification tasks, than their supervised counterparts. Without bells and whistles, a standard MIM pre-trained **Swin**V2-L could achieve state-of-the-art performance on pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the categories are sufficiently covered by the supervised pre-training, MIM models can still achieve highly competitive transfer performance. With a deeper understanding of MIM, we hope that our work can inspire new and solid research in this direction.  
### Inception Transformer. (arXiv:2205.12956v2 [cs.CV] UPDATED)
- Authors : Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, Shuicheng Yan
- Link : [http://arxiv.org/abs/2205.12956](http://arxiv.org/abs/2205.12956)
> ABSTRACT  :  Recent studies show that Transformer has strong capability of building long-range dependencies, yet is incompetent in capturing high frequencies that predominantly convey local information. To tackle this issue, we present a novel and general-purpose Inception Transformer, or iFormer for short, that effectively learns comprehensive features with both high- and low-frequency information in visual data. Specifically, we design an Inception mixer to explicitly graft the advantages of convolution and max-pooling for capturing the high-frequency information to Transformers. Different from recent hybrid frameworks, the Inception mixer brings greater efficiency through a channel splitting mechanism to adopt parallel convolution/max-pooling path and self-attention path as high- and low-frequency mixers, while having the flexibility to model discriminative information scattered within a wide frequency range. Considering that bottom layers play more roles in capturing high-frequency details while top layers more in modeling low-frequency global information, we further introduce a frequency ramp structure, i.e. gradually decreasing the dimensions fed to the high-frequency mixer and increasing those to the low-frequency mixer, which can effectively trade-off high- and low-frequency components across different layers. We benchmark the iFormer on a series of vision tasks, and showcase that it achieves impressive performance on image classification, COCO detection and ADE20K segmentation. For example, our iFormer-S hits the top-1 accuracy of 83.4% on ImageNet-1K, much higher than DeiT-S by 3.6%, and even slightly better than much bigger model **Swin**-B (83.3%) with only 1/4 parameters and 1/3 FLOPs. Code and models will be released at https://github.com/sail-sg/iFormer.  
# Paper List
---
## cs.CV
---
**102** new papers in cs.CV:-) 
1. Towards Diverse and Natural Scene-aware 3D Human Motion Synthesis. (arXiv:2205.13001v1 [cs.CV])
2. People counting system for retail analytics using edge AI. (arXiv:2205.13020v1 [cs.LG])
3. How explainable are adversarially-robust CNNs?. (arXiv:2205.13042v1 [cs.CV])
4. Online Deep Equilibrium Learning for Regularization by Denoising. (arXiv:2205.13051v1 [eess.IV])
5. Designing an Efficient End-to-end Machine Learning Pipeline for **Real-time** Empty-shelf Detection. (arXiv:2205.13060v1 [cs.LG])
6. Exploring Map-based Features for Efficient Attention-based Vehicle Motion Prediction. (arXiv:2205.13071v1 [cs.RO])
7. Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels. (arXiv:2205.13092v1 [cs.CV])
8. VizInspect Pro -- Automated Optical Inspection (AOI) solution. (arXiv:2205.13095v1 [cs.AI])
9. Learning to segment with limited annotations: Self-supervised pretraining with regression and contrastive loss in MRI. (arXiv:2205.13109v1 [cs.CV])
10. Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v1 [cs.CL])
11. Learn to Cluster Faces via Pairwise Classification. (arXiv:2205.13117v1 [cs.CV])
12. Perceptual Learned Source-Channel Coding for High-Fidelity Image Semantic Transmission. (arXiv:2205.13120v1 [cs.CV])
13. To image, or not to image: Class-specific diffractive cameras with all-optical erasure of undesired objects. (arXiv:2205.13122v1 [physics.optics])
14. PixelGame: Infrared small target segmentation as a Nash equilibrium. (arXiv:2205.13124v1 [cs.CV])
15. Prompt-based Learning for Unpaired Image Captioning. (arXiv:2205.13125v1 [cs.CV])
16. Wireless Deep Video Semantic Transmission. (arXiv:2205.13129v1 [cs.CV])
17. MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning. (arXiv:2205.13137v1 [cs.CV])
18. Matryoshka Representations for Adaptive Deployment. (arXiv:2205.13147v1 [cs.LG])
19. Transferable Adversarial Attack based on Integrated Gradients. (arXiv:2205.13152v1 [cs.LG])
20. **Swin**VRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation. (arXiv:2205.13158v1 [cs.CV])
21. HIRL: A General Framework for Hierarchical Image Representation Learning. (arXiv:2205.13159v1 [cs.CV])
22. Light Field Raindrop Removal via 4D Re-sampling. (arXiv:2205.13165v1 [cs.CV])
23. Analyzing the Latent Space of GAN through Local Dimension Estimation. (arXiv:2205.13182v1 [cs.CV])
24. AI for Porosity and Permeability Prediction from Geologic Core X-Ray Micro-Tomography. (arXiv:2205.13189v1 [cs.LG])
25. Tree Reconstruction using Topology Optimisation. (arXiv:2205.13192v1 [cs.CV])
26. Decoupled Pyramid Correlation Network for Liver Tumor Segmentation from CT images. (arXiv:2205.13199v1 [cs.CV])
27. Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v1 [cs.CV])
28. A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning. (arXiv:2205.13218v1 [cs.LG])
29. Penalizing Proposals using Classifiers for Semi-Supervised Object Detection. (arXiv:2205.13219v1 [cs.CV])
30. DGSVis: Visual Analysis of Hierarchical Snapshots in Dynamic Graph. (arXiv:2205.13220v1 [cs.HC])
31. Censor-aware Semi-supervised Learning for Survival Time Prediction from Medical Images. (arXiv:2205.13226v1 [cs.CV])
32. Denial-of-Service Attacks on Learned Image Compression. (arXiv:2205.13253v1 [cs.CV])
33. Task-Customized Self-Supervised Pre-training with Scalable Dynamic Routing. (arXiv:2205.13267v1 [cs.CV])
34. MemeTector: Enforcing deep focus for meme detection. (arXiv:2205.13268v1 [cs.CV])
35. Unsupervised Multi-object Segmentation Using Attention and Soft-argmax. (arXiv:2205.13271v1 [cs.CV])
36. FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices. (arXiv:2205.13272v1 [cs.CV])
37. Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued Convolutional Neural Networks. (arXiv:2205.13273v1 [cs.CV])
38. VIDI: A Video Dataset of Incidents. (arXiv:2205.13277v1 [cs.CV])
39. Semantic Segmentation for Thermal Images: A Comparative Survey. (arXiv:2205.13278v1 [cs.CV])
40. Objects Matter: Learning Object Relation Graph for Robust Camera Relocalization. (arXiv:2205.13280v1 [cs.CV])
41. Surround-view Fisheye Camera Perception for Automated Driving: Overview, Survey and Challenges. (arXiv:2205.13281v1 [cs.CV])
42. On the Eigenvalues of Global Covariance Pooling for Fine-grained Visual Recognition. (arXiv:2205.13282v1 [cs.CV])
43. Analytical Interpretation of Latent Codes in InfoGAN with SAR Images. (arXiv:2205.13294v1 [cs.CV])
44. Social Interpretable Tree for Pedestrian Trajectory Prediction. (arXiv:2205.13296v1 [cs.CV])
45. DeepTechnome: Mitigating Unknown Bias in Deep Learning Based Assessment of CT Images. (arXiv:2205.13297v1 [eess.IV])
46. SARS-CoV-2 Result Interpretation based on Image Analysis of Lateral Flow Devices. (arXiv:2205.13311v1 [cs.LG])
47. Cross-Architecture Self-supervised Video Representation Learning. (arXiv:2205.13313v1 [cs.CV])
48. SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v1 [cs.CV])
49. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v1 [cs.CV])
50. Learning What and Where -- Unsupervised Disentangling Location and Identity Tracking. (arXiv:2205.13349v1 [cs.CV])
51. One-Shot Face Reenactment on Megapixels. (arXiv:2205.13368v1 [cs.CV])
52. BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning. (arXiv:2205.13383v1 [cs.CV])
53. Continual Learning for Visual Search with Backward Consistent Feature Embedding. (arXiv:2205.13384v1 [cs.CV])
54. A Physical-World Adversarial Attack Against 3D Face Recognition. (arXiv:2205.13412v1 [cs.CV])
55. Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation. (arXiv:2205.13425v1 [cs.CV])
56. Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. (arXiv:2205.13445v1 [cs.CV])
57. Continual evaluation for lifelong learning: Identifying the stability gap. (arXiv:2205.13452v1 [cs.LG])
58. 2D versus 3D Convolutional Spiking Neural Networks Trained with Unsupervised STDP for Human Action Recognition. (arXiv:2205.13474v1 [cs.CV])
59. Measuring Perceptual Color Differences of Smartphone Photography. (arXiv:2205.13489v1 [cs.CV])
60. SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation. (arXiv:2205.13490v1 [cs.CV])
61. Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v1 [cs.CV])
62. PREF: Phasorial Embedding Fields for Compact Neural Representations. (arXiv:2205.13524v1 [cs.CV])
63. AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition. (arXiv:2205.13535v1 [cs.CV])
64. BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation. (arXiv:2205.13542v1 [cs.CV])
65. Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v1 [cs.CV])
66. PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural Networks Acceleration. (arXiv:2006.12963v3 [cs.CV] UPDATED)
67. Polygon-free: Unconstrained Scene Text Detection with Box Annotations. (arXiv:2011.13307v3 [cs.CV] UPDATED)
68. DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v4 [cs.CV] UPDATED)
69. Continual Learning for Blind Image Quality Assessment. (arXiv:2102.09717v2 [cs.CV] UPDATED)
70. A Little Energy Goes a Long Way: Build an Energy-Efficient, Accurate Spiking Neural Network from Convolutional Neural Network. (arXiv:2103.00944v3 [cs.CV] UPDATED)
71. Steerable 3D Spherical Neurons. (arXiv:2106.13863v6 [cs.CV] UPDATED)
72. Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v3 [cs.LG] UPDATED)
73. Learning Perceptual Locomotion on Uneven Terrains using Sparse Visual Observations. (arXiv:2109.14026v2 [cs.RO] UPDATED)
74. Mitigating Memorization of Noisy Labels via Regularization between Representations. (arXiv:2110.09022v3 [cs.LG] UPDATED)
75. Towards the Generalization of Contrastive Self-Supervised Learning. (arXiv:2111.00743v3 [cs.LG] UPDATED)
76. Automated pharyngeal phase detection and bolus localization in videofluoroscopic swallowing study: Killing two birds with one stone?. (arXiv:2111.04699v2 [eess.IV] UPDATED)
77. Improved Fine-Tuning by Better Leveraging Pre-Training Data. (arXiv:2111.12292v2 [cs.CV] UPDATED)
78. Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation. (arXiv:2112.02478v2 [eess.IV] UPDATED)
79. Trajectory-Constrained Deep Latent Visual Attention for Improved Local Planning in Presence of Heterogeneous Terrain. (arXiv:2112.04684v3 [cs.RO] UPDATED)
80. Nonlinear Transform Source-Channel Coding for Semantic Communications. (arXiv:2112.10961v2 [cs.IT] UPDATED)
81. Feature Extraction, Classification and Prediction for Hand Hygiene Gestures with KNN Algorithm. (arXiv:2112.15085v2 [cs.CV] UPDATED)
82. Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (arXiv:2201.01080v3 [cs.CV] UPDATED)
83. Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy. (arXiv:2201.02867v3 [eess.IV] UPDATED)
84. RDP-Net: Region Detail Preserving Network for Change Detection. (arXiv:2202.09745v3 [eess.IV] UPDATED)
85. Motion-driven Visual Tempo Learning for Video-based Action Recognition. (arXiv:2202.12116v2 [cs.CV] UPDATED)
86. Towards Creativity Characterization of Generative Models via Group-based Subset Scanning. (arXiv:2203.00523v3 [cs.CV] UPDATED)
87. Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance. (arXiv:2203.06749v3 [cs.CV] UPDATED)
88. A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v2 [cs.CV] UPDATED)
89. What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions. (arXiv:2204.00746v2 [cs.CV] UPDATED)
90. Reinforced Structured State-Evolution for Vision-Language Navigation. (arXiv:2204.09280v2 [cs.CV] UPDATED)
91. From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model. (arXiv:2205.00690v3 [cs.LG] UPDATED)
92. Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v2 [q-bio.QM] UPDATED)
93. When does dough become a bagel? Analyzing the remaining mistakes on ImageNet. (arXiv:2205.04596v2 [cs.CV] UPDATED)
94. Learnable Visual Words for Interpretable Image Recognition. (arXiv:2205.10724v2 [cs.CV] UPDATED)
95. CNNs are Myopic. (arXiv:2205.10760v2 [cs.CV] UPDATED)
96. Super Vision Transformer. (arXiv:2205.11397v2 [cs.CV] UPDATED)
97. Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v2 [cs.LG] UPDATED)
98. Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance. (arXiv:2205.12022v2 [cs.CV] UPDATED)
99. MoCoViT: Mobile Convolutional Vision Transformer. (arXiv:2205.12635v2 [cs.CV] UPDATED)
100. UniInst: Unique Representation for End-to-End Instance Segmentation. (arXiv:2205.12646v2 [cs.CV] UPDATED)
101. Structure Unbiased Adversarial Model for Medical Image Segmentation. (arXiv:2205.12857v2 [eess.IV] UPDATED)
102. Inception Transformer. (arXiv:2205.12956v2 [cs.CV] UPDATED)
## eess.IV
---
**19** new papers in eess.IV:-) 
1. Online Deep Equilibrium Learning for Regularization by Denoising. (arXiv:2205.13051v1 [eess.IV])
2. Learning to segment with limited annotations: Self-supervised pretraining with regression and contrastive loss in MRI. (arXiv:2205.13109v1 [cs.CV])
3. Light Field Raindrop Removal via 4D Re-sampling. (arXiv:2205.13165v1 [cs.CV])
4. Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued Convolutional Neural Networks. (arXiv:2205.13273v1 [cs.CV])
5. Analytical Interpretation of Latent Codes in InfoGAN with SAR Images. (arXiv:2205.13294v1 [cs.CV])
6. DeepTechnome: Mitigating Unknown Bias in Deep Learning Based Assessment of CT Images. (arXiv:2205.13297v1 [eess.IV])
7. SARS-CoV-2 Result Interpretation based on Image Analysis of Lateral Flow Devices. (arXiv:2205.13311v1 [cs.LG])
8. New methods of removing debris and high-throughput counting of cyst nematode eggs extracted from field soil. (arXiv:2205.13363v1 [q-bio.QM])
9. A Physical-World Adversarial Attack Against 3D Face Recognition. (arXiv:2205.13412v1 [cs.CV])
10. Machine Learning Models Are Not Necessarily Biased When Constructed Properly: Evidence from Neuroimaging Studies. (arXiv:2205.13421v1 [cs.LG])
11. Measuring Perceptual Color Differences of Smartphone Photography. (arXiv:2205.13489v1 [cs.CV])
12. Continual Learning for Blind Image Quality Assessment. (arXiv:2102.09717v2 [cs.CV] UPDATED)
13. Automated pharyngeal phase detection and bolus localization in videofluoroscopic swallowing study: Killing two birds with one stone?. (arXiv:2111.04699v2 [eess.IV] UPDATED)
14. Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation. (arXiv:2112.02478v2 [eess.IV] UPDATED)
15. Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy. (arXiv:2201.02867v3 [eess.IV] UPDATED)
16. Unsupervised Learning From Incomplete Measurements for Inverse Problems. (arXiv:2201.12151v3 [stat.ML] UPDATED)
17. RDP-Net: Region Detail Preserving Network for Change Detection. (arXiv:2202.09745v3 [eess.IV] UPDATED)
18. Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v2 [q-bio.QM] UPDATED)
19. Structure Unbiased Adversarial Model for Medical Image Segmentation. (arXiv:2205.12857v2 [eess.IV] UPDATED)
## cs.LG
---
**203** new papers in cs.LG:-) 
1. Uniform Generalization Bound on Time and Inverse Temperature for Gradient Descent Algorithm and its Application to Analysis of Simulated Annealing. (arXiv:2205.12959v1 [cs.LG])
2. Towards Symbolic Time Series Representation Improved by Kernel Density Estimators. (arXiv:2205.12960v1 [cs.LG])
3. Towards Green AI with tensor networks -- Sustainability and innovation enabled by efficient algorithms. (arXiv:2205.12961v1 [cs.LG])
4. QGNN: Value Function Factorisation with Graph Neural Networks. (arXiv:2205.13005v1 [cs.LG])
5. TSEM: Temporally Weighted Spatiotemporal Explainable Neural Network for Multivariate Time Series. (arXiv:2205.13012v1 [cs.LG])
6. BiT: Robustly Binarized Multi-distilled Transformer. (arXiv:2205.13016v1 [cs.LG])
7. People counting system for retail analytics using edge AI. (arXiv:2205.13020v1 [cs.LG])
8. Preference Dynamics Under Personalized Recommendations. (arXiv:2205.13026v1 [cs.LG])
9. Formalizing Preferences Over Runtime Distributions. (arXiv:2205.13028v1 [cs.AI])
10. Concurrent Neural Tree and Data Preprocessing AutoML for Image Classification. (arXiv:2205.13033v1 [cs.LG])
11. EvoVGM: A Deep Variational Generative Model for Evolutionary Parameter Estimation. (arXiv:2205.13034v1 [cs.LG])
12. Improving Subgraph Representation Learning via Multi-View Augmentation. (arXiv:2205.13038v1 [cs.LG])
13. Near-Optimal Goal-Oriented Reinforcement Learning in Non-Stationary Environments. (arXiv:2205.13044v1 [cs.LG])
14. QADAM: Quantization-Aware DNN Accelerator Modeling for Pareto-Optimality. (arXiv:2205.13045v1 [cs.AR])
15. Online Deep Equilibrium Learning for Regularization by Denoising. (arXiv:2205.13051v1 [eess.IV])
16. Scalable and Low-Latency Federated Learning with Cooperative Mobile Edge Networking. (arXiv:2205.13054v1 [cs.DC])
17. Efficient and Near-Optimal Smoothed Online Learning for Generalized Linear Functions. (arXiv:2205.13056v1 [stat.ML])
18. Designing an Efficient End-to-end Machine Learning Pipeline for **Real-time** Empty-shelf Detection. (arXiv:2205.13060v1 [cs.LG])
19. RENs: Relevance Encoding Networks. (arXiv:2205.13061v1 [cs.LG])
20. Urban Rhapsody: Large-scale exploration of urban soundscapes. (arXiv:2205.13064v1 [cs.CY])
21. Semi-supervised Drifted Stream Learning with Short Lookback. (arXiv:2205.13066v1 [cs.LG])
22. Forecasting Patient Demand at Urgent Care Clinics using Machine Learning. (arXiv:2205.13067v1 [cs.LG])
23. Tight Lower Bounds on Worst-Case Guarantees for Zero-Shot Learning with Attributes. (arXiv:2205.13068v1 [cs.LG])
24. Entropy Maximization with Depth: A Variational Principle for Random Neural Networks. (arXiv:2205.13076v1 [cs.LG])
25. Learning to Query Internet Text for Informing Reinforcement Learning Agents. (arXiv:2205.13079v1 [cs.LG])
26. Factorized Structured Regression for Large-Scale Varying Coefficient Models. (arXiv:2205.13080v1 [stat.ML])
27. BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection. (arXiv:2205.13084v1 [cs.LG])
28. Identifying Patient-Specific Root Causes with the Heteroscedastic Noise Model. (arXiv:2205.13085v1 [stat.ML])
29. Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v1 [cs.LG])
30. Optimal Neural Network Approximation of Wasserstein Gradient Direction via Convex Optimization. (arXiv:2205.13098v1 [cs.LG])
31. Deep-XFCT: Deep learning 3D-mineral liberation analysis with micro X-ray fluorescence and computed tomography. (arXiv:2205.13102v1 [cs.LG])
32. Trainable Weight Averaging for Fast Convergence and Better Generalization. (arXiv:2205.13104v1 [cs.LG])
33. Learning to segment with limited annotations: Self-supervised pretraining with regression and contrastive loss in MRI. (arXiv:2205.13109v1 [cs.CV])
34. Contextual Pandora's Box. (arXiv:2205.13114v1 [cs.LG])
35. GraphPMU: Event Clustering via Graph Representation Learning Using Locationally-Scarce Distribution-Level Fundamental and Harmonic PMU Measurements. (arXiv:2205.13116v1 [cs.LG])
36. Understanding Metrics for Paraphrasing. (arXiv:2205.13119v1 [cs.CL])
37. Cali3F: Calibrated Fast Fair Federated Recommendation System. (arXiv:2205.13121v1 [cs.IR])
38. RACE: A Reinforcement Learning Framework for Improved Adaptive Control of NoC Channel Buffers. (arXiv:2205.13130v1 [cs.AR])
39. On the Evolution of A.I. and Machine Learning: Towards Measuring and Understanding Impact, Influence, and Leadership at Premier A.I. Conferences. (arXiv:2205.13131v1 [cs.AI])
40. Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search. (arXiv:2205.13134v1 [cs.AI])
41. Unsupervised Reinforcement Adaptation for Class-Imbalanced Text Classification. (arXiv:2205.13139v1 [cs.CL])
42. Matryoshka Representations for Adaptive Deployment. (arXiv:2205.13147v1 [cs.LG])
43. Grammar Detection for Sentiment Analysis through Improved Viterbi Algorithm. (arXiv:2205.13148v1 [cs.CL])
44. Transferable Adversarial Attack based on Integrated Gradients. (arXiv:2205.13152v1 [cs.LG])
45. Cost-efficient Gaussian Tensor Network Embeddings for Tensor-structured Inputs. (arXiv:2205.13163v1 [math.NA])
46. Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks. (arXiv:2205.13164v1 [cs.CL])
47. On Learning Mixture of Linear Regressions in the Non-Realizable Setting. (arXiv:2205.13166v1 [stat.ML])
48. Distributed Contextual Linear Bandits with Minimax Optimal Communication Cost. (arXiv:2205.13170v1 [cs.LG])
49. AI for Porosity and Permeability Prediction from Geologic Core X-Ray Micro-Tomography. (arXiv:2205.13189v1 [cs.LG])
50. Orthogonal Stochastic Configuration Networks with Adaptive Construction Parameter for Data Analytics. (arXiv:2205.13191v1 [cs.LG])
51. More Recent Advances in (Hyper)Graph Partitioning. (arXiv:2205.13202v1 [cs.DS])
52. $O(N^2)$ Universal Antisymmetry in Fermionic Neural Networks. (arXiv:2205.13205v1 [cs.LG])
53. Sym-NCO: Leveraging Symmetricity for Neural Combinatorial Optimization. (arXiv:2205.13209v1 [cs.LG])
54. Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v1 [cs.CV])
55. SymNMF-Net for The Symmetric NMF Problem. (arXiv:2205.13214v1 [cs.LG])
56. Aggregating Gradients in Encoded Domain for Federated Learning. (arXiv:2205.13216v1 [cs.CR])
57. A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning. (arXiv:2205.13218v1 [cs.LG])
58. Penalizing Proposals using Classifiers for Semi-Supervised Object Detection. (arXiv:2205.13219v1 [cs.CV])
59. QSpeech: Low-Qubit Quantum Speech Application Toolkit. (arXiv:2205.13221v1 [quant-ph])
60. Friends to Help: Saving Federated Learning from Client Dropout. (arXiv:2205.13222v1 [cs.LG])
61. Collaborative Distillation Meta Learning for Simulation Intensive Hardware Design. (arXiv:2205.13225v1 [cs.LG])
62. DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees. (arXiv:2205.13234v1 [cs.LG])
63. Constrained Reinforcement Learning for Short Video Recommendation. (arXiv:2205.13248v1 [cs.LG])
64. DT-SV: A Transformer-based Time-domain Approach for Speaker Verification. (arXiv:2205.13249v1 [cs.SD])
65. Denial-of-Service Attacks on Learned Image Compression. (arXiv:2205.13253v1 [cs.CV])
66. Active Labeling: Streaming Stochastic Gradients. (arXiv:2205.13255v1 [cs.LG])
67. Privacy-Preserving Wavelet Wavelet Neural Network with Fully Homomorphic Encryption. (arXiv:2205.13265v1 [cs.LG])
68. Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued Convolutional Neural Networks. (arXiv:2205.13273v1 [cs.CV])
69. Evaluating Multimodal Interactive Agents. (arXiv:2205.13274v1 [cs.LG])
70. Triangular Contrastive Learning on Molecular Graphs. (arXiv:2205.13279v1 [cs.LG])
71. On the Eigenvalues of Global Covariance Pooling for Fine-grained Visual Recognition. (arXiv:2205.13282v1 [cs.CV])
72. Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks. (arXiv:2205.13283v1 [cs.LG])
73. DeepTechnome: Mitigating Unknown Bias in Deep Learning Based Assessment of CT Images. (arXiv:2205.13297v1 [eess.IV])
74. Federated Split BERT for Heterogeneous Text Classification. (arXiv:2205.13299v1 [cs.CL])
75. Federated Non-negative Matrix Factorization for Short Texts Topic Modeling with Mutual Information. (arXiv:2205.13300v1 [cs.CL])
76. Gaussian Universality of Linear Classifiers with Random Labels in High-Dimension. (arXiv:2205.13303v1 [stat.ML])
77. SARS-CoV-2 Result Interpretation based on Image Analysis of Lateral Flow Devices. (arXiv:2205.13311v1 [cs.LG])
78. Fair Representation Learning through Implicit Path Alignment. (arXiv:2205.13316v1 [cs.LG])
79. Towards Learning Universal Hyperparameter Optimizers with Transformers. (arXiv:2205.13320v1 [cs.LG])
80. The Effect of Task Ordering in Continual Learning. (arXiv:2205.13323v1 [cs.LG])
81. Learning the spatio-temporal relationship between wind and significant wave height using deep learning. (arXiv:2205.13325v1 [stat.ML])
82. How Powerful are K-hop Message Passing Graph Neural Networks. (arXiv:2205.13328v1 [cs.LG])
83. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v1 [cs.CV])
84. Deep Active Learning with Noise Stability. (arXiv:2205.13340v1 [cs.LG])
85. QUICK-FL: Quick Unbiased Compression for Federated Learning. (arXiv:2205.13341v1 [cs.LG])
86. Transfer and Share: Semi-Supervised Learning from Long-Tailed Data. (arXiv:2205.13358v1 [cs.LG])
87. Feature Forgetting in Continual Representation Learning. (arXiv:2205.13359v1 [cs.LG])
88. Multi-fidelity power flow solver. (arXiv:2205.13362v1 [cs.LG])
89. A Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical Representation Learning. (arXiv:2205.13371v1 [cs.LG])
90. BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning. (arXiv:2205.13383v1 [cs.CV])
91. Looking for Out-of-Distribution Environments in Critical Care: A case study with the eICU Database. (arXiv:2205.13398v1 [cs.LG])
92. Your Transformer May Not be as Powerful as You Expect. (arXiv:2205.13401v1 [cs.LG])
93. A Fair Federated Learning Framework With Reinforcement Learning. (arXiv:2205.13415v1 [cs.LG])
94. Avoiding Barren Plateaus with Classical Deep Neural Networks. (arXiv:2205.13418v1 [quant-ph])
95. Machine Learning Models Are Not Necessarily Biased When Constructed Properly: Evidence from Neuroimaging Studies. (arXiv:2205.13421v1 [cs.LG])
96. Opinion Spam Detection: A New Approach Using Machine Learning and Network-Based Algorithms. (arXiv:2205.13422v1 [cs.LG])
97. The Neuro-Symbolic Brain. (arXiv:2205.13440v1 [cs.NE])
98. Principled Knowledge Extrapolation with GANs. (arXiv:2205.13444v1 [cs.LG])
99. Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. (arXiv:2205.13445v1 [cs.CV])
100. Variance-Aware Sparse Linear Bandits. (arXiv:2205.13450v1 [cs.LG])
101. Follow-the-Perturbed-Leader for Adversarial Markov Decision Processes with Bandit Feedback. (arXiv:2205.13451v1 [cs.LG])
102. Continual evaluation for lifelong learning: Identifying the stability gap. (arXiv:2205.13452v1 [cs.LG])
103. AutoTSG: Learning and Synthesis for Incident Troubleshooting. (arXiv:2205.13457v1 [cs.SE])
104. SigMaNet: One Laplacian to Rule Them All. (arXiv:2205.13459v1 [cs.LG])
105. FedAug: Reducing the Local Learning Bias Improves Federated Learning on Heterogeneous Data. (arXiv:2205.13462v1 [cs.LG])
106. Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency. (arXiv:2205.13476v1 [cs.LG])
107. Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse Observations. (arXiv:2205.13479v1 [cs.LG])
108. DeepJoint: Robust Survival Modelling Under Clinical Presence Shift. (arXiv:2205.13481v1 [cs.LG])
109. SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation. (arXiv:2205.13490v1 [cs.CV])
110. Sparse Graph Learning for Spatiotemporal Time Series. (arXiv:2205.13492v1 [cs.LG])
111. Mesoscopic modeling of hidden spiking neurons. (arXiv:2205.13493v1 [q-bio.NC])
112. Censored Quantile Regression Neural Networks. (arXiv:2205.13496v1 [stat.ML])
113. An Analytic Framework for Robust Training of Artificial Neural Networks. (arXiv:2205.13502v1 [cs.LG])
114. Are Transformers Effective for Time Series Forecasting?. (arXiv:2205.13504v1 [cs.AI])
115. A framework for overparameterized learning. (arXiv:2205.13507v1 [cs.LG])
116. Pick up the PACE: Fast and Simple Domain Adaptation via Ensemble Pseudo-Labeling. (arXiv:2205.13508v1 [cs.LG])
117. Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v1 [cs.CV])
118. Transfer learning driven design optimization for inertial confinement fusion. (arXiv:2205.13519v1 [physics.plasm-ph])
119. Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality. (arXiv:2205.13521v1 [cs.AI])
120. Kernel Ridgeless Regression is Inconsistent for Low Dimensions. (arXiv:2205.13525v1 [cs.LG])
121. Subspace clustering in high-dimensions: Phase transitions \& Statistical-to-Computational gap. (arXiv:2205.13527v1 [stat.ML])
122. TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning. (arXiv:2205.13528v1 [cs.LG])
123. Semantic Parsing of Interpage Relations. (arXiv:2205.13530v1 [cs.LG])
124. Training ReLU networks to high uniform accuracy is intractable. (arXiv:2205.13531v1 [cs.LG])
125. Selective Classification Via Neural Network Training Dynamics. (arXiv:2205.13532v1 [cs.LG])
126. Verifying Learning-Based Robotic Navigation Systems. (arXiv:2205.13536v1 [cs.RO])
127. Mitigating barren plateaus of variational quantum eigensolvers. (arXiv:2205.13539v1 [quant-ph])
128. Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v1 [cs.CV])
129. Machine Learning Construction: implications to cybersecurity. (arXiv:1906.10019v3 [cs.LG] UPDATED)
130. Machine Learning Assessment: implications to cybersecurity. (arXiv:1907.12851v4 [stat.ML] UPDATED)
131. Memory AMP. (arXiv:2012.10861v6 [cs.IT] UPDATED)
132. Spherical Message Passing for 3D Graph Networks. (arXiv:2102.05013v3 [cs.LG] UPDATED)
133. Forest Fire Clustering for Single-cell Sequencing with Iterative Label Propagation and Parallelized Monte Carlo Simulation. (arXiv:2103.11802v4 [cs.LG] UPDATED)
134. TrustyAI Explainability Toolkit. (arXiv:2104.12717v2 [cs.AI] UPDATED)
135. Ranking the information content of distance measures. (arXiv:2104.15079v2 [stat.ML] UPDATED)
136. Comparison of Traditional and Hybrid Time Series Models for Forecasting COVID-19 Cases. (arXiv:2105.03266v2 [cs.SI] UPDATED)
137. Independent Asymmetric Embedding for Information Diffusion Prediction on Social Networks. (arXiv:2105.08291v6 [cs.LG] UPDATED)
138. MixR: Data Mixing Augmentation for Regression. (arXiv:2106.03374v3 [cs.LG] UPDATED)
139. Steerable 3D Spherical Neurons. (arXiv:2106.13863v6 [cs.CV] UPDATED)
140. Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v3 [cs.LG] UPDATED)
141. A Penalized Shared-parameter Algorithm for Estimating Optimal Dynamic Treatment Regimens. (arXiv:2107.07875v2 [stat.ML] UPDATED)
142. Epistemic Neural Networks. (arXiv:2107.08924v3 [cs.LG] UPDATED)
143. Amplitude Mean of Functional Data on $\mathbb{S}^2$. (arXiv:2107.13721v4 [stat.ML] UPDATED)
144. DeepTrack: Lightweight Deep Learning for Vehicle Path Prediction in Highways. (arXiv:2108.00505v2 [cs.LG] UPDATED)
145. The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer Linear Networks. (arXiv:2108.11489v2 [stat.ML] UPDATED)
146. Learning Perceptual Locomotion on Uneven Terrains using Sparse Visual Observations. (arXiv:2109.14026v2 [cs.RO] UPDATED)
147. Trustworthy AI: From Principles to Practices. (arXiv:2110.01167v2 [cs.AI] UPDATED)
148. The Neural Testbed: Evaluating Joint Predictions. (arXiv:2110.04629v3 [cs.LG] UPDATED)
149. TIP: Task-Informed Motion Prediction for Intelligent Vehicles. (arXiv:2110.08750v2 [cs.RO] UPDATED)
150. Mitigating Memorization of Noisy Labels via Regularization between Representations. (arXiv:2110.09022v3 [cs.LG] UPDATED)
151. RKHS-SHAP: Shapley Values for Kernel Methods. (arXiv:2110.09167v2 [stat.ML] UPDATED)
152. Gaussian Process Sampling and Optimization with Approximate Upper and Lower Bounds. (arXiv:2110.12087v3 [cs.LG] UPDATED)
153. Towards the Generalization of Contrastive Self-Supervised Learning. (arXiv:2111.00743v3 [cs.LG] UPDATED)
154. Coherent Probabilistic Aggregate Queries on Long-horizon Forecasts. (arXiv:2111.03394v2 [cs.LG] UPDATED)
155. Reliably-stabilizing piecewise-affine neural network controllers. (arXiv:2111.07183v3 [eess.SY] UPDATED)
156. Joint Synthesis of Safety Certificate and Safe Control Policy using Constrained Reinforcement Learning. (arXiv:2111.07695v3 [cs.LG] UPDATED)
157. Improved Fine-Tuning by Better Leveraging Pre-Training Data. (arXiv:2111.12292v2 [cs.CV] UPDATED)
158. FedHM: Efficient Federated Learning for Heterogeneous Models via Low-rank Factorization. (arXiv:2111.14655v2 [cs.LG] UPDATED)
159. Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation. (arXiv:2112.02478v2 [eess.IV] UPDATED)
160. Trajectory-Constrained Deep Latent Visual Attention for Improved Local Planning in Presence of Heterogeneous Terrain. (arXiv:2112.04684v3 [cs.RO] UPDATED)
161. Domain-informed neural networks for interaction localization within astroparticle experiments. (arXiv:2112.07995v2 [hep-ex] UPDATED)
162. Nonlinear Transform Source-Channel Coding for Semantic Communications. (arXiv:2112.10961v2 [cs.IT] UPDATED)
163. FedBalancer: Data and Pace Control for Efficient Federated Learning on Heterogeneous Clients. (arXiv:2201.01601v2 [cs.LG] UPDATED)
164. Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy. (arXiv:2201.02867v3 [eess.IV] UPDATED)
165. Near-Optimal Sparse Allreduce for Distributed Deep Learning. (arXiv:2201.07598v2 [cs.DC] UPDATED)
166. Evaluating Generalization in Classical and Quantum Generative Models. (arXiv:2201.08770v2 [cs.LG] UPDATED)
167. Predicting Physics in Mesh-reduced Space with Temporal Attention. (arXiv:2201.09113v4 [cs.LG] UPDATED)
168. Mask-based Latent Reconstruction for Reinforcement Learning. (arXiv:2201.12096v2 [cs.LG] UPDATED)
169. Unsupervised Learning From Incomplete Measurements for Inverse Problems. (arXiv:2201.12151v3 [stat.ML] UPDATED)
170. Lorentzian Fully Hyperbolic Generative Adversarial Network. (arXiv:2201.12825v2 [cs.LG] UPDATED)
171. Generalization Analysis of Message Passing Neural Networks on Large Random Graphs. (arXiv:2202.00645v5 [cs.LG] UPDATED)
172. Reproducibility in Optimization: Theoretical Framework and Limits. (arXiv:2202.04598v2 [math.OC] UPDATED)
173. ChemicalX: A Deep Learning Library for Drug Pair Scoring. (arXiv:2202.05240v3 [cs.LG] UPDATED)
174. The Shapley Value in Machine Learning. (arXiv:2202.05594v2 [cs.LG] UPDATED)
175. Evolutionary scheduling of university activities based on consumption forecasts to minimise electricity costs. (arXiv:2202.12595v2 [cs.LG] UPDATED)
176. Incremental Inference on Higher-Order Probabilistic Graphical Models Applied to Constraint Satisfaction Problems. (arXiv:2202.12916v2 [cs.LG] UPDATED)
177. Continual Feature Selection: Spurious Features in Continual Learning. (arXiv:2203.01012v2 [cs.LG] UPDATED)
178. A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v2 [cs.CV] UPDATED)
179. Learning to Accelerate by the Methods of Step-size Planning. (arXiv:2204.01705v4 [cs.LG] UPDATED)
180. Worst-case Performance of Greedy Policies in Bandits with Imperfect Context Observations. (arXiv:2204.04773v2 [stat.ML] UPDATED)
181. Domain Adversarial Graph Convolutional Network Based on RSSI and Crowdsensing for Indoor Localization. (arXiv:2204.05184v2 [cs.NI] UPDATED)
182. From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model. (arXiv:2205.00690v3 [cs.LG] UPDATED)
183. Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v2 [q-bio.QM] UPDATED)
184. Stochastic first-order methods for average-reward Markov decision processes. (arXiv:2205.05800v3 [cs.LG] UPDATED)
185. Representation learning with function call graph transformations for malware open set recognition. (arXiv:2205.06918v2 [cs.CR] UPDATED)
186. A Computational Framework of Cortical Microcircuits Approximates Sign-concordant Random Backpropagation. (arXiv:2205.07292v3 [cs.NE] UPDATED)
187. TNN7: A Custom Macro Suite for Implementing Highly Optimized Designs of Neuromorphic TNNs. (arXiv:2205.07410v2 [cs.AR] UPDATED)
188. CARNet: A Dynamic Autoencoder for Learning Latent Dynamics in Autonomous Driving Tasks. (arXiv:2205.08712v2 [cs.LG] UPDATED)
189. IFTT-PIN: A PIN-Entry Method Leveraging the Self-Calibration Paradigm. (arXiv:2205.09534v2 [cs.HC] UPDATED)
190. CNNs are Myopic. (arXiv:2205.10760v2 [cs.CV] UPDATED)
191. Nonparametric likelihood-free inference with Jensen-Shannon divergence for simulator-based models with categorical output. (arXiv:2205.10890v2 [stat.ME] UPDATED)
192. Investigating classification learning curves for automatically generated and labelled plant images. (arXiv:2205.10955v2 [cs.LG] UPDATED)
193. Adaptive Fairness-Aware Online Meta-Learning for Changing Environments. (arXiv:2205.11264v2 [cs.LG] UPDATED)
194. Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v2 [cs.LG] UPDATED)
195. Towards a Defense against Backdoor Attacks in Continual Federated Learning. (arXiv:2205.11736v2 [cs.LG] UPDATED)
196. Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance. (arXiv:2205.12022v2 [cs.CV] UPDATED)
197. ColdGuess: A General and Effective Relational Graph Convolutional Network to Tackle Cold Start Cases. (arXiv:2205.12318v2 [cs.LG] UPDATED)
198. Certified Robustness Against Natural Language Attacks by Causal Intervention. (arXiv:2205.12331v2 [cs.LG] UPDATED)
199. Hardness of Maximum Likelihood Learning of DPPs. (arXiv:2205.12377v2 [cs.CC] UPDATED)
200. Augmentation-induced Consistency Regularization for Classification. (arXiv:2205.12461v2 [cs.LG] UPDATED)
201. Federated Self-supervised Learning for Heterogeneous Clients. (arXiv:2205.12493v2 [cs.LG] UPDATED)
202. Conformal Prediction Intervals with Temporal Dependence. (arXiv:2205.12940v2 [stat.ML] UPDATED)
203. Inception Transformer. (arXiv:2205.12956v2 [cs.CV] UPDATED)
## cs.AI
---
**97** new papers in cs.AI:-) 
1. Towards Symbolic Time Series Representation Improved by Kernel Density Estimators. (arXiv:2205.12960v1 [cs.LG])
2. Towards Green AI with tensor networks -- Sustainability and innovation enabled by efficient algorithms. (arXiv:2205.12961v1 [cs.LG])
3. Open Arms: Open-Source Arms, Hands & Control. (arXiv:2205.12992v1 [cs.RO])
4. An Algorithmic Approach to Emergence. (arXiv:2205.12997v1 [cond-mat.stat-mech])
5. TSEM: Temporally Weighted Spatiotemporal Explainable Neural Network for Multivariate Time Series. (arXiv:2205.13012v1 [cs.LG])
6. Towards Using Data-Centric Approach for Better Code Representation Learning. (arXiv:2205.13022v1 [cs.SE])
7. Formalizing Preferences Over Runtime Distributions. (arXiv:2205.13028v1 [cs.AI])
8. Improving Subgraph Representation Learning via Multi-View Augmentation. (arXiv:2205.13038v1 [cs.LG])
9. How explainable are adversarially-robust CNNs?. (arXiv:2205.13042v1 [cs.CV])
10. Forecasting Patient Demand at Urgent Care Clinics using Machine Learning. (arXiv:2205.13067v1 [cs.LG])
11. Entropy Maximization with Depth: A Variational Principle for Random Neural Networks. (arXiv:2205.13076v1 [cs.LG])
12. BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection. (arXiv:2205.13084v1 [cs.LG])
13. Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v1 [cs.LG])
14. VizInspect Pro -- Automated Optical Inspection (AOI) solution. (arXiv:2205.13095v1 [cs.AI])
15. Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV Conversion. (arXiv:2205.13108v1 [cs.CL])
16. Learning to segment with limited annotations: Self-supervised pretraining with regression and contrastive loss in MRI. (arXiv:2205.13109v1 [cs.CV])
17. Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v1 [cs.CL])
18. On the Evolution of A.I. and Machine Learning: Towards Measuring and Understanding Impact, Influence, and Leadership at Premier A.I. Conferences. (arXiv:2205.13131v1 [cs.AI])
19. Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search. (arXiv:2205.13134v1 [cs.AI])
20. AI for Porosity and Permeability Prediction from Geologic Core X-Ray Micro-Tomography. (arXiv:2205.13189v1 [cs.LG])
21. Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v1 [cs.CV])
22. SymNMF-Net for The Symmetric NMF Problem. (arXiv:2205.13214v1 [cs.LG])
23. Friends to Help: Saving Federated Learning from Client Dropout. (arXiv:2205.13222v1 [cs.LG])
24. Symbiotic Child Emotional Support with Social Robots and Temporal Knowledge Graphs. (arXiv:2205.13229v1 [cs.RO])
25. Denial-of-Service Attacks on Learned Image Compression. (arXiv:2205.13253v1 [cs.CV])
26. Active Labeling: Streaming Stochastic Gradients. (arXiv:2205.13255v1 [cs.LG])
27. FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices. (arXiv:2205.13272v1 [cs.CV])
28. Evaluating Multimodal Interactive Agents. (arXiv:2205.13274v1 [cs.LG])
29. Federated Split BERT for Heterogeneous Text Classification. (arXiv:2205.13299v1 [cs.CL])
30. Federated Non-negative Matrix Factorization for Short Texts Topic Modeling with Mutual Information. (arXiv:2205.13300v1 [cs.CL])
31. Towards Learning Universal Hyperparameter Optimizers with Transformers. (arXiv:2205.13320v1 [cs.LG])
32. How Powerful are K-hop Message Passing Graph Neural Networks. (arXiv:2205.13328v1 [cs.LG])
33. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v1 [cs.CV])
34. QUICK-FL: Quick Unbiased Compression for Federated Learning. (arXiv:2205.13341v1 [cs.LG])
35. Leveraging Causal Inference for Explainable Automatic Program Repair. (arXiv:2205.13342v1 [cs.SE])
36. The Document Vectors Using Cosine Similarity Revisited. (arXiv:2205.13357v1 [cs.CL])
37. Prismal view of ethics. (arXiv:2205.13370v1 [cs.CY])
38. Coalgebraic Fuzzy geometric logic. (arXiv:2205.13387v1 [cs.LO])
39. Multi-objective QUBO Solver: Bi-objective Quadratic Assignment. (arXiv:2205.13399v1 [cs.AI])
40. Avoiding Barren Plateaus with Classical Deep Neural Networks. (arXiv:2205.13418v1 [quant-ph])
41. Opinion Spam Detection: A New Approach Using Machine Learning and Network-Based Algorithms. (arXiv:2205.13422v1 [cs.LG])
42. Jointly Learning Span Extraction and Sequence Labeling for Information Extraction from Business Documents. (arXiv:2205.13434v1 [cs.CL])
43. The Neuro-Symbolic Brain. (arXiv:2205.13440v1 [cs.NE])
44. Deep Reinforcement Learning with Adaptive Hierarchical Reward for MultiMulti-Phase Multi Multi-Objective Dexterous Manipulation. (arXiv:2205.13441v1 [cs.RO])
45. Principled Knowledge Extrapolation with GANs. (arXiv:2205.13444v1 [cs.LG])
46. Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. (arXiv:2205.13445v1 [cs.CV])
47. Variance-Aware Sparse Linear Bandits. (arXiv:2205.13450v1 [cs.LG])
48. Follow-the-Perturbed-Leader for Adversarial Markov Decision Processes with Bandit Feedback. (arXiv:2205.13451v1 [cs.LG])
49. Continual evaluation for lifelong learning: Identifying the stability gap. (arXiv:2205.13452v1 [cs.LG])
50. AutoTSG: Learning and Synthesis for Incident Troubleshooting. (arXiv:2205.13457v1 [cs.SE])
51. Characterising Research Areas in the field of AI. (arXiv:2205.13471v1 [cs.AI])
52. Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency. (arXiv:2205.13476v1 [cs.LG])
53. Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse Observations. (arXiv:2205.13479v1 [cs.LG])
54. SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation. (arXiv:2205.13490v1 [cs.CV])
55. Sparse Graph Learning for Spatiotemporal Time Series. (arXiv:2205.13492v1 [cs.LG])
56. An Analytic Framework for Robust Training of Artificial Neural Networks. (arXiv:2205.13502v1 [cs.LG])
57. Are Transformers Effective for Time Series Forecasting?. (arXiv:2205.13504v1 [cs.AI])
58. Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality. (arXiv:2205.13521v1 [cs.AI])
59. Revealing the **Dark** Secrets of Masked Image Modeling. (arXiv:2205.13543v1 [cs.CV])
60. Memory AMP. (arXiv:2012.10861v6 [cs.IT] UPDATED)
61. TrustyAI Explainability Toolkit. (arXiv:2104.12717v2 [cs.AI] UPDATED)
62. Efficient and robust multi-task learning in the brain with modular latent primitives. (arXiv:2105.14108v2 [cs.AI] UPDATED)
63. Epistemic Neural Networks. (arXiv:2107.08924v3 [cs.LG] UPDATED)
64. An Anytime Hierarchical Approach for Stochastic Task and Motion Planning. (arXiv:2108.12537v2 [cs.RO] UPDATED)
65. Trustworthy AI: From Principles to Practices. (arXiv:2110.01167v2 [cs.AI] UPDATED)
66. The Neural Testbed: Evaluating Joint Predictions. (arXiv:2110.04629v3 [cs.LG] UPDATED)
67. TIP: Task-Informed Motion Prediction for Intelligent Vehicles. (arXiv:2110.08750v2 [cs.RO] UPDATED)
68. Towards the Generalization of Contrastive Self-Supervised Learning. (arXiv:2111.00743v3 [cs.LG] UPDATED)
69. Joint Synthesis of Safety Certificate and Safe Control Policy using Constrained Reinforcement Learning. (arXiv:2111.07695v3 [cs.LG] UPDATED)
70. FedHM: Efficient Federated Learning for Heterogeneous Models via Low-rank Factorization. (arXiv:2111.14655v2 [cs.LG] UPDATED)
71. Unsupervised Dense Information Retrieval with Contrastive Learning. (arXiv:2112.09118v2 [cs.IR] UPDATED)
72. Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (arXiv:2201.01080v3 [cs.CV] UPDATED)
73. Spiker: an FPGA-optimized Hardware acceleration for Spiking Neural Networks. (arXiv:2201.06993v3 [cs.NE] UPDATED)
74. Generalization Analysis of Message Passing Neural Networks on Large Random Graphs. (arXiv:2202.00645v5 [cs.LG] UPDATED)
75. Needs-aware Artificial Intelligence: AI that 'serves [human] needs'. (arXiv:2202.04977v3 [cs.AI] UPDATED)
76. ChemicalX: A Deep Learning Library for Drug Pair Scoring. (arXiv:2202.05240v3 [cs.LG] UPDATED)
77. The Shapley Value in Machine Learning. (arXiv:2202.05594v2 [cs.LG] UPDATED)
78. Continual Feature Selection: Spurious Features in Continual Learning. (arXiv:2203.01012v2 [cs.LG] UPDATED)
79. A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v2 [cs.CV] UPDATED)
80. RFID-Based Indoor Spatial Query Evaluation with Bayesian Filtering Techniques. (arXiv:2204.00747v2 [cs.AI] UPDATED)
81. Learning to Accelerate by the Methods of Step-size Planning. (arXiv:2204.01705v4 [cs.LG] UPDATED)
82. From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model. (arXiv:2205.00690v3 [cs.LG] UPDATED)
83. On the Utility of Prediction Sets in Human-AI Teams. (arXiv:2205.01411v2 [cs.AI] UPDATED)
84. ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation. (arXiv:2205.06457v2 [cs.CL] UPDATED)
85. An Approach for Automatic Construction of an Algorithmic Knowledge Graph from Textual Resources. (arXiv:2205.06854v2 [cs.AI] UPDATED)
86. A Computational Framework of Cortical Microcircuits Approximates Sign-concordant Random Backpropagation. (arXiv:2205.07292v3 [cs.NE] UPDATED)
87. IFTT-PIN: A PIN-Entry Method Leveraging the Self-Calibration Paradigm. (arXiv:2205.09534v2 [cs.HC] UPDATED)
88. CNNs are Myopic. (arXiv:2205.10760v2 [cs.CV] UPDATED)
89. Adaptive Fairness-Aware Online Meta-Learning for Changing Environments. (arXiv:2205.11264v2 [cs.LG] UPDATED)
90. Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v2 [cs.LG] UPDATED)
91. Towards a Defense against Backdoor Attacks in Continual Federated Learning. (arXiv:2205.11736v2 [cs.LG] UPDATED)
92. Adaptive Few-Shot Learning Algorithm for Rare Sound Event Detection. (arXiv:2205.11738v2 [cs.SD] UPDATED)
93. Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance. (arXiv:2205.12022v2 [cs.CV] UPDATED)
94. ColdGuess: A General and Effective Relational Graph Convolutional Network to Tackle Cold Start Cases. (arXiv:2205.12318v2 [cs.LG] UPDATED)
95. Augmentation-induced Consistency Regularization for Classification. (arXiv:2205.12461v2 [cs.LG] UPDATED)
96. UniInst: Unique Representation for End-to-End Instance Segmentation. (arXiv:2205.12646v2 [cs.CV] UPDATED)
97. Inception Transformer. (arXiv:2205.12956v2 [cs.CV] UPDATED)

