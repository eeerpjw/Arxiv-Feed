# Your interest papers
---
## cs.CV
---
### Reinforced **Swin**-Convs Transformer for Underwater Image **Enhancement**. (arXiv:2205.00434v1 [cs.CV])
- Authors : Tingdi Ren, Haiyong Xu, Gangyi Jiang, Mei Yu, Ting Luo
- Link : [http://arxiv.org/abs/2205.00434](http://arxiv.org/abs/2205.00434)
> ABSTRACT  :  Underwater Image **Enhancement** (UIE) technology aims to tackle the challenge of restoring the degraded underwater images due to light absorption and scattering. To address problems, a novel U-Net based Reinforced **Swin**-Convs Transformer for the Underwater Image **Enhancement** method (URSCT-UIE) is proposed. Specifically, with the deficiency of U-Net based on pure convolutions, we embedded the **Swin** Transformer into U-Net for improving the ability to capture the global dependency. Then, given the inadequacy of the **Swin** Transformer capturing the local attention, the reintroduction of convolutions may capture more local attention. Thus, we provide an ingenious manner for the fusion of convolutions and the core attention mechanism to build a Reinforced **Swin**-Convs Transformer Block (RSCTB) for capturing more local attention, which is reinforced in the channel and the spatial attention of the **Swin** Transformer. Finally, the experimental results on available datasets demonstrate that the proposed URSCT-UIE achieves state-of-the-art performance compared with other methods in terms of both subjective and objective evaluations. The code will be released on GitHub after acceptance.  
### Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v1 [eess.IV])
- Authors : Qiaoqiao Ding, Hui Ji, Yuhui Quan, Xiaoqun Zhang
- Link : [http://arxiv.org/abs/2205.00463](http://arxiv.org/abs/2205.00463)
> ABSTRACT  :  Low-dose CT (LDCT) imaging attracted a considerable interest for the reduction of the object's **exposure** to X-ray radiation. In recent years, supervised deep learning has been extensively studied for LDCT image reconstruction, which trains a network over a dataset containing many pairs of normal-dose and low-dose images. However, the challenge on collecting many such pairs in the clinical setup limits the application of such supervised-learning-based methods for LDCT image reconstruction in practice. Aiming at addressing the challenges raised by the collection of training dataset, this paper proposed a unsupervised deep learning method for LDCT image reconstruction, which does not require any external training data. The proposed method is built on a re-parametrization technique for Bayesian inference via deep network with random weights, combined with additional total variational (TV) regularization. The experiments show that the proposed method noticeably outperforms existing dataset-free image reconstruction methods on the test data.  
### Perception and Navigation in Autonomous Systems in the Era of Learning: A Survey. (arXiv:2001.02319v4 [cs.CV] UPDATED)
- Authors : Yang Tang, Chaoqiang Zhao, Jianrui Wang, Chongzhen Zhang, Qiyu Sun, Weixing Zheng, Wenli Du, Feng Qian, Juergen Kurths
- Link : [http://arxiv.org/abs/2001.02319](http://arxiv.org/abs/2001.02319)
> ABSTRACT  :  Autonomous systems possess the features of inferring their own state, understanding their surroundings, and performing autonomous navigation. With the applications of learning systems, like deep learning and reinforcement learning, the visual-based self-state estimation, environment perception and navigation capabilities of autonomous systems have been efficiently addressed, and many new learning-based algorithms have surfaced with respect to autonomous visual perception and navigation. In this review, we focus on the applications of learning-based monocular approaches in ego-motion perception, environment perception and navigation in autonomous systems, which is different from previous reviews that discussed traditional methods. First, we delineate the shortcomings of existing classical visual simultaneous localization and mapping (vSLAM) solutions, which demonstrate the necessity to integrate deep learning techniques. Second, we review the visual-based environmental perception and understanding methods based on deep learning, including deep learning-based monocular depth estimation, monocular ego-motion prediction, image **enhancement**, object detection, semantic segmentation, and their combinations with traditional vSLAM frameworks. Then, we focus on the visual navigation based on learning systems, mainly including reinforcement learning and deep reinforcement learning. Finally, we examine several challenges and promising directions discussed and concluded in related research of learning systems in the era of computer science and robotics.  
### HINet: Half Instance Normalization Network for Image **Restoration**. (arXiv:2105.06086v2 [eess.IV] UPDATED)
- Authors : Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, Chengpeng Chen
- Link : [http://arxiv.org/abs/2105.06086](http://arxiv.org/abs/2105.06086)
> ABSTRACT  :  In this paper, we explore the role of Instance Normalization in low-level vision tasks. Specifically, we present a novel block: Half Instance Normalization Block (HIN Block), to boost the performance of image **restoration** networks. Based on HIN Block, we design a simple and powerful multi-stage network named HINet, which consists of two subnetworks. With the help of HIN Block, HINet surpasses the state-of-the-art (SOTA) on various image **restoration** tasks. For image denoising, we exceed it 0.11dB and 0.28 dB in PSNR on SIDD dataset, with only 7.5% and 30% of its multiplier-accumulator operations (MACs), 6.8 times and 2.9 times speedup respectively. For image deblurring, we get comparable performance with 22.5% of its MACs and 3.3 times speedup on REDS and GoPro datasets. For image deraining, we exceed it by 0.3 dB in PSNR on the average result of multiple datasets with 1.4 times speedup. With HINet, we won 1st place on the NTIRE 2021 Image Deblurring Challenge - Track2. JPEG Artifacts, with a PSNR of 29.70. The code is available at https://github.com/megvii-model/HINet.  
### Depth-supervised **NeRF**: Fewer Views and Faster Training for Free. (arXiv:2107.02791v2 [cs.CV] UPDATED)
- Authors : Kangle Deng, Andrew Liu, Yan Zhu, Deva Ramanan
- Link : [http://arxiv.org/abs/2107.02791](http://arxiv.org/abs/2107.02791)
> ABSTRACT  :  A commonly observed failure mode of Neural Radiance Field (**NeRF**) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-**NeRF** (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current **NeRF** pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as "free" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-**NeRF** can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed **NeRF** methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-**NeRF** can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.  
### Learning-Based Video Coding with Joint Deep Compression and **Enhancement**. (arXiv:2111.14474v2 [eess.IV] UPDATED)
- Authors : Tiesong Zhao, Weize Feng, Hongji Zeng, Yuzhen Niu, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2111.14474](http://arxiv.org/abs/2111.14474)
> ABSTRACT  :  The end-to-end learning-based video compression has attracted substantial attentions by paving another way to compress video signals as stacked visual features. This paper proposes an efficient end-to-end deep video codec with jointly optimized compression and **enhancement** modules (JCEVC). First, we propose a dual-path generative adversarial network (DPEG) to reconstruct video details after compression. An $\alpha$-path facilitates the structure information reconstruction with a large receptive field and multi-frame references, while a $\beta$-path facilitates the reconstruction of local textures. Both paths are fused and co-trained within a generative-adversarial process. Second, we reuse the DPEG network in both motion compensation and quality **enhancement** modules, which are further combined with other necessary modules to formulate our JCEVC framework. Third, we employ a joint training of deep video compression and **enhancement** that further improves the rate-distortion (RD) performance of compression. Compared with x265 LDP very fast mode, our JCEVC reduces the average bit-per-pixel (bpp) by 39.39\%/54.92\% at the same PSNR/MS-SSIM, which outperforms the state-of-the-art deep video codecs by a considerable margin.  
### Head**NeRF**: A **Real-time** **NeRF**-based Parametric Head Model. (arXiv:2112.05637v3 [cs.CV] UPDATED)
- Authors : Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, Juyong Zhang
- Link : [http://arxiv.org/abs/2112.05637](http://arxiv.org/abs/2112.05637)
> ABSTRACT  :  In this paper, we propose Head**NeRF**, a novel **NeRF**-based parametric head model that integrates the neural radiance field to the parametric representation of the human head. It can render high fidelity head images in real-time on modern GPUs, and supports directly controlling the generated images' rendering pose and various semantic attributes. Different from existing related parametric models, we use the neural radiance fields as a novel 3D proxy instead of the traditional 3D textured mesh, which makes that Head**NeRF** is able to generate high fidelity images. However, the computationally expensive rendering process of the original **NeRF** hinders the construction of the parametric **NeRF** model. To address this issue, we adopt the strategy of integrating 2D neural rendering to the rendering process of **NeRF** and design novel loss terms. As a result, the rendering speed of Head**NeRF** can be significantly accelerated, and the rendering time of one frame is reduced from 5s to 25ms. The well designed loss terms also improve the rendering accuracy, and the fine-level details of the human head, such as the gaps between teeth, wrinkles, and beards, can be represented and synthesized by Head**NeRF**. Extensive experimental results and several applications demonstrate its effectiveness. The trained parametric model is available at https://github.com/CrisHY1995/headnerf.  
### Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v3 [cs.CV] UPDATED)
- Authors : Zhenyu Jiang, Chun Hsu, Yuke Zhu
- Link : [http://arxiv.org/abs/2202.08227](http://arxiv.org/abs/2202.08227)
> ABSTRACT  :  Digitizing physical objects into the virtual world has the potential to unlock new research and applications in embodied AI and mixed reality. This work focuses on recreating interactive digital twins of real-world articulated objects, which can be directly imported into virtual environments. We introduce Ditto to learn articulation model estimation and 3D geometry reconstruction of an articulated object through interactive perception. Given a pair of visual observations of an articulated object before and after interaction, Ditto reconstructs part-level geometry and estimates the articulation model of the object. We employ **implicit neural representation**s for joint geometry and articulation modeling. Our experiments show that Ditto effectively builds digital twins of articulated objects in a category-agnostic way. We also apply Ditto to real-world objects and deploy the recreated digital twins in physical simulation. Code and additional results are available at https://ut-austin-rpl.github.io/Ditto  
### Adaptive Cross-Layer Attention for Image **Restoration**. (arXiv:2203.03619v2 [eess.IV] UPDATED)
- Authors : Yancheng Wang, Ning Xu, Yingzhen Yang
- Link : [http://arxiv.org/abs/2203.03619](http://arxiv.org/abs/2203.03619)
> ABSTRACT  :  Non-local attention module has been proven to be crucial for image **restoration**. Conventional non-local attention processes features of each layer separately, so it risks missing correlation between features among different layers. To address this problem, we propose Cross-Layer Attention (CLA) module in this paper. Instead of finding correlated key pixels within the same layer, each query pixel can attend to key pixels at previous layers of the network. In order to further enhance the learning capability and reduce the inference cost of CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two adaptive designs are proposed for ACLA: 1) adaptively selecting the keys for non-local attention at each layer; 2) automatically searching for the insertion locations for ACLA modules. By these two adaptive designs, ACLA dynamically selects the number of keys to be aggregated for non-local attention at layer. In addition, ACLA searches for the optimal insert positions of ACLA modules by a neural architecture search method to render a compact neural network with compelling performance. Extensive experiments on image **restoration** tasks, including single image super-resolution, image denoising, image demosaicing, and image compression artifacts reduction, validate the effectiveness and efficiency of ACLA. The code of CLA and ACLA is available at \url{https://github.com/SDL-ASU/ACLA}.  
## eess.IV
---
### Physics-guided Terahertz Computational Imaging. (arXiv:2205.00327v1 [eess.IV])
- Authors : Tai Su, Chun Hung, Jen Yu, Wen Lin, Hua Yang
- Link : [http://arxiv.org/abs/2205.00327](http://arxiv.org/abs/2205.00327)
> ABSTRACT  :  Visualizing information inside objects is an ever-lasting need to bridge the world from physics, chemistry, biology to computation. Among all tomographic techniques, terahertz (THz) computational imaging has demonstrated its unique sensing features to digitalize multi-dimensional object information in a non-destructive, non-ionizing, and non-invasive way. Applying modern signal processing and physics-guided modalities, THz computational imaging systems are now launched in various application fields in industrial inspection, security screening, chemical inspection and non-destructive evaluation. In this article, we overview recent advances in THz computational imaging modalities in the aspects of system configuration, wave propagation and interaction models, physics-guided algorithm for digitalizing interior information of imaged objects. Several image **restoration** and reconstruction issues based on multi-dimensional THz signals are further discussed, which provides a crosslink between material digitalization, functional property extraction, and multi-dimensional imager utilization from a signal processing perspective.  
### Reinforced **Swin**-Convs Transformer for Underwater Image **Enhancement**. (arXiv:2205.00434v1 [cs.CV])
- Authors : Tingdi Ren, Haiyong Xu, Gangyi Jiang, Mei Yu, Ting Luo
- Link : [http://arxiv.org/abs/2205.00434](http://arxiv.org/abs/2205.00434)
> ABSTRACT  :  Underwater Image **Enhancement** (UIE) technology aims to tackle the challenge of restoring the degraded underwater images due to light absorption and scattering. To address problems, a novel U-Net based Reinforced **Swin**-Convs Transformer for the Underwater Image **Enhancement** method (URSCT-UIE) is proposed. Specifically, with the deficiency of U-Net based on pure convolutions, we embedded the **Swin** Transformer into U-Net for improving the ability to capture the global dependency. Then, given the inadequacy of the **Swin** Transformer capturing the local attention, the reintroduction of convolutions may capture more local attention. Thus, we provide an ingenious manner for the fusion of convolutions and the core attention mechanism to build a Reinforced **Swin**-Convs Transformer Block (RSCTB) for capturing more local attention, which is reinforced in the channel and the spatial attention of the **Swin** Transformer. Finally, the experimental results on available datasets demonstrate that the proposed URSCT-UIE achieves state-of-the-art performance compared with other methods in terms of both subjective and objective evaluations. The code will be released on GitHub after acceptance.  
### Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v1 [eess.IV])
- Authors : Qiaoqiao Ding, Hui Ji, Yuhui Quan, Xiaoqun Zhang
- Link : [http://arxiv.org/abs/2205.00463](http://arxiv.org/abs/2205.00463)
> ABSTRACT  :  Low-dose CT (LDCT) imaging attracted a considerable interest for the reduction of the object's **exposure** to X-ray radiation. In recent years, supervised deep learning has been extensively studied for LDCT image reconstruction, which trains a network over a dataset containing many pairs of normal-dose and low-dose images. However, the challenge on collecting many such pairs in the clinical setup limits the application of such supervised-learning-based methods for LDCT image reconstruction in practice. Aiming at addressing the challenges raised by the collection of training dataset, this paper proposed a unsupervised deep learning method for LDCT image reconstruction, which does not require any external training data. The proposed method is built on a re-parametrization technique for Bayesian inference via deep network with random weights, combined with additional total variational (TV) regularization. The experiments show that the proposed method noticeably outperforms existing dataset-free image reconstruction methods on the test data.  
### Lightweight Image **Enhancement** Network for Mobile Devices Using Self-Feature Extraction and Dense Modulation. (arXiv:2205.00853v1 [eess.IV])
- Authors : Sangwook Baek, Yongsup Park, Youngo Park, Jungmin Lee, Kwangpyo Choi
- Link : [http://arxiv.org/abs/2205.00853](http://arxiv.org/abs/2205.00853)
> ABSTRACT  :  Convolutional neural network (CNN) based image **enhancement** methods such as super-resolution and detail **enhancement** have achieved remarkable performances. However, amounts of operations including convolution and parameters within the networks cost high computing power and need huge memory resource, which limits the applications with on-device requirements. Lightweight image **enhancement** network should restore details, texture, and structural information from low-resolution input images while keeping their fidelity. To address these issues, a lightweight image **enhancement** network is proposed. The proposed network include self-feature extraction module which produces modulation parameters from low-quality image itself, and provides them to modulate the features in the network. Also, dense modulation block is proposed for unit block of the proposed network, which uses dense connections of concatenated features applied in modulation layers. Experimental results demonstrate better performance over existing approaches in terms of both quantitative and qualitative evaluations.  
### HINet: Half Instance Normalization Network for Image **Restoration**. (arXiv:2105.06086v2 [eess.IV] UPDATED)
- Authors : Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, Chengpeng Chen
- Link : [http://arxiv.org/abs/2105.06086](http://arxiv.org/abs/2105.06086)
> ABSTRACT  :  In this paper, we explore the role of Instance Normalization in low-level vision tasks. Specifically, we present a novel block: Half Instance Normalization Block (HIN Block), to boost the performance of image **restoration** networks. Based on HIN Block, we design a simple and powerful multi-stage network named HINet, which consists of two subnetworks. With the help of HIN Block, HINet surpasses the state-of-the-art (SOTA) on various image **restoration** tasks. For image denoising, we exceed it 0.11dB and 0.28 dB in PSNR on SIDD dataset, with only 7.5% and 30% of its multiplier-accumulator operations (MACs), 6.8 times and 2.9 times speedup respectively. For image deblurring, we get comparable performance with 22.5% of its MACs and 3.3 times speedup on REDS and GoPro datasets. For image deraining, we exceed it by 0.3 dB in PSNR on the average result of multiple datasets with 1.4 times speedup. With HINet, we won 1st place on the NTIRE 2021 Image Deblurring Challenge - Track2. JPEG Artifacts, with a PSNR of 29.70. The code is available at https://github.com/megvii-model/HINet.  
### Learning-Based Video Coding with Joint Deep Compression and **Enhancement**. (arXiv:2111.14474v2 [eess.IV] UPDATED)
- Authors : Tiesong Zhao, Weize Feng, Hongji Zeng, Yuzhen Niu, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2111.14474](http://arxiv.org/abs/2111.14474)
> ABSTRACT  :  The end-to-end learning-based video compression has attracted substantial attentions by paving another way to compress video signals as stacked visual features. This paper proposes an efficient end-to-end deep video codec with jointly optimized compression and **enhancement** modules (JCEVC). First, we propose a dual-path generative adversarial network (DPEG) to reconstruct video details after compression. An $\alpha$-path facilitates the structure information reconstruction with a large receptive field and multi-frame references, while a $\beta$-path facilitates the reconstruction of local textures. Both paths are fused and co-trained within a generative-adversarial process. Second, we reuse the DPEG network in both motion compensation and quality **enhancement** modules, which are further combined with other necessary modules to formulate our JCEVC framework. Third, we employ a joint training of deep video compression and **enhancement** that further improves the rate-distortion (RD) performance of compression. Compared with x265 LDP very fast mode, our JCEVC reduces the average bit-per-pixel (bpp) by 39.39\%/54.92\% at the same PSNR/MS-SSIM, which outperforms the state-of-the-art deep video codecs by a considerable margin.  
### Adaptive Cross-Layer Attention for Image **Restoration**. (arXiv:2203.03619v2 [eess.IV] UPDATED)
- Authors : Yancheng Wang, Ning Xu, Yingzhen Yang
- Link : [http://arxiv.org/abs/2203.03619](http://arxiv.org/abs/2203.03619)
> ABSTRACT  :  Non-local attention module has been proven to be crucial for image **restoration**. Conventional non-local attention processes features of each layer separately, so it risks missing correlation between features among different layers. To address this problem, we propose Cross-Layer Attention (CLA) module in this paper. Instead of finding correlated key pixels within the same layer, each query pixel can attend to key pixels at previous layers of the network. In order to further enhance the learning capability and reduce the inference cost of CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two adaptive designs are proposed for ACLA: 1) adaptively selecting the keys for non-local attention at each layer; 2) automatically searching for the insertion locations for ACLA modules. By these two adaptive designs, ACLA dynamically selects the number of keys to be aggregated for non-local attention at layer. In addition, ACLA searches for the optimal insert positions of ACLA modules by a neural architecture search method to render a compact neural network with compelling performance. Extensive experiments on image **restoration** tasks, including single image super-resolution, image denoising, image demosaicing, and image compression artifacts reduction, validate the effectiveness and efficiency of ACLA. The code of CLA and ACLA is available at \url{https://github.com/SDL-ASU/ACLA}.  
## cs.LG
---
### Joint Multisided **Exposure** Fairness for Recommendation. (arXiv:2205.00048v1 [cs.IR])
- Authors : Haolun Wu, Bhaskar Mitra, Chen Ma, Fernando Diaz, Xue Liu
- Link : [http://arxiv.org/abs/2205.00048](http://arxiv.org/abs/2205.00048)
> ABSTRACT  :  Prior research on **exposure** fairness in the context of recommender systems has focused mostly on disparities in the **exposure** of individual or groups of items to individual users of the system. The problem of how individual or groups of items may be systemically under or over exposed to groups of users, or even all users, has received relatively less attention. However, such systemic disparities in information **exposure** can result in observable social harms, such as withholding economic opportunities from historically marginalized groups (allocative harm) or amplifying gendered and racialized stereotypes (representational harm). Previously, Diaz et al. developed the expected **exposure** metric -- that incorporates existing user browsing models that have previously been developed for information retrieval -- to study fairness of content **exposure** to individual users. We extend their proposed framework to formalize a family of **exposure** fairness metrics that model the problem jointly from the perspective of both the consumers and producers. Specifically, we consider group attributes for both types of stakeholders to identify and mitigate fairness concerns that go beyond individual users and items towards more systemic biases in recommendation. Furthermore, we study and discuss the relationships between the different **exposure** fairness dimensions proposed in this paper, as well as demonstrate how stochastic ranking policies can be optimized towards said fairness goals.  
### Depth-supervised **NeRF**: Fewer Views and Faster Training for Free. (arXiv:2107.02791v2 [cs.CV] UPDATED)
- Authors : Kangle Deng, Andrew Liu, Yan Zhu, Deva Ramanan
- Link : [http://arxiv.org/abs/2107.02791](http://arxiv.org/abs/2107.02791)
> ABSTRACT  :  A commonly observed failure mode of Neural Radiance Field (**NeRF**) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-**NeRF** (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current **NeRF** pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as "free" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-**NeRF** can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed **NeRF** methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-**NeRF** can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.  
### Heterogeneous Information Network based Default Analysis on Banking Micro and Small Enterprise Users. (arXiv:2204.11849v2 [q-fin.RM] UPDATED)
- Authors : Zheng Zhang, Yingsheng Ji, Jiachen Shen, Xi Zhang, Guangwen Yang
- Link : [http://arxiv.org/abs/2204.11849](http://arxiv.org/abs/2204.11849)
> ABSTRACT  :  Risk assessment is a substantial problem for financial institutions that has been extensively studied both for its methodological richness and its various practical applications. With the expansion of inclusive finance, recent attentions are paid to micro and small-sized enterprises (MSEs). Compared with large companies, MSEs present a higher **exposure** rate to default owing to their insecure financial stability. Conventional efforts learn classifiers from historical data with elaborate feature engineering. However, the main obstacle for MSEs involves severe deficiency in credit-related information, which may degrade the performance of prediction. Besides, financial activities have diverse explicit and implicit relations, which have not been fully exploited for risk judgement in commercial banks. In particular, the observations on real data show that various relationships between company users have additional power in financial risk analysis. In this paper, we consider a graph of banking data, and propose a novel HIDAM model for the purpose. Specifically, we attempt to incorporate heterogeneous information network with rich attributes on multi-typed nodes and links for modeling the scenario of business banking service. To enhance feature representation of MSEs, we extract interactive information through meta-paths and fully exploit path information. Furthermore, we devise a hierarchical attention mechanism respectively to learn the importance of contents inside each meta-path and the importance of different metapahs. Experimental results verify that HIDAM outperforms state-of-the-art competitors on real-world banking data.  
### Continual Learning for Peer-to-Peer Federated Learning: A Study on Automated Brain Metastasis Identification. (arXiv:2204.13591v2 [cs.LG] UPDATED)
- Authors : Yixing Huang, Christoph Bert, Stefan Fischer, Manuel Schmidt, Andreas Maier, Rainer Fietkau, Florian Putz
- Link : [http://arxiv.org/abs/2204.13591](http://arxiv.org/abs/2204.13591)
> ABSTRACT  :  Due to data privacy constraints, data sharing among multiple centers is restricted. Continual learning, as one approach to peer-to-peer federated learning, can promote multicenter collaboration on deep learning algorithm development by sharing intermediate models instead of training data. This work aims to investigate the feasibility of continual learning for multicenter collaboration on an exemplary application of brain metastasis identification using DeepMedic. 920 T1 MRI contrast enhanced volumes are split to simulate multicenter collaboration scenarios. A continual learning algorithm, synaptic intelligence (SI), is applied to preserve important model weights for training one center after another. In a **bilateral** collaboration scenario, continual learning with SI achieves a sensitivity of 0.917, and naive continual learning without SI achieves a sensitivity of 0.906, while two models trained on internal data solely without continual learning achieve sensitivity of 0.853 and 0.831 only. In a seven-center multilateral collaboration scenario, the models trained on internal datasets (100 volumes each center) without continual learning obtain a mean sensitivity value of 0.699. With single-visit continual learning (i.e., the shared model visits each center only once during training), the sensitivity is improved to 0.788 and 0.849 without SI and with SI, respectively. With iterative continual learning (i.e., the shared model revisits each center multiple times during training), the sensitivity is further improved to 0.914, which is identical to the sensitivity using mixed data for training. Our experiments demonstrate that continual learning can improve brain metastasis identification performance for centers with limited data. This study demonstrates the feasibility of applying continual learning for peer-to-peer federated learning in multicenter collaboration.  
## cs.AI
---
### Joint Multisided **Exposure** Fairness for Recommendation. (arXiv:2205.00048v1 [cs.IR])
- Authors : Haolun Wu, Bhaskar Mitra, Chen Ma, Fernando Diaz, Xue Liu
- Link : [http://arxiv.org/abs/2205.00048](http://arxiv.org/abs/2205.00048)
> ABSTRACT  :  Prior research on **exposure** fairness in the context of recommender systems has focused mostly on disparities in the **exposure** of individual or groups of items to individual users of the system. The problem of how individual or groups of items may be systemically under or over exposed to groups of users, or even all users, has received relatively less attention. However, such systemic disparities in information **exposure** can result in observable social harms, such as withholding economic opportunities from historically marginalized groups (allocative harm) or amplifying gendered and racialized stereotypes (representational harm). Previously, Diaz et al. developed the expected **exposure** metric -- that incorporates existing user browsing models that have previously been developed for information retrieval -- to study fairness of content **exposure** to individual users. We extend their proposed framework to formalize a family of **exposure** fairness metrics that model the problem jointly from the perspective of both the consumers and producers. Specifically, we consider group attributes for both types of stakeholders to identify and mitigate fairness concerns that go beyond individual users and items towards more systemic biases in recommendation. Furthermore, we study and discuss the relationships between the different **exposure** fairness dimensions proposed in this paper, as well as demonstrate how stochastic ranking policies can be optimized towards said fairness goals.  
### Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v3 [cs.CV] UPDATED)
- Authors : Zhenyu Jiang, Chun Hsu, Yuke Zhu
- Link : [http://arxiv.org/abs/2202.08227](http://arxiv.org/abs/2202.08227)
> ABSTRACT  :  Digitizing physical objects into the virtual world has the potential to unlock new research and applications in embodied AI and mixed reality. This work focuses on recreating interactive digital twins of real-world articulated objects, which can be directly imported into virtual environments. We introduce Ditto to learn articulation model estimation and 3D geometry reconstruction of an articulated object through interactive perception. Given a pair of visual observations of an articulated object before and after interaction, Ditto reconstructs part-level geometry and estimates the articulation model of the object. We employ **implicit neural representation**s for joint geometry and articulation modeling. Our experiments show that Ditto effectively builds digital twins of articulated objects in a category-agnostic way. We also apply Ditto to real-world objects and deploy the recreated digital twins in physical simulation. Code and additional results are available at https://ut-austin-rpl.github.io/Ditto  
### Heterogeneous Information Network based Default Analysis on Banking Micro and Small Enterprise Users. (arXiv:2204.11849v2 [q-fin.RM] UPDATED)
- Authors : Zheng Zhang, Yingsheng Ji, Jiachen Shen, Xi Zhang, Guangwen Yang
- Link : [http://arxiv.org/abs/2204.11849](http://arxiv.org/abs/2204.11849)
> ABSTRACT  :  Risk assessment is a substantial problem for financial institutions that has been extensively studied both for its methodological richness and its various practical applications. With the expansion of inclusive finance, recent attentions are paid to micro and small-sized enterprises (MSEs). Compared with large companies, MSEs present a higher **exposure** rate to default owing to their insecure financial stability. Conventional efforts learn classifiers from historical data with elaborate feature engineering. However, the main obstacle for MSEs involves severe deficiency in credit-related information, which may degrade the performance of prediction. Besides, financial activities have diverse explicit and implicit relations, which have not been fully exploited for risk judgement in commercial banks. In particular, the observations on real data show that various relationships between company users have additional power in financial risk analysis. In this paper, we consider a graph of banking data, and propose a novel HIDAM model for the purpose. Specifically, we attempt to incorporate heterogeneous information network with rich attributes on multi-typed nodes and links for modeling the scenario of business banking service. To enhance feature representation of MSEs, we extract interactive information through meta-paths and fully exploit path information. Furthermore, we devise a hierarchical attention mechanism respectively to learn the importance of contents inside each meta-path and the importance of different metapahs. Experimental results verify that HIDAM outperforms state-of-the-art competitors on real-world banking data.  
### Continual Learning for Peer-to-Peer Federated Learning: A Study on Automated Brain Metastasis Identification. (arXiv:2204.13591v2 [cs.LG] UPDATED)
- Authors : Yixing Huang, Christoph Bert, Stefan Fischer, Manuel Schmidt, Andreas Maier, Rainer Fietkau, Florian Putz
- Link : [http://arxiv.org/abs/2204.13591](http://arxiv.org/abs/2204.13591)
> ABSTRACT  :  Due to data privacy constraints, data sharing among multiple centers is restricted. Continual learning, as one approach to peer-to-peer federated learning, can promote multicenter collaboration on deep learning algorithm development by sharing intermediate models instead of training data. This work aims to investigate the feasibility of continual learning for multicenter collaboration on an exemplary application of brain metastasis identification using DeepMedic. 920 T1 MRI contrast enhanced volumes are split to simulate multicenter collaboration scenarios. A continual learning algorithm, synaptic intelligence (SI), is applied to preserve important model weights for training one center after another. In a **bilateral** collaboration scenario, continual learning with SI achieves a sensitivity of 0.917, and naive continual learning without SI achieves a sensitivity of 0.906, while two models trained on internal data solely without continual learning achieve sensitivity of 0.853 and 0.831 only. In a seven-center multilateral collaboration scenario, the models trained on internal datasets (100 volumes each center) without continual learning obtain a mean sensitivity value of 0.699. With single-visit continual learning (i.e., the shared model visits each center only once during training), the sensitivity is improved to 0.788 and 0.849 without SI and with SI, respectively. With iterative continual learning (i.e., the shared model revisits each center multiple times during training), the sensitivity is further improved to 0.914, which is identical to the sensitivity using mixed data for training. Our experiments demonstrate that continual learning can improve brain metastasis identification performance for centers with limited data. This study demonstrates the feasibility of applying continual learning for peer-to-peer federated learning in multicenter collaboration.  
# Paper List
---
## cs.CV
---
**109** new papers in cs.CV:-) 
1. Birds' Eye View: Measuring Behavior and Posture of Chickens as a Metric for Their Well-Being. (arXiv:2205.00069v1 [cs.CV])
2. On Negative Sampling for Audio-Visual Contrastive Learning from Movies. (arXiv:2205.00073v1 [cs.CV])
3. A Simple Method to Boost Human Pose Estimation Accuracy by Correcting the Joint Regressor for the Human3.6m Dataset. (arXiv:2205.00076v1 [cs.CV])
4. Unsupervised Contrastive Learning based Transformer for Lung Nodule Detection. (arXiv:2205.00122v1 [cs.CV])
5. Gaze-enhanced Crossmodal Embeddings for Emotion Recognition. (arXiv:2205.00129v1 [cs.LG])
6. Learn to Understand Negation in Video Retrieval. (arXiv:2205.00132v1 [cs.MM])
7. Multimodal Representation Learning With Text and Images. (arXiv:2205.00142v1 [cs.LG])
8. Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator. (arXiv:2205.00146v1 [cs.CV])
9. AnimalTrack: A Large-scale Benchmark for Multi-Animal Tracking in the Wild. (arXiv:2205.00158v1 [cs.CV])
10. SVTR: Scene Text Recognition with a Single Visual Model. (arXiv:2205.00159v1 [cs.CV])
11. Elucidating Meta-Structures of Noisy Labels in Semantic Segmentation by Deep Neural Networks. (arXiv:2205.00160v1 [cs.CV])
12. ClusterQ: Semantic Feature Distribution Alignment for Data-Free Quantization. (arXiv:2205.00179v1 [cs.CV])
13. Reliable Label Correction is a Good Booster When Learning with Extremely Noisy Labels. (arXiv:2205.00186v1 [cs.CV])
14. "And Then There Were None": Cracking White-box DNN Watermarks via Invariant Neuron Transforms. (arXiv:2205.00199v1 [cs.CR])
15. DefakeHop++: An Enhanced Lightweight Deepfake Detector. (arXiv:2205.00211v1 [cs.CV])
16. Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer. (arXiv:2205.00214v1 [cs.CV])
17. Recognising Known Configurations of Garments For Dual-Arm Robotic Flattening. (arXiv:2205.00225v1 [cs.RO])
18. Unsupervised Visible-light Images Guided Cross-Spectrum Depth Estimation from Dual-Modality Cameras. (arXiv:2205.00257v1 [cs.CV])
19. Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning. (arXiv:2205.00272v1 [cs.CV])
20. Dynamic Curriculum Learning for Great Ape Detection in the Wild. (arXiv:2205.00275v1 [cs.CV])
21. ONCE-3DLanes: Building Monocular 3D Lane Detection. (arXiv:2205.00301v1 [cs.CV])
22. Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs. (arXiv:2205.00303v1 [cs.CV])
23. Source Domain Subset Sampling for Semi-Supervised Domain Adaptation in Semantic Segmentation. (arXiv:2205.00312v1 [cs.CV])
24. LayoutBERT: Masked Language Layout Model for Object Insertion. (arXiv:2205.00347v1 [cs.CV])
25. Visual Spatial Reasoning. (arXiv:2205.00363v1 [cs.CL])
26. RADNet: A Deep Neural Network Model for Robust Perception in Moving Autonomous Systems. (arXiv:2205.00364v1 [cs.CV])
27. Fractional Vegetation Cover Estimation using Hough Lines and Linear Iterative Clustering. (arXiv:2205.00366v1 [cs.CV])
28. Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving. (arXiv:2205.00376v1 [cs.CV])
29. Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition. (arXiv:2205.00380v1 [cs.CV])
30. Convex Combination Consistency between Neighbors for Weakly-supervised Action Localization. (arXiv:2205.00400v1 [cs.CV])
31. Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. (arXiv:2205.00415v1 [cs.CL])
32. UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog. (arXiv:2205.00423v1 [cs.CV])
33. Analysis of Diffractive Neural Networks for Seeing Through Random Diffusers. (arXiv:2205.00428v1 [physics.optics])
34. Reinforced **Swin**-Convs Transformer for Underwater Image **Enhancement**. (arXiv:2205.00434v1 [cs.CV])
35. Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v1 [eess.IV])
36. Preserve Pre-trained Knowledge: Transfer Learning With Self-Distillation For Action Recognition. (arXiv:2205.00506v1 [cs.CV])
37. The Best of Both Worlds: Combining Model-based and Nonparametric Approaches for 3D Human Body Estimation. (arXiv:2205.00508v1 [cs.CV])
38. Deep vs. Shallow Learning: A Benchmark Study in Low Magnitude Earthquake Detection. (arXiv:2205.00525v1 [cs.LG])
39. COUCH: Towards Controllable Human-Chair Interactions. (arXiv:2205.00541v1 [cs.CV])
40. Using a novel fractional-order gradient method for CNN back-propagation. (arXiv:2205.00581v1 [cs.CV])
41. MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries. (arXiv:2205.00613v1 [cs.CV])
42. DFC: Anatomically Informed Fiber Clustering with Self-supervised Deep Learning for Fast and Effective Tractography Parcellation. (arXiv:2205.00627v1 [cs.CV])
43. Design equivariant neural networks for 3D point cloud. (arXiv:2205.00630v1 [cs.CV])
44. Enhancing Adversarial Training with Feature Separability. (arXiv:2205.00637v1 [cs.CV])
45. An Application to Generate Style Guided Compatible Outfit. (arXiv:2205.00663v1 [cs.IR])
46. Revisiting Classical Multiclass Linear Discriminant Analysis with a Novel Prototype-based Interpretable Solution. (arXiv:2205.00668v1 [cs.CV])
47. Deep Video Harmonization with Color Mapping Consistency. (arXiv:2205.00687v1 [cs.CV])
48. Dynamic Graph Message Passing Networks. (arXiv:1908.06955v4 [cs.CV] UPDATED)
49. Perception and Navigation in Autonomous Systems in the Era of Learning: A Survey. (arXiv:2001.02319v4 [cs.CV] UPDATED)
50. Single-Frame based Deep View Synchronization for Unsynchronized Multi-Camera Surveillance. (arXiv:2007.03891v3 [cs.CV] UPDATED)
51. Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation. (arXiv:2009.03787v5 [cs.RO] UPDATED)
52. Long-tailed Recognition by Routing Diverse Distribution-Aware Experts. (arXiv:2010.01809v4 [cs.CV] UPDATED)
53. Shedding Light on Blind Spots: Developing a Reference Architecture to Leverage Video Data for Process Mining. (arXiv:2010.11289v3 [cs.CV] UPDATED)
54. Wide-Area Crowd Counting: Multi-View Fusion Networks for Counting in Large Scenes. (arXiv:2012.00946v2 [cs.CV] UPDATED)
55. CosSGD: Communication-Efficient Federated Learning with a Simple Cosine-Based Quantization. (arXiv:2012.08241v2 [cs.LG] UPDATED)
56. Slimmable Compressive Autoencoders for Practical Neural Image Compression. (arXiv:2103.15726v2 [eess.IV] UPDATED)
57. Federated Generalized Face Presentation Attack Detection. (arXiv:2104.06595v2 [cs.CV] UPDATED)
58. HINet: Half Instance Normalization Network for Image **Restoration**. (arXiv:2105.06086v2 [eess.IV] UPDATED)
59. Probing the Effect of Selection Bias on Generalization: A Thought Experiment. (arXiv:2105.09934v2 [cs.CV] UPDATED)
60. SDOF-Tracker: Fast and Accurate Multiple Human Tracking by Skipped-Detection and Optical-Flow. (arXiv:2106.14259v3 [cs.CV] UPDATED)
61. Boggart: Towards General-Purpose Acceleration of Retrospective Video Analytics. (arXiv:2106.15315v2 [cs.CV] UPDATED)
62. No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v6 [cs.CV] UPDATED)
63. Depth-supervised **NeRF**: Fewer Views and Faster Training for Free. (arXiv:2107.02791v2 [cs.CV] UPDATED)
64. RewriteNet: Reliable Scene Text Editing with Implicit Decomposition of Text Contents and Styles. (arXiv:2107.11041v2 [cs.CV] UPDATED)
65. MMChat: Multi-Modal Chat Dataset on Social Media. (arXiv:2108.07154v3 [cs.CL] UPDATED)
66. Rethinking the Misalignment Problem in Dense Object Detection. (arXiv:2108.12176v5 [cs.CV] UPDATED)
67. Generative Wind Power Curve Modeling Via Machine Vision: A Self-learning Deep Convolutional Network Based Method. (arXiv:2109.00894v2 [cs.CV] UPDATED)
68. Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v5 [eess.IV] UPDATED)
69. Audio-Visual Collaborative Representation Learning for Dynamic Saliency Prediction. (arXiv:2109.08371v3 [cs.CV] UPDATED)
70. HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v6 [cs.CV] UPDATED)
71. Skill Induction and Planning with Latent Language. (arXiv:2110.01517v2 [cs.LG] UPDATED)
72. ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v5 [cs.IT] UPDATED)
73. CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking Detection. (arXiv:2110.11867v2 [cs.CV] UPDATED)
74. SOFT: Softmax-free Transformer with Linear Complexity. (arXiv:2110.11945v3 [cs.CV] UPDATED)
75. A vectorized sea horizon edge filter for maritime video processing tasks. (arXiv:2110.13694v3 [cs.CV] UPDATED)
76. A Survey of Visual Transformers. (arXiv:2111.06091v3 [cs.CV] UPDATED)
77. LIMEcraft: Handcrafted superpixel selection and inspection for Visual eXplanations. (arXiv:2111.08094v2 [cs.CV] UPDATED)
78. Combined Scaling for Open-Vocabulary Image Classification. (arXiv:2111.10050v2 [cs.LG] UPDATED)
79. LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking. (arXiv:2111.11892v2 [cs.CV] UPDATED)
80. SPCL: A New Framework for Domain Adaptive Semantic Segmentation via Semantic Prototype-based Contrastive Learning. (arXiv:2111.12358v2 [cs.CV] UPDATED)
81. Targeted Supervised Contrastive Learning for Long-Tailed Recognition. (arXiv:2111.13998v2 [cs.CV] UPDATED)
82. Learning-Based Video Coding with Joint Deep Compression and **Enhancement**. (arXiv:2111.14474v2 [eess.IV] UPDATED)
83. CSG0: Continual Urban Scene Generation with Zero Forgetting. (arXiv:2112.03252v2 [cs.CV] UPDATED)
84. Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection. (arXiv:2112.04532v2 [cs.CV] UPDATED)
85. Head**NeRF**: A **Real-time** **NeRF**-based Parametric Head Model. (arXiv:2112.05637v3 [cs.CV] UPDATED)
86. Stochastic Planner-Actor-Critic for Unsupervised Deformable Image Registration. (arXiv:2112.07415v2 [eess.IV] UPDATED)
87. Continually Learning Self-Supervised Representations with Projected Functional Regularization. (arXiv:2112.15022v2 [cs.CV] UPDATED)
88. Neural Dual Contouring. (arXiv:2202.01999v2 [cs.CV] UPDATED)
89. Heed the Noise in Performance Evaluations in Neural Architecture Search. (arXiv:2202.02078v2 [cs.NE] UPDATED)
90. Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v2 [eess.SP] UPDATED)
91. Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v3 [cs.CV] UPDATED)
92. A Comprehensive Survey with Quantitative Comparison of Image Analysis Methods for Microorganism Biovolume Measurements. (arXiv:2202.09020v2 [cs.CV] UPDATED)
93. A Molecular Prior Distribution for Bayesian Inference Based on Wilson Statistics. (arXiv:2202.09388v2 [q-bio.QM] UPDATED)
94. Adaptive Cross-Layer Attention for Image **Restoration**. (arXiv:2203.03619v2 [eess.IV] UPDATED)
95. Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers. (arXiv:2203.03682v2 [cs.RO] UPDATED)
96. Progressive End-to-End Object Detection in Crowded Scenes. (arXiv:2203.07669v3 [cs.CV] UPDATED)
97. WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v5 [cs.CV] UPDATED)
98. Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities. (arXiv:2203.14712v2 [cs.CV] UPDATED)
99. PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition. (arXiv:2203.16670v2 [cs.CV] UPDATED)
100. Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v2 [cs.CV] UPDATED)
101. Zero-Shot Logit Adjustment. (arXiv:2204.11822v3 [cs.CV] UPDATED)
102. Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study. (arXiv:2204.12037v2 [cs.CV] UPDATED)
103. A Novel Framework for Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment. (arXiv:2204.12283v3 [q-bio.QM] UPDATED)
104. Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images. (arXiv:2204.12454v2 [cs.CV] UPDATED)
105. Meta-free few-shot learning via representation learning with weight averaging. (arXiv:2204.12466v2 [cs.LG] UPDATED)
106. Coarse-to-fine Q-attention with Tree Expansion. (arXiv:2204.12471v2 [cs.RO] UPDATED)
107. TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v2 [cs.CV] UPDATED)
108. Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v2 [cs.CV] UPDATED)
109. Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (arXiv:2204.14057v2 [cs.SD] CROSS LISTED)
## eess.IV
---
**27** new papers in eess.IV:-) 
1. Fast and Scalable Human Pose Estimation using mmWave Point Cloud. (arXiv:2205.00097v1 [eess.IV])
2. Computational Miniature Mesoscope V2: A deep learning-augmented miniaturized microscope for single-shot 3D high-resolution fluorescence imaging. (arXiv:2205.00123v1 [physics.optics])
3. Motion Compensated Extreme MRI: Multi-Scale Low Rank Reconstructions for Highly Accelerated 3D Dynamic Acquisitions (MoCo-MSLR). (arXiv:2205.00131v1 [physics.med-ph])
4. Terahertz Spatio-Temporal Deep Learning Computed Tomography. (arXiv:2205.00324v1 [eess.IV])
5. Physics-guided Terahertz Computational Imaging. (arXiv:2205.00327v1 [eess.IV])
6. Reinforced **Swin**-Convs Transformer for Underwater Image **Enhancement**. (arXiv:2205.00434v1 [cs.CV])
7. Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v1 [eess.IV])
8. Using a novel fractional-order gradient method for CNN back-propagation. (arXiv:2205.00581v1 [cs.CV])
9. Unsupervised Denoising of Optical Coherence Tomography Images with Dual_Merged CycleWGAN. (arXiv:2205.00698v1 [eess.IV])
10. BSRA: Block-based Super Resolution Accelerator with Hardware Efficient Pixel Attention. (arXiv:2205.00777v1 [cs.AR])
11. Lightweight Image **Enhancement** Network for Mobile Devices Using Self-Feature Extraction and Dense Modulation. (arXiv:2205.00853v1 [eess.IV])
12. Off-resonance artifact correction for magnetic resonance imaging: a review. (arXiv:2205.01028v1 [eess.IV])
13. Generalized MPI Multi-Patch Reconstruction using Clusters of similar System Matrices. (arXiv:2205.01083v1 [physics.med-ph])
14. Fisher Task Distance and Its Application in Neural Architecture Search. (arXiv:2103.12827v5 [cs.LG] UPDATED)
15. Slimmable Compressive Autoencoders for Practical Neural Image Compression. (arXiv:2103.15726v2 [eess.IV] UPDATED)
16. HINet: Half Instance Normalization Network for Image **Restoration**. (arXiv:2105.06086v2 [eess.IV] UPDATED)
17. No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v6 [cs.CV] UPDATED)
18. Objective crystallographic symmetry classifications of noisy and noise-free 2D periodic patterns with strong Fedorov type pseudosymmetries. (arXiv:2108.00829v6 [eess.IV] UPDATED)
19. Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v5 [eess.IV] UPDATED)
20. Learning-Based Video Coding with Joint Deep Compression and **Enhancement**. (arXiv:2111.14474v2 [eess.IV] UPDATED)
21. Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection. (arXiv:2112.04532v2 [cs.CV] UPDATED)
22. Stochastic Planner-Actor-Critic for Unsupervised Deformable Image Registration. (arXiv:2112.07415v2 [eess.IV] UPDATED)
23. Heed the Noise in Performance Evaluations in Neural Architecture Search. (arXiv:2202.02078v2 [cs.NE] UPDATED)
24. Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v2 [eess.SP] UPDATED)
25. A Comprehensive Survey with Quantitative Comparison of Image Analysis Methods for Microorganism Biovolume Measurements. (arXiv:2202.09020v2 [cs.CV] UPDATED)
26. Adaptive Cross-Layer Attention for Image **Restoration**. (arXiv:2203.03619v2 [eess.IV] UPDATED)
27. A Novel Framework for Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment. (arXiv:2204.12283v3 [q-bio.QM] UPDATED)
## cs.LG
---
**181** new papers in cs.LG:-) 
1. Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. (arXiv:2205.00001v1 [cs.AI])
2. Graph Learning from Multivariate Dependent Time Series via a Multi-Attribute Formulation. (arXiv:2205.00007v1 [stat.ML])
3. Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational AI. (arXiv:2205.00029v1 [cs.CL])
4. A Human-Centric Perspective on Fairness and Transparency in Algorithmic Decision-Making. (arXiv:2205.00033v1 [cs.AI])
5. Logically Consistent Adversarial Attacks for Soft Theorem Provers. (arXiv:2205.00047v1 [cs.LG])
6. Joint Multisided **Exposure** Fairness for Recommendation. (arXiv:2205.00048v1 [cs.IR])
7. Prompt Consistency for Zero-Shot Task Generalization. (arXiv:2205.00049v1 [cs.CL])
8. Implicit Regularization Properties of Variance Reduced Stochastic Mirror Descent. (arXiv:2205.00058v1 [stat.ML])
9. The Directional Bias Helps Stochastic Gradient Descent to Generalize in Kernel Regression Models. (arXiv:2205.00061v1 [stat.ML])
10. Doubting AI Predictions: Influence-Driven Second Opinion Recommendation. (arXiv:2205.00072v1 [cs.LG])
11. Infusing Linguistic Knowledge of SMILES into Chemical Language Models. (arXiv:2205.00084v1 [q-bio.QM])
12. Bridging Differential Privacy and Byzantine-Robustness via Model Aggregation. (arXiv:2205.00107v1 [cs.LG])
13. Gaze-enhanced Crossmodal Embeddings for Emotion Recognition. (arXiv:2205.00129v1 [cs.LG])
14. ExSum: From Local Explanations to Model Understanding. (arXiv:2205.00130v1 [cs.CL])
15. Identification of Physical Processes and Unknown Parameters of 3D Groundwater Contaminant Problems via Theory-guided U-net. (arXiv:2205.00134v1 [physics.comp-ph])
16. Multimodal Representation Learning With Text and Images. (arXiv:2205.00142v1 [cs.LG])
17. Operational Adaptation of DNN Classifiers using Elastic Weight Consolidation. (arXiv:2205.00147v1 [cs.LG])
18. Deep Ensemble as a Gaussian Process Approximate Posterior. (arXiv:2205.00163v1 [cs.LG])
19. NeuralEF: Deconstructing Kernels by Deep Neural Networks. (arXiv:2205.00165v1 [cs.LG])
20. An Initial Look at Self-Reprogramming Artificial Intelligence. (arXiv:2205.00167v1 [cs.AI])
21. FEDIC: Federated Learning on Non-IID and Long-Tailed Data via Calibrated Distillation. (arXiv:2205.00172v1 [cs.LG])
22. "And Then There Were None": Cracking White-box DNN Watermarks via Invariant Neuron Transforms. (arXiv:2205.00199v1 [cs.CR])
23. Explainable Artificial Intelligence for Bayesian Neural Networks: Towards trustworthy predictions of ocean dynamics. (arXiv:2205.00202v1 [physics.ao-ph])
24. Software Testing for Machine Learning. (arXiv:2205.00210v1 [cs.SE])
25. StorSeismic: A new paradigm in deep learning for seismic processing. (arXiv:2205.00222v1 [cs.LG])
26. Loss Function Entropy Regularization for Diverse Decision Boundaries. (arXiv:2205.00224v1 [cs.LG])
27. Approximating Permutations with Neural Network Components for Travelling Photographer Problem. (arXiv:2205.00242v1 [cs.LG])
28. PGD: A Large-scale Professional Go Dataset for Data-driven Analytics. (arXiv:2205.00254v1 [cs.AI])
29. Heterogeneous Graph Neural Networks using Self-supervised Reciprocally Contrastive Learning. (arXiv:2205.00256v1 [cs.LG])
30. Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound. (arXiv:2205.00263v1 [cs.LG])
31. Deep Learning-Enabled Semantic Communication Systems with Task-Unaware Transmitter and Dynamic Data. (arXiv:2205.00271v1 [cs.IT])
32. Understanding the Generalization Performance of Spectral Clustering Algorithms. (arXiv:2205.00281v1 [cs.LG])
33. Leveraging Emotion-specific Features to Improve Transformer Performance for Emotion Classification. (arXiv:2205.00283v1 [cs.CL])
34. Learning Effective SDEs from Brownian Dynamics Simulations of Colloidal Particles. (arXiv:2205.00286v1 [math.DS])
35. TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning. (arXiv:2205.00293v1 [cs.LG])
36. SHAPE: An Unified Approach to Evaluate the Contribution and Cooperation of Individual Modalities. (arXiv:2205.00302v1 [cs.LG])
37. Learning to Get Up. (arXiv:2205.00307v1 [cs.GR])
38. FairSR: Fairness-aware Sequential Recommendation through Multi-Task Learning with Preference Graph Embeddings. (arXiv:2205.00313v1 [cs.IR])
39. Foundational Models for Continual Learning: An Empirical Study of Latent Replay. (arXiv:2205.00329v1 [cs.LG])
40. Engineering flexible machine learning systems by traversing functionally invariant paths in weight space. (arXiv:2205.00334v1 [cs.LG])
41. End-to-End Signal Classification in Signed Cumulative Distribution Transform Space. (arXiv:2205.00348v1 [eess.SP])
42. Orthogonal Statistical Learning with Self-Concordant Loss. (arXiv:2205.00350v1 [stat.ML])
43. Graph Anisotropic Diffusion. (arXiv:2205.00354v1 [cs.LG])
44. Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees. (arXiv:2205.00359v1 [cs.LG])
45. Combined Learning of Neural Network Weights for Privacy in Collaborative Tasks. (arXiv:2205.00361v1 [cs.LG])
46. A Simple Duality Proof for Wasserstein Distributionally Robust Optimization. (arXiv:2205.00362v1 [math.OC])
47. Detecting COVID-19 Conspiracy Theories with Transformers and TF-IDF. (arXiv:2205.00377v1 [cs.CL])
48. Abnormal-aware Multi-person Evaluation System with Improved Fuzzy Weighting. (arXiv:2205.00388v1 [cs.CY])
49. Neural Network Optimal Feedback Control with Guaranteed Local Stability. (arXiv:2205.00394v1 [math.OC])
50. A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness. (arXiv:2205.00403v1 [cs.LG])
51. Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. (arXiv:2205.00415v1 [cs.CL])
52. Uniform Manifold Approximation with Two-phase Optimization. (arXiv:2205.00420v1 [cs.LG])
53. TinyLight: Adaptive Traffic Signal Control on Devices with Extremely Limited Resources. (arXiv:2205.00427v1 [cs.LG])
54. Differentially Private Multivariate Time Series Forecasting of Aggregated Human Mobility With Deep Learning: Input or Gradient Perturbation?. (arXiv:2205.00436v1 [cs.LG])
55. Adaptive Online Optimization with Predictions: Static and Dynamic Environments. (arXiv:2205.00446v1 [math.OC])
56. Molecular Identification from AFM images using the IUPAC Nomenclature and Attribute Multimodal Recurrent Neural Networks. (arXiv:2205.00449v1 [cond-mat.mtrl-sci])
57. An Analysis of the Features Considerable for NFT Recommendations. (arXiv:2205.00456v1 [cs.IR])
58. Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation. (arXiv:2205.00459v1 [cs.NE])
59. Reward Systems for Trustworthy Medical Federated Learning. (arXiv:2205.00470v1 [cs.LG])
60. A Survey of Decentralized Online Learning. (arXiv:2205.00473v1 [cs.LG])
61. None Class Ranking Loss for Document-Level Relation Extraction. (arXiv:2205.00476v1 [cs.CL])
62. Ridgeless Regression with Random Features. (arXiv:2205.00477v1 [cs.LG])
63. Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank HMMs and PCFGs. (arXiv:2205.00484v1 [cs.CL])
64. On the speed of uniform convergence in Mercer's theorem. (arXiv:2205.00487v1 [cs.LG])
65. Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. (arXiv:2205.00501v1 [cs.HC])
66. Domain Adaptation meets Individual Fairness. And they get along. (arXiv:2205.00504v1 [stat.ML])
67. Preserve Pre-trained Knowledge: Transfer Learning With Self-Distillation For Action Recognition. (arXiv:2205.00506v1 [cs.CV])
68. An Early Fault Detection Method of Rotating Machines Based on Multiple Feature Fusion with Stacking Architecture. (arXiv:2205.00511v1 [cs.LG])
69. Accurate non-stationary short-term traffic flow prediction method. (arXiv:2205.00517v1 [cs.LG])
70. Deep Learning with Logical Constraints. (arXiv:2205.00523v1 [cs.AI])
71. Deep vs. Shallow Learning: A Benchmark Study in Low Magnitude Earthquake Detection. (arXiv:2205.00525v1 [cs.LG])
72. Generalized Reference Kernel for One-class Classification. (arXiv:2205.00534v1 [cs.LG])
73. Can Information Behaviour Inform Machine Learning?. (arXiv:2205.00538v1 [cs.LG])
74. Federated Semi-Supervised Classification of Multimedia Flows for 3D Networks. (arXiv:2205.00550v1 [cs.LG])
75. Experimental quantum pattern recognition in IBMQ and diamond NVs. (arXiv:2205.00561v1 [quant-ph])
76. Thermodynamically Consistent Machine-Learned Internal State Variable Approach for Data-Driven Modeling of Path-Dependent Materials. (arXiv:2205.00578v1 [math.NA])
77. Data-driven control of spatiotemporal chaos with reduced-order neural ODE-based models and reinforcement learning. (arXiv:2205.00579v1 [cs.LG])
78. Using a novel fractional-order gradient method for CNN back-propagation. (arXiv:2205.00581v1 [cs.CV])
79. Forecasting Market Changes using Variational Inference. (arXiv:2205.00605v1 [q-fin.ST])
80. Physics-aware Reduced-order Modeling of Transonic Flow via $\beta$-Variational Autoencoder. (arXiv:2205.00608v1 [physics.flu-dyn])
81. LoopStack: a Lightweight Tensor Algebra Compiler Stack. (arXiv:2205.00618v1 [cs.LG])
82. Community detection in multiplex networks based on orthogonal nonnegative matrix tri-factorization. (arXiv:2205.00626v1 [cs.SI])
83. The Multivariate Community Hawkes Model for Dependent Relational Events in Continuous-time Networks. (arXiv:2205.00639v1 [stat.ME])
84. Skeptical binary inferences in multi-label problems with sets of probabilities. (arXiv:2205.00662v1 [stat.ML])
85. Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study). (arXiv:2205.00664v1 [cs.LG])
86. Optimal Rates for Distributed Learning with Random Features. (arXiv:1906.03155v3 [cs.LG] UPDATED)
87. Dynamic Graph Message Passing Networks. (arXiv:1908.06955v4 [cs.CV] UPDATED)
88. Global Entity Disambiguation with BERT. (arXiv:1909.00426v5 [cs.CL] UPDATED)
89. A Family of Pairwise Multi-Marginal Optimal Transports that Define a Generalized Metric. (arXiv:2001.11114v5 [cs.LG] UPDATED)
90. On the Ambiguity of Rank-Based Evaluation of Entity Alignment or Link Prediction Methods. (arXiv:2002.06914v4 [cs.LG] UPDATED)
91. Learning Context-aware Task Reasoning for Efficient Meta-reinforcement Learning. (arXiv:2003.01373v2 [cs.LG] UPDATED)
92. A Comparison of Methods for Treatment Assignment with an Application to Playlist Generation. (arXiv:2004.11532v5 [econ.EM] UPDATED)
93. Relational reasoning and generalization using non-symbolic neural networks. (arXiv:2006.07968v3 [cs.LG] UPDATED)
94. Finite-Sample Guarantees for Wasserstein Distributionally Robust Optimization: Breaking the Curse of Dimensionality. (arXiv:2009.04382v3 [cs.LG] UPDATED)
95. Anomaly Detection in Large Labeled Multi-Graph Databases. (arXiv:2010.03600v2 [cs.DB] UPDATED)
96. Learning Not to Learn: Nature versus Nurture in Silico. (arXiv:2010.04466v3 [cs.LG] UPDATED)
97. Shedding Light on Blind Spots: Developing a Reference Architecture to Leverage Video Data for Process Mining. (arXiv:2010.11289v3 [cs.CV] UPDATED)
98. CosSGD: Communication-Efficient Federated Learning with a Simple Cosine-Based Quantization. (arXiv:2012.08241v2 [cs.LG] UPDATED)
99. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. (arXiv:2101.03961v2 [cs.LG] UPDATED)
100. The impact of prior knowledge on causal structure learning. (arXiv:2102.00473v4 [cs.AI] UPDATED)
101. Derivation of the Backpropagation Algorithm Based on Derivative Amplification Coefficients. (arXiv:2102.04320v2 [cs.LG] UPDATED)
102. Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v3 [cs.SI] UPDATED)
103. Fisher Task Distance and Its Application in Neural Architecture Search. (arXiv:2103.12827v5 [cs.LG] UPDATED)
104. New Insights on Reducing Abrupt Representation Change in Online Continual Learning. (arXiv:2104.05025v3 [cs.LG] UPDATED)
105. Supervising Model Attention with Human Explanations for Robust Natural Language Inference. (arXiv:2104.08142v3 [cs.CL] UPDATED)
106. Data-Driven Reachability Analysis from Noisy Data. (arXiv:2105.07229v2 [eess.SY] UPDATED)
107. Algorithms from Invariants: Smoothed Analysis of Orbit Recovery over $SO(3)$. (arXiv:2106.02680v3 [cs.DS] UPDATED)
108. MoCL: Data-driven Molecular Fingerprint via Knowledge-aware Contrastive Learning from Molecular Graph. (arXiv:2106.04509v2 [physics.bio-ph] UPDATED)
109. CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v3 [cs.CL] UPDATED)
110. Personalized Federated Learning with Contextualized Generalization. (arXiv:2106.13044v2 [cs.LG] UPDATED)
111. A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification. (arXiv:2107.01461v4 [cs.SD] UPDATED)
112. A Deep Transfer Learning Approach on Identifying Glitch Wave-form in Gravitational Wave Data. (arXiv:2107.01863v5 [gr-qc] UPDATED)
113. Depth-supervised **NeRF**: Fewer Views and Faster Training for Free. (arXiv:2107.02791v2 [cs.CV] UPDATED)
114. What Do You Get When You Cross Beam Search with Nucleus Sampling?. (arXiv:2107.09729v3 [cs.CL] UPDATED)
115. Mapping Research Topics in Software Testing: A Bibliometric Analysis. (arXiv:2109.04086v2 [cs.DL] UPDATED)
116. An objective function for order preserving hierarchical clustering. (arXiv:2109.04266v3 [cs.LG] UPDATED)
117. Discriminative Similarity for Data Clustering. (arXiv:2109.08675v2 [cs.LG] UPDATED)
118. Federated Feature Selection for Cyber-Physical Systems of Systems. (arXiv:2109.11323v2 [cs.LG] UPDATED)
119. Non-Euclidean Self-Organizing Maps. (arXiv:2109.11769v2 [cs.LG] UPDATED)
120. Parameter-Free Reduction of the Estimation Bias in Deep Reinforcement Learning for Continuous Control. (arXiv:2109.11788v2 [cs.LG] UPDATED)
121. Efficient and passive learning of networked dynamical systems driven by non-white exogenous inputs. (arXiv:2110.00852v2 [cs.LG] UPDATED)
122. Skill Induction and Planning with Latent Language. (arXiv:2110.01517v2 [cs.LG] UPDATED)
123. EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits. (arXiv:2110.03177v7 [cs.LG] UPDATED)
124. ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v5 [cs.IT] UPDATED)
125. Provably Efficient Multi-Agent Reinforcement Learning with Fully Decentralized Communication. (arXiv:2110.07392v2 [cs.LG] UPDATED)
126. CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training. (arXiv:2110.07731v2 [cs.CL] UPDATED)
127. Hydra: A System for Large Multi-Model Deep Learning. (arXiv:2110.08633v5 [cs.DC] UPDATED)
128. Neural Network Compatible Off-Policy Natural Actor-Critic Algorithm. (arXiv:2110.10017v2 [cs.LG] UPDATED)
129. SOFT: Softmax-free Transformer with Linear Complexity. (arXiv:2110.11945v3 [cs.CV] UPDATED)
130. Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching. (arXiv:2110.14457v2 [cs.LG] UPDATED)
131. Fairer LP-based Online Allocation via Analytic Center. (arXiv:2110.14621v4 [cs.DS] UPDATED)
132. Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text. (arXiv:2110.15718v3 [cs.CL] UPDATED)
133. Selecting the number of clusters, clustering models, and algorithms. A unifying approach based on the quadratic discriminant score. (arXiv:2111.02302v2 [stat.ML] UPDATED)
134. Implicit vs Unfolded Graph Neural Networks. (arXiv:2111.06592v2 [cs.LG] UPDATED)
135. LIMEcraft: Handcrafted superpixel selection and inspection for Visual eXplanations. (arXiv:2111.08094v2 [cs.CV] UPDATED)
136. SMACE: A New Method for the Interpretability of Composite Decision Systems. (arXiv:2111.08749v3 [cs.LG] UPDATED)
137. Combined Scaling for Open-Vocabulary Image Classification. (arXiv:2111.10050v2 [cs.LG] UPDATED)
138. Effective and efficient structure learning with pruning and model averaging strategies. (arXiv:2112.00398v2 [cs.LG] UPDATED)
139. Fast Sparse Decision Tree Optimization via Reference Ensembles. (arXiv:2112.00798v5 [cs.LG] UPDATED)
140. Machine Learning in Nuclear Physics. (arXiv:2112.02309v2 [nucl-th] UPDATED)
141. Risk and optimal policies in bandit experiments. (arXiv:2112.06363v4 [econ.EM] UPDATED)
142. Stochastic Coded Federated Learning with Convergence and Privacy Guarantees. (arXiv:2201.10092v3 [cs.LG] UPDATED)
143. Leveraging class abstraction for commonsense reinforcement learning via residual policy gradient methods. (arXiv:2201.12126v2 [cs.AI] UPDATED)
144. Composing a surrogate observation operator for sequential data assimilation. (arXiv:2201.12514v2 [cs.LG] UPDATED)
145. Stochastic 2D Signal Generative Model with Wavelet Packets Basis Regarded as a Random Variable and Bayes Optimal Processing. (arXiv:2202.00568v2 [eess.SP] UPDATED)
146. Neural Dual Contouring. (arXiv:2202.01999v2 [cs.CV] UPDATED)
147. Supervised Contrastive Learning for Product Matching. (arXiv:2202.02098v2 [cs.LG] UPDATED)
148. Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v2 [eess.SP] UPDATED)
149. Personalized Public Policy Analysis in Social Sciences using Causal-Graphical Normalizing Flows. (arXiv:2202.03281v2 [cs.LG] UPDATED)
150. Group-Agent Reinforcement Learning. (arXiv:2202.05135v3 [cs.LG] UPDATED)
151. Quantum Lazy Training. (arXiv:2202.08232v2 [quant-ph] UPDATED)
152. ST-MoE: Designing Stable and Transferable Sparse Expert Models. (arXiv:2202.08906v2 [cs.CL] UPDATED)
153. Quantification of Actual Road User Behavior on the Basis of Given Traffic Rules. (arXiv:2202.09269v2 [cs.RO] UPDATED)
154. Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v4 [eess.AS] UPDATED)
155. ApacheJIT: A Large Dataset for Just-In-Time Defect Prediction. (arXiv:2203.00101v2 [cs.SE] UPDATED)
156. DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition. (arXiv:2203.00545v2 [cs.CL] UPDATED)
157. Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space. (arXiv:2203.00614v2 [cs.LG] UPDATED)
158. Personalized Federated Learning With Graph. (arXiv:2203.00829v5 [cs.LG] UPDATED)
159. Renyi Fair Information Bottleneck for Image Classification. (arXiv:2203.04950v2 [cs.LG] UPDATED)
160. Dazzle: Using Optimized Generative Adversarial Networks to Address Security Data Class Imbalance Issue. (arXiv:2203.11410v2 [cs.CR] UPDATED)
161. On Supervised Feature Selection from High Dimensional Feature Spaces. (arXiv:2203.11924v2 [cs.LG] UPDATED)
162. Neural Implicit Flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data. (arXiv:2204.03216v3 [cs.LG] UPDATED)
163. KCD: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media. (arXiv:2204.04046v2 [cs.LG] UPDATED)
164. Interpretable AI for policy-making in pandemics. (arXiv:2204.04256v2 [cs.LG] UPDATED)
165. TASAC: a twin-actor reinforcement learning framework with stochastic policy for batch process control. (arXiv:2204.10685v2 [cs.LG] UPDATED)
166. Long-term Spatio-temporal Forecasting via Dynamic Multiple-Graph Attention. (arXiv:2204.11008v3 [cs.LG] UPDATED)
167. Zero-Shot Logit Adjustment. (arXiv:2204.11822v3 [cs.CV] UPDATED)
168. Heterogeneous Information Network based Default Analysis on Banking Micro and Small Enterprise Users. (arXiv:2204.11849v2 [q-fin.RM] UPDATED)
169. Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study. (arXiv:2204.12037v2 [cs.CV] UPDATED)
170. Thompson Sampling for Bandit Learning in Matching Markets. (arXiv:2204.12048v2 [cs.LG] UPDATED)
171. Time-triggered Federated Learning over Wireless Networks. (arXiv:2204.12426v2 [cs.LG] UPDATED)
172. Meta-free few-shot learning via representation learning with weight averaging. (arXiv:2204.12466v2 [cs.LG] UPDATED)
173. Coarse-to-fine Q-attention with Tree Expansion. (arXiv:2204.12471v2 [cs.RO] UPDATED)
174. Anomaly Detection by Leveraging Incomplete Anomalous Knowledge with Anomaly-Aware Bidirectional GANs. (arXiv:2204.13335v2 [cs.LG] UPDATED)
175. WeaNF: Weak Supervision with Normalizing Flows. (arXiv:2204.13409v2 [cs.CL] UPDATED)
176. Cumulative Stay-time Representation for Electronic Health Records in Medical Event Time Prediction. (arXiv:2204.13451v2 [cs.LG] UPDATED)
177. An Explainable Regression Framework for Predicting Remaining Useful Life of Machines. (arXiv:2204.13574v2 [cs.LG] UPDATED)
178. Continual Learning for Peer-to-Peer Federated Learning: A Study on Automated Brain Metastasis Identification. (arXiv:2204.13591v2 [cs.LG] UPDATED)
179. Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v2 [cs.CV] UPDATED)
180. Training Language Models with Natural Language Feedback. (arXiv:2204.14146v2 [cs.CL] UPDATED)
181. RoSA: A Robust Self-Aligned Framework for Node-Node Graph Contrastive Learning. (arXiv:2204.13846v1 [cs.LG] CROSS LISTED)
## cs.AI
---
**99** new papers in cs.AI:-) 
1. Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. (arXiv:2205.00001v1 [cs.AI])
2. A Theory of Natural Intelligence. (arXiv:2205.00002v1 [cs.AI])
3. Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational AI. (arXiv:2205.00029v1 [cs.CL])
4. A Human-Centric Perspective on Fairness and Transparency in Algorithmic Decision-Making. (arXiv:2205.00033v1 [cs.AI])
5. What do we Really Know about State of the Art NER?. (arXiv:2205.00034v1 [cs.CL])
6. Joint Multisided **Exposure** Fairness for Recommendation. (arXiv:2205.00048v1 [cs.IR])
7. Who's the Expert? On Multi-source Belief Change. (arXiv:2205.00077v1 [cs.AI])
8. Infusing Linguistic Knowledge of SMILES into Chemical Language Models. (arXiv:2205.00084v1 [q-bio.QM])
9. A Scalable 5,6-Qubit Grover's Quantum Search Algorithm. (arXiv:2205.00117v1 [quant-ph])
10. Unsupervised Contrastive Learning based Transformer for Lung Nodule Detection. (arXiv:2205.00122v1 [cs.CV])
11. SciEv: Finding Scientific Evidence Papers for Scientific News. (arXiv:2205.00126v1 [cs.IR])
12. Identification of Physical Processes and Unknown Parameters of 3D Groundwater Contaminant Problems via Theory-guided U-net. (arXiv:2205.00134v1 [physics.comp-ph])
13. An Initial Look at Self-Reprogramming Artificial Intelligence. (arXiv:2205.00167v1 [cs.AI])
14. Trust in Human-AI Interaction: Scoping Out Models, Measures, and Methods. (arXiv:2205.00189v1 [cs.HC])
15. Software Testing for Machine Learning. (arXiv:2205.00210v1 [cs.SE])
16. An attention model for the formation of collectives in real-world domains. (arXiv:2205.00215v1 [cs.AI])
17. A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction. (arXiv:2205.00241v1 [cs.CL])
18. PGD: A Large-scale Professional Go Dataset for Data-driven Analytics. (arXiv:2205.00254v1 [cs.AI])
19. Leveraging Emotion-specific Features to Improve Transformer Performance for Emotion Classification. (arXiv:2205.00283v1 [cs.CL])
20. A Survey of Machine Narrative Reading Comprehension Assessments. (arXiv:2205.00299v1 [cs.AI])
21. Artificial Intelligence and Medicine: A literature review. (arXiv:2205.00322v1 [q-bio.QM])
22. Foundational Models for Continual Learning: An Empirical Study of Latent Replay. (arXiv:2205.00329v1 [cs.LG])
23. Engineering flexible machine learning systems by traversing functionally invariant paths in weight space. (arXiv:2205.00334v1 [cs.LG])
24. Opponent Modeling in Negotiation Dialogues by Related Data Adaptation. (arXiv:2205.00344v1 [cs.CL])
25. LayoutBERT: Masked Language Layout Model for Object Insertion. (arXiv:2205.00347v1 [cs.CV])
26. Combined Learning of Neural Network Weights for Privacy in Collaborative Tasks. (arXiv:2205.00361v1 [cs.LG])
27. Visual Spatial Reasoning. (arXiv:2205.00363v1 [cs.CL])
28. Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition. (arXiv:2205.00380v1 [cs.CV])
29. Learning user-defined sub-goals using memory editing in reinforcement learning. (arXiv:2205.00399v1 [cs.AI])
30. Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. (arXiv:2205.00415v1 [cs.CL])
31. TinyLight: Adaptive Traffic Signal Control on Devices with Extremely Limited Resources. (arXiv:2205.00427v1 [cs.LG])
32. Drone Flocking Optimization using NSGA-II and Principal Component Analysis. (arXiv:2205.00432v1 [cs.RO])
33. ETMS@IITKGP at SemEval-2022 Task 10: Structured Sentiment Analysis Using A Generative Approach. (arXiv:2205.00440v1 [cs.CL])
34. MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. (arXiv:2205.00445v1 [cs.CL])
35. The Ludii Game Description Language is Universal. (arXiv:2205.00451v1 [cs.AI])
36. Shape Change and Control of Pressure-based Soft Agents. (arXiv:2205.00467v1 [cs.RO])
37. Boost decoding performance of finite geometry LDPC codes with deep learning tactics. (arXiv:2205.00481v1 [cs.IT])
38. Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. (arXiv:2205.00501v1 [cs.HC])
39. Domain Adaptation meets Individual Fairness. And they get along. (arXiv:2205.00504v1 [stat.ML])
40. Deep Learning with Logical Constraints. (arXiv:2205.00523v1 [cs.AI])
41. Experimental quantum pattern recognition in IBMQ and diamond NVs. (arXiv:2205.00561v1 [quant-ph])
42. Adversarial Plannning. (arXiv:2205.00566v1 [cs.CR])
43. MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries. (arXiv:2205.00613v1 [cs.CV])
44. Semantically Informed Slang Interpretation. (arXiv:2205.00616v1 [cs.CL])
45. Re-defining Radiology Quality Assurance (QA) -- Artificial Intelligence (AI)-Based QA by Restricted Investigation of Unequal Scores (AQUARIUS). (arXiv:2205.00629v1 [cs.HC])
46. Design equivariant neural networks for 3D point cloud. (arXiv:2205.00630v1 [cs.CV])
47. Enhancing Adversarial Training with Feature Separability. (arXiv:2205.00637v1 [cs.CV])
48. Skeptical binary inferences in multi-label problems with sets of probabilities. (arXiv:2205.00662v1 [stat.ML])
49. Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study). (arXiv:2205.00664v1 [cs.LG])
50. Deep Video Harmonization with Color Mapping Consistency. (arXiv:2205.00687v1 [cs.CV])
51. Learning Context-aware Task Reasoning for Efficient Meta-reinforcement Learning. (arXiv:2003.01373v2 [cs.LG] UPDATED)
52. Relational reasoning and generalization using non-symbolic neural networks. (arXiv:2006.07968v3 [cs.LG] UPDATED)
53. Anomaly Detection in Large Labeled Multi-Graph Databases. (arXiv:2010.03600v2 [cs.DB] UPDATED)
54. Learning Not to Learn: Nature versus Nurture in Silico. (arXiv:2010.04466v3 [cs.LG] UPDATED)
55. Shedding Light on Blind Spots: Developing a Reference Architecture to Leverage Video Data for Process Mining. (arXiv:2010.11289v3 [cs.CV] UPDATED)
56. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. (arXiv:2101.03961v2 [cs.LG] UPDATED)
57. The impact of prior knowledge on causal structure learning. (arXiv:2102.00473v4 [cs.AI] UPDATED)
58. Robust optimal policies for team Markov games. (arXiv:2105.07405v2 [math.OC] UPDATED)
59. Probing the Effect of Selection Bias on Generalization: A Thought Experiment. (arXiv:2105.09934v2 [cs.CV] UPDATED)
60. What Do You Get When You Cross Beam Search with Nucleus Sampling?. (arXiv:2107.09729v3 [cs.CL] UPDATED)
61. Parameter-Free Reduction of the Estimation Bias in Deep Reinforcement Learning for Continuous Control. (arXiv:2109.11788v2 [cs.LG] UPDATED)
62. SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction. (arXiv:2109.12093v2 [cs.CL] UPDATED)
63. Efficient and passive learning of networked dynamical systems driven by non-white exogenous inputs. (arXiv:2110.00852v2 [cs.LG] UPDATED)
64. Exploration of AI-Oriented Power System Dynamic Simulations. (arXiv:2110.00931v3 [eess.SY] UPDATED)
65. Skill Induction and Planning with Latent Language. (arXiv:2110.01517v2 [cs.LG] UPDATED)
66. Neural Network Compatible Off-Policy Natural Actor-Critic Algorithm. (arXiv:2110.10017v2 [cs.LG] UPDATED)
67. SOFT: Softmax-free Transformer with Linear Complexity. (arXiv:2110.11945v3 [cs.CV] UPDATED)
68. Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text. (arXiv:2110.15718v3 [cs.CL] UPDATED)
69. Calculating Question Similarity is Enough: A New Method for KBQA Tasks. (arXiv:2111.07658v4 [cs.CL] UPDATED)
70. SMACE: A New Method for the Interpretability of Composite Decision Systems. (arXiv:2111.08749v3 [cs.LG] UPDATED)
71. Effective and efficient structure learning with pruning and model averaging strategies. (arXiv:2112.00398v2 [cs.LG] UPDATED)
72. Fast Sparse Decision Tree Optimization via Reference Ensembles. (arXiv:2112.00798v5 [cs.LG] UPDATED)
73. JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering. (arXiv:2112.02732v2 [cs.CL] UPDATED)
74. Stochastic Planner-Actor-Critic for Unsupervised Deformable Image Registration. (arXiv:2112.07415v2 [eess.IV] UPDATED)
75. Verification of Neural-Network Control Systems by Integrating Taylor Models and Zonotopes. (arXiv:2112.09197v2 [eess.SY] UPDATED)
76. Leveraging class abstraction for commonsense reinforcement learning via residual policy gradient methods. (arXiv:2201.12126v2 [cs.AI] UPDATED)
77. Personalized Public Policy Analysis in Social Sciences using Causal-Graphical Normalizing Flows. (arXiv:2202.03281v2 [cs.LG] UPDATED)
78. On the Relationship between Shy and Warded Datalog+/-. (arXiv:2202.06285v2 [cs.LO] UPDATED)
79. Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v3 [cs.CV] UPDATED)
80. ApacheJIT: A Large Dataset for Just-In-Time Defect Prediction. (arXiv:2203.00101v2 [cs.SE] UPDATED)
81. Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers. (arXiv:2203.03682v2 [cs.RO] UPDATED)
82. A meta-probabilistic-programming language for bisimulation of probabilistic and non-well-founded type systems. (arXiv:2203.15970v2 [cs.AI] UPDATED)
83. KCD: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media. (arXiv:2204.04046v2 [cs.LG] UPDATED)
84. Interpretable AI for policy-making in pandemics. (arXiv:2204.04256v2 [cs.LG] UPDATED)
85. Recent Advances and New Frontiers in Spiking Neural Networks. (arXiv:2204.07050v4 [cs.NE] UPDATED)
86. Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks. (arXiv:2204.07705v2 [cs.CL] UPDATED)
87. Long-term Spatio-temporal Forecasting via Dynamic Multiple-Graph Attention. (arXiv:2204.11008v3 [cs.LG] UPDATED)
88. Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v3 [cs.CL] UPDATED)
89. Heterogeneous Information Network based Default Analysis on Banking Micro and Small Enterprise Users. (arXiv:2204.11849v2 [q-fin.RM] UPDATED)
90. Learning First-Order Symbolic Planning Representations That Are Grounded. (arXiv:2204.11902v3 [cs.AI] UPDATED)
91. Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study. (arXiv:2204.12037v2 [cs.CV] UPDATED)
92. Coarse-to-fine Q-attention with Tree Expansion. (arXiv:2204.12471v2 [cs.RO] UPDATED)
93. TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v2 [cs.CV] UPDATED)
94. An Explainable Regression Framework for Predicting Remaining Useful Life of Machines. (arXiv:2204.13574v2 [cs.LG] UPDATED)
95. Continual Learning for Peer-to-Peer Federated Learning: A Study on Automated Brain Metastasis Identification. (arXiv:2204.13591v2 [cs.LG] UPDATED)
96. Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v2 [cs.CV] UPDATED)
97. Training Language Models with Natural Language Feedback. (arXiv:2204.14146v2 [cs.CL] UPDATED)
98. Human-in-the-loop online multi-agent approach to increase trustworthiness in ML models through trust scores and data augmentation. (arXiv:2204.14255v2 [cs.AI] UPDATED)
99. RoSA: A Robust Self-Aligned Framework for Node-Node Graph Contrastive Learning. (arXiv:2204.13846v1 [cs.LG] CROSS LISTED)

