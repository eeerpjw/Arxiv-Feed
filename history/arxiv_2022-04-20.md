# Your interest papers
---
## cs.CV
---
### CapillaryX: A Software Design Pattern for Analyzing Medical Images in **Real-time** using Deep Learning. (arXiv:2204.08462v1 [eess.IV])
- Authors : Maged Abdalla, Helmy Abdou, Paulo Ferreira, Eric Jul, Tuyen Trung
- Link : [http://arxiv.org/abs/2204.08462](http://arxiv.org/abs/2204.08462)
> ABSTRACT  :  Recent advances in digital imaging, e.g., increased number of pixels captured, have meant that the volume of data to be processed and analyzed from these images has also increased. Deep learning algorithms are state-of-the-art for analyzing such images, given their high accuracy when trained with a large data volume of data. Nevertheless, such analysis requires considerable computational power, making such algorithms time- and resource-demanding. Such high demands can be met by using third-party cloud service providers. However, analyzing medical images using such services raises several legal and privacy challenges and does not necessarily provide real-time results. This paper provides a computing architecture that locally and in parallel can analyze medical images in real-time using deep learning thus avoiding the legal and privacy challenges stemming from uploading data to a third-party cloud provider. To make local image processing efficient on modern multi-core processors, we utilize parallel execution to offset the resource-intensive demands of deep neural networks. We focus on a specific medical-industrial case study, namely the quantifying of blood vessels in microcirculation images for which we have developed a working system. It is currently used in an industrial, clinical research setting as part of an e-health application. Our results show that our system is approximately 78% faster than its serial system counterpart and 12% faster than a master-slave parallel system architecture.  
### CorrGAN: Input Transformation Technique Against Natural Corruptions. (arXiv:2204.08623v1 [cs.LG])
- Authors : Mirazul Haque, Wei Yang
- Link : [http://arxiv.org/abs/2204.08623](http://arxiv.org/abs/2204.08623)
> ABSTRACT  :  Because of the increasing accuracy of Deep Neural Networks (DNNs) on different tasks, a lot of **real time**s systems are utilizing DNNs. These DNNs are vulnerable to adversarial perturbations and corruptions. Specifically, natural corruptions like fog, blur, contrast etc can affect the prediction of DNN in an autonomous vehicle. In **real time**, these corruptions are needed to be detected and also the corrupted inputs are needed to be de-noised to be predicted correctly. In this work, we propose CorrGAN approach, which can generate benign input when a corrupted input is provided. In this framework, we train Generative Adversarial Network (GAN) with novel intermediate output-based loss function. The GAN can denoise the corrupted input and generate benign input. Through experimentation, we show that up to 75.2% of the corrupted misclassified inputs can be classified correctly by DNN using CorrGAN.  
### Interaction-Aware Labeled Multi-Bernoulli Filter. (arXiv:2204.08655v1 [eess.SP])
- Authors : Nida Ishtiaq, Amirali Khodadadian, Alireza Bab, Reza Hoseinnezhad
- Link : [http://arxiv.org/abs/2204.08655](http://arxiv.org/abs/2204.08655)
> ABSTRACT  :  Tracking multiple objects through time is an important part of an intelligent transportation system. Random finite set (RFS)-based filters are one of the emerging techniques for tracking multiple objects. In multi-object tracking (MOT), a common assumption is that each object is moving independent of its surroundings. But in many real-world applications, target objects interact with one another and the environment. Such interactions, when considered for tracking, are usually modeled by an interactive motion model which is application specific. In this paper, we present a novel approach to incorporate target interactions within the prediction step of an RFS-based multi-target filter, i.e. labeled multi-Bernoulli (LMB) filter. The method has been developed for two practical applications of tracking a coordinated swarm and vehicles. The method has been tested for a complex vehicle tracking dataset and compared with the LMB filter through the OSPA and OSPA$^{(2)}$ metrics. The results demonstrate that the proposed interaction-aware method depicts considerable performance **enhancement** over the LMB filter in terms of the selected metrics.  
### CTCNet: A CNN-Transformer Cooperation Network for Face Image Super-Resolution. (arXiv:2204.08696v1 [cs.CV])
- Authors : Guangwei Gao, Zixiang Xu, Juncheng Li, Jian Yang, Tieyong Zeng, Jun Qi
- Link : [http://arxiv.org/abs/2204.08696](http://arxiv.org/abs/2204.08696)
> ABSTRACT  :  Recently, deep convolution neural networks (CNNs) steered face super-resolution methods have achieved great progress in restoring degraded facial details by jointly training with facial priors. However, these methods have some obvious limitations. On the one hand, multi-task joint learning requires additional marking on the dataset, and the introduced prior network will significantly increase the computational cost of the model. On the other hand, the limited receptive field of CNN will reduce the fidelity and naturalness of the reconstructed facial images, resulting in suboptimal reconstructed images. In this work, we propose an efficient CNN-Transformer Cooperation Network (CTCNet) for face super-resolution tasks, which uses the multi-scale connected encoder-decoder architecture as the backbone. Specifically, we first devise a novel Local-Global Feature Cooperation Module (LGCM), which is composed of a Facial Structure Attention Unit (FSAU) and a Transformer block, to promote the consistency of local facial detail and global facial structure **restoration** simultaneously. Then, we design an efficient Local Feature Refinement Module (LFRM) to enhance the local facial structure information. Finally, to further improve the **restoration** of fine facial details, we present a Multi-scale Feature Fusion Unit (MFFU) to adaptively fuse the features from different stages in the encoder procedure. Comprehensive evaluations on various datasets have assessed that the proposed CTCNet can outperform other state-of-the-art methods significantly.  
### NAFSSR: Stereo Image Super-Resolution Using NAFNet. (arXiv:2204.08714v1 [cs.CV])
- Authors : Xiaojie Chu, Liangyu Chen, Wenqing Yu
- Link : [http://arxiv.org/abs/2204.08714](http://arxiv.org/abs/2204.08714)
> ABSTRACT  :  Stereo image super-resolution aims at enhancing the quality of super-resolution results by utilizing the complementary information provided by binocular systems. To obtain reasonable performance, most methods focus on finely designing modules, loss functions, and etc. to exploit information from another viewpoint. This has the side effect of increasing system complexity, making it difficult for researchers to evaluate new ideas and compare methods. This paper inherits a strong and simple image **restoration** model, NAFNet, for single-view feature extraction and extends it by adding cross attention modules to fuse features between views to adapt to binocular scenarios. The proposed baseline for stereo image super-resolution is noted as NAFSSR. Furthermore, training/testing strategies are proposed to fully exploit the performance of NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In particular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012, KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models will be released at https://github.com/megvii-research/NAFNet.  
### Edge-enhanced Feature Distillation Network for Efficient Super-Resolution. (arXiv:2204.08759v1 [cs.CV])
- Authors : Yan Wang
- Link : [http://arxiv.org/abs/2204.08759](http://arxiv.org/abs/2204.08759)
> ABSTRACT  :  With the recently massive development in convolution neural networks, numerous lightweight CNN-based image super-resolution methods have been proposed for practical deployments on edge devices. However, most existing methods focus on one specific aspect: network or loss design, which leads to the difficulty of minimizing the model size. To address the issue, we conclude block devising, architecture searching, and loss design to obtain a more efficient SR structure. In this paper, we proposed an edge-enhanced feature distillation network, named EFDN, to preserve the high-frequency information under constrained resources. In detail, we build an edge-enhanced convolution block based on the existing reparameterization methods. Meanwhile, we propose edge-enhanced gradient loss to calibrate the reparameterized path training. Experimental results show that our edge-enhanced strategies preserve the edge and significantly improve the final **restoration** quality. Code is available at https://github.com/icandle/EFDN.  
### Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment. (arXiv:2204.08763v1 [cs.CV])
- Authors : Yue Cao, Zhaolin Wan, Dongwei Ren, Zifei Yan, Wangmeng Zuo
- Link : [http://arxiv.org/abs/2204.08763](http://arxiv.org/abs/2204.08763)
> ABSTRACT  :  Full-reference (FR) image quality assessment (IQA) evaluates the visual quality of a distorted image by measuring its perceptual difference with pristine-quality reference, and has been widely used in low-level vision tasks. Pairwise labeled data with mean opinion score (MOS) are required in training FR-IQA model, but is time-consuming and cumbersome to collect. In contrast, unlabeled data can be easily collected from an image degradation or **restoration** process, making it encouraging to exploit unlabeled training data to boost FR-IQA performance. Moreover, due to the distribution inconsistency between labeled and unlabeled data, outliers may occur in unlabeled data, further increasing the training difficulty. In this paper, we suggest to incorporate semi-supervised and positive-unlabeled (PU) learning for exploiting unlabeled data while mitigating the adverse effect of outliers. Particularly, by treating all labeled data as positive samples, PU learning is leveraged to identify negative samples (i.e., outliers) from unlabeled data. Semi-supervised learning (SSL) is further deployed to exploit positive unlabeled data by dynamically generating pseudo-MOS. We adopt a dual-branch network including reference and distortion branches. Furthermore, spatial attention is introduced in the reference branch to concentrate more on the informative regions, and sliced Wasserstein distance is used for robust difference map computation to address the misalignment issues caused by images recovered by GAN models. Extensive experiments show that our method performs favorably against state-of-the-arts on the benchmark datasets PIPAL, KADID-10k, TID2013, LIVE and CSIQ.  
### A qualitative investigation of optical flow algorithms for video denoising. (arXiv:2204.08791v1 [cs.CV])
- Authors : Hannes Fassold
- Link : [http://arxiv.org/abs/2204.08791](http://arxiv.org/abs/2204.08791)
> ABSTRACT  :  A good optical flow estimation is crucial in many video analysis and **restoration** algorithms employed in application fields like media industry, industrial inspection and automotive. In this work, we investigate how well optical flow algorithms perform qualitatively when integrated into a state of the art video denoising algorithm. Both classic optical flow algorithms (e.g. TV-L1) as well as recent deep learning based algorithm (like RAFT or BMBC) will be taken into account. For the qualitative investigation, we will employ realistic content with challenging characteristic (noisy content, large motion etc.) instead of the standard images used in most publications.  
### SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation. (arXiv:2204.08808v1 [cs.CV])
- Authors : Binhui Xie, Shuang Li, Mingjia Li, Chi Harold, Gao Huang, Guoren Wang
- Link : [http://arxiv.org/abs/2204.08808](http://arxiv.org/abs/2204.08808)
> ABSTRACT  :  Domain adaptive semantic segmentation attempts to make satisfactory dense predictions on an unlabeled target domain by utilizing the model trained on a labeled source domain. One solution is self-training, which retrains models with target pseudo labels. Many methods tend to alleviate noisy pseudo labels, however, they ignore intrinsic connections among cross-domain pixels with similar semantic concepts. Thus, they would struggle to deal with the semantic variations across domains, leading to less discrimination and poor generalization. In this work, we propose Semantic-Guided Pixel Contrast (SePiCo), a novel one-stage adaptation framework that highlights the semantic concepts of individual pixel to promote learning of class-discriminative and class-balanced pixel embedding space across domains. Specifically, to explore proper semantic concepts, we first investigate a centroid-aware pixel contrast that employs the category centroids of the entire source domain or a single source image to guide the learning of discriminative features. Considering the possible lack of category diversity in semantic concepts, we then blaze a trail of distributional perspective to involve a sufficient quantity of instances, namely distribution-aware pixel contrast, in which we approximate the true distribution of each semantic category from the statistics of labeled source data. Moreover, such an optimization objective can derive a closed-form upper bound by implicitly involving an infinite number of (dis)similar pairs. Extensive experiments show that SePiCo not only helps stabilize training but also yields discriminative features, making significant progress in both daytime and **night**time scenarios. Most notably, SePiCo establishes excellent results on tasks of GTAV/SYNTHIA-to-Cityscapes and Cityscapes-to-**Dark** Zurich, improving by 12.8, 8.8, and 9.2 mIoUs compared to the previous best method, respectively.  
### UID2021: An Underwater Image Dataset for Evaluation of No-reference Quality Assessment Metrics. (arXiv:2204.08813v1 [cs.CV])
- Authors : Guojia Hou, Yuxuan Li, Huan Yang, Kunqian Li, Zhenkuan Pan
- Link : [http://arxiv.org/abs/2204.08813](http://arxiv.org/abs/2204.08813)
> ABSTRACT  :  Achieving subjective and objective quality assessment of underwater images is of high significance in underwater visual perception and image/video processing. However, the development of underwater image quality assessment (UIQA) is limited for the lack of comprehensive human subjective user study with publicly available dataset and reliable objective UIQA metric. To address this issue, we establish a large-scale underwater image dataset, dubbed UID2021, for evaluating no-reference UIQA metrics. The constructed dataset contains 60 multiply degraded underwater images collected from various sources, covering six common underwater scenes (i.e. bluish scene, bluish-green scene, greenish scene, hazy scene, **low-light** scene, and turbid scene), and their corresponding 900 quality improved versions generated by employing fifteen state-of-the-art underwater image **enhancement** and **restoration** algorithms. Mean opinion scores (MOS) for UID2021 are also obtained by using the pair comparison sorting method with 52 observers. Both in-air NR-IQA and underwater-specific algorithms are tested on our constructed dataset to fairly compare the performance and analyze their strengths and weaknesses. Our proposed UID2021 dataset enables ones to evaluate NR UIQA algorithms comprehensively and paves the way for further research on UIQA. Our UID2021 will be a free download and utilized for research purposes at: https://github.com/Hou-Guojia/UID2021.  
### Towards Efficient Single Image Dehazing and Desnowing. (arXiv:2204.08899v1 [cs.CV])
- Authors : Tian Ye, Sixiang Chen, Yun Liu, Erkang Chen, Yuche Li
- Link : [http://arxiv.org/abs/2204.08899](http://arxiv.org/abs/2204.08899)
> ABSTRACT  :  Removing adverse weather conditions like rain, fog, and snow from images is a challenging problem. Although the current recovery algorithms targeting a specific condition have made impressive progress, it is not flexible enough to deal with various degradation types. We propose an efficient and compact image **restoration** network named DAN-Net (Degradation-Adaptive Neural Network) to address this problem, which consists of multiple compact expert networks with one adaptive gated neural. A single expert network efficiently addresses specific degradation in nasty winter scenes relying on the compact architecture and three novel components. Based on the Mixture of Experts strategy, DAN-Net captures degradation information from each input image to adaptively modulate the outputs of task-specific expert networks to remove various adverse winter weather conditions. Specifically, it adopts a lightweight Adaptive Gated Neural Network to estimate gated attention maps of the input image, while different task-specific experts with the same topology are jointly dispatched to process the degraded image. Such novel image **restoration** pipeline handles different types of severe weather scenes effectively and efficiently. It also enjoys the benefit of coordinate boosting in which the whole network outperforms each expert trained without coordination.    Extensive experiments demonstrate that the presented manner outperforms the state-of-the-art single-task methods on image quality and has better inference efficiency. Furthermore, we have collected the first real-world winter scenes dataset to evaluate winter image **restoration** methods, which contains various hazy and snowy images snapped in winter. Both the dataset and source code will be publicly available.  
### Global-and-Local Collaborative Learning for Co-Salient Object Detection. (arXiv:2204.08917v1 [cs.CV])
- Authors : Runmin Cong, Ning Yang, **Chongyi Li**, Huazhu Fu, Yao Zhao, Qingming Huang, Sam Kwong
- Link : [http://arxiv.org/abs/2204.08917](http://arxiv.org/abs/2204.08917)
> ABSTRACT  :  The goal of co-salient object detection (CoSOD) is to discover salient objects that commonly appear in a query group containing two or more relevant images. Therefore, how to effectively extract inter-image correspondence is crucial for the CoSOD task. In this paper, we propose a global-and-local collaborative learning architecture, which includes a global correspondence modeling (GCM) and a local correspondence modeling (LCM) to capture comprehensive inter-image corresponding relationship among different images from the global and local perspectives. Firstly, we treat different images as different time slices and use 3D convolution to integrate all intra features intuitively, which can more fully extract the global group semantics. Secondly, we design a pairwise correlation transformation (PCT) to explore similarity correspondence between pairwise images and combine the multiple local pairwise correspondences to generate the local inter-image relationship. Thirdly, the inter-image relationships of the GCM and LCM are integrated through a global-and-local correspondence aggregation (GLA) module to explore more comprehensive inter-image collaboration cues. Finally, the intra- and inter-features are adaptively integrated by an intra-and-inter weighting fusion (AEWF) module to learn co-saliency features and predict the co-saliency map. The proposed GLNet is evaluated on three prevailing CoSOD benchmark datasets, demonstrating that our model trained on a small dataset (about 3k images) still outperforms eleven state-of-the-art competitors trained on some large datasets (about 8k-200k images).  
### MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment. (arXiv:2204.08958v1 [cs.CV])
- Authors : Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, Yujiu Yang
- Link : [http://arxiv.org/abs/2204.08958](http://arxiv.org/abs/2204.08958)
> ABSTRACT  :  No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual quality of images in accordance with human subjective perception. Unfortunately, existing NR-IQA methods are far from meeting the needs of predicting accurate quality scores on GAN-based distortion images. To this end, we propose Multi-dimension Attention Network for no-reference Image Quality Assessment (MANIQA) to improve the performance on GAN-based distortion. We firstly extract features via ViT, then to strengthen global and local interactions, we propose the Transposed Attention Block (TAB) and the Scale **Swin** Transformer Block (SSTB). These two modules apply attention mechanisms across the channel and spatial dimension, respectively. In this multi-dimensional manner, the modules cooperatively increase the interaction among different regions of images globally and locally. Finally, a dual branch structure for patch-weighted quality prediction is applied to predict the final score depending on the weight of each patch's score. Experimental results demonstrate that MANIQA outperforms state-of-the-art methods on four standard datasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our method ranked first place in the final testing phase of the NTIRE 2022 Perceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and models are available at https://github.com/IIGROUP/MANIQA.  
### Rendering **Night**time Image Via Cascaded Color and Brightness Compensation. (arXiv:2204.08970v1 [cs.CV])
- Authors : Zhihao Li, Si Yi, Zhan Ma
- Link : [http://arxiv.org/abs/2204.08970](http://arxiv.org/abs/2204.08970)
> ABSTRACT  :  Image signal processing (ISP) is crucial for camera imaging, and neural networks (NN) solutions are extensively deployed for daytime scenes. The lack of sufficient **night**time image dataset and insights on **night**time illumination characteristics poses a great challenge for high-quality rendering using existing NN ISPs. To tackle it, we first built a high-resolution **night**time RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert professionals. Meanwhile, to best capture the characteristics of **night**time illumination light sources, we develop the CBUnet, a two-stage NN ISP to cascade the compensation of color and brightness attributes. Experiments show that our method has better visual quality compared to traditional ISP pipeline, and is ranked at the second place in the NTIRE 2022 **Night** Photography Rendering Challenge for two tracks by respective People's and Professional Photographer's choices. The code and relevant materials are avaiable on our website: https://njuvision.github.io/CBUnet.  
### Shallow **camera pipeline** for **night** photography rendering. (arXiv:2204.08972v1 [cs.CV])
- Authors : Simone Zini, Claudio Rota, Marco Buzzelli, Simone Bianco, Raimondo Schettini
- Link : [http://arxiv.org/abs/2204.08972](http://arxiv.org/abs/2204.08972)
> ABSTRACT  :  We introduce a **camera pipeline** for rendering visually pleasing photographs in **low light** conditions, as part of the NTIRE2022 **Night** Photography Rendering challenge. Given the nature of the task, where the objective is verbally defined by an expert photographer instead of relying on explicit ground truth images, we design an handcrafted solution, characterized by a shallow structure and by a low parameter count. Our pipeline exploits a local light enhancer as a form of **high dynamic range** correction, followed by a global adjustment of the image histogram to prevent washed-out results. We proportionally apply image denoising to **dark**er regions, where it is more easily perceived, without losing details on brighter regions. The solution reached the fifth place in the competition, with a preference vote count comparable to those of other entries, based on deep convolutional neural networks. Code is available at www.github.com/AvailableAfterAcceptance.  
### A comparison of different atmospheric turbulence simulation methods for image **restoration**. (arXiv:2204.08974v1 [cs.CV])
- Authors : Nithin Gopalakrishnan, Kangfu Mei
- Link : [http://arxiv.org/abs/2204.08974](http://arxiv.org/abs/2204.08974)
> ABSTRACT  :  Atmospheric turbulence deteriorates the quality of images captured by long-range imaging systems by introducing blur and geometric distortions to the captured scene. This leads to a drastic drop in performance when computer vision algorithms like object/face recognition and detection are performed on these images. In recent years, various deep learning-based atmospheric turbulence mitigation methods have been proposed in the literature. These methods are often trained using synthetically generated images and tested on real-world images. Hence, the performance of these **restoration** methods depends on the type of simulation used for training the network. In this paper, we systematically evaluate the effectiveness of various turbulence simulation methods on image **restoration**. In particular, we evaluate the performance of two state-or-the-art **restoration** networks using six simulations method on a real-world LRFID dataset consisting of face images degraded by turbulence. This paper will provide guidance to the researchers and practitioners working in this field to choose the suitable data generation models for training deep models for turbulence mitigation. The implementation codes for the simulation methods, source codes for the networks, and the pre-trained models will be publicly made available.  
### CSRNet: Cascaded Selective Resolution Network for **Real-time** Semantic Segmentation. (arXiv:2106.04400v2 [cs.CV] UPDATED)
- Authors : Jingjing Xiong, Man Po, Yin Yu, Chang Zhou, Pengfei Xian, Weifeng Ou
- Link : [http://arxiv.org/abs/2106.04400](http://arxiv.org/abs/2106.04400)
> ABSTRACT  :  **Real-time** semantic segmentation has received considerable attention due to growing demands in many practical applications, such as autonomous vehicles, robotics, etc. Existing real-time segmentation approaches often utilize feature fusion to improve segmentation accuracy. However, they fail to fully consider the feature information at different resolutions and the receptive fields of the networks are relatively limited, thereby compromising the performance. To tackle this problem, we propose a light Cascaded Selective Resolution Network (CSRNet) to improve the performance of real-time segmentation through multiple context information embedding and enhanced feature aggregation. The proposed network builds a three-stage segmentation system, which integrates feature information from low resolution to high resolution and achieves feature refinement progressively. CSRNet contains two critical modules: the Shorted Pyramid Fusion Module (SPFM) and the Selective Resolution Module (SRM). The SPFM is a computationally efficient module to incorporate the global context information and significantly enlarge the receptive field at each stage. The SRM is designed to fuse multi-resolution feature maps with various receptive fields, which assigns soft channel attentions across the feature maps and helps to remedy the problem caused by multi-scale objects. Comprehensive experiments on two well-known datasets demonstrate that the proposed CSRNet effectively improves the performance for real-time segmentation.  
### A comprehensive benchmark analysis for sand dust image reconstruction. (arXiv:2202.03031v2 [eess.IV] UPDATED)
- Authors : Yazhong Si, Fan Yang, Ya Guo, Wei Zhang, Yipu Yang
- Link : [http://arxiv.org/abs/2202.03031](http://arxiv.org/abs/2202.03031)
> ABSTRACT  :  Numerous sand dust image **enhancement** algorithms have been proposed in recent years. To our best acknowledge, however, most methods evaluated their performance with no-reference way using few selected real-world images from internet. It is unclear how to quantitatively analysis the performance of the algorithms in a supervised way and how we could gauge the progress in the field. Moreover, due to the absence of large-scale benchmark datasets, there are no well-known reports of data-driven based method for sand dust image **enhancement** up till now. To advance the development of deep learning-based algorithms for sand dust image reconstruction, while enabling supervised objective evaluation of algorithm performance. In this paper, we presented a comprehensive perceptual study and analysis of real-world sand dust images, then constructed a Sand-dust Image Reconstruction Benchmark (SIRB) for training Convolutional Neural Networks (CNNs) and evaluating algorithms performance. In addition, we adopted the existing image transformation neural network trained on SIRB as baseline to illustrate the generalization of SIRB for training CNNs. Finally, we conducted the qualitative and quantitative evaluation to demonstrate the performance and limitations of the state-of-the-arts (SOTA), which shed light on future research in sand dust image reconstruction.  
### Nested Collaborative Learning for Long-Tailed Visual Recognition. (arXiv:2203.15359v2 [cs.CV] UPDATED)
- Authors : Jun Li, Zichang Tan, Jun Wan, Zhen Lei, Guodong Guo
- Link : [http://arxiv.org/abs/2203.15359](http://arxiv.org/abs/2203.15359)
> ABSTRACT  :  The networks trained on the long-tailed dataset vary remarkably, despite the same training settings, which shows the great uncertainty in long-tailed learning. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL), which tackles the problem by collaboratively learning multiple experts together. NCL consists of two core components, namely Nested Individual Learning (NIL) and Nested Balanced Online Distillation (NBOD), which focus on the individual supervised learning for each single expert and the knowledge transferring among multiple experts, respectively. To learn representations more thoroughly, both NIL and NBOD are formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Regarding the learning in the partial perspective, we specifically select the negative categories with high predicted scores as the hard categories by using a proposed Hard Category Mining (HCM). In the NCL, the learning from two perspectives is nested, highly related and complementary, and helps the network to capture not only global and robust features but also meticulous distinguishing ability. Moreover, self-supervision is further utilized for feature **enhancement**. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether by using a single model or an ensemble.  
## eess.IV
---
### CapillaryX: A Software Design Pattern for Analyzing Medical Images in **Real-time** using Deep Learning. (arXiv:2204.08462v1 [eess.IV])
- Authors : Maged Abdalla, Helmy Abdou, Paulo Ferreira, Eric Jul, Tuyen Trung
- Link : [http://arxiv.org/abs/2204.08462](http://arxiv.org/abs/2204.08462)
> ABSTRACT  :  Recent advances in digital imaging, e.g., increased number of pixels captured, have meant that the volume of data to be processed and analyzed from these images has also increased. Deep learning algorithms are state-of-the-art for analyzing such images, given their high accuracy when trained with a large data volume of data. Nevertheless, such analysis requires considerable computational power, making such algorithms time- and resource-demanding. Such high demands can be met by using third-party cloud service providers. However, analyzing medical images using such services raises several legal and privacy challenges and does not necessarily provide real-time results. This paper provides a computing architecture that locally and in parallel can analyze medical images in real-time using deep learning thus avoiding the legal and privacy challenges stemming from uploading data to a third-party cloud provider. To make local image processing efficient on modern multi-core processors, we utilize parallel execution to offset the resource-intensive demands of deep neural networks. We focus on a specific medical-industrial case study, namely the quantifying of blood vessels in microcirculation images for which we have developed a working system. It is currently used in an industrial, clinical research setting as part of an e-health application. Our results show that our system is approximately 78% faster than its serial system counterpart and 12% faster than a master-slave parallel system architecture.  
### Edge-enhanced Feature Distillation Network for Efficient Super-Resolution. (arXiv:2204.08759v1 [cs.CV])
- Authors : Yan Wang
- Link : [http://arxiv.org/abs/2204.08759](http://arxiv.org/abs/2204.08759)
> ABSTRACT  :  With the recently massive development in convolution neural networks, numerous lightweight CNN-based image super-resolution methods have been proposed for practical deployments on edge devices. However, most existing methods focus on one specific aspect: network or loss design, which leads to the difficulty of minimizing the model size. To address the issue, we conclude block devising, architecture searching, and loss design to obtain a more efficient SR structure. In this paper, we proposed an edge-enhanced feature distillation network, named EFDN, to preserve the high-frequency information under constrained resources. In detail, we build an edge-enhanced convolution block based on the existing reparameterization methods. Meanwhile, we propose edge-enhanced gradient loss to calibrate the reparameterized path training. Experimental results show that our edge-enhanced strategies preserve the edge and significantly improve the final **restoration** quality. Code is available at https://github.com/icandle/EFDN.  
### Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment. (arXiv:2204.08763v1 [cs.CV])
- Authors : Yue Cao, Zhaolin Wan, Dongwei Ren, Zifei Yan, Wangmeng Zuo
- Link : [http://arxiv.org/abs/2204.08763](http://arxiv.org/abs/2204.08763)
> ABSTRACT  :  Full-reference (FR) image quality assessment (IQA) evaluates the visual quality of a distorted image by measuring its perceptual difference with pristine-quality reference, and has been widely used in low-level vision tasks. Pairwise labeled data with mean opinion score (MOS) are required in training FR-IQA model, but is time-consuming and cumbersome to collect. In contrast, unlabeled data can be easily collected from an image degradation or **restoration** process, making it encouraging to exploit unlabeled training data to boost FR-IQA performance. Moreover, due to the distribution inconsistency between labeled and unlabeled data, outliers may occur in unlabeled data, further increasing the training difficulty. In this paper, we suggest to incorporate semi-supervised and positive-unlabeled (PU) learning for exploiting unlabeled data while mitigating the adverse effect of outliers. Particularly, by treating all labeled data as positive samples, PU learning is leveraged to identify negative samples (i.e., outliers) from unlabeled data. Semi-supervised learning (SSL) is further deployed to exploit positive unlabeled data by dynamically generating pseudo-MOS. We adopt a dual-branch network including reference and distortion branches. Furthermore, spatial attention is introduced in the reference branch to concentrate more on the informative regions, and sliced Wasserstein distance is used for robust difference map computation to address the misalignment issues caused by images recovered by GAN models. Extensive experiments show that our method performs favorably against state-of-the-arts on the benchmark datasets PIPAL, KADID-10k, TID2013, LIVE and CSIQ.  
### UID2021: An Underwater Image Dataset for Evaluation of No-reference Quality Assessment Metrics. (arXiv:2204.08813v1 [cs.CV])
- Authors : Guojia Hou, Yuxuan Li, Huan Yang, Kunqian Li, Zhenkuan Pan
- Link : [http://arxiv.org/abs/2204.08813](http://arxiv.org/abs/2204.08813)
> ABSTRACT  :  Achieving subjective and objective quality assessment of underwater images is of high significance in underwater visual perception and image/video processing. However, the development of underwater image quality assessment (UIQA) is limited for the lack of comprehensive human subjective user study with publicly available dataset and reliable objective UIQA metric. To address this issue, we establish a large-scale underwater image dataset, dubbed UID2021, for evaluating no-reference UIQA metrics. The constructed dataset contains 60 multiply degraded underwater images collected from various sources, covering six common underwater scenes (i.e. bluish scene, bluish-green scene, greenish scene, hazy scene, **low-light** scene, and turbid scene), and their corresponding 900 quality improved versions generated by employing fifteen state-of-the-art underwater image **enhancement** and **restoration** algorithms. Mean opinion scores (MOS) for UID2021 are also obtained by using the pair comparison sorting method with 52 observers. Both in-air NR-IQA and underwater-specific algorithms are tested on our constructed dataset to fairly compare the performance and analyze their strengths and weaknesses. Our proposed UID2021 dataset enables ones to evaluate NR UIQA algorithms comprehensively and paves the way for further research on UIQA. Our UID2021 will be a free download and utilized for research purposes at: https://github.com/Hou-Guojia/UID2021.  
### MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment. (arXiv:2204.08958v1 [cs.CV])
- Authors : Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, Yujiu Yang
- Link : [http://arxiv.org/abs/2204.08958](http://arxiv.org/abs/2204.08958)
> ABSTRACT  :  No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual quality of images in accordance with human subjective perception. Unfortunately, existing NR-IQA methods are far from meeting the needs of predicting accurate quality scores on GAN-based distortion images. To this end, we propose Multi-dimension Attention Network for no-reference Image Quality Assessment (MANIQA) to improve the performance on GAN-based distortion. We firstly extract features via ViT, then to strengthen global and local interactions, we propose the Transposed Attention Block (TAB) and the Scale **Swin** Transformer Block (SSTB). These two modules apply attention mechanisms across the channel and spatial dimension, respectively. In this multi-dimensional manner, the modules cooperatively increase the interaction among different regions of images globally and locally. Finally, a dual branch structure for patch-weighted quality prediction is applied to predict the final score depending on the weight of each patch's score. Experimental results demonstrate that MANIQA outperforms state-of-the-art methods on four standard datasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our method ranked first place in the final testing phase of the NTIRE 2022 Perceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and models are available at https://github.com/IIGROUP/MANIQA.  
### Rendering **Night**time Image Via Cascaded Color and Brightness Compensation. (arXiv:2204.08970v1 [cs.CV])
- Authors : Zhihao Li, Si Yi, Zhan Ma
- Link : [http://arxiv.org/abs/2204.08970](http://arxiv.org/abs/2204.08970)
> ABSTRACT  :  Image signal processing (ISP) is crucial for camera imaging, and neural networks (NN) solutions are extensively deployed for daytime scenes. The lack of sufficient **night**time image dataset and insights on **night**time illumination characteristics poses a great challenge for high-quality rendering using existing NN ISPs. To tackle it, we first built a high-resolution **night**time RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert professionals. Meanwhile, to best capture the characteristics of **night**time illumination light sources, we develop the CBUnet, a two-stage NN ISP to cascade the compensation of color and brightness attributes. Experiments show that our method has better visual quality compared to traditional ISP pipeline, and is ranked at the second place in the NTIRE 2022 **Night** Photography Rendering Challenge for two tracks by respective People's and Professional Photographer's choices. The code and relevant materials are avaiable on our website: https://njuvision.github.io/CBUnet.  
### A comparison of different atmospheric turbulence simulation methods for image **restoration**. (arXiv:2204.08974v1 [cs.CV])
- Authors : Nithin Gopalakrishnan, Kangfu Mei
- Link : [http://arxiv.org/abs/2204.08974](http://arxiv.org/abs/2204.08974)
> ABSTRACT  :  Atmospheric turbulence deteriorates the quality of images captured by long-range imaging systems by introducing blur and geometric distortions to the captured scene. This leads to a drastic drop in performance when computer vision algorithms like object/face recognition and detection are performed on these images. In recent years, various deep learning-based atmospheric turbulence mitigation methods have been proposed in the literature. These methods are often trained using synthetically generated images and tested on real-world images. Hence, the performance of these **restoration** methods depends on the type of simulation used for training the network. In this paper, we systematically evaluate the effectiveness of various turbulence simulation methods on image **restoration**. In particular, we evaluate the performance of two state-or-the-art **restoration** networks using six simulations method on a real-world LRFID dataset consisting of face images degraded by turbulence. This paper will provide guidance to the researchers and practitioners working in this field to choose the suitable data generation models for training deep models for turbulence mitigation. The implementation codes for the simulation methods, source codes for the networks, and the pre-trained models will be publicly made available.  
### A comprehensive benchmark analysis for sand dust image reconstruction. (arXiv:2202.03031v2 [eess.IV] UPDATED)
- Authors : Yazhong Si, Fan Yang, Ya Guo, Wei Zhang, Yipu Yang
- Link : [http://arxiv.org/abs/2202.03031](http://arxiv.org/abs/2202.03031)
> ABSTRACT  :  Numerous sand dust image **enhancement** algorithms have been proposed in recent years. To our best acknowledge, however, most methods evaluated their performance with no-reference way using few selected real-world images from internet. It is unclear how to quantitatively analysis the performance of the algorithms in a supervised way and how we could gauge the progress in the field. Moreover, due to the absence of large-scale benchmark datasets, there are no well-known reports of data-driven based method for sand dust image **enhancement** up till now. To advance the development of deep learning-based algorithms for sand dust image reconstruction, while enabling supervised objective evaluation of algorithm performance. In this paper, we presented a comprehensive perceptual study and analysis of real-world sand dust images, then constructed a Sand-dust Image Reconstruction Benchmark (SIRB) for training Convolutional Neural Networks (CNNs) and evaluating algorithms performance. In addition, we adopted the existing image transformation neural network trained on SIRB as baseline to illustrate the generalization of SIRB for training CNNs. Finally, we conducted the qualitative and quantitative evaluation to demonstrate the performance and limitations of the state-of-the-arts (SOTA), which shed light on future research in sand dust image reconstruction.  
## cs.LG
---
### CapillaryX: A Software Design Pattern for Analyzing Medical Images in **Real-time** using Deep Learning. (arXiv:2204.08462v1 [eess.IV])
- Authors : Maged Abdalla, Helmy Abdou, Paulo Ferreira, Eric Jul, Tuyen Trung
- Link : [http://arxiv.org/abs/2204.08462](http://arxiv.org/abs/2204.08462)
> ABSTRACT  :  Recent advances in digital imaging, e.g., increased number of pixels captured, have meant that the volume of data to be processed and analyzed from these images has also increased. Deep learning algorithms are state-of-the-art for analyzing such images, given their high accuracy when trained with a large data volume of data. Nevertheless, such analysis requires considerable computational power, making such algorithms time- and resource-demanding. Such high demands can be met by using third-party cloud service providers. However, analyzing medical images using such services raises several legal and privacy challenges and does not necessarily provide real-time results. This paper provides a computing architecture that locally and in parallel can analyze medical images in real-time using deep learning thus avoiding the legal and privacy challenges stemming from uploading data to a third-party cloud provider. To make local image processing efficient on modern multi-core processors, we utilize parallel execution to offset the resource-intensive demands of deep neural networks. We focus on a specific medical-industrial case study, namely the quantifying of blood vessels in microcirculation images for which we have developed a working system. It is currently used in an industrial, clinical research setting as part of an e-health application. Our results show that our system is approximately 78% faster than its serial system counterpart and 12% faster than a master-slave parallel system architecture.  
### So2Sat POP -- A Curated Benchmark Data Set for Population Estimation from Space on a Continental Scale. (arXiv:2204.08524v1 [cs.LG])
- Authors : Sugandha Doda, Yuanyuan Wang, Matthias Kahl, Eike Jens, Hannes Taubenb, Xiao Xiang
- Link : [http://arxiv.org/abs/2204.08524](http://arxiv.org/abs/2204.08524)
> ABSTRACT  :  Obtaining a dynamic population distribution is key to many decision-making processes such as urban planning, disaster management and most importantly helping the government to better allocate socio-technical supply. For the aspiration of these objectives, good population data is essential. The traditional method of collecting population data through the census is expensive and tedious. In recent years, machine learning methods have been developed to estimate the population distribution. Most of the methods use data sets that are either developed on a small scale or not publicly available yet. Thus, the development and evaluation of the new methods become challenging. We fill this gap by providing a comprehensive data set for population estimation in 98 European cities. The data set comprises digital elevation model, local climate zone, land use classifications, **night**time lights in combination with multi-spectral Sentinel-2 imagery, and data from the Open Street Map initiative. We anticipate that it would be a valuable addition to the research community for the development of sophisticated machine learning-based approaches in the field of population estimation.  
### CorrGAN: Input Transformation Technique Against Natural Corruptions. (arXiv:2204.08623v1 [cs.LG])
- Authors : Mirazul Haque, Wei Yang
- Link : [http://arxiv.org/abs/2204.08623](http://arxiv.org/abs/2204.08623)
> ABSTRACT  :  Because of the increasing accuracy of Deep Neural Networks (DNNs) on different tasks, a lot of **real time**s systems are utilizing DNNs. These DNNs are vulnerable to adversarial perturbations and corruptions. Specifically, natural corruptions like fog, blur, contrast etc can affect the prediction of DNN in an autonomous vehicle. In **real time**, these corruptions are needed to be detected and also the corrupted inputs are needed to be de-noised to be predicted correctly. In this work, we propose CorrGAN approach, which can generate benign input when a corrupted input is provided. In this framework, we train Generative Adversarial Network (GAN) with novel intermediate output-based loss function. The GAN can denoise the corrupted input and generate benign input. Through experimentation, we show that up to 75.2% of the corrupted misclassified inputs can be classified correctly by DNN using CorrGAN.  
### GraphHop++: New Insights into GraphHop and Its **Enhancement**. (arXiv:2204.08646v1 [cs.LG])
- Authors : Tian Xie, Rajgopal Kannan, Jay Kuo
- Link : [http://arxiv.org/abs/2204.08646](http://arxiv.org/abs/2204.08646)
> ABSTRACT  :  An enhanced label propagation (LP) method called GraphHop has been proposed recently. It outperforms graph convolutional networks (GCNs) in the semi-supervised node classification task on various networks. Although the performance of GraphHop was explained intuitively with joint node attributes and labels smoothening, its rigorous mathematical treatment is lacking. In this paper, new insights into GraphHop are provided by analyzing it from a constrained optimization viewpoint. We show that GraphHop offers an alternate optimization to a certain regularization problem defined on graphs. Based on this interpretation, we propose two ideas to improve GraphHop furthermore, which leads to GraphHop++. We conduct extensive experiments to demonstrate the effectiveness and efficiency of GraphHop++. It is observed that GraphHop++ outperforms all other benchmarking methods, including GraphHop, consistently on five test datasets as well as an object recognition task at extremely low label rates (i.e., 1, 2, 4, 8, 16, and 20 labeled samples per class).  
### Model reduction for the material point method via an **implicit neural representation** of the deformation map. (arXiv:2109.12390v2 [cs.LG] UPDATED)
- Authors : Peter Yichen, Maurizio Chiaramonte, Eitan Grinspun, Kevin Carlberg
- Link : [http://arxiv.org/abs/2109.12390](http://arxiv.org/abs/2109.12390)
> ABSTRACT  :  This work proposes a model-reduction approach for the material point method on nonlinear manifolds. Our technique approximates the $\textit{kinematics}$ by approximating the deformation map using an **implicit neural representation** that restricts deformation trajectories to reside on a low-dimensional manifold. By explicitly approximating the deformation map, its spatiotemporal gradients -- in particular the deformation gradient and the velocity -- can be computed via analytical differentiation. In contrast to typical model-reduction techniques that construct a linear or nonlinear manifold to approximate the (finite number of) degrees of freedom characterizing a given spatial discretization, the use of an **implicit neural representation** enables the proposed method to approximate the $\textit{continuous}$ deformation map. This allows the kinematic approximation to remain agnostic to the discretization. Consequently, the technique supports dynamic discretizations -- including resolution changes -- during the course of the online reduced-order-model simulation.    To generate $\textit{dynamics}$ for the generalized coordinates, we propose a family of projection techniques. At each time step, these techniques: (1) Calculate full-space kinematics at quadrature points, (2) Calculate the full-space dynamics for a subset of `sample' material points, and (3) Calculate the reduced-space dynamics by projecting the updated full-space position and velocity onto the low-dimensional manifold and tangent space, respectively. We achieve significant computational speedup via hyper-reduction that ensures all three steps execute on only a small subset of the problem's spatial domain. Large-scale numerical examples with millions of material points illustrate the method's ability to gain an order of magnitude computational-cost saving -- indeed $\textit{real-time simulations}$ -- with negligible errors.  
### Risk-Aware Learning for Scalable Voltage Optimization in Distribution Grids. (arXiv:2110.01490v2 [cs.LG] UPDATED)
- Authors : Shanny Lin, Shaohui Liu, Hao Zhu
- Link : [http://arxiv.org/abs/2110.01490](http://arxiv.org/abs/2110.01490)
> ABSTRACT  :  **Real-time** coordination of distributed energy resources (DERs) is crucial for regulating the voltage profile in distribution grids. By capitalizing on a scalable neural network (NN) architecture, one can attain decentralized DER decisions to address the lack of real-time communications. This paper develops an advanced learning-enabled DER coordination scheme by accounting for the potential risks associated with reactive power prediction and voltage deviation. Such risks are quantified by the conditional value-at-risk (CVaR) using the worst-case samples only, and we propose a mini-batch selection algorithm to address the training speed issue in minimizing the CVaR-regularized loss. Numerical tests using real-world data on the IEEE 123-bus test case have demonstrated the computation and safety improvements of the proposed risk-aware learning algorithm for decentralized DER decision making, especially in terms of reducing feeder voltage violations.  
### Conditional Injective Flows for Bayesian Imaging. (arXiv:2204.07664v2 [cs.LG] UPDATED)
- Authors : AmirEhsan Khorashadizadeh, Konik Kothari, Leonardo Salsi, Ali Aghababaei, Maarten de, Ivan Dokmani
- Link : [http://arxiv.org/abs/2204.07664](http://arxiv.org/abs/2204.07664)
> ABSTRACT  :  Most deep learning models for computational imaging regress a single reconstructed image. In practice, however, ill-posedness, nonlinearity, model mismatch, and noise often conspire to make such point estimates misleading or insufficient. The Bayesian approach models images and (noisy) measurements as jointly distributed random vectors and aims to approximate the posterior distribution of unknowns. Recent variational inference methods based on conditional normalizing flows are a promising alternative to traditional MCMC methods, but they come with drawbacks: excessive memory and compute demands for moderate to high resolution images and underwhelming performance on hard nonlinear problems. In this work, we propose C-Trumpets -- conditional injective flows specifically designed for imaging problems, which greatly diminish these challenges. Injectivity reduces memory footprint and training time while low-dimensional latent space together with architectural innovations like fixed-volume-change layers and skip-connection revnet layers, C-Trumpets outperform regular conditional flow models on a variety of imaging and image **restoration** tasks, including limited-view CT and nonlinear inverse scattering, with a lower compute and memory budget. C-Trumpets enable fast approximation of point estimates like MMSE or MAP as well as physically-meaningful uncertainty quantification.  
## cs.AI
---
### So2Sat POP -- A Curated Benchmark Data Set for Population Estimation from Space on a Continental Scale. (arXiv:2204.08524v1 [cs.LG])
- Authors : Sugandha Doda, Yuanyuan Wang, Matthias Kahl, Eike Jens, Hannes Taubenb, Xiao Xiang
- Link : [http://arxiv.org/abs/2204.08524](http://arxiv.org/abs/2204.08524)
> ABSTRACT  :  Obtaining a dynamic population distribution is key to many decision-making processes such as urban planning, disaster management and most importantly helping the government to better allocate socio-technical supply. For the aspiration of these objectives, good population data is essential. The traditional method of collecting population data through the census is expensive and tedious. In recent years, machine learning methods have been developed to estimate the population distribution. Most of the methods use data sets that are either developed on a small scale or not publicly available yet. Thus, the development and evaluation of the new methods become challenging. We fill this gap by providing a comprehensive data set for population estimation in 98 European cities. The data set comprises digital elevation model, local climate zone, land use classifications, **night**time lights in combination with multi-spectral Sentinel-2 imagery, and data from the Open Street Map initiative. We anticipate that it would be a valuable addition to the research community for the development of sophisticated machine learning-based approaches in the field of population estimation.  
### GraphHop++: New Insights into GraphHop and Its **Enhancement**. (arXiv:2204.08646v1 [cs.LG])
- Authors : Tian Xie, Rajgopal Kannan, Jay Kuo
- Link : [http://arxiv.org/abs/2204.08646](http://arxiv.org/abs/2204.08646)
> ABSTRACT  :  An enhanced label propagation (LP) method called GraphHop has been proposed recently. It outperforms graph convolutional networks (GCNs) in the semi-supervised node classification task on various networks. Although the performance of GraphHop was explained intuitively with joint node attributes and labels smoothening, its rigorous mathematical treatment is lacking. In this paper, new insights into GraphHop are provided by analyzing it from a constrained optimization viewpoint. We show that GraphHop offers an alternate optimization to a certain regularization problem defined on graphs. Based on this interpretation, we propose two ideas to improve GraphHop furthermore, which leads to GraphHop++. We conduct extensive experiments to demonstrate the effectiveness and efficiency of GraphHop++. It is observed that GraphHop++ outperforms all other benchmarking methods, including GraphHop, consistently on five test datasets as well as an object recognition task at extremely low label rates (i.e., 1, 2, 4, 8, 16, and 20 labeled samples per class).  
# Paper List
---
## cs.CV
---
**136** new papers in cs.CV:-) 
1. SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix Data Augmentation. (arXiv:2204.08458v1 [cs.CV])
2. 3D Convolutional Networks for Action Recognition: Application to Sport Gesture Recognition. (arXiv:2204.08460v1 [cs.CV])
3. Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification. (arXiv:2204.08461v1 [cs.CV])
4. CapillaryX: A Software Design Pattern for Analyzing Medical Images in **Real-time** using Deep Learning. (arXiv:2204.08462v1 [eess.IV])
5. Machine Learning-Based Automated Thermal Comfort Prediction: Integration of Low-Cost Thermal and Visual Cameras for Higher Accuracy. (arXiv:2204.08463v1 [cs.CV])
6. Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography. (arXiv:2204.08466v1 [eess.IV])
7. IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v1 [eess.IV])
8. Face recognition with small and large size databases. (arXiv:2204.08468v1 [cs.CV])
9. Hand Geometry Based Recognition with a MLP Classifier. (arXiv:2204.08469v1 [cs.CV])
10. U-Net and its variants for Medical Image Segmentation : A short review. (arXiv:2204.08470v1 [eess.IV])
11. Simultaneous Multiple-Prompt Guided Generation Using Differentiable Optimal Transport. (arXiv:2204.08472v1 [cs.CV])
12. Self Supervised Lesion Recognition For Breast Ultrasound Diagnosis. (arXiv:2204.08477v1 [eess.IV])
13. Enhancing Non-mass Breast Ultrasound Cancer Classification With Knowledge Transfer. (arXiv:2204.08478v1 [eess.IV])
14. Inductive Biases for Object-Centric Representations of Complex Textures. (arXiv:2204.08479v1 [cs.CV])
15. Active Learning Helps Pretrained Models Learn the Intended Task. (arXiv:2204.08491v1 [cs.LG])
16. DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning. (arXiv:2204.08499v1 [cs.LG])
17. Spot the Difference: A Novel Task for Embodied Agents in Changing Environments. (arXiv:2204.08502v1 [cs.CV])
18. Dress Code: High-Resolution Multi-Category Virtual Try-On. (arXiv:2204.08532v1 [cs.CV])
19. A Novel Region Duplication Detection Algorithm Based on Hybrid Approach. (arXiv:2204.08545v1 [cs.CV])
20. Cylin-Painting: Seamless 360{\deg} Panoramic Image Outpainting and Beyond with Cylinder-Style Convolutions. (arXiv:2204.08563v1 [cs.CV])
21. VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance. (arXiv:2204.08583v1 [cs.CV])
22. A Region-Based Deep Learning Approach to Automated Retail Checkout. (arXiv:2204.08584v1 [cs.CV])
23. A Tour of Visualization Techniques for Computer Vision Datasets. (arXiv:2204.08601v1 [cs.CV])
24. Image Data Augmentation for Deep Learning: A Survey. (arXiv:2204.08610v1 [cs.CV])
25. Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors. (arXiv:2204.08612v1 [cs.CV])
26. Self-Supervised Equivariant Learning for Oriented Keypoint Detection. (arXiv:2204.08613v1 [cs.CV])
27. CorrGAN: Input Transformation Technique Against Natural Corruptions. (arXiv:2204.08623v1 [cs.LG])
28. Topology and geometry of data manifold in deep learning. (arXiv:2204.08624v1 [cs.LG])
29. Quaternion Optimized Model with Sparse Regularization for Color Image Recovery. (arXiv:2204.08629v1 [cs.CV])
30. Interaction-Aware Labeled Multi-Bernoulli Filter. (arXiv:2204.08655v1 [eess.SP])
31. ActAR: Actor-Driven Pose Embeddings for Video Action Recognition. (arXiv:2204.08671v1 [cs.CV])
32. Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. (arXiv:2204.08680v1 [cs.CV])
33. A Thin Format Vision-Based Tactile Sensor with A Micro Lens Array (MLA). (arXiv:2204.08691v1 [cs.RO])
34. CTCNet: A CNN-Transformer Cooperation Network for Face Image Super-Resolution. (arXiv:2204.08696v1 [cs.CV])
35. Software Engineering Approaches for TinyML based IoT Embedded Vision: A Systematic Literature Review. (arXiv:2204.08702v1 [cs.SE])
36. Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing. (arXiv:2204.08707v1 [cs.CV])
37. NAFSSR: Stereo Image Super-Resolution Using NAFNet. (arXiv:2204.08714v1 [cs.CV])
38. Shape-Aware Monocular 3D Object Detection. (arXiv:2204.08717v1 [cs.CV])
39. Multimodal Token Fusion for Vision Transformers. (arXiv:2204.08721v1 [cs.CV])
40. Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks. (arXiv:2204.08726v1 [cs.LG])
41. Proposal-free Lidar Panoptic Segmentation with Pillar-level Affinity. (arXiv:2204.08744v1 [cs.CV])
42. Augmentation of Atmospheric Turbulence Effects on Thermal Adapted Object Detection Models. (arXiv:2204.08745v1 [cs.CV])
43. Multi-View Spatial-Temporal Network for Continuous Sign Language Recognition. (arXiv:2204.08747v1 [cs.CV])
44. Dynamic Point Cloud Denoising via Gradient Fields. (arXiv:2204.08755v1 [cs.CV])
45. Edge-enhanced Feature Distillation Network for Efficient Super-Resolution. (arXiv:2204.08759v1 [cs.CV])
46. Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment. (arXiv:2204.08763v1 [cs.CV])
47. Modeling Missing Annotations for Incremental Learning in Object Detection. (arXiv:2204.08766v1 [cs.CV])
48. Binary Multi Channel Morphological Neural Network. (arXiv:2204.08768v1 [cs.CV])
49. GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning. (arXiv:2204.08770v1 [cs.CV])
50. Sensor Data Fusion in Top-View Grid Maps using Evidential Reasoning with Advanced Conflict Resolution. (arXiv:2204.08780v1 [cs.CV])
51. ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v1 [cs.CV])
52. A qualitative investigation of optical flow algorithms for video denoising. (arXiv:2204.08791v1 [cs.CV])
53. Two-Stream Graph Convolutional Network for Intra-oral Scanner Image Segmentation. (arXiv:2204.08797v1 [eess.IV])
54. An Energy-Based Prior for Generative Saliency. (arXiv:2204.08803v1 [cs.CV])
55. SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation. (arXiv:2204.08808v1 [cs.CV])
56. UID2021: An Underwater Image Dataset for Evaluation of No-reference Quality Assessment Metrics. (arXiv:2204.08813v1 [cs.CV])
57. An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions. (arXiv:2204.08817v1 [cs.CV])
58. Semi-supervised 3D shape segmentation with multilevel consistency and part substitution. (arXiv:2204.08824v1 [cs.CV])
59. Detect-and-describe: Joint learning framework for detection and description of objects. (arXiv:2204.08828v1 [cs.CV])
60. Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations. (arXiv:2204.08839v1 [cs.CV])
61. Core Box Image Recognition and its Improvement with a New Augmentation Technique. (arXiv:2204.08853v1 [cs.CV])
62. OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching. (arXiv:2204.08870v1 [cs.CV])
63. Less than Few: Self-Shot Video Instance Segmentation. (arXiv:2204.08874v1 [cs.CV])
64. Invertible Mask Network for Face Privacy-Preserving. (arXiv:2204.08895v1 [cs.CV])
65. Towards Efficient Single Image Dehazing and Desnowing. (arXiv:2204.08899v1 [cs.CV])
66. Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing. (arXiv:2204.08906v1 [cs.CV])
67. Self-Calibrated Efficient Transformer for Lightweight Super-Resolution. (arXiv:2204.08913v1 [cs.CV])
68. Global-and-Local Collaborative Learning for Co-Salient Object Detection. (arXiv:2204.08917v1 [cs.CV])
69. Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data. (arXiv:2204.08932v1 [cs.CV])
70. Deep learning-based surrogate model for 3-D patient-specific computational fluid dynamics. (arXiv:2204.08939v1 [physics.med-ph])
71. Missingness Bias in Model Debugging. (arXiv:2204.08945v1 [cs.CV])
72. Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity. (arXiv:2204.08954v1 [cs.LG])
73. MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment. (arXiv:2204.08958v1 [cs.CV])
74. Rendering **Night**time Image Via Cascaded Color and Brightness Compensation. (arXiv:2204.08970v1 [cs.CV])
75. Shallow **camera pipeline** for **night** photography rendering. (arXiv:2204.08972v1 [cs.CV])
76. A comparison of different atmospheric turbulence simulation methods for image **restoration**. (arXiv:2204.08974v1 [cs.CV])
77. Real-Time Face Recognition System. (arXiv:2204.08978v1 [cs.CV])
78. Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones. (arXiv:2204.08989v1 [eess.SP])
79. Dual-Domain Image Synthesis using Segmentation-Guided GAN. (arXiv:2204.09015v1 [cs.CV])
80. Unsupervised detection of ash dieback disease (Hymenoscyphus fraxineus) using diffusion-based hyperspectral image clustering. (arXiv:2204.09041v1 [cs.CV])
81. Good, Better, Best: Textual Distractors Generation for Multiple-Choice Visual Question Answering via Reinforcement Learning. (arXiv:1910.09134v3 [cs.CV] UPDATED)
82. A Survey on Deep Hashing Methods. (arXiv:2003.03369v3 [cs.CV] UPDATED)
83. Perceptron Synthesis Network: Rethinking the Action Scale Variances in Videos. (arXiv:2007.11460v3 [cs.CV] UPDATED)
84. NeuralAnnot: Neural Annotator for 3D Human Mesh Training Sets. (arXiv:2011.11232v5 [cs.CV] UPDATED)
85. Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation. (arXiv:2011.11534v4 [cs.CV] UPDATED)
86. Efficient Density Ratio-Guided Subsampling of Conditional GANs, With Conditioning on a Class or a Continuous Variable. (arXiv:2103.11166v4 [cs.CV] UPDATED)
87. Machine learning method for light field refocusing. (arXiv:2103.16020v3 [eess.IV] UPDATED)
88. Fourier Image Transformer. (arXiv:2104.02555v3 [cs.CV] UPDATED)
89. Distilling and Transferring Knowledge via cGAN-generated Samples for Image Classification and Regression. (arXiv:2104.03164v3 [cs.CV] UPDATED)
90. M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection. (arXiv:2104.09770v3 [cs.CV] UPDATED)
91. Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation. (arXiv:2104.14639v2 [cs.CV] UPDATED)
92. CSRNet: Cascaded Selective Resolution Network for **Real-time** Semantic Segmentation. (arXiv:2106.04400v2 [cs.CV] UPDATED)
93. Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v3 [eess.IV] UPDATED)
94. An overview of mixing augmentation methods and augmentation strategies. (arXiv:2107.09887v2 [cs.CV] UPDATED)
95. A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v4 [eess.IV] UPDATED)
96. HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v5 [cs.CV] UPDATED)
97. Coarse-to-Fine Reasoning for Visual Question Answering. (arXiv:2110.02526v2 [cs.CV] UPDATED)
98. MPSN: Motion-aware Pseudo Siamese Network for Indoor Video Head Detection in Buildings. (arXiv:2110.03302v4 [cs.CV] UPDATED)
99. FastDOG: Fast Discrete Optimization on GPU. (arXiv:2111.10270v3 [math.OC] UPDATED)
100. Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling. (arXiv:2111.12698v2 [cs.CV] UPDATED)
101. Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v2 [cs.LG] UPDATED)
102. Joint Symmetry Detection and Shape Matching for Non-Rigid Point Cloud. (arXiv:2112.02713v2 [cs.CV] UPDATED)
103. I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v5 [cs.CV] UPDATED)
104. Dual-Key Multimodal Backdoors for Visual Question Answering. (arXiv:2112.07668v3 [cs.CV] UPDATED)
105. SPTS: Single-Point Text Spotting. (arXiv:2112.07917v3 [cs.CV] UPDATED)
106. The Wanderings of Odysseus in 3D Scenes. (arXiv:2112.09251v2 [cs.CV] UPDATED)
107. Learned Queries for Efficient Local Attention. (arXiv:2112.11435v2 [cs.CV] UPDATED)
108. Cross Modal Retrieval with Querybank Normalisation. (arXiv:2112.12777v3 [cs.CV] UPDATED)
109. Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v2 [cs.GR] UPDATED)
110. Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning. (arXiv:2201.09671v3 [cs.LG] UPDATED)
111. Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices. (arXiv:2201.09933v2 [cs.CV] UPDATED)
112. Bootstrapped Representation Learning for Skeleton-Based Action Recognition. (arXiv:2202.02232v2 [cs.CV] UPDATED)
113. A comprehensive benchmark analysis for sand dust image reconstruction. (arXiv:2202.03031v2 [eess.IV] UPDATED)
114. Point-Level Region Contrast for Object Detection Pre-Training. (arXiv:2202.04639v2 [cs.CV] UPDATED)
115. FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v3 [cs.CV] UPDATED)
116. GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges. (arXiv:2202.06511v3 [cs.CV] UPDATED)
117. H4D: Human 4D Modeling by Learning Neural Compositional Representation. (arXiv:2203.01247v2 [cs.CV] UPDATED)
118. BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v4 [cs.CV] UPDATED)
119. WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v4 [cs.CV] UPDATED)
120. Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v4 [eess.IV] UPDATED)
121. R3M: A Universal Visual Representation for Robot Manipulation. (arXiv:2203.12601v2 [cs.RO] UPDATED)
122. Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v2 [cs.CV] UPDATED)
123. Nested Collaborative Learning for Long-Tailed Visual Recognition. (arXiv:2203.15359v2 [cs.CV] UPDATED)
124. Ball 3D Localization From A Single Calibrated Image. (arXiv:2204.00003v3 [cs.CV] UPDATED)
125. Joint Learning of Feature Extraction and Cost Aggregation for Semantic Correspondence. (arXiv:2204.02164v2 [cs.CV] UPDATED)
126. M$^2$BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation. (arXiv:2204.05088v2 [cs.CV] UPDATED)
127. Transparent Shape from Single Polarization Images. (arXiv:2204.06331v2 [cs.CV] UPDATED)
128. Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v3 [cs.CV] UPDATED)
129. Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis. (arXiv:2204.06929v2 [eess.IV] UPDATED)
130. Attention Mechanism based Cognition-level Scene Understanding. (arXiv:2204.08027v2 [cs.CV] UPDATED)
131. Semi-Supervised Super-Resolution. (arXiv:2204.08192v2 [eess.IV] UPDATED)
132. MHSCNet: A Multimodal Hierarchical Shot-aware Convolutional Network for Video Summarization. (arXiv:2204.08352v2 [cs.CV] UPDATED)
133. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v2 [cs.CL] UPDATED)
134. Revisiting Consistency Regularization for Semi-supervised Change Detection in Remote Sensing Images. (arXiv:2204.08454v2 [cs.CV] UPDATED)
135. Land Cover Classification from Remote Sensing Images Based on Multi-Scale Fully Convolutional Network. (arXiv:2008.00168v2 [cs.CV] CROSS LISTED)
136. Binary Segmentation of Seismic Facies Using Encoder-Decoder Neural Networks. (arXiv:2012.03675v1 [eess.IV] CROSS LISTED)
## eess.IV
---
**33** new papers in eess.IV:-) 
1. 3D Convolutional Networks for Action Recognition: Application to Sport Gesture Recognition. (arXiv:2204.08460v1 [cs.CV])
2. CapillaryX: A Software Design Pattern for Analyzing Medical Images in **Real-time** using Deep Learning. (arXiv:2204.08462v1 [eess.IV])
3. Machine Learning-Based Automated Thermal Comfort Prediction: Integration of Low-Cost Thermal and Visual Cameras for Higher Accuracy. (arXiv:2204.08463v1 [cs.CV])
4. Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography. (arXiv:2204.08466v1 [eess.IV])
5. IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v1 [eess.IV])
6. U-Net and its variants for Medical Image Segmentation : A short review. (arXiv:2204.08470v1 [eess.IV])
7. Self Supervised Lesion Recognition For Breast Ultrasound Diagnosis. (arXiv:2204.08477v1 [eess.IV])
8. Enhancing Non-mass Breast Ultrasound Cancer Classification With Knowledge Transfer. (arXiv:2204.08478v1 [eess.IV])
9. Quaternion Optimized Model with Sparse Regularization for Color Image Recovery. (arXiv:2204.08629v1 [cs.CV])
10. A Thin Format Vision-Based Tactile Sensor with A Micro Lens Array (MLA). (arXiv:2204.08691v1 [cs.RO])
11. Edge-enhanced Feature Distillation Network for Efficient Super-Resolution. (arXiv:2204.08759v1 [cs.CV])
12. Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment. (arXiv:2204.08763v1 [cs.CV])
13. Two-Stream Graph Convolutional Network for Intra-oral Scanner Image Segmentation. (arXiv:2204.08797v1 [eess.IV])
14. UID2021: An Underwater Image Dataset for Evaluation of No-reference Quality Assessment Metrics. (arXiv:2204.08813v1 [cs.CV])
15. Deep learning-based surrogate model for 3-D patient-specific computational fluid dynamics. (arXiv:2204.08939v1 [physics.med-ph])
16. MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment. (arXiv:2204.08958v1 [cs.CV])
17. Per Clip Lagrangian Multiplier Optimisation for HEVC. (arXiv:2204.08965v1 [eess.IV])
18. Per-clip adaptive Lagrangian multiplier optimisation with low-resolution proxies. (arXiv:2204.08966v1 [eess.IV])
19. Rendering **Night**time Image Via Cascaded Color and Brightness Compensation. (arXiv:2204.08970v1 [cs.CV])
20. A comparison of different atmospheric turbulence simulation methods for image **restoration**. (arXiv:2204.08974v1 [cs.CV])
21. Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones. (arXiv:2204.08989v1 [eess.SP])
22. Perceptron Synthesis Network: Rethinking the Action Scale Variances in Videos. (arXiv:2007.11460v3 [cs.CV] UPDATED)
23. Machine learning method for light field refocusing. (arXiv:2103.16020v3 [eess.IV] UPDATED)
24. Fourier Image Transformer. (arXiv:2104.02555v3 [cs.CV] UPDATED)
25. Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v3 [eess.IV] UPDATED)
26. A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v4 [eess.IV] UPDATED)
27. HyperPCA: a Powerful Tool to Extract Elemental Maps from Noisy Data Obtained in LIBS Mapping of Materials. (arXiv:2111.15187v2 [physics.app-ph] UPDATED)
28. A comprehensive benchmark analysis for sand dust image reconstruction. (arXiv:2202.03031v2 [eess.IV] UPDATED)
29. Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v4 [eess.IV] UPDATED)
30. Ball 3D Localization From A Single Calibrated Image. (arXiv:2204.00003v3 [cs.CV] UPDATED)
31. Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis. (arXiv:2204.06929v2 [eess.IV] UPDATED)
32. Semi-Supervised Super-Resolution. (arXiv:2204.08192v2 [eess.IV] UPDATED)
33. Revisiting Consistency Regularization for Semi-supervised Change Detection in Remote Sensing Images. (arXiv:2204.08454v2 [cs.CV] UPDATED)
## cs.LG
---
**132** new papers in cs.LG:-) 
1. SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix Data Augmentation. (arXiv:2204.08458v1 [cs.CV])
2. Comparative analysis of machine learning and numerical modeling for combined heat transfer in Polymethylmethacrylate. (arXiv:2204.08459v1 [cs.LG])
3. 3D Convolutional Networks for Action Recognition: Application to Sport Gesture Recognition. (arXiv:2204.08460v1 [cs.CV])
4. Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification. (arXiv:2204.08461v1 [cs.CV])
5. CapillaryX: A Software Design Pattern for Analyzing Medical Images in **Real-time** using Deep Learning. (arXiv:2204.08462v1 [eess.IV])
6. Machine Learning-Based Automated Thermal Comfort Prediction: Integration of Low-Cost Thermal and Visual Cameras for Higher Accuracy. (arXiv:2204.08463v1 [cs.CV])
7. Intelligent Spatial Interpolation-based Frost Prediction Methodology using Artificial Neural Networks with Limited Local Data. (arXiv:2204.08465v1 [cs.LG])
8. IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v1 [eess.IV])
9. Simultaneous Multiple-Prompt Guided Generation Using Differentiable Optimal Transport. (arXiv:2204.08472v1 [cs.CV])
10. AB/BA analysis: A framework for estimating keyword spotting recall improvement while maintaining audio privacy. (arXiv:2204.08474v1 [cs.SD])
11. Predictive analytics for appointment bookings. (arXiv:2204.08475v1 [cs.LG])
12. Research on Domain Information Mining and Theme Evolution of Scientific Papers. (arXiv:2204.08476v1 [cs.DL])
13. Inductive Biases for Object-Centric Representations of Complex Textures. (arXiv:2204.08479v1 [cs.CV])
14. Active Learning Helps Pretrained Models Learn the Intended Task. (arXiv:2204.08491v1 [cs.LG])
15. DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning. (arXiv:2204.08499v1 [cs.LG])
16. CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v1 [cs.SI])
17. Active-learning-based non-intrusive Model Order Reduction. (arXiv:2204.08523v1 [cs.LG])
18. So2Sat POP -- A Curated Benchmark Data Set for Population Estimation from Space on a Continental Scale. (arXiv:2204.08524v1 [cs.LG])
19. An Optimal Time Variable Learning Framework for Deep Neural Networks. (arXiv:2204.08528v1 [math.OC])
20. Improving Information Cascade Modeling by Social Topology and Dual Role User Dependency. (arXiv:2204.08529v1 [cs.SI])
21. Twitter Dataset on the Russo-Ukrainian War. (arXiv:2204.08530v1 [cs.SI])
22. CBR-iKB: A Case-Based Reasoning Approach for Question Answering over Incomplete Knowledge Bases. (arXiv:2204.08554v1 [cs.CL])
23. Learning Similarity Preserving Binary Codes for Recommender Systems. (arXiv:2204.08569v1 [cs.IR])
24. A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability. (arXiv:2204.08570v1 [cs.LG])
25. Expert-Calibrated Learning for Online Optimization with Switching Costs. (arXiv:2204.08572v1 [cs.LG])
26. Training and Evaluation of Deep Policies using Reinforcement Learning and Generative Models. (arXiv:2204.08573v1 [cs.LG])
27. Adaptive Noisy Data Augmentation for Regularized Estimation and Inference in Generalized Linear Models. (arXiv:2204.08574v1 [stat.ML])
28. On Parametric Optimal Execution and Machine Learning Surrogates. (arXiv:2204.08581v1 [q-fin.TR])
29. MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. (arXiv:2204.08582v1 [cs.CL])
30. INFOrmation Prioritization through EmPOWERment in Visual Model-Based RL. (arXiv:2204.08585v1 [cs.RO])
31. Spatial-Temporal Hypergraph Self-Supervised Learning for Crime Prediction. (arXiv:2204.08587v1 [cs.LG])
32. G2GT: Retrosynthesis Prediction with Graph to Graph Attention Neural Network and Self-Training. (arXiv:2204.08608v1 [q-bio.QM])
33. "Flux+Mutability": A Conditional Generative Approach to One-Class Classification and Anomaly Detection. (arXiv:2204.08609v1 [cs.LG])
34. Poisons that are learned faster are more effective. (arXiv:2204.08615v1 [cs.LG])
35. Equity in Resident Crowdsourcing: Measuring Under-reporting without Ground Truth Data. (arXiv:2204.08620v1 [stat.AP])
36. Proximal Implicit ODE Solvers for Accelerating Learning Neural ODEs. (arXiv:2204.08621v1 [math.NA])
37. CorrGAN: Input Transformation Technique Against Natural Corruptions. (arXiv:2204.08623v1 [cs.LG])
38. Topology and geometry of data manifold in deep learning. (arXiv:2204.08624v1 [cs.LG])
39. GraphHop++: New Insights into GraphHop and Its **Enhancement**. (arXiv:2204.08646v1 [cs.LG])
40. Learning Forward Dynamics Model and Informed Trajectory Sampler for Safe Quadruped Navigation. (arXiv:2204.08647v1 [cs.RO])
41. On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules. (arXiv:2204.08653v1 [cs.SE])
42. Mono vs Multilingual BERT for Hate Speech Detection and Text Classification: A Case Study in Marathi. (arXiv:2204.08669v1 [cs.CL])
43. A Score-based Geometric Model for Molecular Dynamics Simulations. (arXiv:2204.08672v1 [cs.CE])
44. Investigation of a Data Split Strategy Involving the Time Axis in Adverse Event Prediction Using Machine Learning. (arXiv:2204.08682v1 [cs.LG])
45. Imbalanced Classification via a Tabular Translation GAN. (arXiv:2204.08683v1 [cs.LG])
46. Independence Testing for Bounded Degree Bayesian Network. (arXiv:2204.08690v1 [cs.DS])
47. Software Engineering Approaches for TinyML based IoT Embedded Vision: A Systematic Literature Review. (arXiv:2204.08702v1 [cs.SE])
48. Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks. (arXiv:2204.08726v1 [cs.LG])
49. Neural Collapse Inspired Attraction-Repulsion-Balanced Loss for Imbalanced Learning. (arXiv:2204.08735v1 [cs.LG])
50. Table-based Fact Verification with Self-adaptive Mixture of Experts. (arXiv:2204.08753v1 [cs.AI])
51. Binary Multi Channel Morphological Neural Network. (arXiv:2204.08768v1 [cs.CV])
52. GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning. (arXiv:2204.08770v1 [cs.CV])
53. EXIT: Extrapolation and Interpolation-based Neural Controlled Differential Equations for Time-series Classification and Forecasting. (arXiv:2204.08771v1 [cs.LG])
54. LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations. (arXiv:2204.08781v1 [cs.LG])
55. ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v1 [cs.CV])
56. Making Progress Based on False Discoveries. (arXiv:2204.08809v1 [cs.LG])
57. Radio Galaxy Zoo: Using semi-supervised learning to leverage large unlabelled data-sets for radio galaxy classification under data-set shift. (arXiv:2204.08816v1 [astro-ph.GA])
58. System Analysis for Responsible Design of Modern AI/ML Systems. (arXiv:2204.08836v1 [cs.LG])
59. Rumor Detection with Self-supervised Learning on Texts and Social Graph. (arXiv:2204.08838v1 [cs.SI])
60. Compressed Empirical Measures (in finite dimensions). (arXiv:2204.08847v1 [stat.ML])
61. Antipatterns in Software Classification Taxonomies. (arXiv:2204.08880v1 [cs.SE])
62. GestureLens: Visual Analysis of Gestures in Presentation Videos. (arXiv:2204.08894v1 [cs.HC])
63. Feature Structure Distillation for BERT Transferring. (arXiv:2204.08922v1 [cs.CL])
64. Learning heuristics for A*. (arXiv:2204.08938v1 [cs.AI])
65. CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex. (arXiv:2204.08941v1 [cs.DB])
66. Missingness Bias in Model Debugging. (arXiv:2204.08945v1 [cs.CV])
67. Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity. (arXiv:2204.08954v1 [cs.LG])
68. COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation. (arXiv:2204.08957v1 [cs.LG])
69. When Is Partially Observable Reinforcement Learning Not Scary?. (arXiv:2204.08967v1 [cs.LG])
70. Deep learning based closed-loop optimization of geothermal reservoir production. (arXiv:2204.08987v1 [cs.LG])
71. CPU- and GPU-based Distributed Sampling in Dirichlet Process Mixtures for Large-scale Analysis. (arXiv:2204.08988v1 [cs.LG])
72. Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones. (arXiv:2204.08989v1 [eess.SP])
73. Study of Robust Sparsity-Aware RLS algorithms with Jointly-Optimized Parameters for Impulsive Noise Environments. (arXiv:2204.08990v1 [eess.SP])
74. Benchmarking Domain Generalization on EEG-based Emotion Recognition. (arXiv:2204.09016v1 [eess.SP])
75. Hybrid Transformer Network for Different Horizons-based Enriched Wind Speed Forecasting. (arXiv:2204.09019v1 [eess.SP])
76. A stochastic Stein Variational Newton method. (arXiv:2204.09039v1 [stat.ML])
77. Unsupervised detection of ash dieback disease (Hymenoscyphus fraxineus) using diffusion-based hyperspectral image clustering. (arXiv:2204.09041v1 [cs.CV])
78. Accelerating Inhibitor Discovery for Multiple SARS-CoV-2 Targets with a Single, Sequence-Guided Deep Generative Framework. (arXiv:2204.09042v1 [q-bio.QM])
79. Energy-Based Continuous Inverse Optimal Control. (arXiv:1904.05453v6 [cs.LG] UPDATED)
80. Distributed Learning of Deep Neural Networks using Independent Subnet Training. (arXiv:1910.02120v7 [cs.LG] UPDATED)
81. Meta-Learning through Hebbian Plasticity in Random Networks. (arXiv:2007.02686v5 [cs.NE] UPDATED)
82. Perceptron Synthesis Network: Rethinking the Action Scale Variances in Videos. (arXiv:2007.11460v3 [cs.CV] UPDATED)
83. An Intuitive Tutorial to Gaussian Processes Regression. (arXiv:2009.10862v4 [stat.ML] UPDATED)
84. Model-free Neural Counterfactual Regret Minimization with Bootstrap Learning. (arXiv:2012.01870v4 [cs.LG] UPDATED)
85. DEUP: Direct Epistemic Uncertainty Prediction. (arXiv:2102.08501v2 [cs.LG] UPDATED)
86. A Brief Survey on Deep Learning Based Data Hiding. (arXiv:2103.01607v2 [cs.CR] UPDATED)
87. Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability. (arXiv:2103.07162v3 [cs.CL] UPDATED)
88. Efficient Density Ratio-Guided Subsampling of Conditional GANs, With Conditioning on a Class or a Continuous Variable. (arXiv:2103.11166v4 [cs.CV] UPDATED)
89. Machine learning method for light field refocusing. (arXiv:2103.16020v3 [eess.IV] UPDATED)
90. Fourier Image Transformer. (arXiv:2104.02555v3 [cs.CV] UPDATED)
91. Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage. (arXiv:2107.03920v4 [stat.ML] UPDATED)
92. FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks. (arXiv:2107.06419v6 [cs.LG] UPDATED)
93. Learning Augmented Online Facility Location. (arXiv:2107.08277v2 [cs.DS] UPDATED)
94. Greedification Operators for Policy Optimization: Investigating Forward and Reverse KL Divergences. (arXiv:2107.08285v2 [cs.LG] UPDATED)
95. An Exploration of Learnt Representations of W Jets. (arXiv:2109.10919v3 [hep-ph] UPDATED)
96. Model reduction for the material point method via an **implicit neural representation** of the deformation map. (arXiv:2109.12390v2 [cs.LG] UPDATED)
97. Reversible Gromov-Monge Sampler for Simulation-Based Inference. (arXiv:2109.14090v2 [stat.ME] UPDATED)
98. Risk-Aware Learning for Scalable Voltage Optimization in Distribution Grids. (arXiv:2110.01490v2 [cs.LG] UPDATED)
99. Equivalence Analysis between Counterfactual Regret Minimization and Online Mirror Descent. (arXiv:2110.04961v2 [cs.LG] UPDATED)
100. Convex-Concave Min-Max Stackelberg Games. (arXiv:2110.05192v5 [cs.GT] UPDATED)
101. 2021 Drexel Society of Artificial Intelligence Research Conference. (arXiv:2110.05263v3 [cs.AI] UPDATED)
102. Deep Federated Learning for Autonomous Driving. (arXiv:2110.05754v2 [cs.LG] UPDATED)
103. Neural Stochastic Partial Differential Equations: Resolution-Invariant Learning of Continuous Spatiotemporal Dynamics. (arXiv:2110.10249v5 [cs.LG] UPDATED)
104. OneFlow: Redesign the Distributed Deep Learning Framework from Scratch. (arXiv:2110.15032v6 [cs.DC] UPDATED)
105. Using Shapley Values and Variational Autoencoders to Explain Predictive Models with Dependent Mixed Features. (arXiv:2111.13507v2 [stat.ML] UPDATED)
106. Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v2 [cs.LG] UPDATED)
107. HyperPCA: a Powerful Tool to Extract Elemental Maps from Noisy Data Obtained in LIBS Mapping of Materials. (arXiv:2111.15187v2 [physics.app-ph] UPDATED)
108. Joint Symmetry Detection and Shape Matching for Non-Rigid Point Cloud. (arXiv:2112.02713v2 [cs.CV] UPDATED)
109. Step-unrolled Denoising Autoencoders for Text Generation. (arXiv:2112.06749v3 [cs.CL] UPDATED)
110. Stochastic Saddle Point Problems with Decision-Dependent Distributions. (arXiv:2201.02313v2 [math.OC] UPDATED)
111. Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v2 [cs.GR] UPDATED)
112. Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning. (arXiv:2201.09671v3 [cs.LG] UPDATED)
113. Separating Rule Discovery and Global Solution Composition in a Learning Classifier System. (arXiv:2202.01677v2 [cs.LG] UPDATED)
114. TinyM$^2$Net: A Flexible System Algorithm Co-designed Multimodal Learning Framework for Tiny Devices. (arXiv:2202.04303v3 [cs.LG] UPDATED)
115. Limitations of Deep Learning for Inverse Problems on Digital Hardware. (arXiv:2202.13490v2 [cs.LG] UPDATED)
116. An Open Challenge for Inductive Link Prediction on Knowledge Graphs. (arXiv:2203.01520v2 [cs.LG] UPDATED)
117. BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v4 [cs.CV] UPDATED)
118. A Unified Framework for Rank-based Evaluation Metrics for Link Prediction in Knowledge Graphs. (arXiv:2203.07544v2 [cs.LG] UPDATED)
119. R3M: A Universal Visual Representation for Robot Manipulation. (arXiv:2203.12601v2 [cs.RO] UPDATED)
120. Reinforcement Learning Guided by Provable Normative Compliance. (arXiv:2203.16275v2 [cs.AI] UPDATED)
121. On the Efficiency of Integrating Self-supervised Learning and Meta-learning for User-defined Few-shot Keyword Spotting. (arXiv:2204.00352v2 [cs.LG] UPDATED)
122. Efficient comparison of sentence embeddings. (arXiv:2204.00820v2 [cs.CL] UPDATED)
123. BigDL 2.0: Seamless Scaling of AI Pipelines from Laptops to Distributed Cluster. (arXiv:2204.01715v2 [cs.LG] UPDATED)
124. Neural Lagrangian Schr\"odinger Bridge. (arXiv:2204.04853v2 [cs.LG] UPDATED)
125. Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v3 [cs.CV] UPDATED)
126. Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis. (arXiv:2204.06929v2 [eess.IV] UPDATED)
127. Interpretable Fault Diagnosis of Rolling Element Bearings with Temporal Logic Neural Network. (arXiv:2204.07579v2 [cs.LG] UPDATED)
128. Conditional Injective Flows for Bayesian Imaging. (arXiv:2204.07664v2 [cs.LG] UPDATED)
129. Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning. (arXiv:2204.07729v2 [cs.LG] UPDATED)
130. Differentiable Time-Frequency Scattering in Kymatio. (arXiv:2204.08269v2 [cs.SD] UPDATED)
131. Revisiting Consistency Regularization for Semi-supervised Change Detection in Remote Sensing Images. (arXiv:2204.08454v2 [cs.CV] UPDATED)
132. Binary Segmentation of Seismic Facies Using Encoder-Decoder Neural Networks. (arXiv:2012.03675v1 [eess.IV] CROSS LISTED)
## cs.AI
---
**58** new papers in cs.AI:-) 
1. SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix Data Augmentation. (arXiv:2204.08458v1 [cs.CV])
2. Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography. (arXiv:2204.08466v1 [eess.IV])
3. IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v1 [eess.IV])
4. U-Net and its variants for Medical Image Segmentation : A short review. (arXiv:2204.08470v1 [eess.IV])
5. AI for human assessment: What do professional assessors need?. (arXiv:2204.08471v1 [cs.HC])
6. CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v1 [cs.SI])
7. So2Sat POP -- A Curated Benchmark Data Set for Population Estimation from Space on a Continental Scale. (arXiv:2204.08524v1 [cs.LG])
8. Dress Code: High-Resolution Multi-Category Virtual Try-On. (arXiv:2204.08532v1 [cs.CV])
9. CBR-iKB: A Case-Based Reasoning Approach for Question Answering over Incomplete Knowledge Bases. (arXiv:2204.08554v1 [cs.CL])
10. MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. (arXiv:2204.08582v1 [cs.CL])
11. INFOrmation Prioritization through EmPOWERment in Visual Model-Based RL. (arXiv:2204.08585v1 [cs.RO])
12. Spatial-Temporal Hypergraph Self-Supervised Learning for Crime Prediction. (arXiv:2204.08587v1 [cs.LG])
13. Example-based Synthesis of Static Analysis Rules. (arXiv:2204.08643v1 [cs.SE])
14. Artificial Intelligence for Imaging Cherenkov Detectors at the EIC. (arXiv:2204.08645v1 [physics.ins-det])
15. GraphHop++: New Insights into GraphHop and Its **Enhancement**. (arXiv:2204.08646v1 [cs.LG])
16. LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation. (arXiv:2204.08649v1 [cs.CL])
17. Interventional Behavior Prediction: Avoiding Overly Confident Anticipation in Interactive Prediction. (arXiv:2204.08665v1 [cs.RO])
18. Many Episode Learning in a Modular Embodied Agent via End-to-End Interaction. (arXiv:2204.08687v1 [cs.AI])
19. Table-based Fact Verification with Self-adaptive Mixture of Experts. (arXiv:2204.08753v1 [cs.AI])
20. IndicXNLI: Evaluating Multilingual Inference for Indian Languages. (arXiv:2204.08776v1 [cs.CL])
21. Where Was COVID-19 First Discovered? Designing a Question-Answering System for Pandemic Situations. (arXiv:2204.08787v1 [cs.CL])
22. RNNCTPs: A Neural Symbolic Reasoning Method Using Dynamic Knowledge Partitioning Technology. (arXiv:2204.08810v1 [cs.AI])
23. SmartSales: Sales Script Extraction and Analysis from Sales Chatlog. (arXiv:2204.08811v1 [cs.CL])
24. A Convolutional-Attentional Neural Framework for Structure-Aware Performance-Score Synchronization. (arXiv:2204.08822v1 [cs.SD])
25. On the Influence of Explainable AI on Automation Bias. (arXiv:2204.08859v1 [cs.HC])
26. ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs. (arXiv:2204.08875v1 [cs.CL])
27. Invertible Mask Network for Face Privacy-Preserving. (arXiv:2204.08895v1 [cs.CV])
28. Model Checking Strategic Abilities in Information-sharing Systems. (arXiv:2204.08896v1 [cs.LO])
29. Feature Structure Distillation for BERT Transferring. (arXiv:2204.08922v1 [cs.CL])
30. Learning heuristics for A*. (arXiv:2204.08938v1 [cs.AI])
31. Missingness Bias in Model Debugging. (arXiv:2204.08945v1 [cs.CV])
32. COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation. (arXiv:2204.08957v1 [cs.LG])
33. When Is Partially Observable Reinforcement Learning Not Scary?. (arXiv:2204.08967v1 [cs.LG])
34. Disappeared Command: Spoofing Attack On Automatic Speech Recognition Systems with Sound Masking. (arXiv:2204.08977v1 [cs.SD])
35. Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones. (arXiv:2204.08989v1 [eess.SP])
36. Learning Visual Shape Control of Novel 3D Deformable Objects from Partial-View Point Clouds. (arXiv:2110.04685v2 [cs.RO] UPDATED)
37. 2021 Drexel Society of Artificial Intelligence Research Conference. (arXiv:2110.05263v3 [cs.AI] UPDATED)
38. OneFlow: Redesign the Distributed Deep Learning Framework from Scratch. (arXiv:2110.15032v6 [cs.DC] UPDATED)
39. Large Scale Diverse Combinatorial Optimization: ESPN Fantasy Football Player Trades. (arXiv:2111.02859v3 [cs.AI] UPDATED)
40. Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v2 [cs.LG] UPDATED)
41. The signature and cusp geometry of hyperbolic knots. (arXiv:2111.15323v2 [math.GT] UPDATED)
42. Joint Symmetry Detection and Shape Matching for Non-Rigid Point Cloud. (arXiv:2112.02713v2 [cs.CV] UPDATED)
43. Supervised Contrastive Learning for Recommendation. (arXiv:2201.03144v2 [cs.IR] UPDATED)
44. Separating Rule Discovery and Global Solution Composition in a Learning Classifier System. (arXiv:2202.01677v2 [cs.LG] UPDATED)
45. Limitations of Deep Learning for Inverse Problems on Digital Hardware. (arXiv:2202.13490v2 [cs.LG] UPDATED)
46. An Open Challenge for Inductive Link Prediction on Knowledge Graphs. (arXiv:2203.01520v2 [cs.LG] UPDATED)
47. Scalable Verification of GNN-based Job Schedulers. (arXiv:2203.03153v2 [cs.AI] UPDATED)
48. A Unified Framework for Rank-based Evaluation Metrics for Link Prediction in Knowledge Graphs. (arXiv:2203.07544v2 [cs.LG] UPDATED)
49. R3M: A Universal Visual Representation for Robot Manipulation. (arXiv:2203.12601v2 [cs.RO] UPDATED)
50. Enabling hand gesture customization on wrist-worn devices. (arXiv:2203.15239v2 [cs.HC] UPDATED)
51. Reinforcement Learning Guided by Provable Normative Compliance. (arXiv:2203.16275v2 [cs.AI] UPDATED)
52. Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v3 [cs.CV] UPDATED)
53. Interpretable Fault Diagnosis of Rolling Element Bearings with Temporal Logic Neural Network. (arXiv:2204.07579v2 [cs.LG] UPDATED)
54. Attention Mechanism based Cognition-level Scene Understanding. (arXiv:2204.08027v2 [cs.CV] UPDATED)
55. Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning. (arXiv:2204.08143v2 [cs.CL] UPDATED)
56. Revisiting Consistency Regularization for Semi-supervised Change Detection in Remote Sensing Images. (arXiv:2204.08454v2 [cs.CV] UPDATED)
57. Land Cover Classification from Remote Sensing Images Based on Multi-Scale Fully Convolutional Network. (arXiv:2008.00168v2 [cs.CV] CROSS LISTED)
58. Perspectives on risk prioritization of data center vulnerabilities using rank aggregation and multi-objective optimization. (arXiv:2202.07466v1 [cs.CR] CROSS LISTED)

