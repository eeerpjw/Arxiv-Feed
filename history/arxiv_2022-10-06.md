# Your interest papers
---
## cs.CV
---
### **Low-Light** Image **Restoration** Based on Retina Model using Neural Networks. (arXiv:2210.01806v1 [eess.IV])
- Authors : Yurui Ming, Yuanyuan Liang
- Link : [http://arxiv.org/abs/2210.01806](http://arxiv.org/abs/2210.01806)
> ABSTRACT  :  We report the possibility of using a simple neural network for effortless **restoration** of **low-light** images inspired by the retina model, which mimics the neurophysiological principles and dynamics of various types of optical neurons. The proposed neural network model saves the cost of computational overhead in contrast with traditional signal-processing models, and generates results comparable with complicated deep learning models from the subjective perceptual perspective. This work shows that to directly simulate the functionalities of retinal neurons using neural networks not only avoids the manually seeking for the optimal parameters, but also paves the way to build corresponding artificial versions for certain neurobiological organizations.  
### A Perceptual Quality Metric for Video Frame Interpolation. (arXiv:2210.01879v1 [cs.CV])
- Authors : Qiqi Hou, Abhijay Ghildyal, Feng Liu
- Link : [http://arxiv.org/abs/2210.01879](http://arxiv.org/abs/2210.01879)
> ABSTRACT  :  Research on video frame interpolation has made significant progress in recent years. However, existing methods mostly use off-the-shelf metrics to measure the quality of interpolation results with the exception of a few methods that employ user studies, which is time-consuming. As video frame interpolation results often exhibit unique artifacts, existing quality metrics sometimes are not consistent with human perception when measuring the interpolation results. Some recent deep learning-based perceptual quality metrics are shown more consistent with human judgments, but their performance on videos is compromised since they do not consider temporal information. In this paper, we present a dedicated perceptual quality metric for measuring video frame interpolation results. Our method learns perceptual features directly from videos instead of individual frames. It compares pyramid features extracted from video frames and employs **Swin** Transformer blocks-based spatio-temporal modules to extract spatio-temporal information. To train our metric, we collected a new video frame interpolation quality assessment dataset. Our experiments show that our dedicated quality metric outperforms state-of-the-art methods when measuring video frame interpolation results. Our code and model are made publicly available at \url{https://github.com/hqqxyy/VFIPS}.  
### Graph Classification via Discriminative Edge Feature Learning. (arXiv:2210.02060v1 [cs.CV])
- Authors : Yang Yi, Xuequan Lu, Shang Gao, Antonio Robles, Yuejie Zhang
- Link : [http://arxiv.org/abs/2210.02060](http://arxiv.org/abs/2210.02060)
> ABSTRACT  :  Spectral graph convolutional neural networks (GCNNs) have been producing encouraging results in graph classification tasks. However, most spectral GCNNs utilize fixed graphs when aggregating node features, while omitting edge feature learning and failing to get an optimal graph structure. Moreover, many existing graph datasets do not provide initialized edge features, further restraining the ability of learning edge features via spectral GCNNs. In this paper, we try to address this issue by designing an edge feature scheme and an add-on layer between every two stacked graph convolution layers in GCNN. Both are lightweight while effective in filling the gap between edge feature learning and performance **enhancement** of graph classification. The edge feature scheme makes edge features adapt to node representations at different graph convolution layers. The add-on layers help adjust the edge features to an optimal graph structure. To test the effectiveness of our method, we take Euclidean positions as initial node features and extract graphs with semantic information from point cloud objects. The node features of our extracted graphs are more scalable for edge feature learning than most existing graph datasets (in one-hot encoded label format). Three new graph datasets are constructed based on ModelNet40, ModelNet10 and ShapeNet Part datasets. Experimental results show that our method outperforms state-of-the-art graph classification methods on the new datasets by reaching 96.56% overall accuracy on Graph-ModelNet40, 98.79% on Graph-ModelNet10 and 97.91% on Graph-ShapeNet Part. The constructed graph datasets will be released to the community.  
### Hiding Images in Deep Probabilistic Models. (arXiv:2210.02257v1 [cs.CR])
- Authors : Haoyu Chen, Linqi Song, Zhenxing Qian, Xinpeng Zhang, **Kede Ma**
- Link : [http://arxiv.org/abs/2210.02257](http://arxiv.org/abs/2210.02257)
> ABSTRACT  :  Data hiding with deep neural networks (DNNs) has experienced impressive successes in recent years. A prevailing scheme is to train an autoencoder, consisting of an encoding network to embed (or transform) secret messages in (or into) a carrier, and a decoding network to extract the hidden messages. This scheme may suffer from several limitations regarding practicability, security, and embedding capacity. In this work, we describe a different computational framework to hide images in deep probabilistic models. Specifically, we use a DNN to model the probability density of cover images, and hide a secret image in one particular location of the learned distribution. As an instantiation, we adopt a SinGAN, a pyramid of generative adversarial networks (GANs), to learn the patch distribution of one cover image. We hide the secret image by fitting a deterministic mapping from a fixed set of noise maps (generated by an embedding key) to the secret image during patch distribution learning. The stego SinGAN, behaving as the original SinGAN, is publicly communicated; only the receiver with the embedding key is able to extract the secret image. We demonstrate the feasibility of our SinGAN approach in terms of extraction accuracy and model security. Moreover, we show the flexibility of the proposed method in terms of hiding multiple images for different receivers and obfuscating the secret image.  
### Intensity Mapping Functions For **HDR** Panorama Imaging: Weighted Histogram Averaging. (arXiv:2111.07283v4 [cs.CV] UPDATED)
- Authors : Yilun Xu, Zhengguo Li, Weihai Chen, Changyun Wen
- Link : [http://arxiv.org/abs/2111.07283](http://arxiv.org/abs/2111.07283)
> ABSTRACT  :  It is challenging to stitch multiple images with different **exposure**s due to possible color distortion and loss of details in the brightest and **dark**est regions of input images. In this paper, a novel intensity mapping algorithm is first proposed by introducing a new concept of weighted histogram averaging (WHA). The proposed WHA algorithm leverages the correspondence between the histogram bins of two images which are built up by using the non-decreasing property of the intensity mapping functions (IMFs). The WHA algorithm is then adopted to synthesize a set of differently exposed panorama images. The intermediate panorama images are finally fused via a state-of-the-art multi-scale **exposure** fusion (MEF) algorithm to produce the final panorama image. Extensive experiments indicate that the proposed WHA algorithm significantly surpasses the related state-of-the-art intensity mapping methods. The proposed **high dynamic range** (**HDR**) stitching algorithm also preserves details in the brightest and **dark**est regions of the input images well. The related materials will be publicly accessible at https://github.com/yilun-xu/WHA for reproducible research.  
### Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework. (arXiv:2203.11991v3 [cs.CV] UPDATED)
- Authors : Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan
- Link : [http://arxiv.org/abs/2203.11991](http://arxiv.org/abs/2203.11991)
> ABSTRACT  :  The current popular two-stream, two-stage tracking framework extracts the template and the search region features separately and then performs relation modeling, thus the extracted features lack the awareness of the target and have limited target-background discriminability. To tackle the above issue, we propose a novel one-stream tracking (OSTrack) framework that unifies feature learning and relation modeling by bridging the template-search image pairs with bidirectional information flows. In this way, discriminative target-oriented features can be dynamically extracted by mutual guidance. Since no extra heavy relation modeling module is needed and the implementation is highly parallelized, the proposed tracker runs at a fast speed. To further improve the inference efficiency, an in-network candidate early elimination module is proposed based on the strong similarity prior calculated in the one-stream framework. As a unified framework, OSTrack achieves state-of-the-art performance on multiple benchmarks, in particular, it shows impressive results on the one-shot tracking benchmark GOT-10k, i.e., achieving 73.7% AO, improving the existing best result (**Swin**Track) by 4.3\%. Besides, our method maintains a good performance-speed trade-off and shows faster convergence. The code and models are available at https://github.com/botaoye/OSTrack.  
### A Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v2 [eess.IV] UPDATED)
- Authors : Qiaoqiao Ding, Hui Ji, Yuhui Quan, Xiaoqun Zhang
- Link : [http://arxiv.org/abs/2205.00463](http://arxiv.org/abs/2205.00463)
> ABSTRACT  :  Low-dose CT (LDCT) imaging attracted a considerable interest for the reduction of the object's **exposure** to X-ray radiation. In recent years, supervised deep learning (DL) has been extensively studied for LDCT image reconstruction, which trains a network over a dataset containing many pairs of normal-dose and low-dose images. However, the challenge on collecting many such pairs in the clinical setup limits the application of such supervised-learning-based methods for LDCT image reconstruction in practice. Aiming at addressing the challenges raised by the collection of training dataset, this paper proposed a unsupervised deep learning method for LDCT image reconstruction, which does not require any external training data. The proposed method is built on a re-parametrization technique for Bayesian inference via deep network with random weights, combined with additional total variational~(TV) regularization. The experiments show that the proposed method noticeably outperforms existing dataset-free image reconstruction methods on the test data.  
### Retinal Image **Restoration** and Vessel Segmentation using Modified Cycle-CBAM and CBAM-UNet. (arXiv:2209.04234v2 [eess.IV] UPDATED)
- Authors : Alnur Alimanov, Md Baharul
- Link : [http://arxiv.org/abs/2209.04234](http://arxiv.org/abs/2209.04234)
> ABSTRACT  :  Clinical screening with low-quality fundus images is challenging and significantly leads to misdiagnosis. This paper addresses the issue of improving the retinal image quality and vessel segmentation through retinal image **restoration**. More specifically, a cycle-consistent generative adversarial network (CycleGAN) with a convolution block attention module (CBAM) is used for retinal image **restoration**. A modified UNet is used for retinal vessel segmentation for the restored retinal images (CBAM-UNet). The proposed model consists of two generators and two discriminators. Generators translate images from one domain to another, i.e., from low to high quality and vice versa. Discriminators classify generated and original images. The retinal vessel segmentation model uses downsampling, bottlenecking, and upsampling layers to generate segmented images. The CBAM has been used to enhance the feature extraction of these models. The proposed method does not require paired image datasets, which are challenging to produce. Instead, it uses unpaired data that consists of low- and high-quality fundus images retrieved from publicly available datasets. The **restoration** performance of the proposed method was evaluated using full-reference evaluation metrics, e.g., peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). The retinal vessel segmentation performance was compared with the ground-truth fundus images. The proposed method can significantly reduce the degradation effects caused by out-of-focus blurring, color distortion, low, high, and uneven illumination. Experimental results show the effectiveness of the proposed method for retinal image **restoration** and vessel segmentation.  
### Soft Diffusion: Score Matching for General Corruptions. (arXiv:2209.05442v2 [cs.CV] UPDATED)
- Authors : Giannis Daras, Mauricio Delbracio, Hossein Talebi, **Peyman Milanfar**
- Link : [http://arxiv.org/abs/2209.05442](http://arxiv.org/abs/2209.05442)
> ABSTRACT  :  We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, \textit{that after corruption}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art FID score $1.85$ on CelebA-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion.  
## eess.IV
---
### **Low-Light** Image **Restoration** Based on Retina Model using Neural Networks. (arXiv:2210.01806v1 [eess.IV])
- Authors : Yurui Ming, Yuanyuan Liang
- Link : [http://arxiv.org/abs/2210.01806](http://arxiv.org/abs/2210.01806)
> ABSTRACT  :  We report the possibility of using a simple neural network for effortless **restoration** of **low-light** images inspired by the retina model, which mimics the neurophysiological principles and dynamics of various types of optical neurons. The proposed neural network model saves the cost of computational overhead in contrast with traditional signal-processing models, and generates results comparable with complicated deep learning models from the subjective perceptual perspective. This work shows that to directly simulate the functionalities of retinal neurons using neural networks not only avoids the manually seeking for the optimal parameters, but also paves the way to build corresponding artificial versions for certain neurobiological organizations.  
### A Perceptual Quality Metric for Video Frame Interpolation. (arXiv:2210.01879v1 [cs.CV])
- Authors : Qiqi Hou, Abhijay Ghildyal, Feng Liu
- Link : [http://arxiv.org/abs/2210.01879](http://arxiv.org/abs/2210.01879)
> ABSTRACT  :  Research on video frame interpolation has made significant progress in recent years. However, existing methods mostly use off-the-shelf metrics to measure the quality of interpolation results with the exception of a few methods that employ user studies, which is time-consuming. As video frame interpolation results often exhibit unique artifacts, existing quality metrics sometimes are not consistent with human perception when measuring the interpolation results. Some recent deep learning-based perceptual quality metrics are shown more consistent with human judgments, but their performance on videos is compromised since they do not consider temporal information. In this paper, we present a dedicated perceptual quality metric for measuring video frame interpolation results. Our method learns perceptual features directly from videos instead of individual frames. It compares pyramid features extracted from video frames and employs **Swin** Transformer blocks-based spatio-temporal modules to extract spatio-temporal information. To train our metric, we collected a new video frame interpolation quality assessment dataset. Our experiments show that our dedicated quality metric outperforms state-of-the-art methods when measuring video frame interpolation results. Our code and model are made publicly available at \url{https://github.com/hqqxyy/VFIPS}.  
### A Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v2 [eess.IV] UPDATED)
- Authors : Qiaoqiao Ding, Hui Ji, Yuhui Quan, Xiaoqun Zhang
- Link : [http://arxiv.org/abs/2205.00463](http://arxiv.org/abs/2205.00463)
> ABSTRACT  :  Low-dose CT (LDCT) imaging attracted a considerable interest for the reduction of the object's **exposure** to X-ray radiation. In recent years, supervised deep learning (DL) has been extensively studied for LDCT image reconstruction, which trains a network over a dataset containing many pairs of normal-dose and low-dose images. However, the challenge on collecting many such pairs in the clinical setup limits the application of such supervised-learning-based methods for LDCT image reconstruction in practice. Aiming at addressing the challenges raised by the collection of training dataset, this paper proposed a unsupervised deep learning method for LDCT image reconstruction, which does not require any external training data. The proposed method is built on a re-parametrization technique for Bayesian inference via deep network with random weights, combined with additional total variational~(TV) regularization. The experiments show that the proposed method noticeably outperforms existing dataset-free image reconstruction methods on the test data.  
### Retinal Image **Restoration** and Vessel Segmentation using Modified Cycle-CBAM and CBAM-UNet. (arXiv:2209.04234v2 [eess.IV] UPDATED)
- Authors : Alnur Alimanov, Md Baharul
- Link : [http://arxiv.org/abs/2209.04234](http://arxiv.org/abs/2209.04234)
> ABSTRACT  :  Clinical screening with low-quality fundus images is challenging and significantly leads to misdiagnosis. This paper addresses the issue of improving the retinal image quality and vessel segmentation through retinal image **restoration**. More specifically, a cycle-consistent generative adversarial network (CycleGAN) with a convolution block attention module (CBAM) is used for retinal image **restoration**. A modified UNet is used for retinal vessel segmentation for the restored retinal images (CBAM-UNet). The proposed model consists of two generators and two discriminators. Generators translate images from one domain to another, i.e., from low to high quality and vice versa. Discriminators classify generated and original images. The retinal vessel segmentation model uses downsampling, bottlenecking, and upsampling layers to generate segmented images. The CBAM has been used to enhance the feature extraction of these models. The proposed method does not require paired image datasets, which are challenging to produce. Instead, it uses unpaired data that consists of low- and high-quality fundus images retrieved from publicly available datasets. The **restoration** performance of the proposed method was evaluated using full-reference evaluation metrics, e.g., peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). The retinal vessel segmentation performance was compared with the ground-truth fundus images. The proposed method can significantly reduce the degradation effects caused by out-of-focus blurring, color distortion, low, high, and uneven illumination. Experimental results show the effectiveness of the proposed method for retinal image **restoration** and vessel segmentation.  
## cs.LG
---
### **Low-Light** Image **Restoration** Based on Retina Model using Neural Networks. (arXiv:2210.01806v1 [eess.IV])
- Authors : Yurui Ming, Yuanyuan Liang
- Link : [http://arxiv.org/abs/2210.01806](http://arxiv.org/abs/2210.01806)
> ABSTRACT  :  We report the possibility of using a simple neural network for effortless **restoration** of **low-light** images inspired by the retina model, which mimics the neurophysiological principles and dynamics of various types of optical neurons. The proposed neural network model saves the cost of computational overhead in contrast with traditional signal-processing models, and generates results comparable with complicated deep learning models from the subjective perceptual perspective. This work shows that to directly simulate the functionalities of retinal neurons using neural networks not only avoids the manually seeking for the optimal parameters, but also paves the way to build corresponding artificial versions for certain neurobiological organizations.  
### Graph Classification via Discriminative Edge Feature Learning. (arXiv:2210.02060v1 [cs.CV])
- Authors : Yang Yi, Xuequan Lu, Shang Gao, Antonio Robles, Yuejie Zhang
- Link : [http://arxiv.org/abs/2210.02060](http://arxiv.org/abs/2210.02060)
> ABSTRACT  :  Spectral graph convolutional neural networks (GCNNs) have been producing encouraging results in graph classification tasks. However, most spectral GCNNs utilize fixed graphs when aggregating node features, while omitting edge feature learning and failing to get an optimal graph structure. Moreover, many existing graph datasets do not provide initialized edge features, further restraining the ability of learning edge features via spectral GCNNs. In this paper, we try to address this issue by designing an edge feature scheme and an add-on layer between every two stacked graph convolution layers in GCNN. Both are lightweight while effective in filling the gap between edge feature learning and performance **enhancement** of graph classification. The edge feature scheme makes edge features adapt to node representations at different graph convolution layers. The add-on layers help adjust the edge features to an optimal graph structure. To test the effectiveness of our method, we take Euclidean positions as initial node features and extract graphs with semantic information from point cloud objects. The node features of our extracted graphs are more scalable for edge feature learning than most existing graph datasets (in one-hot encoded label format). Three new graph datasets are constructed based on ModelNet40, ModelNet10 and ShapeNet Part datasets. Experimental results show that our method outperforms state-of-the-art graph classification methods on the new datasets by reaching 96.56% overall accuracy on Graph-ModelNet40, 98.79% on Graph-ModelNet10 and 97.91% on Graph-ShapeNet Part. The constructed graph datasets will be released to the community.  
### Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers. (arXiv:2210.02317v1 [cs.RO])
- Authors : Yan Wang, Gautham Vasan, Rupam Mahmood
- Link : [http://arxiv.org/abs/2210.02317](http://arxiv.org/abs/2210.02317)
> ABSTRACT  :  **Real-time** learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC's performance degrades heavily on a resource-limited local computer. Strikingly, when all computations of the learning system are deployed on a remote workstation, SAC fails to compensate for the performance loss, indicating that, without careful consideration, using a powerful remote computer may not result in performance improvement. However, a carefully chosen distribution of computations of SAC consistently and substantially improves its performance on both tasks. On the other hand, the performance of PPO remains largely unaffected by the distribution of computations. In addition, when all computations happen solely on a powerful tethered computer, the performance of our system remains on par with an existing system that is well-tuned for using a single machine. ReLoD is the only publicly available system for real-time RL that applies to multiple robots for vision-based tasks.  
### Soft Diffusion: Score Matching for General Corruptions. (arXiv:2209.05442v2 [cs.CV] UPDATED)
- Authors : Giannis Daras, Mauricio Delbracio, Hossein Talebi, **Peyman Milanfar**
- Link : [http://arxiv.org/abs/2209.05442](http://arxiv.org/abs/2209.05442)
> ABSTRACT  :  We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, \textit{that after corruption}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art FID score $1.85$ on CelebA-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion.  
## cs.AI
---
### **Low-Light** Image **Restoration** Based on Retina Model using Neural Networks. (arXiv:2210.01806v1 [eess.IV])
- Authors : Yurui Ming, Yuanyuan Liang
- Link : [http://arxiv.org/abs/2210.01806](http://arxiv.org/abs/2210.01806)
> ABSTRACT  :  We report the possibility of using a simple neural network for effortless **restoration** of **low-light** images inspired by the retina model, which mimics the neurophysiological principles and dynamics of various types of optical neurons. The proposed neural network model saves the cost of computational overhead in contrast with traditional signal-processing models, and generates results comparable with complicated deep learning models from the subjective perceptual perspective. This work shows that to directly simulate the functionalities of retinal neurons using neural networks not only avoids the manually seeking for the optimal parameters, but also paves the way to build corresponding artificial versions for certain neurobiological organizations.  
### Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers. (arXiv:2210.02317v1 [cs.RO])
- Authors : Yan Wang, Gautham Vasan, Rupam Mahmood
- Link : [http://arxiv.org/abs/2210.02317](http://arxiv.org/abs/2210.02317)
> ABSTRACT  :  **Real-time** learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC's performance degrades heavily on a resource-limited local computer. Strikingly, when all computations of the learning system are deployed on a remote workstation, SAC fails to compensate for the performance loss, indicating that, without careful consideration, using a powerful remote computer may not result in performance improvement. However, a carefully chosen distribution of computations of SAC consistently and substantially improves its performance on both tasks. On the other hand, the performance of PPO remains largely unaffected by the distribution of computations. In addition, when all computations happen solely on a powerful tethered computer, the performance of our system remains on par with an existing system that is well-tuned for using a single machine. ReLoD is the only publicly available system for real-time RL that applies to multiple robots for vision-based tasks.  
### Soft Diffusion: Score Matching for General Corruptions. (arXiv:2209.05442v2 [cs.CV] UPDATED)
- Authors : Giannis Daras, Mauricio Delbracio, Hossein Talebi, **Peyman Milanfar**
- Link : [http://arxiv.org/abs/2209.05442](http://arxiv.org/abs/2209.05442)
> ABSTRACT  :  We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, \textit{that after corruption}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art FID score $1.85$ on CelebA-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion.  
# Paper List
---
## cs.CV
---
**112** new papers in cs.CV:-) 
1. **Low-Light** Image **Restoration** Based on Retina Model using Neural Networks. (arXiv:2210.01806v1 [eess.IV])
2. MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models. (arXiv:2210.01820v1 [cs.CV])
3. Centerpoints Are All You Need in Overhead Imagery. (arXiv:2210.01857v1 [cs.CV])
4. Capturing and Animation of Body and Clothing from Monocular Video. (arXiv:2210.01868v1 [cs.CV])
5. A Perceptual Quality Metric for Video Frame Interpolation. (arXiv:2210.01879v1 [cs.CV])
6. Self-supervised Pre-training for Semantic Segmentation in an Indoor Scene. (arXiv:2210.01884v1 [cs.CV])
7. Multi-view Human Body Mesh Translator. (arXiv:2210.01886v1 [cs.CV])
8. Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures. (arXiv:2210.01887v1 [cs.CV])
9. AdaWAC: Adaptively Weighted Augmentation Consistency Regularization for Volumetric Medical Image Segmentation. (arXiv:2210.01891v1 [cs.CV])
10. Supervised Metric Learning for Retrieval via Contextual Similarity Optimization. (arXiv:2210.01908v1 [cs.LG])
11. Grounding Language with Visual Affordances over Unstructured Data. (arXiv:2210.01911v1 [cs.RO])
12. Dfferentiable Raycasting for Self-supervised Occupancy Forecasting. (arXiv:2210.01917v1 [cs.CV])
13. When and why vision-language models behave like bag-of-words models, and what to do about it?. (arXiv:2210.01936v1 [cs.CV])
14. Affection: Learning Affective Explanations for Real-World Visual Data. (arXiv:2210.01946v1 [cs.CV])
15. The Calibration Generalization Gap. (arXiv:2210.01964v1 [cs.LG])
16. Meta-Ensemble Parameter Learning. (arXiv:2210.01973v1 [cs.CV])
17. Cloud removal Using Atmosphere Model. (arXiv:2210.01981v1 [cs.CV])
18. ImpressLearn: Continual Learning via Combined Task Impressions. (arXiv:2210.01987v1 [cs.CV])
19. Multi-Camera Collaborative Depth Prediction via Consistent Structure Estimation. (arXiv:2210.02009v1 [cs.CV])
20. InterFace:Adjustable Angular Margin Inter-class Loss for Deep Face Recognition. (arXiv:2210.02018v1 [cs.CV])
21. Exploring Effective Knowledge Transfer for Few-shot Object Detection. (arXiv:2210.02021v1 [cs.CV])
22. GMMSeg: Gaussian Mixture based Generative Semantic Segmentation Models. (arXiv:2210.02025v1 [cs.CV])
23. Inharmonious Region Localization with Auxiliary Style Feature. (arXiv:2210.02029v1 [cs.CV])
24. Point Cloud Recognition with Position-to-Structure Attention Transformers. (arXiv:2210.02030v1 [cs.CV])
25. Learning Video-independent Eye Contact Segmentation from In-the-Wild Videos. (arXiv:2210.02033v1 [cs.CV])
26. Inharmonious Region Localization via Recurrent Self-Reasoning. (arXiv:2210.02036v1 [cs.CV])
27. MOTSLAM: MOT-assisted monocular dynamic SLAM using single-view depth estimation. (arXiv:2210.02038v1 [cs.CV])
28. Natural Color Fool: Towards Boosting Black-box Unrestricted Attacks. (arXiv:2210.02041v1 [cs.CV])
29. Coarse-to-Fine Point Cloud Registration with SE(3)-Equivariant Representations. (arXiv:2210.02045v1 [cs.CV])
30. Graph Classification via Discriminative Edge Feature Learning. (arXiv:2210.02060v1 [cs.CV])
31. Advanced Deep Learning Architectures for Accurate Detection of Subsurface Tile Drainage Pipes from Remote Sensing Images. (arXiv:2210.02071v1 [cs.CV])
32. Two Video Data Sets for Tracking and Retrieval of Out of Distribution Objects. (arXiv:2210.02074v1 [cs.CV])
33. On the Learning Mechanisms in Physical Reasoning. (arXiv:2210.02075v1 [cs.LG])
34. Exploring The Role of Mean Teachers in Self-supervised Masked Auto-Encoders. (arXiv:2210.02077v1 [cs.CV])
35. Locate before Answering: Answer Guided Question Localization for Video Question Answering. (arXiv:2210.02081v1 [cs.CV])
36. Jitter Does Matter: Adapting Gaze Estimation to New Domains. (arXiv:2210.02082v1 [cs.CV])
37. WUDA: Unsupervised Domain Adaptation Based on Weak Source Domain Labels. (arXiv:2210.02088v1 [cs.CV])
38. Centralized Feature Pyramid for Object Detection. (arXiv:2210.02093v1 [cs.CV])
39. Relational Proxies: Emergent Relationships as Fine-Grained Discriminators. (arXiv:2210.02149v1 [cs.CV])
40. Differentiable Mathematical Programming for Object-Centric Representation Learning. (arXiv:2210.02159v1 [cs.LG])
41. CFL-Net: Image Forgery Localization Using Contrastive Learning. (arXiv:2210.02182v1 [cs.CV])
42. A Generalizable Artificial Intelligence Model for COVID-19 Classification Task Using Chest X-ray Radiographs: Evaluated Over Four Clinical Datasets with 15,097 Patients. (arXiv:2210.02189v1 [eess.IV])
43. On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks. (arXiv:2210.02191v1 [cs.LG])
44. Comprint: Image Forgery Detection and Localization using Compression Fingerprints. (arXiv:2210.02227v1 [cs.CV])
45. Decanus to Legatus: Synthetic training for 2D-3D human pose lifting. (arXiv:2210.02231v1 [cs.CV])
46. HeartSpot: Privatized and Explainable Data Compression for Cardiomegaly Detection. (arXiv:2210.02241v1 [eess.IV])
47. LDEdit: Towards Generalized Text Guided Image Manipulation via Latent Diffusion Models. (arXiv:2210.02249v1 [cs.CV])
48. Granularity-aware Adaptation for Image Retrieval over Multiple Tasks. (arXiv:2210.02254v1 [cs.CV])
49. Hiding Images in Deep Probabilistic Models. (arXiv:2210.02257v1 [cs.CR])
50. Weak-shot Semantic Segmentation via Dual Similarity Transfer. (arXiv:2210.02270v1 [cs.CV])
51. Novel Radiomic Measurements of Tumor- Associated Vasculature Morphology on Clinical Imaging as a Biomarker of Treatment Response in Multiple Cancers. (arXiv:2210.02273v1 [q-bio.QM])
52. Progressive Denoising Model for Fine-Grained Text-to-Image Generation. (arXiv:2210.02291v1 [cs.CV])
53. SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations. (arXiv:2210.02299v1 [cs.CV])
54. Imagen Video: High Definition Video Generation with Diffusion Models. (arXiv:2210.02303v1 [cs.CV])
55. Multi-stream Fusion for Class Incremental Learning in Pill Image Classification. (arXiv:2210.02313v1 [cs.CV])
56. FQDet: Fast-converging Query-based Detector. (arXiv:2210.02318v1 [cs.CV])
57. Promising or Elusive? Unsupervised Object Segmentation from Real-world Single Images. (arXiv:2210.02324v1 [cs.CV])
58. Learning Across Domains and Devices: Style-Driven Source-Free Domain Adaptation in Clustered Federated Learning. (arXiv:2210.02326v1 [cs.CV])
59. clip2latent: Text driven sampling of a pre-trained StyleGAN using denoising diffusion and CLIP. (arXiv:2210.02347v1 [cs.CV])
60. Fitting a Directional Microstructure Model to Diffusion-Relaxation MRI Data with Self-Supervised Machine Learning. (arXiv:2210.02349v1 [eess.IV])
61. Image Masking for Robust Self-Supervised Monocular Depth Estimation. (arXiv:2210.02357v1 [cs.CV])
62. SoccerNet 2022 Challenges Results. (arXiv:2210.02365v1 [cs.CV])
63. Spatio-Temporal Learnable Proposals for End-to-End Video Object Detection. (arXiv:2210.02368v1 [cs.CV])
64. NeuralMeshing: Differentiable Meshing of Implicit Neural Representations. (arXiv:2210.02382v1 [cs.CV])
65. Variational prompt tuning improves generalization of vision-language models. (arXiv:2210.02390v1 [cs.CV])
66. Geometry Driven Progressive Warping for One-Shot Face Animation. (arXiv:2210.02391v1 [cs.CV])
67. Temporally Consistent Video Transformer for Long-Term Video Prediction. (arXiv:2210.02396v1 [cs.CV])
68. Phenaki: Variable Length Video Generation From Open Domain Textual Description. (arXiv:2210.02399v1 [cs.CV])
69. Medical Image Retrieval via Nearest Neighbor Search on Pre-trained Image Features. (arXiv:2210.02401v1 [cs.CV])
70. A deep learning model for brain vessel segmentation in 3DRA with arteriovenous malformations. (arXiv:2210.02416v1 [eess.IV])
71. DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics. (arXiv:2210.02438v1 [cs.RO])
72. Making Your First Choice: To Address Cold Start Problem in Vision Active Learning. (arXiv:2210.02442v1 [cs.CV])
73. Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection. (arXiv:2210.02443v1 [cs.CV])
74. Universal Medical Image Segmentation using 3D Fabric Image Representation Encoding Networks. (arXiv:2006.15578v3 [eess.IV] UPDATED)
75. Improved Real-Time Monocular SLAM Using Semantic Segmentation on Selective Frames. (arXiv:2105.00114v4 [cs.CV] UPDATED)
76. What and When to Look?: Temporal Span Proposal Network for Video Relation Detection. (arXiv:2107.07154v2 [cs.CV] UPDATED)
77. FocusNet: Classifying Better by Focusing on Confusing Classes. (arXiv:2110.07307v3 [cs.CV] UPDATED)
78. Intensity Mapping Functions For **HDR** Panorama Imaging: Weighted Histogram Averaging. (arXiv:2111.07283v4 [cs.CV] UPDATED)
79. Neural Residual Flow Fields for Efficient Video Representations. (arXiv:2201.04329v2 [cs.CV] UPDATED)
80. LTC-SUM: Lightweight Client-driven Personalized Video Summarization Framework Using 2D CNN. (arXiv:2201.09049v2 [cs.CV] UPDATED)
81. Towards Adversarially Robust Deepfake Detection: An Ensemble Approach. (arXiv:2202.05687v2 [cs.LG] UPDATED)
82. Learning with Free Object Segments for Long-Tailed Instance Segmentation. (arXiv:2202.11124v3 [cs.CV] UPDATED)
83. Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection. (arXiv:2203.00307v2 [cs.CV] UPDATED)
84. Diffusion Models for Medical Anomaly Detection. (arXiv:2203.04306v2 [eess.IV] UPDATED)
85. Training from a Better Start Point: Active Self-Semi-Supervised Learning for Few Labeled Samples. (arXiv:2203.04560v2 [cs.CV] UPDATED)
86. Decoupled Mixup for Data-efficient Learning. (arXiv:2203.10761v2 [cs.LG] UPDATED)
87. Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework. (arXiv:2203.11991v3 [cs.CV] UPDATED)
88. A Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v2 [eess.IV] UPDATED)
89. Non-rigid Point Cloud Registration with Neural Deformation Pyramid. (arXiv:2205.12796v3 [cs.CV] UPDATED)
90. On the duality between contrastive and non-contrastive self-supervised learning. (arXiv:2206.02574v2 [cs.LG] UPDATED)
91. GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions. (arXiv:2206.02780v2 [cs.CV] UPDATED)
92. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v3 [cs.AI] UPDATED)
93. Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks. (arXiv:2206.08916v2 [cs.CV] UPDATED)
94. SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders. (arXiv:2206.10207v3 [cs.CV] UPDATED)
95. Boosting R-CNN: Reweighting R-CNN Samples by RPN's Error for Underwater Object Detection. (arXiv:2206.13728v3 [cs.CV] UPDATED)
96. A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images. (arXiv:2207.06489v4 [eess.IV] UPDATED)
97. Fine-grained Few-shot Recognition by Deep Object Parsing. (arXiv:2207.07110v3 [cs.CV] UPDATED)
98. PlaneSDF-based Change Detection for Long-term Dense Mapping. (arXiv:2207.08323v2 [cs.RO] UPDATED)
99. Self-Distilled Vision Transformer for Domain Generalization. (arXiv:2207.12392v3 [cs.CV] UPDATED)
100. Robotic Dough Shaping. (arXiv:2208.00386v2 [cs.RO] UPDATED)
101. Self-supervised learning with rotation-invariant kernels. (arXiv:2208.00789v2 [cs.CV] UPDATED)
102. T4DT: Tensorizing Time for Learning Temporal 3D Visual Data. (arXiv:2208.01421v2 [cs.CV] UPDATED)
103. Learning to Incorporate Texture Saliency Adaptive Attention to Image Cartoonization. (arXiv:2208.01587v2 [cs.CV] UPDATED)
104. Detecting COVID-19 from digitized ECG printouts using 1D convolutional neural networks. (arXiv:2208.05433v2 [eess.IV] UPDATED)
105. PatchDropout: Economizing Vision Transformers Using Patch Dropout. (arXiv:2208.07220v2 [cs.CV] UPDATED)
106. Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment. (arXiv:2208.13628v2 [cs.CV] UPDATED)
107. Retinal Image **Restoration** and Vessel Segmentation using Modified Cycle-CBAM and CBAM-UNet. (arXiv:2209.04234v2 [eess.IV] UPDATED)
108. Soft Diffusion: Score Matching for General Corruptions. (arXiv:2209.05442v2 [cs.CV] UPDATED)
109. Differentiable Frequency-based Disentanglement for Aerial Video Action Recognition. (arXiv:2209.09194v2 [cs.CV] UPDATED)
110. IoU-Enhanced Attention for End-to-End Task Specific Object Detection. (arXiv:2209.10391v2 [cs.CV] UPDATED)
111. Dataset Distillation using Parameter Pruning. (arXiv:2209.14609v2 [cs.CV] UPDATED)
112. ROAD-R: The Autonomous Driving Dataset with Logical Requirements. (arXiv:2210.01597v2 [cs.LG] UPDATED)
## eess.IV
---
**21** new papers in eess.IV:-) 
1. **Low-Light** Image **Restoration** Based on Retina Model using Neural Networks. (arXiv:2210.01806v1 [eess.IV])
2. A Perceptual Quality Metric for Video Frame Interpolation. (arXiv:2210.01879v1 [cs.CV])
3. Calibrating Data Mismatches in Deep Learning-Based Quantitative Ultrasound Using Setting Transfer Functions. (arXiv:2210.01958v1 [eess.IV])
4. Advanced Deep Learning Architectures for Accurate Detection of Subsurface Tile Drainage Pipes from Remote Sensing Images. (arXiv:2210.02071v1 [cs.CV])
5. Development and validation of deep learning based embryo selection across multiple days of transfer. (arXiv:2210.02120v1 [q-bio.QM])
6. A Generalizable Artificial Intelligence Model for COVID-19 Classification Task Using Chest X-ray Radiographs: Evaluated Over Four Clinical Datasets with 15,097 Patients. (arXiv:2210.02189v1 [eess.IV])
7. PriorNet: lesion segmentation in PET-CT including prior tumor appearance information. (arXiv:2210.02203v1 [eess.IV])
8. Join, select, and insert: efficient out-of-core algorithms for hierarchical segmentation trees. (arXiv:2210.02218v1 [eess.IV])
9. HeartSpot: Privatized and Explainable Data Compression for Cardiomegaly Detection. (arXiv:2210.02241v1 [eess.IV])
10. Channel Modeling for UAV-to-Ground Communications with Posture Variation and Fuselage Scattering Effect. (arXiv:2210.02245v1 [eess.SP])
11. Fitting a Directional Microstructure Model to Diffusion-Relaxation MRI Data with Self-Supervised Machine Learning. (arXiv:2210.02349v1 [eess.IV])
12. Domain Adaptation for Unknown Image Distortions in Instance Segmentation. (arXiv:2210.02386v1 [eess.IV])
13. A deep learning model for brain vessel segmentation in 3DRA with arteriovenous malformations. (arXiv:2210.02416v1 [eess.IV])
14. Universal Medical Image Segmentation using 3D Fabric Image Representation Encoding Networks. (arXiv:2006.15578v3 [eess.IV] UPDATED)
15. Diffusion Models for Medical Anomaly Detection. (arXiv:2203.04306v2 [eess.IV] UPDATED)
16. A Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v2 [eess.IV] UPDATED)
17. An untrained deep learning method for reconstructing dynamic magnetic resonance images from accelerated model-based data. (arXiv:2205.01604v3 [eess.IV] UPDATED)
18. A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images. (arXiv:2207.06489v4 [eess.IV] UPDATED)
19. Detecting COVID-19 from digitized ECG printouts using 1D convolutional neural networks. (arXiv:2208.05433v2 [eess.IV] UPDATED)
20. Retinal Image **Restoration** and Vessel Segmentation using Modified Cycle-CBAM and CBAM-UNet. (arXiv:2209.04234v2 [eess.IV] UPDATED)
21. STEMPO -- dynamic X-ray tomography phantom. (arXiv:2209.12471v2 [eess.IV] UPDATED)
## cs.LG
---
**213** new papers in cs.LG:-) 
1. BayesFT: Bayesian Optimization for Fault Tolerant Neural Network Architecture. (arXiv:2210.01795v1 [cs.LG])
2. Multi-objective Deep Data Generation with Correlated Property Control. (arXiv:2210.01796v1 [cs.LG])
3. Ten Years after ImageNet: A 360{\deg} Perspective on AI. (arXiv:2210.01797v1 [cs.LG])
4. Latent Hierarchical Causal Structure Discovery with Rank Constraints. (arXiv:2210.01798v1 [cs.LG])
5. STGIN: A Spatial Temporal Graph-Informer Network for Long Sequence Traffic Speed Forecasting. (arXiv:2210.01799v1 [cs.LG])
6. Bayesian Q-learning With Imperfect Expert Demonstrations. (arXiv:2210.01800v1 [cs.LG])
7. Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation. (arXiv:2210.01801v1 [cs.LG])
8. Alternating Differentiation for Optimization Layers. (arXiv:2210.01802v1 [cs.LG])
9. Federated Graph-based Networks with Shared Embedding. (arXiv:2210.01803v1 [cs.LG])
10. CostNet: An End-to-End Framework for Goal-Directed Reinforcement Learning. (arXiv:2210.01805v1 [cs.LG])
11. **Low-Light** Image **Restoration** Based on Retina Model using Neural Networks. (arXiv:2210.01806v1 [eess.IV])
12. TripleE: Easy Domain Generalization via Episodic Replay. (arXiv:2210.01807v1 [cs.LG])
13. Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees. (arXiv:2210.01808v1 [cs.LG])
14. Invariant Aggregator for Defending Federated Backdoor Attacks. (arXiv:2210.01834v1 [cs.LG])
15. Detecting Anomalies within Smart Buildings using Do-It-Yourself Internet of Things. (arXiv:2210.01840v1 [cs.LG])
16. Explaining Patterns in Data with Language Models via Interpretable Autoprompting. (arXiv:2210.01848v1 [cs.LG])
17. Multifaceted Hierarchical Report Identification for Non-Functional Bugs in Deep Learning Frameworks. (arXiv:2210.01855v1 [cs.SE])
18. Efficient Prototype Selection via Multi-Armed Bandits. (arXiv:2210.01860v1 [cs.LG])
19. Group Personalized Federated Learning. (arXiv:2210.01863v1 [stat.ML])
20. Recycling Scraps: Improving Private Learning by Leveraging Intermediate Checkpoints. (arXiv:2210.01864v1 [cs.LG])
21. Uncertainty-Aware Meta-Learning for Multimodal Task Distributions. (arXiv:2210.01881v1 [cs.LG])
22. A Collaborative Approach to the Analysis of the COVID-19 Response in Africa. (arXiv:2210.01882v1 [cs.LG])
23. Contrastive Learning Can Find An Optimal Basis For Approximately View-Invariant Functions. (arXiv:2210.01883v1 [cs.LG])
24. Bicriteria Approximation Algorithms for Priority Matroid Median. (arXiv:2210.01888v1 [cs.DS])
25. AdaWAC: Adaptively Weighted Augmentation Consistency Regularization for Volumetric Medical Image Segmentation. (arXiv:2210.01891v1 [cs.CV])
26. Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v1 [cs.NE])
27. Reproducible Bandits. (arXiv:2210.01898v1 [cs.LG])
28. Representing missing values through polar encoding. (arXiv:2210.01905v1 [cs.LG])
29. Tree Mover's Distance: Bridging Graph Metrics and Stability of Graph Neural Networks. (arXiv:2210.01906v1 [cs.LG])
30. A Self-Play Posterior Sampling Algorithm for Zero-Sum Markov Games. (arXiv:2210.01907v1 [cs.LG])
31. Supervised Metric Learning for Retrieval via Contextual Similarity Optimization. (arXiv:2210.01908v1 [cs.LG])
32. Learning Signal Temporal Logic through Neural Network for Interpretable Classification. (arXiv:2210.01910v1 [cs.FL])
33. Grounding Language with Visual Affordances over Unstructured Data. (arXiv:2210.01911v1 [cs.RO])
34. Non-Parametric and Regularized Dynamical Wasserstein Barycenters for Time-Series Analysis. (arXiv:2210.01918v1 [cs.LG])
35. Benchmarking Learnt Radio Localisation under Distribution Shift. (arXiv:2210.01930v1 [cs.LG])
36. Regression-Based Elastic Metric Learning on Shape Spaces of Elastic Curves. (arXiv:2210.01932v1 [cs.LG])
37. When and why vision-language models behave like bag-of-words models, and what to do about it?. (arXiv:2210.01936v1 [cs.CV])
38. On the Robustness of Deep Clustering Models: Adversarial Attacks and Defenses. (arXiv:2210.01940v1 [cs.LG])
39. SIMPLE: A Gradient Estimator for $k$-Subset Sampling. (arXiv:2210.01941v1 [cs.LG])
40. IGNiteR: News Recommendation in Microblogging Applications (Extended Version). (arXiv:2210.01942v1 [cs.IR])
41. A Framework for Large Scale Synthetic Graph Dataset Generation. (arXiv:2210.01944v1 [cs.LG])
42. Robust Fair Clustering: A Novel Fairness Attack and Defense Framework. (arXiv:2210.01953v1 [cs.LG])
43. Learning Dynamic Abstract Representations for Sample-Efficient Reinforcement Learning. (arXiv:2210.01955v1 [cs.LG])
44. Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering. (arXiv:2210.01959v1 [cs.CL])
45. Split Federated Learning on Micro-controllers: A Keyword Spotting Showcase. (arXiv:2210.01961v1 [cs.LG])
46. The Calibration Generalization Gap. (arXiv:2210.01964v1 [cs.LG])
47. Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v1 [cs.LG])
48. Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurement. (arXiv:2210.01970v1 [cs.LG])
49. Meta-Ensemble Parameter Learning. (arXiv:2210.01973v1 [cs.CV])
50. Towards Prototype-Based Self-Explainable Graph Neural Network. (arXiv:2210.01974v1 [cs.LG])
51. Cloud removal Using Atmosphere Model. (arXiv:2210.01981v1 [cs.CV])
52. A Multi-Stage Automated Online Network Data Stream Analytics Framework for IIoT Systems. (arXiv:2210.01985v1 [cs.LG])
53. MAtt: A Manifold Attention Network for EEG Decoding. (arXiv:2210.01986v1 [cs.LG])
54. ImpressLearn: Continual Learning via Combined Task Impressions. (arXiv:2210.01987v1 [cs.CV])
55. Waveformer: Linear-Time Attention with Forward and Backward Wavelet Transform. (arXiv:2210.01989v1 [cs.CL])
56. Conformalized Fairness via Quantile Regression. (arXiv:2210.02015v1 [stat.ML])
57. Multi-task Self-supervised Graph Neural Networks Enable Stronger Task Generalization. (arXiv:2210.02016v1 [cs.LG])
58. Atari-5: Distilling the Arcade Learning Environment down to Five Games. (arXiv:2210.02019v1 [cs.AI])
59. DreamShard: Generalizable Embedding Table Placement for Recommender Systems. (arXiv:2210.02023v1 [cs.LG])
60. GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks. (arXiv:2210.02040v1 [cs.LG])
61. FedMT: Federated Learning with Mixed-type Labels. (arXiv:2210.02042v1 [cs.LG])
62. Learning to Act: Novel Integration of Algorithms and Models for Epidemic Preparedness. (arXiv:2210.02055v1 [cs.LG])
63. Graph Classification via Discriminative Edge Feature Learning. (arXiv:2210.02060v1 [cs.CV])
64. Non-Convergence and Limit Cycles in the Adam optimizer. (arXiv:2210.02070v1 [cs.LG])
65. Advanced Deep Learning Architectures for Accurate Detection of Subsurface Tile Drainage Pipes from Remote Sensing Images. (arXiv:2210.02071v1 [cs.CV])
66. On the Learning Mechanisms in Physical Reasoning. (arXiv:2210.02075v1 [cs.LG])
67. Multi-View Independent Component Analysis with Shared and Individual Sources. (arXiv:2210.02083v1 [cs.LG])
68. Bilinear Exponential Family of MDPs: Frequentist Regret Bound with Tractable Exploration and Planning. (arXiv:2210.02087v1 [cs.LG])
69. Transformer-based conditional generative adversarial network for multivariate time series generation. (arXiv:2210.02089v1 [cs.LG])
70. Tripletformer for Probabilistic Interpolation of Asynchronous Time Series. (arXiv:2210.02091v1 [cs.LG])
71. Functional Central Limit Theorem and Strong Law of Large Numbers for Stochastic Gradient Langevin Dynamics. (arXiv:2210.02092v1 [math.PR])
72. ChemAlgebra: Algebraic Reasoning on Chemical Reactions. (arXiv:2210.02095v1 [cs.LG])
73. Teaching Yourself:c Graph Self-Distillation on Neighborhood for Node Classification. (arXiv:2210.02097v1 [cs.LG])
74. Automated Graph Self-supervised Learning via Multi-teacher Knowledge Distillation. (arXiv:2210.02099v1 [cs.LG])
75. Optimization-Informed Neural Networks. (arXiv:2210.02113v1 [math.OC])
76. ISFL: Trustworthy Federated Learning for Non-i.i.d. Data with Local Importance Sampling. (arXiv:2210.02119v1 [cs.LG])
77. Development and validation of deep learning based embryo selection across multiple days of transfer. (arXiv:2210.02120v1 [q-bio.QM])
78. Stock Volatility Prediction using Time Series and Deep Learning Approach. (arXiv:2210.02126v1 [q-fin.CP])
79. Personalized Decentralized Bilevel Optimization over Stochastic and Directed Networks. (arXiv:2210.02129v1 [stat.ML])
80. Common Vulnerability Scoring System Prediction based on Open Source Intelligence Information Sources. (arXiv:2210.02143v1 [cs.CR])
81. SECOE: Alleviating Sensors Failure in Machine Learning-Coupled IoT Systems. (arXiv:2210.02144v1 [cs.LG])
82. Relational Proxies: Emergent Relationships as Fine-Grained Discriminators. (arXiv:2210.02149v1 [cs.CV])
83. Fine-Tuning with Differential Privacy Necessitates an Additional Hyperparameter Search. (arXiv:2210.02156v1 [cs.LG])
84. The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks. (arXiv:2210.02157v1 [stat.ML])
85. Differentiable Mathematical Programming for Object-Centric Representation Learning. (arXiv:2210.02159v1 [cs.LG])
86. Bayesian Quadrature for Probability Threshold Robustness of Partially Undefined Functions. (arXiv:2210.02168v1 [cs.LG])
87. CW-ERM: Improving Autonomous Driving Planning with Closed-loop Weighted Empirical Risk Minimization. (arXiv:2210.02174v1 [cs.LG])
88. Feature Importance for Time Series Data: Improving KernelSHAP. (arXiv:2210.02176v1 [cs.LG])
89. Multi-objective optimization via equivariant deep hypervolume approximation. (arXiv:2210.02177v1 [cs.LG])
90. DISCOVER: Deep identification of symbolic open-form PDEs via enhanced reinforcement-learning. (arXiv:2210.02181v1 [cs.LG])
91. Rediscovery of Numerical L\"uscher's Formula from the Neural Network. (arXiv:2210.02184v1 [hep-lat])
92. TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. (arXiv:2210.02186v1 [cs.LG])
93. A Generalizable Artificial Intelligence Model for COVID-19 Classification Task Using Chest X-ray Radiographs: Evaluated Over Four Clinical Datasets with 15,097 Patients. (arXiv:2210.02189v1 [eess.IV])
94. Domain Discrepancy Aware Distillation for Model Aggregation in Federated Learning. (arXiv:2210.02190v1 [cs.LG])
95. On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks. (arXiv:2210.02191v1 [cs.LG])
96. Are All Losses Created Equal: A Neural Collapse Perspective. (arXiv:2210.02192v1 [cs.LG])
97. A machine learning based algorithm selection method to solve the minimum cost flow problem. (arXiv:2210.02195v1 [cs.LG])
98. Hierarchical Neyman-Pearson Classification for Prioritizing Severe Disease Categories in COVID-19 Patient Data. (arXiv:2210.02197v1 [cs.LG])
99. MTSMAE: Masked Autoencoders for Multivariate Time-Series Forecasting. (arXiv:2210.02199v1 [cs.LG])
100. Machine learning in bioprocess development: From promise to practice. (arXiv:2210.02200v1 [cs.LG])
101. A new family of Constitutive Artificial Neural Networks towards automated model discovery. (arXiv:2210.02202v1 [cs.LG])
102. Game Theoretic Rating in N-player general-sum games with Equilibria. (arXiv:2210.02205v1 [cs.GT])
103. On the Statistical Complexity of Estimation and Testing under Privacy Constraints. (arXiv:2210.02215v1 [cs.LG])
104. Neural Distillation as a State Representation Bottleneck in Reinforcement Learning. (arXiv:2210.02224v1 [cs.LG])
105. Null Hypothesis Test for Anomaly Detection. (arXiv:2210.02226v1 [hep-ph])
106. Thermal (and Hybrid Thermal/Audio) Side-Channel Attacks on Keyboard Input. (arXiv:2210.02234v1 [cs.CR])
107. Over-the-Air Federated Learning with Privacy Protection via Correlated Additive Perturbations. (arXiv:2210.02235v1 [cs.LG])
108. On Neural Consolidation for Transfer in Reinforcement Learning. (arXiv:2210.02240v1 [cs.LG])
109. HeartSpot: Privatized and Explainable Data Compression for Cardiomegaly Detection. (arXiv:2210.02241v1 [eess.IV])
110. Cost Aware Asynchronous Multi-Agent Active Search. (arXiv:2210.02259v1 [cs.LG])
111. Extending Conformal Prediction to Hidden Markov Models with Exact Validity via de Finetti's Theorem for Markov Chains. (arXiv:2210.02271v1 [stat.ME])
112. Probabilistic reconciliation of forecasts via importance sampling. (arXiv:2210.02286v1 [stat.ML])
113. TC-SKNet with GridMask for Low-complexity Classification of Acoustic scene. (arXiv:2210.02287v1 [cs.SD])
114. Multiclass Learnability Beyond the PAC Framework: Universal Rates and Partial Concept Classes. (arXiv:2210.02297v1 [cs.LG])
115. SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations. (arXiv:2210.02299v1 [cs.CV])
116. Imagen Video: High Definition Video Generation with Diffusion Models. (arXiv:2210.02303v1 [cs.CV])
117. Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers. (arXiv:2210.02317v1 [cs.RO])
118. Promising or Elusive? Unsupervised Object Segmentation from Real-world Single Images. (arXiv:2210.02324v1 [cs.CV])
119. Learning Across Domains and Devices: Style-Driven Source-Free Domain Adaptation in Clustered Federated Learning. (arXiv:2210.02326v1 [cs.CV])
120. Revisiting Graph Contrastive Learning from the Perspective of Graph Spectrum. (arXiv:2210.02330v1 [cs.LG])
121. Using Full-Text Content to Characterize and Identify Best Seller Books. (arXiv:2210.02334v1 [cs.CL])
122. Particle clustering in turbulence: Prediction of spatial and statistical properties with deep learning. (arXiv:2210.02339v1 [astro-ph.EP])
123. Visual Backtracking Teleoperation: A Data Collection Protocol for Offline Image-Based Reinforcement Learning. (arXiv:2210.02343v1 [cs.RO])
124. Fitting a Directional Microstructure Model to Diffusion-Relaxation MRI Data with Self-Supervised Machine Learning. (arXiv:2210.02349v1 [eess.IV])
125. A kernel-based quantum random forest for improved classification. (arXiv:2210.02355v1 [quant-ph])
126. Dynamical systems' based neural networks. (arXiv:2210.02373v1 [cs.LG])
127. Goal Recognition as a Deep Learning Task: the GRNet Approach. (arXiv:2210.02377v1 [cs.AI])
128. Variational prompt tuning improves generalization of vision-language models. (arXiv:2210.02390v1 [cs.CV])
129. Geometry Driven Progressive Warping for One-Shot Face Animation. (arXiv:2210.02391v1 [cs.CV])
130. Temporally Consistent Video Transformer for Long-Term Video Prediction. (arXiv:2210.02396v1 [cs.CV])
131. ciDATGAN: Conditional Inputs for Tabular GANs. (arXiv:2210.02404v1 [cs.LG])
132. The Vendi Score: A Diversity Evaluation Metric for Machine Learning. (arXiv:2210.02410v1 [cs.LG])
133. Dynamical Isometry for Residual Networks. (arXiv:2210.02411v1 [cs.LG])
134. How Erd\"os and R\'enyi Win the Lottery. (arXiv:2210.02412v1 [cs.LG])
135. GLM-130B: An Open Bilingual Pre-trained Model. (arXiv:2210.02414v1 [cs.CL])
136. A Fourier Approach to Mixture Learning. (arXiv:2210.02415v1 [cs.LG])
137. A deep learning model for brain vessel segmentation in 3DRA with arteriovenous malformations. (arXiv:2210.02416v1 [eess.IV])
138. Explanation Uncertainty with Decision Boundary Awareness. (arXiv:2210.02419v1 [cs.LG])
139. DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics. (arXiv:2210.02438v1 [cs.RO])
140. Making Your First Choice: To Address Cold Start Problem in Vision Active Learning. (arXiv:2210.02442v1 [cs.CV])
141. Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection. (arXiv:2210.02443v1 [cs.CV])
142. Tree-based Intelligent Intrusion Detection System in Internet of Vehicles. (arXiv:1910.08635v2 [cs.LG] UPDATED)
143. Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v14 [cs.AI] UPDATED)
144. Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-Start Users. (arXiv:2005.12979v5 [cs.IR] UPDATED)
145. A Systematic Survey on Deep Generative Models for Graph Generation. (arXiv:2007.06686v3 [cs.LG] UPDATED)
146. On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice. (arXiv:2007.15745v3 [cs.LG] UPDATED)
147. The Variational Method of Moments. (arXiv:2012.09422v3 [cs.LG] UPDATED)
148. Beyond Impute-Then-Regress: Adapting Prediction to Missing Data. (arXiv:2104.03158v2 [stat.ML] UPDATED)
149. Understanding the Eluder Dimension. (arXiv:2104.06970v3 [cs.LG] UPDATED)
150. Truthful Self-Play. (arXiv:2106.03007v3 [stat.ML] UPDATED)
151. A Coupled Design of Exploiting Record Similarity for Practical Vertical Federated Learning. (arXiv:2106.06312v2 [cs.LG] UPDATED)
152. Joint Majorization-Minimization for Nonnegative Matrix Factorization with the $\beta$-divergence. (arXiv:2106.15214v3 [cs.LG] UPDATED)
153. SPEAR : Semi-supervised Data Programming in Python. (arXiv:2108.00373v3 [cs.LG] UPDATED)
154. Stochastic coordinate transformations with applications to robust machine learning. (arXiv:2110.01729v2 [stat.ML] UPDATED)
155. Efficient Estimation in NPIV Models: A Comparison of Various Neural Networks-Based Estimators. (arXiv:2110.06763v4 [econ.EM] UPDATED)
156. Does your graph need a confidence boost? Convergent boosted smoothing on graphs with tabular node features. (arXiv:2110.13413v2 [cs.LG] UPDATED)
157. TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches. (arXiv:2111.04867v4 [cs.DC] UPDATED)
158. Revisiting and Advancing Fast Adversarial Training Through The Lens of Bi-Level Optimization. (arXiv:2112.12376v6 [cs.LG] UPDATED)
159. Deep Generative Model for Periodic Graphs. (arXiv:2201.11932v3 [cs.LG] UPDATED)
160. Can Adversarial Training Be Manipulated By Non-Robust Features?. (arXiv:2201.13329v3 [cs.LG] UPDATED)
161. Dependence model assessment and selection with DecoupleNets. (arXiv:2202.03406v2 [stat.ML] UPDATED)
162. Understanding Rare Spurious Correlations in Neural Networks. (arXiv:2202.05189v3 [cs.LG] UPDATED)
163. Understanding Curriculum Learning in Policy Optimization for Solving Combinatorial Optimization Problems. (arXiv:2202.05423v2 [cs.LG] UPDATED)
164. Fast Adversarial Training with Noise Augmentation: A Unified Perspective on RandStart and GradAlign. (arXiv:2202.05488v2 [cs.LG] UPDATED)
165. Towards Adversarially Robust Deepfake Detection: An Ensemble Approach. (arXiv:2202.05687v2 [cs.LG] UPDATED)
166. Multi-Instance Causal Representation Learning for Instance Label Prediction and Out-of-Distribution Generalization. (arXiv:2202.12570v3 [cs.LG] UPDATED)
167. VaiPhy: a Variational Inference Based Algorithm for Phylogeny. (arXiv:2203.01121v2 [q-bio.PE] UPDATED)
168. Fully-Connected Network on Noncompact Symmetric Space and Ridgelet Transform based on Helgason-Fourier Analysis. (arXiv:2203.01631v2 [cs.LG] UPDATED)
169. Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse. (arXiv:2203.06768v3 [cs.LG] UPDATED)
170. Semi-Discrete Normalizing Flows through Differentiable Tessellation. (arXiv:2203.06832v3 [cs.LG] UPDATED)
171. Nearest Neighbor Classifier with Margin Penalty for Active Learning. (arXiv:2203.09174v3 [cs.IR] UPDATED)
172. Decoupled Mixup for Data-efficient Learning. (arXiv:2203.10761v2 [cs.LG] UPDATED)
173. A Perspective on Neural Capacity Estimation: Viability and Reliability. (arXiv:2203.11793v2 [cs.IT] UPDATED)
174. On the Efficiency of Integrating Self-supervised Learning and Meta-learning for User-defined Few-shot Keyword Spotting. (arXiv:2204.00352v3 [cs.LG] UPDATED)
175. Learnable latent embeddings for joint behavioral and neural analysis. (arXiv:2204.00673v2 [cs.LG] UPDATED)
176. Accelerated Training of Physics-Informed Neural Networks (PINNs) using Meshless Discretizations. (arXiv:2205.09332v3 [cs.LG] UPDATED)
177. Pessimism for Offline Linear Contextual Bandits using $\ell_p$ Confidence Sets. (arXiv:2205.10671v2 [cs.LG] UPDATED)
178. A Quadrature Rule combining Control Variates and Adaptive Importance Sampling. (arXiv:2205.11890v2 [stat.ML] UPDATED)
179. Generalised Implicit Neural Representations. (arXiv:2205.15674v2 [cs.LG] UPDATED)
180. coVariance Neural Networks. (arXiv:2205.15856v2 [cs.LG] UPDATED)
181. Learning a Restricted Boltzmann Machine using biased Monte Carlo sampling. (arXiv:2206.01310v2 [cs.LG] UPDATED)
182. On the duality between contrastive and non-contrastive self-supervised learning. (arXiv:2206.02574v2 [cs.LG] UPDATED)
183. GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions. (arXiv:2206.02780v2 [cs.CV] UPDATED)
184. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v3 [cs.AI] UPDATED)
185. Look, Radiate, and Learn: Self-supervised Localisation via Radio-Visual Correspondence. (arXiv:2206.06424v2 [cs.LG] UPDATED)
186. All you need is feedback: Communication with block attention feedback codes. (arXiv:2206.09457v2 [cs.IT] UPDATED)
187. EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. (arXiv:2206.09674v2 [cs.CL] UPDATED)
188. Integral Probability Metrics PAC-Bayes Bounds. (arXiv:2207.00614v4 [stat.ML] UPDATED)
189. Uncertainty-Aware Mixed-Variable Machine Learning for Materials Design. (arXiv:2207.04994v3 [stat.ML] UPDATED)
190. Continual Meta-Reinforcement Learning for UAV-Aided Vehicular Wireless Networks. (arXiv:2207.06131v2 [cs.LG] UPDATED)
191. Stochastic Functional Analysis and Multilevel Vector Field Anomaly Detection. (arXiv:2207.06229v2 [stat.ML] UPDATED)
192. TCT: Convexifying Federated Learning using Bootstrapped Neural Tangent Kernels. (arXiv:2207.06343v2 [cs.LG] UPDATED)
193. A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images. (arXiv:2207.06489v4 [eess.IV] UPDATED)
194. Self-Distilled Vision Transformer for Domain Generalization. (arXiv:2207.12392v3 [cs.CV] UPDATED)
195. PatchDropout: Economizing Vision Transformers Using Patch Dropout. (arXiv:2208.07220v2 [cs.CV] UPDATED)
196. Learned Indexing in Proteins: Extended Work on Substituting Complex Distance Calculations with Embedding and Clustering Techniques. (arXiv:2208.08910v2 [cs.IR] UPDATED)
197. A Domain Generalization Approach for Out-Of-Distribution 12-lead ECG Classification with Convolutional Neural Networks. (arXiv:2208.09656v2 [cs.LG] UPDATED)
198. Latent Variable Models in the Era of Industrial Big Data: Extension and Beyond. (arXiv:2208.10847v2 [eess.SY] UPDATED)
199. Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment. (arXiv:2208.13628v2 [cs.CV] UPDATED)
200. On the Trade-Off between Actionable Explanations and the Right to be Forgotten. (arXiv:2208.14137v2 [cs.LG] UPDATED)
201. Soft Diffusion: Score Matching for General Corruptions. (arXiv:2209.05442v2 [cs.CV] UPDATED)
202. On the State of the Art in Authorship Attribution and Authorship Verification. (arXiv:2209.06869v2 [cs.CL] UPDATED)
203. Computational Complexity of Sub-Linear Convergent Algorithms. (arXiv:2209.14558v2 [cs.LG] UPDATED)
204. Dataset Distillation using Parameter Pruning. (arXiv:2209.14609v2 [cs.CV] UPDATED)
205. Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v2 [stat.ML] UPDATED)
206. Composition of Differential Privacy & Privacy Amplification by Subsampling. (arXiv:2210.00597v2 [cs.CR] UPDATED)
207. Efficient acoustic feature transformation in mismatched environments using a Guided-GAN. (arXiv:2210.00721v2 [cs.SD] UPDATED)
208. FedDig: Robust Federated Learning Using Data Digest to Represent Absent Clients. (arXiv:2210.00737v2 [cs.LG] UPDATED)
209. Random Weight Factorization Improves the Training of Continuous Neural Representations. (arXiv:2210.01274v2 [cs.LG] UPDATED)
210. Less is More: Task-aware Layer-wise Distillation for Language Model Compression. (arXiv:2210.01351v2 [cs.CL] UPDATED)
211. ROAD-R: The Autonomous Driving Dataset with Logical Requirements. (arXiv:2210.01597v2 [cs.LG] UPDATED)
212. Using Entropy Measures for Monitoring the Evolution of Activity Patterns. (arXiv:2210.01736v2 [cs.LG] UPDATED)
213. NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning. (arXiv:2210.00120v1 [cs.RO] CROSS LISTED)
## cs.AI
---
**101** new papers in cs.AI:-) 
1. BayesFT: Bayesian Optimization for Fault Tolerant Neural Network Architecture. (arXiv:2210.01795v1 [cs.LG])
2. Multi-objective Deep Data Generation with Correlated Property Control. (arXiv:2210.01796v1 [cs.LG])
3. Ten Years after ImageNet: A 360{\deg} Perspective on AI. (arXiv:2210.01797v1 [cs.LG])
4. Latent Hierarchical Causal Structure Discovery with Rank Constraints. (arXiv:2210.01798v1 [cs.LG])
5. STGIN: A Spatial Temporal Graph-Informer Network for Long Sequence Traffic Speed Forecasting. (arXiv:2210.01799v1 [cs.LG])
6. Bayesian Q-learning With Imperfect Expert Demonstrations. (arXiv:2210.01800v1 [cs.LG])
7. Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation. (arXiv:2210.01801v1 [cs.LG])
8. Alternating Differentiation for Optimization Layers. (arXiv:2210.01802v1 [cs.LG])
9. Federated Graph-based Networks with Shared Embedding. (arXiv:2210.01803v1 [cs.LG])
10. CostNet: An End-to-End Framework for Goal-Directed Reinforcement Learning. (arXiv:2210.01805v1 [cs.LG])
11. **Low-Light** Image **Restoration** Based on Retina Model using Neural Networks. (arXiv:2210.01806v1 [eess.IV])
12. TripleE: Easy Domain Generalization via Episodic Replay. (arXiv:2210.01807v1 [cs.LG])
13. Detecting Anomalies within Smart Buildings using Do-It-Yourself Internet of Things. (arXiv:2210.01840v1 [cs.LG])
14. Learning Perception-Aware Agile Flight in Cluttered Environments. (arXiv:2210.01841v1 [cs.RO])
15. Explaining Patterns in Data with Language Models via Interpretable Autoprompting. (arXiv:2210.01848v1 [cs.LG])
16. Centerpoints Are All You Need in Overhead Imagery. (arXiv:2210.01857v1 [cs.CV])
17. Efficient Prototype Selection via Multi-Armed Bandits. (arXiv:2210.01860v1 [cs.LG])
18. Memory in humans and deep language models: Linking hypotheses for model augmentation. (arXiv:2210.01869v1 [cs.CL])
19. Opportunistic Qualitative Planning in Stochastic Systems with Incomplete Preferences over Reachability Objectives. (arXiv:2210.01878v1 [cs.AI])
20. Uncertainty-Aware Meta-Learning for Multimodal Task Distributions. (arXiv:2210.01881v1 [cs.LG])
21. Multi-view Human Body Mesh Translator. (arXiv:2210.01886v1 [cs.CV])
22. Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v1 [cs.NE])
23. A Self-Play Posterior Sampling Algorithm for Zero-Sum Markov Games. (arXiv:2210.01907v1 [cs.LG])
24. Grounding Language with Visual Affordances over Unstructured Data. (arXiv:2210.01911v1 [cs.RO])
25. When and why vision-language models behave like bag-of-words models, and what to do about it?. (arXiv:2210.01936v1 [cs.CV])
26. On the Robustness of Deep Clustering Models: Adversarial Attacks and Defenses. (arXiv:2210.01940v1 [cs.LG])
27. SIMPLE: A Gradient Estimator for $k$-Subset Sampling. (arXiv:2210.01941v1 [cs.LG])
28. IGNiteR: News Recommendation in Microblogging Applications (Extended Version). (arXiv:2210.01942v1 [cs.IR])
29. Learning Dynamic Abstract Representations for Sample-Efficient Reinforcement Learning. (arXiv:2210.01955v1 [cs.LG])
30. Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering. (arXiv:2210.01959v1 [cs.CL])
31. COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models. (arXiv:2210.01963v1 [cs.CL])
32. The Calibration Generalization Gap. (arXiv:2210.01964v1 [cs.LG])
33. Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v1 [cs.LG])
34. Towards Prototype-Based Self-Explainable Graph Neural Network. (arXiv:2210.01974v1 [cs.LG])
35. GAPX: Generalized Autoregressive Paraphrase-Identification X. (arXiv:2210.01979v1 [cs.CL])
36. Manipulation and Peer Mechanisms: A Survey. (arXiv:2210.01984v1 [cs.AI])
37. Atari-5: Distilling the Arcade Learning Environment down to Five Games. (arXiv:2210.02019v1 [cs.AI])
38. Clustering Semantic Predicates in the Open Research Knowledge Graph. (arXiv:2210.02034v1 [cs.DL])
39. GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks. (arXiv:2210.02040v1 [cs.LG])
40. Natural Color Fool: Towards Boosting Black-box Unrestricted Attacks. (arXiv:2210.02041v1 [cs.CV])
41. FedMT: Federated Learning with Mixed-type Labels. (arXiv:2210.02042v1 [cs.LG])
42. Contextualized Generative Retrieval. (arXiv:2210.02068v1 [cs.IR])
43. On the Learning Mechanisms in Physical Reasoning. (arXiv:2210.02075v1 [cs.LG])
44. Transformer-based conditional generative adversarial network for multivariate time series generation. (arXiv:2210.02089v1 [cs.LG])
45. ISFL: Trustworthy Federated Learning for Non-i.i.d. Data with Local Importance Sampling. (arXiv:2210.02119v1 [cs.LG])
46. Domain Discrepancy Aware Distillation for Model Aggregation in Federated Learning. (arXiv:2210.02190v1 [cs.LG])
47. On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks. (arXiv:2210.02191v1 [cs.LG])
48. Are All Losses Created Equal: A Neural Collapse Perspective. (arXiv:2210.02192v1 [cs.LG])
49. MTSMAE: Masked Autoencoders for Multivariate Time-Series Forecasting. (arXiv:2210.02199v1 [cs.LG])
50. PriorNet: lesion segmentation in PET-CT including prior tumor appearance information. (arXiv:2210.02203v1 [eess.IV])
51. CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations. (arXiv:2210.02223v1 [cs.CL])
52. Neural Distillation as a State Representation Bottleneck in Reinforcement Learning. (arXiv:2210.02224v1 [cs.LG])
53. Comprint: Image Forgery Detection and Localization using Compression Fingerprints. (arXiv:2210.02227v1 [cs.CV])
54. On Neural Consolidation for Transfer in Reinforcement Learning. (arXiv:2210.02240v1 [cs.LG])
55. From Intelligent Agents to Trustworthy Human-Centred Multiagent Systems. (arXiv:2210.02260v1 [cs.MA])
56. Spatial-Temporal-Aware Safe Multi-Agent Reinforcement Learning of Connected Autonomous Vehicles in Challenging Scenarios. (arXiv:2210.02300v1 [cs.RO])
57. Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers. (arXiv:2210.02317v1 [cs.RO])
58. Promising or Elusive? Unsupervised Object Segmentation from Real-world Single Images. (arXiv:2210.02324v1 [cs.CV])
59. Goal Recognition as a Deep Learning Task: the GRNet Approach. (arXiv:2210.02377v1 [cs.AI])
60. Variational prompt tuning improves generalization of vision-language models. (arXiv:2210.02390v1 [cs.CV])
61. Temporally Consistent Video Transformer for Long-Term Video Prediction. (arXiv:2210.02396v1 [cs.CV])
62. Phenaki: Variable Length Video Generation From Open Domain Textual Description. (arXiv:2210.02399v1 [cs.CV])
63. Medical Image Retrieval via Nearest Neighbor Search on Pre-trained Image Features. (arXiv:2210.02401v1 [cs.CV])
64. The Influence of Explainable Artificial Intelligence: Nudging Behaviour or Boosting Capability?. (arXiv:2210.02407v1 [cs.HC])
65. GLM-130B: An Open Bilingual Pre-trained Model. (arXiv:2210.02414v1 [cs.CL])
66. Making Your First Choice: To Address Cold Start Problem in Vision Active Learning. (arXiv:2210.02442v1 [cs.CV])
67. Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection. (arXiv:2210.02443v1 [cs.CV])
68. Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v14 [cs.AI] UPDATED)
69. What and When to Look?: Temporal Span Proposal Network for Video Relation Detection. (arXiv:2107.07154v2 [cs.CV] UPDATED)
70. Human Perception of Audio Deepfakes. (arXiv:2107.09667v5 [cs.HC] UPDATED)
71. PCNN: A physics-constrained neural network for multiphase flows. (arXiv:2109.08965v3 [physics.flu-dyn] UPDATED)
72. Rome was built in 1776: A Case Study on Factual Correctness in Knowledge-Grounded Response Generation. (arXiv:2110.05456v2 [cs.CL] UPDATED)
73. LTC-SUM: Lightweight Client-driven Personalized Video Summarization Framework Using 2D CNN. (arXiv:2201.09049v2 [cs.CV] UPDATED)
74. Fast Adversarial Training with Noise Augmentation: A Unified Perspective on RandStart and GradAlign. (arXiv:2202.05488v2 [cs.LG] UPDATED)
75. Decoupled Mixup for Data-efficient Learning. (arXiv:2203.10761v2 [cs.LG] UPDATED)
76. Computable Artificial General Intelligence. (arXiv:2205.10513v6 [cs.AI] UPDATED)
77. Efficient Policy Iteration for Robust Markov Decision Processes via Regularization. (arXiv:2205.14327v2 [cs.AI] UPDATED)
78. Generalised Implicit Neural Representations. (arXiv:2205.15674v2 [cs.LG] UPDATED)
79. Human-AI Shared Control via Policy Dissection. (arXiv:2206.00152v4 [cs.RO] UPDATED)
80. On the duality between contrastive and non-contrastive self-supervised learning. (arXiv:2206.02574v2 [cs.LG] UPDATED)
81. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v3 [cs.AI] UPDATED)
82. A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction. (arXiv:2206.05224v2 [cs.CL] UPDATED)
83. All you need is feedback: Communication with block attention feedback codes. (arXiv:2206.09457v2 [cs.IT] UPDATED)
84. EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. (arXiv:2206.09674v2 [cs.CL] UPDATED)
85. Fine-grained Few-shot Recognition by Deep Object Parsing. (arXiv:2207.07110v3 [cs.CV] UPDATED)
86. Self-Distilled Vision Transformer for Domain Generalization. (arXiv:2207.12392v3 [cs.CV] UPDATED)
87. Robotic Dough Shaping. (arXiv:2208.00386v2 [cs.RO] UPDATED)
88. Self-supervised learning with rotation-invariant kernels. (arXiv:2208.00789v2 [cs.CV] UPDATED)
89. A Domain Generalization Approach for Out-Of-Distribution 12-lead ECG Classification with Convolutional Neural Networks. (arXiv:2208.09656v2 [cs.LG] UPDATED)
90. On the Trade-Off between Actionable Explanations and the Right to be Forgotten. (arXiv:2208.14137v2 [cs.LG] UPDATED)
91. SANCL: Multimodal Review Helpfulness Prediction with Selective Attention and Natural Contrastive Learning. (arXiv:2209.05040v5 [cs.CL] UPDATED)
92. Soft Diffusion: Score Matching for General Corruptions. (arXiv:2209.05442v2 [cs.CV] UPDATED)
93. On the State of the Art in Authorship Attribution and Authorship Verification. (arXiv:2209.06869v2 [cs.CL] UPDATED)
94. Will It Blend? Mixing Training Paradigms & Prompting for Argument Quality Prediction. (arXiv:2209.08966v2 [cs.CL] UPDATED)
95. Dataset Distillation using Parameter Pruning. (arXiv:2209.14609v2 [cs.CV] UPDATED)
96. Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v2 [stat.ML] UPDATED)
97. Less is More: Task-aware Layer-wise Distillation for Language Model Compression. (arXiv:2210.01351v2 [cs.CL] UPDATED)
98. Pay Self-Attention to Audio-Visual Navigation. (arXiv:2210.01353v2 [cs.SD] UPDATED)
99. Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings. (arXiv:2210.01448v2 [cs.SD] UPDATED)
100. ROAD-R: The Autonomous Driving Dataset with Logical Requirements. (arXiv:2210.01597v2 [cs.LG] UPDATED)
101. Learning the Spectrogram Temporal Resolution for Audio Classification. (arXiv:2210.01719v2 [cs.SD] UPDATED)

