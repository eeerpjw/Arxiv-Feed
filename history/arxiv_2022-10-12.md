# Your interest papers
---
## cs.CV
---
### ACRNet: Attention Cube Regression Network for Multi-view **Real-time** 3D Human Pose Estimation in Telemedicine. (arXiv:2210.05130v1 [cs.CV])
- Authors : Boce Hu, Chenfei Zhu, Xupeng Ai
- Link : [http://arxiv.org/abs/2210.05130](http://arxiv.org/abs/2210.05130)
> ABSTRACT  :  Human pose estimation (HPE) for 3D skeleton reconstruction in telemedicine has long received attention. Although the development of deep learning has made HPE methods in telemedicine simpler and easier to use, addressing low accuracy and high latency remains a big challenge. In this paper, we propose a novel multi-view Attention Cube Regression Network (ACRNet), which regresses the 3D position of joints in **real time** by aggregating informative attention points on each cube surface. More specially, a cube whose each surface contains uniformly distributed attention points with specific coordinate values is first created to wrap the target from the main view. Then, our network regresses the 3D position of each joint by summing and averaging the coordinates of attention points on each surface after being weighted. To verify our method, we first tested ACRNet on the open-source ITOP dataset; meanwhile, we collected a new multi-view upper body movement dataset (UBM) on the trunk support trainer (TruST) to validate the capability of our model in real rehabilitation scenarios. Experimental results demonstrate the superiority of ACRNet compared with other state-of-the-art methods. We also validate the efficacy of each module in ACRNet. Furthermore, Our work analyzes the performance of ACRNet under the medical monitoring indicator. Because of the high accuracy and running speed, our model is suitable for real-time telemedicine settings. The source code is available at https://github.com/BoceHu/ACRNet  
### Markup-to-Image Diffusion Models with Scheduled Sampling. (arXiv:2210.05147v1 [cs.LG])
- Authors : Yuntian Deng, Noriyuki Kojima
- Link : [http://arxiv.org/abs/2210.05147](http://arxiv.org/abs/2210.05147)
> ABSTRACT  :  Building on recent advances in image generation, we present a fully data-driven approach to rendering markup into images. The approach is based on diffusion models, which parameterize the distribution of data using a sequence of denoising operations on top of a Gaussian noise distribution. We view the diffusion denoising process as a sequential decision making process, and show that it exhibits compounding errors similar to **exposure** bias issues in imitation learning problems. To mitigate these issues, we adapt the scheduled sampling algorithm to diffusion training. We conduct experiments on four markup datasets: mathematical formulas (LaTeX), table layouts (HTML), sheet music (LilyPond), and molecular images (SMILES). These experiments each verify the effectiveness of the diffusion process and the use of scheduled sampling to fix generation issues. These results also show that the markup-to-image task presents a useful controlled compositional setting for diagnosing and analyzing generative image models.  
### Deep Fourier Up-Sampling. (arXiv:2210.05171v1 [cs.CV])
- Authors : Man Zhou, Hu Yu, Jie Huang, Feng Zhao, **Jinwei Gu**, Chen Change, Deyu Meng, **Chongyi Li**
- Link : [http://arxiv.org/abs/2210.05171](http://arxiv.org/abs/2210.05171)
> ABSTRACT  :  Existing convolutional neural networks widely adopt spatial down-/up-sampling for multi-scale modeling. However, spatial up-sampling operators (\emph{e.g.}, interpolation, transposed convolution, and un-pooling) heavily depend on local pixel attention, incapably exploring the global dependency. In contrast, the Fourier domain obeys the nature of global modeling according to the spectral convolution theorem. Unlike the spatial domain that performs up-sampling with the property of local similarity, up-sampling in the Fourier domain is more challenging as it does not follow such a local property. In this study, we propose a theoretically sound Deep Fourier Up-Sampling (FourierUp) to solve these issues. We revisit the relationships between spatial and Fourier domains and reveal the transform rules on the features of different resolutions in the Fourier domain, which provide key insights for FourierUp's designs. FourierUp as a generic operator consists of three key components: 2D discrete Fourier transform, Fourier dimension increase rules, and 2D inverse Fourier transform, which can be directly integrated with existing networks. Extensive experiments across multiple computer vision tasks, including object detection, image segmentation, image de-raining, image dehazing, and guided image super-resolution, demonstrate the consistent performance gains obtained by introducing our FourierUp.  
### Neighbourhood Representative Sampling for Efficient End-to-end Video Quality Assessment. (arXiv:2210.05357v1 [cs.CV])
- Authors : Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou, Wenxiu Sun, Qiong Yan, **Jinwei Gu**, Weisi Lin
- Link : [http://arxiv.org/abs/2210.05357](http://arxiv.org/abs/2210.05357)
> ABSTRACT  :  The increased resolution of real-world videos presents a dilemma between efficiency and accuracy for deep Video Quality Assessment (VQA). On the one hand, keeping the original resolution will lead to unacceptable computational costs. On the other hand, existing practices, such as resizing and cropping, will change the quality of original videos due to the loss of details and contents, and are therefore harmful to quality assessment. With the obtained insight from the study of spatial-temporal redundancy in the human visual system and visual coding theory, we observe that quality information around a neighbourhood is typically similar, motivating us to investigate an effective quality-sensitive neighbourhood representatives scheme for VQA. In this work, we propose a unified scheme, spatial-temporal grid mini-cube sampling (St-GMS) to get a novel type of sample, named fragments. Full-resolution videos are first divided into mini-cubes with preset spatial-temporal grids, then the temporal-aligned quality representatives are sampled to compose the fragments that serve as inputs for VQA. In addition, we design the Fragment Attention Network (FANet), a network architecture tailored specifically for fragments. With fragments and FANet, the proposed efficient end-to-end FAST-VQA and FasterVQA achieve significantly better performance than existing approaches on all VQA benchmarks while requiring only 1/1612 FLOPs compared to the current state-of-the-art. Codes, models and demos are available at https://github.com/timothyhtimothy/FAST-VQA-and-FasterVQA.  
### PP-StructureV2: A Stronger Document Analysis System. (arXiv:2210.05391v1 [cs.CV])
- Authors : Chenxia Li, Ruoyu Guo, Jun Zhou, Mengtao An, Yuning Du, Lingfeng Zhu, Yi Liu, Xiaoguang Hu, Dianhai Yu
- Link : [http://arxiv.org/abs/2210.05391](http://arxiv.org/abs/2210.05391)
> ABSTRACT  :  A large amount of document data exists in unstructured form such as raw images without any text information. Designing a practical document image analysis system is a meaningful but challenging task. In previous work, we proposed an intelligent document analysis system PP-Structure. In order to further upgrade the function and performance of PP-Structure, we propose PP-StructureV2 in this work, which contains two subsystems: Layout Information Extraction and Key Information Extraction. Firstly, we integrate Image Direction Correction module and Layout **Restoration** module to enhance the functionality of the system. Secondly, 8 practical strategies are utilized in PP-StructureV2 for better performance. For Layout Analysis model, we introduce ultra light-weight detector PP-PicoDet and knowledge distillation algorithm FGD for model lightweighting, which increased the inference speed by 11 times with comparable mAP. For Table Recognition model, we utilize PP-LCNet, CSP-PAN and SLAHead to optimize the backbone module, feature fusion module and decoding module, respectively, which improved the table structure accuracy by 6\% with comparable inference speed. For Key Information Extraction model, we introduce VI-LayoutXLM which is a visual-feature independent LayoutXLM architecture, TB-YX sorting algorithm and U-DML knowledge distillation algorithm, which brought 2.8\% and 9.1\% improvement respectively on the Hmean of Semantic Entity Recognition and Relation Extraction tasks. All the above mentioned models and code are open-sourced in the GitHub repository PaddleOCR.  
### Retinex Image **Enhancement** Based on Sequential Decomposition With a Plug-and-Play Framework. (arXiv:2210.05436v1 [eess.IV])
- Authors : Tingting Wu, Wenna Wu, Ying Yang, Lei Fan, Tieyong Zeng
- Link : [http://arxiv.org/abs/2210.05436](http://arxiv.org/abs/2210.05436)
> ABSTRACT  :  The Retinex model is one of the most representative and effective methods for **low-light** image **enhancement**. However, the Retinex model does not explicitly tackle the noise problem, and shows unsatisfactory enhancing results. In recent years, due to the excellent performance, deep learning models have been widely used in **low-light** image **enhancement**. However, these methods have two limitations: i) The desirable performance can only be achieved by deep learning when a large number of labeled data are available. However, it is not easy to curate massive low/normal-light paired data; ii) Deep learning is notoriously a black-box model [1]. It is difficult to explain their inner-working mechanism and understand their behaviors. In this paper, using a sequential Retinex decomposition strategy, we design a plug-and-play framework based on the Retinex theory for simultaneously image **enhancement** and noise removal. Meanwhile, we develop a convolutional neural network-based (CNN-based) denoiser into our proposed plug-and-play framework to generate a reflectance component. The final enhanced image is produced by integrating the illumination and reflectance with gamma correction. The proposed plug-and-play framework can facilitate both post hoc and ad hoc interpretability. Extensive experiments on different datasets demonstrate that our framework outcompetes the state-of-the-art methods in both image **enhancement** and denoising.  
### Parallel Augmentation and Dual **Enhancement** for Occluded Person Re-identification. (arXiv:2210.05438v1 [cs.CV])
- Authors : Zi wang, Huaibo Huang, Aihua Zheng, Chenglong Li, Ran He
- Link : [http://arxiv.org/abs/2210.05438](http://arxiv.org/abs/2210.05438)
> ABSTRACT  :  Occluded person re-identification (Re-ID), the task of searching for the same person's images in occluded environments, has attracted lots of attention in the past decades. Recent approaches concentrate on improving performance on occluded data by data/feature augmentation or using extra models to predict occlusions. However, they ignore the imbalance problem in the test set and not fully utilize the information from the training data. To alleviate the above problems, we propose a simple but effective method with Parallel Augmentation and Dual **Enhancement** (PADE) that is robust on both occluded and non-occluded data, and does not require any auxiliary clues. First, we design a parallel augmentation mechanism (PAM) for occluded Re-ID to generate more suitable occluded data to mitigate the negative effects of unbalanced data. Second, we propose the dual **enhancement** strategy (DES)for global and local features to promote the context information and details. Experimental results on widely used occluded datasets (OccludedDuke, Partial-REID, and Occluded-ReID) and non-occluded datasets (Market-1501 and DukeMTMC-reID) validate the effectiveness of our method. The code will be available soon.  
### Enabling **ISP**-less Low-Power Computer Vision. (arXiv:2210.05451v1 [cs.CV])
- Authors : Gourav Datta, Zeyu Liu, Zihan Yin, Linyu Sun
- Link : [http://arxiv.org/abs/2210.05451](http://arxiv.org/abs/2210.05451)
> ABSTRACT  :  In order to deploy current computer vision (CV) models on resource-constrained low-power devices, recent works have proposed in-sensor and in-pixel computing approaches that try to partly/fully bypass the image signal processor (**ISP**) and yield significant bandwidth reduction between the image sensor and the CV processing unit by downsampling the activation maps in the initial convolutional neural network (CNN) layers. However, direct inference on the raw images degrades the test accuracy due to the difference in covariance of the raw images captured by the image sensors compared to the **ISP**-processed images used for training. Moreover, it is difficult to train deep CV models on raw images, because most (if not all) large-scale open-source datasets consist of RGB images. To mitigate this concern, we propose to invert the **ISP** pipeline, which can convert the RGB images of any dataset to its raw counterparts, and enable model training on raw images. We release the raw version of the COCO dataset, a large-scale benchmark for generic high-level vision tasks. For **ISP**-less CV systems, training on these raw images result in a 7.1% increase in test accuracy on the visual wake works (VWW) dataset compared to relying on training with traditional **ISP**-processed RGB datasets. To further improve the accuracy of **ISP**-less CV models and to increase the energy and bandwidth benefits obtained by in-sensor/in-pixel computing, we propose an energy-efficient form of analog in-pixel demosaicing that may be coupled with in-pixel CNN computations. When evaluated on raw images captured by real sensors from the PASCALRAW dataset, our approach results in a 8.1% increase in mAP. Lastly, we demonstrate a further 20.5% increase in mAP by using a novel application of few-shot learning with thirty shots each for the novel PASCALRAW dataset, constituting 3 classes.  
### MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v4 [cs.CV] UPDATED)
- Authors : Qi Yang, Yujie Zhang, Siheng Chen, Yiling Xu, Jun Sun, Zhan Ma
- Link : [http://arxiv.org/abs/2103.02850](http://arxiv.org/abs/2103.02850)
> ABSTRACT  :  In this paper, we propose a new distortion quantification method for point clouds, the multiscale potential energy discrepancy (MPED). Currently, there is a lack of effective distortion quantification for a variety of point cloud perception tasks. Specifically, for dense point clouds, a distortion quantification method is used to predict human subjective scores and optimize the selection of human perception tasks parameters, such as compression and **enhancement**. For sparse point clouds, a distortion quantification methods is work as a loss function to guide the training of deep neural networks for unsupervised learning tasks (e.g., point cloud reconstruction, completion and upsampling). Therefore, an effective distortion quantification should be differentiable, distortion discriminable and have a low computational complexity. However, current distortion quantification cannot satisfy all three conditions. To fill this gap, we propose a new point cloud feature description method, the point potential energy (PPE), inspired by the classical physics. We regard the point clouds are systems that have potential energy and the distortion can change the total potential energy. By evaluating at various neighborhood sizes, the proposed MPED achieves global-local tradeoffs, capturing distortion in a multiscale fashion. We further theoretically show that classical Chamfer distance is a special case of our MPED. Extensive experiments show the proposed MPED superior to current methods on both human and machine perception tasks. Our code is avaliable at https://github.com/Qi-Yangsjtu/MPED.  
### HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions. (arXiv:2207.14284v3 [cs.CV] UPDATED)
- Authors : Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Nam Lim, Jiwen Lu
- Link : [http://arxiv.org/abs/2207.14284](http://arxiv.org/abs/2207.14284)
> ABSTRACT  :  Recent progress in vision Transformers exhibits great success in various tasks driven by the new spatial modeling mechanism based on dot-product self-attention. In this paper, we show that the key ingredients behind the vision Transformers, namely input-adaptive, long-range and high-order spatial interactions, can also be efficiently implemented with a convolution-based framework. We present the Recursive Gated Convolution ($\textit{g}^\textit{n}$Conv) that performs high-order spatial interactions with gated convolutions and recursive designs. The new operation is highly flexible and customizable, which is compatible with various variants of convolution and extends the two-order interactions in self-attention to arbitrary orders without introducing significant extra computation. $\textit{g}^\textit{n}$Conv can serve as a plug-and-play module to improve various vision Transformers and convolution-based models. Based on the operation, we construct a new family of generic vision backbones named HorNet. Extensive experiments on ImageNet classification, COCO object detection and ADE20K semantic segmentation show HorNet outperform **Swin** Transformers and ConvNeXt by a significant margin with similar overall architecture and training configurations. HorNet also shows favorable scalability to more training data and larger model sizes. Apart from the effectiveness in visual encoders, we also show $\textit{g}^\textit{n}$Conv can be applied to task-specific decoders and consistently improve dense prediction performance with less computation. Our results demonstrate that $\textit{g}^\textit{n}$Conv can be a new basic module for visual modeling that effectively combines the merits of both vision Transformers and CNNs. Code is available at https://github.com/raoyongming/HorNet  
## eess.IV
---
### Retinex Image **Enhancement** Based on Sequential Decomposition With a Plug-and-Play Framework. (arXiv:2210.05436v1 [eess.IV])
- Authors : Tingting Wu, Wenna Wu, Ying Yang, Lei Fan, Tieyong Zeng
- Link : [http://arxiv.org/abs/2210.05436](http://arxiv.org/abs/2210.05436)
> ABSTRACT  :  The Retinex model is one of the most representative and effective methods for **low-light** image **enhancement**. However, the Retinex model does not explicitly tackle the noise problem, and shows unsatisfactory enhancing results. In recent years, due to the excellent performance, deep learning models have been widely used in **low-light** image **enhancement**. However, these methods have two limitations: i) The desirable performance can only be achieved by deep learning when a large number of labeled data are available. However, it is not easy to curate massive low/normal-light paired data; ii) Deep learning is notoriously a black-box model [1]. It is difficult to explain their inner-working mechanism and understand their behaviors. In this paper, using a sequential Retinex decomposition strategy, we design a plug-and-play framework based on the Retinex theory for simultaneously image **enhancement** and noise removal. Meanwhile, we develop a convolutional neural network-based (CNN-based) denoiser into our proposed plug-and-play framework to generate a reflectance component. The final enhanced image is produced by integrating the illumination and reflectance with gamma correction. The proposed plug-and-play framework can facilitate both post hoc and ad hoc interpretability. Extensive experiments on different datasets demonstrate that our framework outcompetes the state-of-the-art methods in both image **enhancement** and denoising.  
### Enabling **ISP**-less Low-Power Computer Vision. (arXiv:2210.05451v1 [cs.CV])
- Authors : Gourav Datta, Zeyu Liu, Zihan Yin, Linyu Sun
- Link : [http://arxiv.org/abs/2210.05451](http://arxiv.org/abs/2210.05451)
> ABSTRACT  :  In order to deploy current computer vision (CV) models on resource-constrained low-power devices, recent works have proposed in-sensor and in-pixel computing approaches that try to partly/fully bypass the image signal processor (**ISP**) and yield significant bandwidth reduction between the image sensor and the CV processing unit by downsampling the activation maps in the initial convolutional neural network (CNN) layers. However, direct inference on the raw images degrades the test accuracy due to the difference in covariance of the raw images captured by the image sensors compared to the **ISP**-processed images used for training. Moreover, it is difficult to train deep CV models on raw images, because most (if not all) large-scale open-source datasets consist of RGB images. To mitigate this concern, we propose to invert the **ISP** pipeline, which can convert the RGB images of any dataset to its raw counterparts, and enable model training on raw images. We release the raw version of the COCO dataset, a large-scale benchmark for generic high-level vision tasks. For **ISP**-less CV systems, training on these raw images result in a 7.1% increase in test accuracy on the visual wake works (VWW) dataset compared to relying on training with traditional **ISP**-processed RGB datasets. To further improve the accuracy of **ISP**-less CV models and to increase the energy and bandwidth benefits obtained by in-sensor/in-pixel computing, we propose an energy-efficient form of analog in-pixel demosaicing that may be coupled with in-pixel CNN computations. When evaluated on raw images captured by real sensors from the PASCALRAW dataset, our approach results in a 8.1% increase in mAP. Lastly, we demonstrate a further 20.5% increase in mAP by using a novel application of few-shot learning with thirty shots each for the novel PASCALRAW dataset, constituting 3 classes.  
### MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v4 [cs.CV] UPDATED)
- Authors : Qi Yang, Yujie Zhang, Siheng Chen, Yiling Xu, Jun Sun, Zhan Ma
- Link : [http://arxiv.org/abs/2103.02850](http://arxiv.org/abs/2103.02850)
> ABSTRACT  :  In this paper, we propose a new distortion quantification method for point clouds, the multiscale potential energy discrepancy (MPED). Currently, there is a lack of effective distortion quantification for a variety of point cloud perception tasks. Specifically, for dense point clouds, a distortion quantification method is used to predict human subjective scores and optimize the selection of human perception tasks parameters, such as compression and **enhancement**. For sparse point clouds, a distortion quantification methods is work as a loss function to guide the training of deep neural networks for unsupervised learning tasks (e.g., point cloud reconstruction, completion and upsampling). Therefore, an effective distortion quantification should be differentiable, distortion discriminable and have a low computational complexity. However, current distortion quantification cannot satisfy all three conditions. To fill this gap, we propose a new point cloud feature description method, the point potential energy (PPE), inspired by the classical physics. We regard the point clouds are systems that have potential energy and the distortion can change the total potential energy. By evaluating at various neighborhood sizes, the proposed MPED achieves global-local tradeoffs, capturing distortion in a multiscale fashion. We further theoretically show that classical Chamfer distance is a special case of our MPED. Extensive experiments show the proposed MPED superior to current methods on both human and machine perception tasks. Our code is avaliable at https://github.com/Qi-Yangsjtu/MPED.  
## cs.LG
---
### Markup-to-Image Diffusion Models with Scheduled Sampling. (arXiv:2210.05147v1 [cs.LG])
- Authors : Yuntian Deng, Noriyuki Kojima
- Link : [http://arxiv.org/abs/2210.05147](http://arxiv.org/abs/2210.05147)
> ABSTRACT  :  Building on recent advances in image generation, we present a fully data-driven approach to rendering markup into images. The approach is based on diffusion models, which parameterize the distribution of data using a sequence of denoising operations on top of a Gaussian noise distribution. We view the diffusion denoising process as a sequential decision making process, and show that it exhibits compounding errors similar to **exposure** bias issues in imitation learning problems. To mitigate these issues, we adapt the scheduled sampling algorithm to diffusion training. We conduct experiments on four markup datasets: mathematical formulas (LaTeX), table layouts (HTML), sheet music (LilyPond), and molecular images (SMILES). These experiments each verify the effectiveness of the diffusion process and the use of scheduled sampling to fix generation issues. These results also show that the markup-to-image task presents a useful controlled compositional setting for diagnosing and analyzing generative image models.  
### ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret. (arXiv:2206.04122v2 [cs.GT] UPDATED)
- Authors : Stephen McAleer, Gabriele Farina, Marc Lanctot, Tuomas Sandholm
- Link : [http://arxiv.org/abs/2206.04122](http://arxiv.org/abs/2206.04122)
> ABSTRACT  :  Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promising line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art -- DREAM and neural fictitious self play (NFSP) -- on a number of games and the difference becomes dramatic as game size increases. In the very large game of **dark** chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over $90\%$ of the time.  
## cs.AI
---
### Neighbourhood Representative Sampling for Efficient End-to-end Video Quality Assessment. (arXiv:2210.05357v1 [cs.CV])
- Authors : Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou, Wenxiu Sun, Qiong Yan, **Jinwei Gu**, Weisi Lin
- Link : [http://arxiv.org/abs/2210.05357](http://arxiv.org/abs/2210.05357)
> ABSTRACT  :  The increased resolution of real-world videos presents a dilemma between efficiency and accuracy for deep Video Quality Assessment (VQA). On the one hand, keeping the original resolution will lead to unacceptable computational costs. On the other hand, existing practices, such as resizing and cropping, will change the quality of original videos due to the loss of details and contents, and are therefore harmful to quality assessment. With the obtained insight from the study of spatial-temporal redundancy in the human visual system and visual coding theory, we observe that quality information around a neighbourhood is typically similar, motivating us to investigate an effective quality-sensitive neighbourhood representatives scheme for VQA. In this work, we propose a unified scheme, spatial-temporal grid mini-cube sampling (St-GMS) to get a novel type of sample, named fragments. Full-resolution videos are first divided into mini-cubes with preset spatial-temporal grids, then the temporal-aligned quality representatives are sampled to compose the fragments that serve as inputs for VQA. In addition, we design the Fragment Attention Network (FANet), a network architecture tailored specifically for fragments. With fragments and FANet, the proposed efficient end-to-end FAST-VQA and FasterVQA achieve significantly better performance than existing approaches on all VQA benchmarks while requiring only 1/1612 FLOPs compared to the current state-of-the-art. Codes, models and demos are available at https://github.com/timothyhtimothy/FAST-VQA-and-FasterVQA.  
### ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret. (arXiv:2206.04122v2 [cs.GT] UPDATED)
- Authors : Stephen McAleer, Gabriele Farina, Marc Lanctot, Tuomas Sandholm
- Link : [http://arxiv.org/abs/2206.04122](http://arxiv.org/abs/2206.04122)
> ABSTRACT  :  Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promising line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art -- DREAM and neural fictitious self play (NFSP) -- on a number of games and the difference becomes dramatic as game size increases. In the very large game of **dark** chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over $90\%$ of the time.  
# Paper List
---
## cs.CV
---
**145** new papers in cs.CV:-) 
1. Improving The Reconstruction Quality by Overfitted Decoder Bias in Neural Image Compression. (arXiv:2210.04898v1 [eess.IV])
2. NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields. (arXiv:2210.04932v1 [cs.RO])
3. An Action Is Worth Multiple Words: Handling Ambiguity in Action Recognition. (arXiv:2210.04933v1 [cs.CV])
4. Deep Insights of Learning based Micro Expression Recognition: A Perspective on Promises, Challenges and Research Needs. (arXiv:2210.04935v1 [cs.CV])
5. EarthNets: Empowering AI in Earth Observation. (arXiv:2210.04936v1 [cs.CV])
6. Masked Autoencoders for Low dose CT denoising. (arXiv:2210.04944v1 [cs.CV])
7. f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation. (arXiv:2210.04955v1 [cs.CV])
8. Domain-guided data augmentation for deep learning on medical imaging. (arXiv:2210.04977v1 [cs.CV])
9. Label-free segmentation from cardiac ultrasound using self-supervised learning. (arXiv:2210.04979v1 [cs.CV])
10. Loop Unrolled Shallow Equilibrium Regularizer (LUSER) -- A Memory-Efficient Inverse Problem Solver. (arXiv:2210.04987v1 [eess.IV])
11. Learning with an Evolving Class Ontology. (arXiv:2210.04993v1 [cs.CV])
12. Graph2Vid: Flow graph to Video Grounding forWeakly-supervised Multi-Step Localization. (arXiv:2210.04996v1 [cs.CV])
13. Social Media Personal Event Notifier Using NLP and Machine Learning. (arXiv:2210.05001v1 [cs.CV])
14. Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware. (arXiv:2210.05006v1 [cs.LG])
15. Fast Hierarchical Learning for Few-Shot Object Detection. (arXiv:2210.05008v1 [cs.CV])
16. LidarNAS: Unifying and Searching Neural Architectures for 3D Point Clouds. (arXiv:2210.05018v1 [cs.CV])
17. Using Deep Learning to Improve Early Diagnosis of Pneumonia in Underdeveloped Countries. (arXiv:2210.05023v1 [eess.IV])
18. Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks. (arXiv:2210.05038v1 [cs.CL])
19. Contrastive Video-Language Learning with Fine-grained Frame Sampling. (arXiv:2210.05039v1 [cs.LG])
20. AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization. (arXiv:2210.05060v1 [cs.CV])
21. Improving Dense Contrastive Learning with Dense Negative Pairs. (arXiv:2210.05063v1 [cs.CV])
22. VER: Scaling On-Policy RL Leads to the Emergence of Navigation in Embodied Rearrangement. (arXiv:2210.05064v1 [cs.LG])
23. Self-supervised Model Based on Masked Autoencoders Advance CT Scans Classification. (arXiv:2210.05073v1 [eess.IV])
24. Repainting and Imitating Learning for Lane Detection. (arXiv:2210.05097v1 [cs.CV])
25. 3D Matting: A Benchmark Study on Soft Segmentation Method for Pulmonary Nodules Applied in Computed Tomography. (arXiv:2210.05104v1 [eess.IV])
26. Deep learning model compression using network sensitivity and gradients. (arXiv:2210.05111v1 [cs.LG])
27. DA-VSR: Domain Adaptable Volumetric Super-Resolution For Medical Images. (arXiv:2210.05117v1 [eess.IV])
28. Boosting Adversarial Robustness From The Perspective of Effective Margin Regularization. (arXiv:2210.05118v1 [cs.LG])
29. Exploring CNN-based models for image's aesthetic score prediction with using ensemble. (arXiv:2210.05119v1 [cs.CV])
30. Tackling Instance-Dependent Label Noise with Dynamic Distribution Calibration. (arXiv:2210.05126v1 [cs.LG])
31. Multi-Object Navigation with dynamically learned neural implicit representations. (arXiv:2210.05129v1 [cs.CV])
32. ACRNet: Attention Cube Regression Network for Multi-view **Real-time** 3D Human Pose Estimation in Telemedicine. (arXiv:2210.05130v1 [cs.CV])
33. X-NeRF: Explicit Neural Radiance Field for Multi-Scene 360$^{\circ} $ Insufficient RGB-D Views. (arXiv:2210.05135v1 [cs.CV])
34. Markup-to-Image Diffusion Models with Scheduled Sampling. (arXiv:2210.05147v1 [cs.LG])
35. UGformer for Robust Left Atrium and Scar Segmentation Across Scanners. (arXiv:2210.05151v1 [cs.CV])
36. TriangleNet: Edge Prior Augmented Network for Semantic Segmentation through Cross-Task Consistency. (arXiv:2210.05152v1 [cs.CV])
37. The Fast and Accurate Approach to Detection and Segmentation of Melanoma Skin Cancer using Fine-tuned Yolov3 and SegNet Based on Deep Transfer Learning. (arXiv:2210.05167v1 [eess.IV])
38. Deep Fourier Up-Sampling. (arXiv:2210.05171v1 [cs.CV])
39. BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation. (arXiv:2210.05174v1 [cs.CV])
40. Variability Matters : Evaluating inter-rater variability in histopathology for robust cell detection. (arXiv:2210.05175v1 [cs.CV])
41. Fine-Grained Image Style Transfer with Visual Transformers. (arXiv:2210.05176v1 [cs.CV])
42. Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach. (arXiv:2210.05177v1 [cs.LG])
43. Robust Human Matting via Semantic Guidance. (arXiv:2210.05210v1 [cs.CV])
44. DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation. (arXiv:2210.05232v1 [cs.CV])
45. It Takes Two: Masked Appearance-Motion Modeling for Self-supervised Video Transformer Pre-training. (arXiv:2210.05234v1 [cs.CV])
46. Multi-site Diagnostic Classification Of Schizophrenia Using 3D CNN On Aggregated Task-based fMRI Data. (arXiv:2210.05240v1 [cs.CV])
47. Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization. (arXiv:2210.05242v1 [cs.CV])
48. Cluster-level pseudo-labelling for source-free cross-domain facial expression recognition. (arXiv:2210.05246v1 [cs.CV])
49. EOCSA: Predicting Prognosis of Epithelial Ovarian Cancer with Whole Slide Histopathological Images. (arXiv:2210.05258v1 [eess.IV])
50. EnsembleMOT: A Step towards Ensemble Learning of Multiple Object Tracking. (arXiv:2210.05278v1 [cs.CV])
51. ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning. (arXiv:2210.05280v1 [cs.CV])
52. Computer Vision based inspection on post-earthquake with UAV synthetic dataset. (arXiv:2210.05282v1 [cs.CV])
53. Weakly-Supervised Optical Flow Estimation for Time-of-Flight. (arXiv:2210.05298v1 [cs.CV])
54. CD-FSOD: A Benchmark for Cross-domain Few-shot Object Detection. (arXiv:2210.05311v1 [cs.CV])
55. Memory transformers for full context and high-resolution 3D Medical Segmentation. (arXiv:2210.05313v1 [cs.CV])
56. CASAPose: Class-Adaptive and Semantic-Aware Multi-Object Pose Estimation. (arXiv:2210.05318v1 [cs.CV])
57. Gender Stereotyping Impact in Facial Expression Recognition. (arXiv:2210.05332v1 [cs.CV])
58. MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v1 [cs.CV])
59. Printing variability of copy detection patterns. (arXiv:2210.05343v1 [cs.CR])
60. Neighbourhood Representative Sampling for Efficient End-to-end Video Quality Assessment. (arXiv:2210.05357v1 [cs.CV])
61. Uncertainty-Aware Unsupervised Image Deblurring with Deep Priors Guided by Domain Knowledge. (arXiv:2210.05361v1 [cs.CV])
62. Race Bias Analysis of Bona Fide Errors in face anti-spoofing. (arXiv:2210.05366v1 [cs.CV])
63. Stable and Efficient Adversarial Training through Local Linearization. (arXiv:2210.05373v1 [cs.LG])
64. Sequential Ensembling for Semantic Segmentation. (arXiv:2210.05387v1 [cs.CV])
65. PP-StructureV2: A Stronger Document Analysis System. (arXiv:2210.05391v1 [cs.CV])
66. TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning. (arXiv:2210.05392v1 [cs.CV])
67. Continual Learning by Modeling Intra-Class Variation. (arXiv:2210.05398v1 [cs.LG])
68. Exploring Interactions and Regulations in Collaborative Learning: An Interdisciplinary Multimodal Dataset. (arXiv:2210.05419v1 [cs.CV])
69. Learning to Locate Visual Answer in Video Corpus Using Question. (arXiv:2210.05423v1 [cs.CV])
70. Retinex Image **Enhancement** Based on Sequential Decomposition With a Plug-and-Play Framework. (arXiv:2210.05436v1 [eess.IV])
71. DPANET:Dual Pooling Attention Network for Semantic Segmentation. (arXiv:2210.05437v1 [cs.CV])
72. Parallel Augmentation and Dual **Enhancement** for Occluded Person Re-identification. (arXiv:2210.05438v1 [cs.CV])
73. CIRCA: comprehensible online system in support of chest X-rays-based COVID-19 diagnosis. (arXiv:2210.05440v1 [eess.IV])
74. Enabling **ISP**-less Low-Power Computer Vision. (arXiv:2210.05451v1 [cs.CV])
75. FreGAN: Exploiting Frequency Components for Training GANs under Limited Data. (arXiv:2210.05461v1 [cs.CV])
76. Large-to-small Image Resolution Asymmetry in Deep Metric Learning. (arXiv:2210.05463v1 [cs.CV])
77. Aggregating Layers for Deepfake Detection. (arXiv:2210.05478v1 [cs.CV])
78. Frequency-Aware Self-Supervised Monocular Depth Estimation. (arXiv:2210.05479v1 [cs.CV])
79. Map-free Visual Relocalization: Metric Pose Relative to a Single Image. (arXiv:2210.05494v1 [cs.CV])
80. Finding the global semantic representation in GAN through Frechet Mean. (arXiv:2210.05509v1 [cs.CV])
81. ViFiCon: Vision and Wireless Association Via Self-Supervised Contrastive Learning. (arXiv:2210.05513v1 [cs.CV])
82. DeepMLE: A Robust Deep Maximum Likelihood Estimator for Two-view Structure from Motion. (arXiv:2210.05517v1 [cs.CV])
83. Autonomous Asteroid Characterization Through Nanosatellite Swarming. (arXiv:2210.05518v1 [eess.SP])
84. Style-Guided Inference of Transformer for High-resolution Image Synthesis. (arXiv:2210.05533v1 [cs.CV])
85. Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation. (arXiv:2210.05534v1 [cs.CV])
86. What does a deep neural network confidently perceive? The effective dimension of high certainty class manifolds and their low confidence boundaries. (arXiv:2210.05546v1 [cs.LG])
87. Evaluating Unsupervised Denoising Requires Unsupervised Metrics. (arXiv:2210.05553v1 [cs.CV])
88. ViLPAct: A Benchmark for Compositional Generalization on Multimodal Human Activities. (arXiv:2210.05556v1 [cs.CV])
89. OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions. (arXiv:2210.05557v1 [cs.CV])
90. Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance. (arXiv:2210.05559v1 [cs.CV])
91. Hypergraph Convolutional Networks for Weakly-Supervised Semantic Segmentation. (arXiv:2210.05564v1 [cs.CV])
92. The Equalization Losses: Gradient-Driven Training for Long-tailed Object Recognition. (arXiv:2210.05566v1 [cs.CV])
93. Global Spectral Filter Memory Network for Video Object Segmentation. (arXiv:2210.05567v1 [cs.CV])
94. Improving Long-tailed Object Detection with Image-Level Supervision by Multi-Task Collaborative Learning. (arXiv:2210.05568v1 [cs.CV])
95. Motion Aware Self-Supervision for Generic Event Boundary Detection. (arXiv:2210.05574v1 [cs.CV])
96. Unifying Graph Embedding Features with Graph Convolutional Networks for Skeleton-based Action Recognition. (arXiv:2003.03007v2 [cs.CV] UPDATED)
97. Scene Graph to Image Generation with Contextualized Object Layout Refinement. (arXiv:2009.10939v4 [cs.CV] UPDATED)
98. Manipulation-Oriented Object Perception in Clutter through Affordance Coordinate Frames. (arXiv:2010.08202v4 [cs.RO] UPDATED)
99. Class-incremental learning: survey and performance evaluation on image classification. (arXiv:2010.15277v3 [cs.LG] UPDATED)
100. Content-Adaptive Pixel Discretization to Improve Model Robustness. (arXiv:2012.01699v4 [cs.CV] UPDATED)
101. MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v4 [cs.CV] UPDATED)
102. CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge Distillation. (arXiv:2104.05892v4 [eess.IV] UPDATED)
103. Geometric Model Checking of Continuous Space. (arXiv:2105.06194v4 [cs.LO] UPDATED)
104. Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v3 [cs.LG] UPDATED)
105. Exploring Localization for Self-supervised Fine-grained Contrastive Learning. (arXiv:2106.15788v4 [cs.CV] UPDATED)
106. Ordered Attention for Coherent Visual Storytelling. (arXiv:2108.02180v3 [cs.CV] UPDATED)
107. MEMO: Test Time Robustness via Adaptation and Augmentation. (arXiv:2110.09506v3 [cs.LG] UPDATED)
108. Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. (arXiv:2110.10832v4 [cs.LG] UPDATED)
109. Image-Based CLIP-Guided Essence Transfer. (arXiv:2110.12427v4 [cs.CV] UPDATED)
110. GenURL: A General Framework for Unsupervised Representation Learning. (arXiv:2110.14553v2 [cs.LG] UPDATED)
111. Lightweight Transformer Backbone for Medical Object Detection. (arXiv:2111.11546v3 [cs.CV] UPDATED)
112. Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition. (arXiv:2112.12175v4 [cs.CV] UPDATED)
113. Quasi-Taylor Samplers for Diffusion Generative Models based on Ideal Derivatives. (arXiv:2112.13339v2 [stat.ML] UPDATED)
114. An Optimal Transport Perspective on Unpaired Image Super-Resolution. (arXiv:2202.01116v2 [eess.IV] UPDATED)
115. Skeleton-Based Action Segmentation with Multi-Stage Spatial-Temporal Graph Convolutional Neural Networks. (arXiv:2202.01727v2 [cs.CV] UPDATED)
116. OG-SGG: Ontology-Guided Scene Graph Generation. A Case Study in Transfer Learning for Telepresence Robotics. (arXiv:2202.10201v2 [cs.RO] UPDATED)
117. Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v5 [eess.IV] UPDATED)
118. SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution. (arXiv:2204.02227v2 [cs.CV] UPDATED)
119. Application of belief functions to medical image segmentation: A review. (arXiv:2205.01733v2 [cs.CV] UPDATED)
120. Ultrafast image categorization in vivo and in silico. (arXiv:2205.03635v3 [q-bio.NC] UPDATED)
121. Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation. (arXiv:2205.08316v2 [cs.RO] UPDATED)
122. Unsupervised Learning of Depth, Camera Pose and Optical Flow from Monocular Video. (arXiv:2205.09821v2 [cs.CV] UPDATED)
123. Clustering as Attention: Unified Image Segmentation with Hierarchical Clustering. (arXiv:2205.09949v3 [cs.CV] UPDATED)
124. Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v3 [cs.CV] UPDATED)
125. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v3 [cs.CV] UPDATED)
126. Compressible-composable NeRF via Rank-residual Decomposition. (arXiv:2205.14870v2 [cs.CV] UPDATED)
127. Elucidating the Design Space of Diffusion-Based Generative Models. (arXiv:2206.00364v2 [cs.CV] UPDATED)
128. EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v5 [cs.CV] UPDATED)
129. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v5 [cs.AI] UPDATED)
130. Neural Deformable Voxel Grid for Fast Optimization of Dynamic View Synthesis. (arXiv:2206.07698v2 [cs.CV] UPDATED)
131. Object Representations as Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation. (arXiv:2207.00787v2 [cs.LG] UPDATED)
132. A Deep Dive into Deep Cluster. (arXiv:2207.11839v2 [cs.CV] UPDATED)
133. WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v2 [cs.CL] UPDATED)
134. HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions. (arXiv:2207.14284v3 [cs.CV] UPDATED)
135. One for All: One-stage Referring Expression Comprehension with Dynamic Reasoning. (arXiv:2208.00361v2 [cs.CV] UPDATED)
136. Rethinking the Evaluation of Unbiased Scene Graph Generation. (arXiv:2208.01909v2 [cs.CV] UPDATED)
137. Localized Sparse Incomplete Multi-view Clustering. (arXiv:2208.02998v2 [cs.CV] UPDATED)
138. Unified Fully and Timestamp Supervised Temporal Action Segmentation via Sequence to Sequence Translation. (arXiv:2209.00638v2 [cs.CV] UPDATED)
139. Deliberated Domain Bridging for Domain Adaptive Semantic Segmentation. (arXiv:2209.07695v3 [cs.CV] UPDATED)
140. Imbalanced Node Processing Method in Graph Neural Network Classification Task. (arXiv:2209.08514v2 [cs.LG] UPDATED)
141. Toward an Over-parameterized Direct-Fit Model of Visual Perception. (arXiv:2210.03850v2 [cs.CV] UPDATED)
142. AMPose: Alternatively Mixed Global-Local Attention Model for 3D Human Pose Estimation. (arXiv:2210.04216v2 [cs.CV] UPDATED)
143. Red-Teaming the Stable Diffusion Safety Filter. (arXiv:2210.04610v2 [cs.AI] UPDATED)
144. LMQFormer: A Laplace-Prior-Guided Mask Query Transformer for Lightweight Snow Removal. (arXiv:2210.04787v2 [cs.CV] UPDATED)
145. What the DAAM: Interpreting Stable Diffusion Using Cross Attention. (arXiv:2210.04885v2 [cs.CV] UPDATED)
## eess.IV
---
**25** new papers in eess.IV:-) 
1. Improving The Reconstruction Quality by Overfitted Decoder Bias in Neural Image Compression. (arXiv:2210.04898v1 [eess.IV])
2. Loop Unrolled Shallow Equilibrium Regularizer (LUSER) -- A Memory-Efficient Inverse Problem Solver. (arXiv:2210.04987v1 [eess.IV])
3. Using Deep Learning to Improve Early Diagnosis of Pneumonia in Underdeveloped Countries. (arXiv:2210.05023v1 [eess.IV])
4. Self-supervised Model Based on Masked Autoencoders Advance CT Scans Classification. (arXiv:2210.05073v1 [eess.IV])
5. 3D Matting: A Benchmark Study on Soft Segmentation Method for Pulmonary Nodules Applied in Computed Tomography. (arXiv:2210.05104v1 [eess.IV])
6. DA-VSR: Domain Adaptable Volumetric Super-Resolution For Medical Images. (arXiv:2210.05117v1 [eess.IV])
7. The Fast and Accurate Approach to Detection and Segmentation of Melanoma Skin Cancer using Fine-tuned Yolov3 and SegNet Based on Deep Transfer Learning. (arXiv:2210.05167v1 [eess.IV])
8. EOCSA: Predicting Prognosis of Epithelial Ovarian Cancer with Whole Slide Histopathological Images. (arXiv:2210.05258v1 [eess.IV])
9. Weakly-Supervised Optical Flow Estimation for Time-of-Flight. (arXiv:2210.05298v1 [cs.CV])
10. Retinex Image **Enhancement** Based on Sequential Decomposition With a Plug-and-Play Framework. (arXiv:2210.05436v1 [eess.IV])
11. CIRCA: comprehensible online system in support of chest X-rays-based COVID-19 diagnosis. (arXiv:2210.05440v1 [eess.IV])
12. Modeling of Energy Consumption and Streaming Video QoE using a Crowdsourcing Dataset. (arXiv:2210.05444v1 [eess.IV])
13. Enabling **ISP**-less Low-Power Computer Vision. (arXiv:2210.05451v1 [cs.CV])
14. High-precision Density Mapping of Marine Debris and Floating Plastics via Satellite Imagery. (arXiv:2210.05468v1 [eess.IV])
15. Simulating single-photon detector array sensors for depth imaging. (arXiv:2210.05644v1 [eess.IV])
16. Unifying Graph Embedding Features with Graph Convolutional Networks for Skeleton-based Action Recognition. (arXiv:2003.03007v2 [cs.CV] UPDATED)
17. Improving Workflow Integration with xPath: Design and Evaluation of a Human-AI Diagnosis System in Pathology. (arXiv:2006.12683v4 [cs.HC] UPDATED)
18. Automated Detection and Forecasting of COVID-19 using Deep Learning Techniques: A Review. (arXiv:2007.10785v4 [cs.LG] UPDATED)
19. MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v4 [cs.CV] UPDATED)
20. CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge Distillation. (arXiv:2104.05892v4 [eess.IV] UPDATED)
21. An Optimal Transport Perspective on Unpaired Image Super-Resolution. (arXiv:2202.01116v2 [eess.IV] UPDATED)
22. Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v5 [eess.IV] UPDATED)
23. Sensing Theorems for Unsupervised Learning in Linear Inverse Problems. (arXiv:2203.12513v2 [stat.ML] UPDATED)
24. Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v3 [cs.CV] UPDATED)
25. Channel Modeling for UAV-to-Ground Communications with Posture Variation and Fuselage Scattering Effect. (arXiv:2210.02245v2 [eess.SP] UPDATED)
## cs.LG
---
**235** new papers in cs.LG:-) 
1. Equivariant Shape-Conditioned Generation of 3D Molecules for Ligand-Based Drug Design. (arXiv:2210.04893v1 [physics.chem-ph])
2. Improving The Reconstruction Quality by Overfitted Decoder Bias in Neural Image Compression. (arXiv:2210.04898v1 [eess.IV])
3. Meta-Principled Family of Hyperparameter Scaling Strategies. (arXiv:2210.04909v1 [cs.LG])
4. NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields. (arXiv:2210.04932v1 [cs.RO])
5. Masked Autoencoders for Low dose CT denoising. (arXiv:2210.04944v1 [cs.CV])
6. Reaching Goals is Hard: Settling the Sample Complexity of the Stochastic Shortest Path. (arXiv:2210.04946v1 [cs.LG])
7. A Hybrid Active-Passive Approach to Imbalanced Nonstationary Data Stream Classification. (arXiv:2210.04949v1 [cs.LG])
8. f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation. (arXiv:2210.04955v1 [cs.CV])
9. Mining Causality from Continuous-time Dynamics Models: An Application to Tsunami Forecasting. (arXiv:2210.04958v1 [cs.LG])
10. Characterization of anomalous diffusion through convolutional transformers. (arXiv:2210.04959v1 [cs.LG])
11. Multi-step Planning for Automated Hyperparameter Optimization with OptFormer. (arXiv:2210.04971v1 [cs.LG])
12. Function-space regularized R\'enyi divergences. (arXiv:2210.04974v1 [stat.ML])
13. On Designing Day Ahead and Same Day Ridership Level Prediction Models for City-Scale Transit Networks Using Noisy APC Data. (arXiv:2210.04989v1 [cs.LG])
14. Learning with an Evolving Class Ontology. (arXiv:2210.04993v1 [cs.CV])
15. Sampling-based inference for large linear models, with application to linearised Laplace. (arXiv:2210.04994v1 [stat.ML])
16. FEAMOE: Fair, Explainable and Adaptive Mixture of Experts. (arXiv:2210.04995v1 [cs.LG])
17. Social Media Personal Event Notifier Using NLP and Machine Learning. (arXiv:2210.05001v1 [cs.CV])
18. Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware. (arXiv:2210.05006v1 [cs.LG])
19. The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective. (arXiv:2210.05021v1 [cs.LG])
20. Contrastive Video-Language Learning with Fine-grained Frame Sampling. (arXiv:2210.05039v1 [cs.LG])
21. Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling. (arXiv:2210.05043v1 [cs.CL])
22. Risk Automatic Prediction for Social Economy Companies using Camels. (arXiv:2210.05052v1 [cs.LG])
23. InQMAD: Incremental Quantum Measurement Anomaly Detection. (arXiv:2210.05061v1 [cs.LG])
24. Relational Attention: Generalizing Transformers for Graph-Structured Tasks. (arXiv:2210.05062v1 [cs.LG])
25. VER: Scaling On-Policy RL Leads to the Emergence of Navigation in Embodied Rearrangement. (arXiv:2210.05064v1 [cs.LG])
26. Self-supervised Model Based on Masked Autoencoders Advance CT Scans Classification. (arXiv:2210.05073v1 [eess.IV])
27. Approximation of nearly-periodic symplectic maps via structure-preserving neural networks. (arXiv:2210.05087v1 [cs.LG])
28. IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v1 [cs.CL])
29. COMBO: Pre-Training Representations of Binary Code Using Contrastive Learning. (arXiv:2210.05102v1 [cs.SE])
30. Functional Constrained Optimization for Risk Aversion and Sparsity Control. (arXiv:2210.05108v1 [math.OC])
31. Deep learning model compression using network sensitivity and gradients. (arXiv:2210.05111v1 [cs.LG])
32. Boosting Adversarial Robustness From The Perspective of Effective Margin Regularization. (arXiv:2210.05118v1 [cs.LG])
33. Human-AI Coordination via Human-Regularized Search and Learning. (arXiv:2210.05125v1 [cs.AI])
34. Tackling Instance-Dependent Label Noise with Dynamic Distribution Calibration. (arXiv:2210.05126v1 [cs.LG])
35. Multi-Object Navigation with dynamically learned neural implicit representations. (arXiv:2210.05129v1 [cs.CV])
36. Markup-to-Image Diffusion Models with Scheduled Sampling. (arXiv:2210.05147v1 [cs.LG])
37. DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability. (arXiv:2210.05148v1 [cs.SD])
38. DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning. (arXiv:2210.05150v1 [cs.LG])
39. Understanding the Failure of Batch Normalization for Transformers in NLP. (arXiv:2210.05153v1 [cs.CL])
40. Contrastive Trajectory Similarity Learning with Dual-Feature Attention. (arXiv:2210.05155v1 [cs.DB])
41. ConserWeightive Behavioral Cloning for Reliable Offline Reinforcement Learning. (arXiv:2210.05158v1 [cs.LG])
42. Combining datasets to increase the number of samples and improve model fitting. (arXiv:2210.05165v1 [stat.ML])
43. LARF: Two-level Attention-based Random Forests with a Mixture of Contamination Models. (arXiv:2210.05168v1 [cs.LG])
44. On Explainability in AI-Solutions: A Cross-Domain Survey. (arXiv:2210.05173v1 [cs.AI])
45. Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach. (arXiv:2210.05177v1 [cs.LG])
46. Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials. (arXiv:2210.05178v1 [cs.RO])
47. Edge-Cloud Cooperation for DNN Inference via Reinforcement Learning and Supervised Learning. (arXiv:2210.05182v1 [cs.LG])
48. Meta-Learning with Self-Improving Momentum Target. (arXiv:2210.05185v1 [cs.LG])
49. Broad-persistent Advice for Interactive Reinforcement Learning Scenarios. (arXiv:2210.05187v1 [cs.AI])
50. Neural Networks are Decision Trees. (arXiv:2210.05189v1 [cs.LG])
51. Kernelized multi-graph matching. (arXiv:2210.05206v1 [cs.LG])
52. On Scrambling Phenomena for Randomly Initialized Recurrent Networks. (arXiv:2210.05212v1 [cs.LG])
53. From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models. (arXiv:2210.05230v1 [cs.CL])
54. Planning Assembly Sequence with Graph Transformer. (arXiv:2210.05236v1 [cs.AI])
55. Multi-site Diagnostic Classification Of Schizophrenia Using 3D CNN On Aggregated Task-based fMRI Data. (arXiv:2210.05240v1 [cs.CV])
56. Efficient debiasing with contrastive weight pruning. (arXiv:2210.05247v1 [cs.LG])
57. Self-supervised debiasing using low rank regularization. (arXiv:2210.05248v1 [cs.LG])
58. Constrained Deployment Optimization in Integrated Access and Backhaul Networks. (arXiv:2210.05253v1 [cs.NI])
59. Rethinking the Event Coding Pipeline with Prompt Entailment. (arXiv:2210.05257v1 [cs.CL])
60. EOCSA: Predicting Prognosis of Epithelial Ovarian Cancer with Whole Slide Histopathological Images. (arXiv:2210.05258v1 [eess.IV])
61. Factors of Influence of the Overestimation Bias of Q-Learning. (arXiv:2210.05262v1 [stat.ML])
62. Component-Wise Natural Gradient Descent -- An Efficient Neural Network Optimization. (arXiv:2210.05268v1 [cs.LG])
63. Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design. (arXiv:2210.05274v1 [cs.LG])
64. RoHNAS: A Neural Architecture Search Framework with Conjoint Optimization for Adversarial Robustness and Hardware Efficiency of Convolutional and Capsule Networks. (arXiv:2210.05276v1 [cs.LG])
65. Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity. (arXiv:2210.05279v1 [cs.LG])
66. Computer Vision based inspection on post-earthquake with UAV synthetic dataset. (arXiv:2210.05282v1 [cs.CV])
67. Intrinsic Dimension for Large-Scale Geometric Learning. (arXiv:2210.05301v1 [cs.LG])
68. Learning Control Policies for Region Stabilization in Stochastic Systems. (arXiv:2210.05304v1 [cs.LG])
69. Learning Control Policies for Stochastic Systems with Reach-avoid Guarantees. (arXiv:2210.05308v1 [cs.LG])
70. Synthetic Model Combination: An Instance-wise Approach to Unsupervised Ensemble Learning. (arXiv:2210.05320v1 [cs.LG])
71. Label Noise-Robust Learning using a Confidence-Based Sieving Strategy. (arXiv:2210.05330v1 [cs.LG])
72. Generalization Analysis on Learning with a Concurrent Verifier. (arXiv:2210.05331v1 [cs.LG])
73. SGD with large step sizes learns sparse features. (arXiv:2210.05337v1 [cs.LG])
74. FusionDeepMF: A Dual Embedding based Deep Fusion Model for Recommendation. (arXiv:2210.05338v1 [cs.IR])
75. Constructing Prediction Intervals with Neural Networks: An Empirical Evaluation of Bootstrapping and Conformal Inference Methods. (arXiv:2210.05354v1 [stat.ML])
76. Multi-User Reinforcement Learning with Low Rank Rewards. (arXiv:2210.05355v1 [cs.LG])
77. Race Bias Analysis of Bona Fide Errors in face anti-spoofing. (arXiv:2210.05366v1 [cs.CV])
78. Learning Credit Assignment for Cooperative Reinforcement Learning. (arXiv:2210.05367v1 [cs.LG])
79. DeepPerform: An Efficient Approach for Performance Testing of Resource-Constrained Neural Networks. (arXiv:2210.05370v1 [cs.LG])
80. A global analysis of global optimisation. (arXiv:2210.05371v1 [cs.LG])
81. Stable and Efficient Adversarial Training through Local Linearization. (arXiv:2210.05373v1 [cs.LG])
82. Break the Wall Between Homophily and Heterophily for Graph Representation Learning. (arXiv:2210.05382v1 [cs.LG])
83. Sequential Ensembling for Semantic Segmentation. (arXiv:2210.05387v1 [cs.CV])
84. Scaling Directed Controller Synthesis via Reinforcement Learning. (arXiv:2210.05393v1 [cs.LG])
85. Computationally-efficient initialisation of GPs: The generalised variogram method. (arXiv:2210.05394v1 [cs.LG])
86. Continual Learning by Modeling Intra-Class Variation. (arXiv:2210.05398v1 [cs.LG])
87. LECO: Learnable Episodic Count for Task-Specific Intrinsic Reward. (arXiv:2210.05409v1 [cs.LG])
88. Class-Specific Explainability for Deep Time Series Classifiers. (arXiv:2210.05411v1 [cs.LG])
89. COVID-19-related Nepali Tweets Classification in a Low Resource Setting. (arXiv:2210.05425v1 [cs.CL])
90. Non-Asymptotic Analysis of a UCB-based Top Two Algorithm. (arXiv:2210.05431v1 [stat.ML])
91. QuCNN : A Quantum Convolutional Neural Network with Entanglement Based Backpropagation. (arXiv:2210.05443v1 [quant-ph])
92. Disentangling Causal Effects from Sets of Interventions in the Presence of Unobserved Confounders. (arXiv:2210.05446v1 [stat.ML])
93. Unlabelled Sample Compression Schemes for Intersection-Closed Classes and Extremal Classes. (arXiv:2210.05455v1 [cs.LG])
94. High-precision Density Mapping of Marine Debris and Floating Plastics via Satellite Imagery. (arXiv:2210.05468v1 [eess.IV])
95. Block Format Error Bounds and Optimal Block Size Selection. (arXiv:2210.05470v1 [cs.LG])
96. GENIE: Higher-Order Denoising Diffusion Solvers. (arXiv:2210.05475v1 [stat.ML])
97. Aggregating Layers for Deepfake Detection. (arXiv:2210.05478v1 [cs.CV])
98. Frequency-Aware Self-Supervised Monocular Depth Estimation. (arXiv:2210.05479v1 [cs.CV])
99. Architectural Optimization over Subgroups for Equivariant Neural Networks. (arXiv:2210.05484v1 [cs.LG])
100. Pooling Strategies for Simplicial Convolutional Networks. (arXiv:2210.05490v1 [eess.SP])
101. Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning. (arXiv:2210.05492v1 [cs.GT])
102. MAgNet: Mesh Agnostic Neural PDE Solver. (arXiv:2210.05495v1 [cs.LG])
103. Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration. (arXiv:2210.05506v1 [cs.LG])
104. Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data. (arXiv:2210.05508v1 [cs.DB])
105. Robust and Controllable Object-Centric Learning through Energy-based Models. (arXiv:2210.05519v1 [cs.LG])
106. A hybrid neural-network and finite-difference method for solving Poisson equation with jump discontinuities on interfaces. (arXiv:2210.05523v1 [math.NA])
107. What does a deep neural network confidently perceive? The effective dimension of high certainty class manifolds and their low confidence boundaries. (arXiv:2210.05546v1 [cs.LG])
108. Continual Training of Language Models for Few-Shot Learning. (arXiv:2210.05549v1 [cs.CL])
109. OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions. (arXiv:2210.05557v1 [cs.CV])
110. Causal and counterfactual views of missing data models. (arXiv:2210.05558v1 [stat.ME])
111. Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance. (arXiv:2210.05559v1 [cs.CV])
112. Schedule-Robust Online Continual Learning. (arXiv:2210.05561v1 [cs.LG])
113. Misspecified Phase Retrieval with Generative Priors. (arXiv:2210.05571v1 [stat.ML])
114. Knowledge-Driven New Drug Recommendation. (arXiv:2210.05572v1 [cs.LG])
115. Motion Aware Self-Supervision for Generic Event Boundary Detection. (arXiv:2210.05574v1 [cs.CV])
116. What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?. (arXiv:2210.05577v1 [cs.LG])
117. Benefits of Permutation-Equivariance in Auction Mechanisms. (arXiv:2210.05579v1 [cs.GT])
118. Local Function Complexity for Active Learning via Mixture of Gaussian Processes. (arXiv:1902.10664v4 [cs.LG] UPDATED)
119. QubitHD: A Stochastic Acceleration Method for HD Computing-Based Machine Learning. (arXiv:1911.12446v3 [cs.LG] UPDATED)
120. Generative Modeling with Denoising Auto-Encoders and Langevin Sampling. (arXiv:2002.00107v4 [stat.ML] UPDATED)
121. Unifying Graph Embedding Features with Graph Convolutional Networks for Skeleton-based Action Recognition. (arXiv:2003.03007v2 [cs.CV] UPDATED)
122. Automated Detection and Forecasting of COVID-19 using Deep Learning Techniques: A Review. (arXiv:2007.10785v4 [cs.LG] UPDATED)
123. NodeSig: Binary Node Embeddings via Random Walk Diffusion. (arXiv:2010.00261v2 [cs.LG] UPDATED)
124. Personalised Meta-path Generation for Heterogeneous GNNs. (arXiv:2010.13735v2 [cs.LG] UPDATED)
125. Class-incremental learning: survey and performance evaluation on image classification. (arXiv:2010.15277v3 [cs.LG] UPDATED)
126. Content-Adaptive Pixel Discretization to Improve Model Robustness. (arXiv:2012.01699v4 [cs.CV] UPDATED)
127. A New Look and Convergence Rate of Federated Multi-Task Learning with Laplacian Regularization. (arXiv:2102.07148v5 [cs.LG] UPDATED)
128. An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks. (arXiv:2102.10423v2 [cs.LG] UPDATED)
129. CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge Distillation. (arXiv:2104.05892v4 [eess.IV] UPDATED)
130. Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning. (arXiv:2104.10219v3 [eess.SY] UPDATED)
131. Laplace Matching for fast Approximate Inference in Latent Gaussian Models. (arXiv:2105.03109v2 [cs.LG] UPDATED)
132. Maximizing Mutual Information Across Feature and Topology Views for Learning Graph Representations. (arXiv:2105.06715v3 [cs.LG] UPDATED)
133. GAL: Gradient Assisted Learning for Decentralized Multi-Organization Collaborations. (arXiv:2106.01425v4 [cs.LG] UPDATED)
134. SemiFL: Semi-Supervised Federated Learning for Unlabeled Clients with Alternate Training. (arXiv:2106.01432v4 [cs.LG] UPDATED)
135. Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v3 [cs.LG] UPDATED)
136. Positively Weighted Kernel Quadrature via Subsampling. (arXiv:2107.09597v4 [math.NA] UPDATED)
137. Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v2 [cs.CR] UPDATED)
138. Generalizing Dynamic Mode Decomposition: Balancing Accuracy and Expressiveness in Koopman Approximations. (arXiv:2108.03712v3 [eess.SY] UPDATED)
139. Load Balancing in Compute Clusters with Delayed Feedback. (arXiv:2109.08548v2 [cs.DC] UPDATED)
140. MEMO: Test Time Robustness via Adaptation and Augmentation. (arXiv:2110.09506v3 [cs.LG] UPDATED)
141. Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. (arXiv:2110.10832v4 [cs.LG] UPDATED)
142. GenURL: A General Framework for Unsupervised Representation Learning. (arXiv:2110.14553v2 [cs.LG] UPDATED)
143. Adaptive Discretization in Online Reinforcement Learning. (arXiv:2110.15843v3 [stat.ML] UPDATED)
144. Distribution-Free Model for Community Detection. (arXiv:2111.07495v3 [cs.SI] UPDATED)
145. NeuralPDE: Modelling Dynamical Systems from Data. (arXiv:2111.07671v3 [cs.LG] UPDATED)
146. Robust On-Policy Sampling for Data-Efficient Policy Evaluation in Reinforcement Learning. (arXiv:2111.14552v2 [cs.LG] UPDATED)
147. NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation. (arXiv:2112.02721v2 [cs.CL] UPDATED)
148. RafterNet: Probabilistic predictions in multi-response regression. (arXiv:2112.03377v2 [cs.LG] UPDATED)
149. Quasi-Taylor Samplers for Diffusion Generative Models based on Ideal Derivatives. (arXiv:2112.13339v2 [stat.ML] UPDATED)
150. Non-Asymptotic Guarantees for Robust Statistical Learning under Infinite Variance Assumption. (arXiv:2201.03182v2 [stat.ML] UPDATED)
151. AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees. (arXiv:2201.07984v4 [cs.AI] UPDATED)
152. Understanding the Effects of Second-Order Approximations in Natural Policy Gradient Reinforcement Learning. (arXiv:2201.09104v2 [cs.LG] UPDATED)
153. Towards Safe Reinforcement Learning with a Safety Editor Policy. (arXiv:2201.12427v3 [cs.LG] UPDATED)
154. Efficient Reinforcement Learning in Block MDPs: A Model-free Representation Learning Approach. (arXiv:2202.00063v3 [cs.LG] UPDATED)
155. An Optimal Transport Perspective on Unpaired Image Super-Resolution. (arXiv:2202.01116v2 [eess.IV] UPDATED)
156. Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs. (arXiv:2202.05441v3 [cs.LG] UPDATED)
157. PFGE: Parsimonious Fast Geometric Ensembling of DNNs. (arXiv:2202.06658v6 [cs.LG] UPDATED)
158. End-to-End Training of Both Translation Models in the Back-Translation Framework. (arXiv:2202.08465v2 [cs.CL] UPDATED)
159. A Quantitative Geometric Approach to Neural-Network Smoothness. (arXiv:2203.01212v2 [cs.LG] UPDATED)
160. ZIN: When and How to Learn Invariance Without Environment Partition?. (arXiv:2203.05818v2 [cs.LG] UPDATED)
161. Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v5 [eess.IV] UPDATED)
162. Sensing Theorems for Unsupervised Learning in Linear Inverse Problems. (arXiv:2203.12513v2 [stat.ML] UPDATED)
163. FLUTE: A Scalable, Extensible Framework for High-Performance Federated Learning Simulations. (arXiv:2203.13789v2 [cs.LG] UPDATED)
164. Graph Neural Networks are Dynamic Programmers. (arXiv:2203.15544v3 [cs.LG] UPDATED)
165. Weakly supervised causal representation learning. (arXiv:2203.16437v3 [stat.ML] UPDATED)
166. Assessment of convolutional recurrent autoencoder network for learning wave propagation. (arXiv:2204.05573v2 [physics.flu-dyn] UPDATED)
167. Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD. (arXiv:2204.12446v4 [stat.ML] UPDATED)
168. RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning. (arXiv:2204.12581v3 [cs.LG] UPDATED)
169. Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v5 [cs.LG] UPDATED)
170. Gold-standard solutions to the Schr\"odinger equation using deep learning: How much physics do we need?. (arXiv:2205.09438v3 [cs.LG] UPDATED)
171. Set-based Meta-Interpolation for Few-Task Meta-Learning. (arXiv:2205.09990v2 [cs.LG] UPDATED)
172. Bayesian Active Learning with Fully Bayesian Gaussian Processes. (arXiv:2205.10186v2 [cs.LG] UPDATED)
173. Statistical inference as Green's functions. (arXiv:2205.11366v2 [cond-mat.stat-mech] UPDATED)
174. Instance-Based Uncertainty Estimation for Gradient-Boosted Regression Trees. (arXiv:2205.11412v2 [cs.LG] UPDATED)
175. Federated Distillation based Indoor Localization for IoT Networks. (arXiv:2205.11440v2 [eess.SP] UPDATED)
176. Learning Interacting Dynamical Systems with Latent Gaussian Process ODEs. (arXiv:2205.11894v2 [cs.LG] UPDATED)
177. Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v3 [cs.CV] UPDATED)
178. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v3 [cs.CV] UPDATED)
179. Generalization Bounds for Gradient Methods via Discrete and Continuous Prior. (arXiv:2205.13799v4 [cs.LG] UPDATED)
180. Flowification: Everything is a Normalizing Flow. (arXiv:2205.15209v2 [cs.LG] UPDATED)
181. Elucidating the Design Space of Diffusion-Based Generative Models. (arXiv:2206.00364v2 [cs.CV] UPDATED)
182. Collaborative Learning of Discrete Distributions under Heterogeneity and Communication Constraints. (arXiv:2206.00707v3 [stat.ML] UPDATED)
183. When does return-conditioned supervised learning work for offline reinforcement learning?. (arXiv:2206.01079v2 [cs.LG] UPDATED)
184. Excess risk analysis for epistemic uncertainty with application to variational inference. (arXiv:2206.01606v2 [stat.ML] UPDATED)
185. Is $L^2$ Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Network?. (arXiv:2206.02016v3 [cs.LG] UPDATED)
186. Infinite Recommendation Networks: A Data-Centric Approach. (arXiv:2206.02626v2 [cs.IR] UPDATED)
187. Adversarial Reprogramming Revisited. (arXiv:2206.03466v2 [cs.LG] UPDATED)
188. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v5 [cs.AI] UPDATED)
189. Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with Communication Compression. (arXiv:2206.03665v2 [cs.LG] UPDATED)
190. ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret. (arXiv:2206.04122v2 [cs.GT] UPDATED)
191. Universality and approximation bounds for echo state networks with random weights. (arXiv:2206.05669v2 [cs.LG] UPDATED)
192. Taxonomy of Benchmarks in Graph Representation Learning. (arXiv:2206.07729v3 [cs.LG] UPDATED)
193. Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning. (arXiv:2206.08686v2 [cs.RO] UPDATED)
194. Mirror Descent with Relative Smoothness in Measure Spaces, with application to Sinkhorn and EM. (arXiv:2206.08873v2 [math.OC] UPDATED)
195. EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. (arXiv:2206.09674v3 [cs.CL] UPDATED)
196. GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks. (arXiv:2206.09677v4 [cs.LG] UPDATED)
197. Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v2 [cs.LG] UPDATED)
198. pyKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models. (arXiv:2206.11460v2 [cs.LG] UPDATED)
199. Adversarial Robustness of Deep Neural Networks: A Survey from a Formal Verification Perspective. (arXiv:2206.12227v2 [cs.CR] UPDATED)
200. AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection. (arXiv:2206.15476v2 [cs.LG] UPDATED)
201. Object Representations as Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation. (arXiv:2207.00787v2 [cs.LG] UPDATED)
202. Error Analysis of Tensor-Train Cross Approximation. (arXiv:2207.04327v2 [cs.LG] UPDATED)
203. Automatic differentiation and the optimization of differential equation models in biology. (arXiv:2207.04487v2 [q-bio.QM] UPDATED)
204. A Newton-CG based barrier method for finding a second-order stationary point of nonconvex conic optimization with complexity guarantees. (arXiv:2207.05697v2 [math.OC] UPDATED)
205. Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v3 [cs.LG] UPDATED)
206. GEM-2: Next Generation Molecular Property Prediction Network by Modeling Full-range Many-body Interactions. (arXiv:2208.05863v3 [cs.LG] UPDATED)
207. Algorithmic Differentiation for Automatized Modelling of Machine Learned Force Fields. (arXiv:2208.12104v2 [physics.chem-ph] UPDATED)
208. Graph Neural Networks for Low-Energy Event Classification & Reconstruction in IceCube. (arXiv:2209.03042v3 [hep-ex] UPDATED)
209. CWP: Instance complexity weighted channel-wise soft masks for network pruning. (arXiv:2209.03534v2 [cs.LG] UPDATED)
210. Multiobjective Ranking and Selection Using Stochastic Kriging. (arXiv:2209.03919v2 [stat.ML] UPDATED)
211. Langevin Autoencoders for Learning Deep Latent Variable Models. (arXiv:2209.07036v2 [cs.LG] UPDATED)
212. Imbalanced Node Processing Method in Graph Neural Network Classification Task. (arXiv:2209.08514v2 [cs.LG] UPDATED)
213. Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning. (arXiv:2209.10113v2 [cs.LG] UPDATED)
214. Concordance based Survival Cobra with regression type weak learners. (arXiv:2209.11919v3 [stat.ML] UPDATED)
215. Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social-Text Classification. (arXiv:2209.13017v2 [cs.CL] UPDATED)
216. LL-GNN: Low Latency Graph Neural Networks on FPGAs for Particle Detectors. (arXiv:2209.14065v3 [cs.AR] UPDATED)
217. Towards Lightweight Black-Box Attacks against Deep Neural Networks. (arXiv:2209.14826v3 [cs.LG] UPDATED)
218. GIDN: A Lightweight Graph Inception Diffusion Network for High-efficient Link Prediction. (arXiv:2210.01301v2 [cs.LG] UPDATED)
219. Group Personalized Federated Learning. (arXiv:2210.01863v2 [stat.ML] UPDATED)
220. GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks. (arXiv:2210.02040v3 [cs.LG] UPDATED)
221. CW-ERM: Improving Autonomous Driving Planning with Closed-loop Weighted Empirical Risk Minimization. (arXiv:2210.02174v2 [cs.LG] UPDATED)
222. Null Hypothesis Test for Anomaly Detection. (arXiv:2210.02226v2 [hep-ph] UPDATED)
223. Trust in Motion: Capturing Trust Ascendancy in Open-Source Projects using Hybrid AI. (arXiv:2210.02656v2 [cs.SE] UPDATED)
224. Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data. (arXiv:2210.02974v2 [cs.AI] UPDATED)
225. Understanding Practices, Challenges, and Opportunities for User-Driven Algorithm Auditing in Industry Practice. (arXiv:2210.03709v2 [cs.HC] UPDATED)
226. GRANITE: A Graph Neural Network Model for Basic Block Throughput Estimation. (arXiv:2210.03894v2 [cs.LG] UPDATED)
227. Asymptotically Unbiased Instance-wise Regularized Partial AUC Optimization: Theory and Algorithm. (arXiv:2210.03967v2 [cs.LG] UPDATED)
228. Event-Driven Tactile Learning with Various Location Spiking Neurons. (arXiv:2210.04277v2 [cs.NE] UPDATED)
229. Asymmetric Temperature Scaling Makes Larger Networks Teach Well Again. (arXiv:2210.04427v2 [cs.LG] UPDATED)
230. Everything is Varied: The Surprising Impact of Individual Variation on ML Robustness in Medicine. (arXiv:2210.04555v2 [cs.LG] UPDATED)
231. Red-Teaming the Stable Diffusion Safety Filter. (arXiv:2210.04610v2 [cs.AI] UPDATED)
232. AutoMap: Automatic Medical Code Mapping for Clinical Prediction Model Deployment. (arXiv:2203.02446v1 [cs.AI] CROSS LISTED)
233. ASTRO: An AST-Assisted Approach for Generalizable Neural Clone Detection. (arXiv:2208.08067v1 [cs.SE] CROSS LISTED)
234. Over-the-Air Federated Learning with Privacy Protection via Correlated Additive Perturbations. (arXiv:2210.02235v1 [cs.LG] CROSS LISTED)
235. Detecting Label Errors in Token Classification Data. (arXiv:2210.03920v1 [cs.CL] CROSS LISTED)
## cs.AI
---
**140** new papers in cs.AI:-) 
1. NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields. (arXiv:2210.04932v1 [cs.RO])
2. Reaching Goals is Hard: Settling the Sample Complexity of the Stochastic Shortest Path. (arXiv:2210.04946v1 [cs.LG])
3. Generating Executable Action Plans with Environmentally-Aware Language Models. (arXiv:2210.04964v1 [cs.RO])
4. Multi-step Planning for Automated Hyperparameter Optimization with OptFormer. (arXiv:2210.04971v1 [cs.LG])
5. Domain-guided data augmentation for deep learning on medical imaging. (arXiv:2210.04977v1 [cs.CV])
6. Label-free segmentation from cardiac ultrasound using self-supervised learning. (arXiv:2210.04979v1 [cs.CV])
7. Simulating Coverage Path Planning with Roomba. (arXiv:2210.04988v1 [cs.RO])
8. Extracting or Guessing? Improving Faithfulness of Event Temporal Relation Extraction. (arXiv:2210.04992v1 [cs.CL])
9. Learning with an Evolving Class Ontology. (arXiv:2210.04993v1 [cs.CV])
10. Sampling-based inference for large linear models, with application to linearised Laplace. (arXiv:2210.04994v1 [stat.ML])
11. FEAMOE: Fair, Explainable and Adaptive Mixture of Experts. (arXiv:2210.04995v1 [cs.LG])
12. Graph2Vid: Flow graph to Video Grounding forWeakly-supervised Multi-Step Localization. (arXiv:2210.04996v1 [cs.CV])
13. Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware. (arXiv:2210.05006v1 [cs.LG])
14. Exploring Adaptive MCTS with TD Learning in miniXCOM. (arXiv:2210.05014v1 [cs.AI])
15. Generalized Optimality Guarantees for Solving Continuous Observation POMDPs through Particle Belief MDP Approximation. (arXiv:2210.05015v1 [cs.AI])
16. Probabilities of Causation: Adequate Size of Experimental and Observational Samples. (arXiv:2210.05027v1 [cs.AI])
17. Unit Selection: Case Study and Comparison with A/B Test Heuristic. (arXiv:2210.05030v1 [cs.AI])
18. Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis. (arXiv:2210.05035v1 [cs.CL])
19. Automated Audio Captioning via Fusion of Low- and High- Dimensional Features. (arXiv:2210.05037v1 [cs.SD])
20. Neurosymbolic Programming for Science. (arXiv:2210.05050v1 [cs.AI])
21. Relational Attention: Generalizing Transformers for Graph-Structured Tasks. (arXiv:2210.05062v1 [cs.LG])
22. VER: Scaling On-Policy RL Leads to the Emergence of Navigation in Embodied Rearrangement. (arXiv:2210.05064v1 [cs.LG])
23. How to construct the symmetric cycle of length 5 using Haj\'os construction with an adapted Rank Genetic Algorithm. (arXiv:2210.05080v1 [math.CO])
24. Checks and Strategies for Enabling Code-Switched Machine Translation. (arXiv:2210.05096v1 [cs.CL])
25. Leveraging Artificial Intelligence on Binary Code Comprehension. (arXiv:2210.05103v1 [cs.SE])
26. Human-AI Coordination via Human-Regularized Search and Learning. (arXiv:2210.05125v1 [cs.AI])
27. Tackling Instance-Dependent Label Noise with Dynamic Distribution Calibration. (arXiv:2210.05126v1 [cs.LG])
28. DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability. (arXiv:2210.05148v1 [cs.SD])
29. DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning. (arXiv:2210.05150v1 [cs.LG])
30. ConserWeightive Behavioral Cloning for Reliable Offline Reinforcement Learning. (arXiv:2210.05158v1 [cs.LG])
31. Can Language Models Be Specific? How?. (arXiv:2210.05159v1 [cs.CL])
32. LARF: Two-level Attention-based Random Forests with a Mixture of Contamination Models. (arXiv:2210.05168v1 [cs.LG])
33. On Explainability in AI-Solutions: A Cross-Domain Survey. (arXiv:2210.05173v1 [cs.AI])
34. Fine-Grained Image Style Transfer with Visual Transformers. (arXiv:2210.05176v1 [cs.CV])
35. Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach. (arXiv:2210.05177v1 [cs.LG])
36. Broad-persistent Advice for Interactive Reinforcement Learning Scenarios. (arXiv:2210.05187v1 [cs.AI])
37. Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval. (arXiv:2210.05188v1 [cs.CL])
38. Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA. (arXiv:2210.05197v1 [cs.CL])
39. On Scrambling Phenomena for Randomly Initialized Recurrent Networks. (arXiv:2210.05212v1 [cs.LG])
40. CHAE: Fine-Grained Controllable Story Generation with Characters, Actions and Emotions. (arXiv:2210.05221v1 [cs.CL])
41. Regret Analysis of the Stochastic Direct Search Method for Blind Resource Allocation. (arXiv:2210.05222v1 [cs.AI])
42. Planning Assembly Sequence with Graph Transformer. (arXiv:2210.05236v1 [cs.AI])
43. Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization. (arXiv:2210.05242v1 [cs.CV])
44. PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction. (arXiv:2210.05245v1 [cs.CL])
45. Cluster-level pseudo-labelling for source-free cross-domain facial expression recognition. (arXiv:2210.05246v1 [cs.CV])
46. Efficient debiasing with contrastive weight pruning. (arXiv:2210.05247v1 [cs.LG])
47. Self-supervised debiasing using low rank regularization. (arXiv:2210.05248v1 [cs.LG])
48. Deep Spectro-temporal Artifacts for Detecting Synthesized Speech. (arXiv:2210.05254v1 [cs.SD])
49. Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling. (arXiv:2210.05261v1 [cs.CL])
50. Factors of Influence of the Overestimation Bias of Q-Learning. (arXiv:2210.05262v1 [stat.ML])
51. GAN You Hear Me? Reclaiming Unconditional Speech Synthesis from Diffusion Models. (arXiv:2210.05271v1 [cs.SD])
52. Intrinsic Dimension for Large-Scale Geometric Learning. (arXiv:2210.05301v1 [cs.LG])
53. Learning Control Policies for Region Stabilization in Stochastic Systems. (arXiv:2210.05304v1 [cs.LG])
54. Learning Control Policies for Stochastic Systems with Reach-avoid Guarantees. (arXiv:2210.05308v1 [cs.LG])
55. Client Error Clustering Approaches in Content Delivery Networks (CDN). (arXiv:2210.05314v1 [cs.NI])
56. A Causal Analysis of Harm. (arXiv:2210.05327v1 [cs.AI])
57. A new perspective on Digital Twins: Imparting intelligence and agency to entities. (arXiv:2210.05350v1 [cs.HC])
58. Neighbourhood Representative Sampling for Efficient End-to-end Video Quality Assessment. (arXiv:2210.05357v1 [cs.CV])
59. Mind's Eye: Grounded Language Model Reasoning through Simulation. (arXiv:2210.05359v1 [cs.CL])
60. Break the Wall Between Homophily and Heterophily for Graph Representation Learning. (arXiv:2210.05382v1 [cs.LG])
61. Scaling Directed Controller Synthesis via Reinforcement Learning. (arXiv:2210.05393v1 [cs.LG])
62. Continual Learning by Modeling Intra-Class Variation. (arXiv:2210.05398v1 [cs.LG])
63. Code Librarian: A Software Package Recommendation System. (arXiv:2210.05406v1 [cs.SE])
64. LECO: Learnable Episodic Count for Task-Specific Intrinsic Reward. (arXiv:2210.05409v1 [cs.LG])
65. A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning. (arXiv:2210.05448v1 [cs.MA])
66. Low Complexity Convolutional Neural Networks for Equalization in Optical Fiber Transmission. (arXiv:2210.05454v1 [eess.SP])
67. Instance Regularization for Discriminative Language Model Pre-training. (arXiv:2210.05471v1 [cs.CL])
68. Frequency-Aware Self-Supervised Monocular Depth Estimation. (arXiv:2210.05479v1 [cs.CV])
69. Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning. (arXiv:2210.05492v1 [cs.GT])
70. Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks. (arXiv:2210.05498v1 [cs.CL])
71. Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration. (arXiv:2210.05506v1 [cs.LG])
72. Bi-Phase Enhanced IVFPQ for Time-Efficient Ad-hoc Retrieval. (arXiv:2210.05521v1 [cs.IR])
73. Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems. (arXiv:2210.05528v1 [cs.CL])
74. Continual Training of Language Models for Few-Shot Learning. (arXiv:2210.05549v1 [cs.CL])
75. OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions. (arXiv:2210.05557v1 [cs.CV])
76. Knowledge-Driven New Drug Recommendation. (arXiv:2210.05572v1 [cs.LG])
77. Motion Aware Self-Supervision for Generic Event Boundary Detection. (arXiv:2210.05574v1 [cs.CV])
78. Benefits of Permutation-Equivariance in Auction Mechanisms. (arXiv:2210.05579v1 [cs.GT])
79. From Eye-blinks to State Construction: Diagnostic Benchmarks for Online Representation Learning. (arXiv:2011.04590v4 [cs.AI] UPDATED)
80. Geometric Model Checking of Continuous Space. (arXiv:2105.06194v4 [cs.LO] UPDATED)
81. Load Balancing in Compute Clusters with Delayed Feedback. (arXiv:2109.08548v2 [cs.DC] UPDATED)
82. The Tensor Brain: A Unified Theory of Perception, Memory and Semantic Decoding. (arXiv:2109.13392v3 [cs.AI] UPDATED)
83. Double Trouble: How to not explain a text classifier's decisions using counterfactuals synthesized by masked language models?. (arXiv:2110.11929v4 [cs.CL] UPDATED)
84. GenURL: A General Framework for Unsupervised Representation Learning. (arXiv:2110.14553v2 [cs.LG] UPDATED)
85. The signature and cusp geometry of hyperbolic knots. (arXiv:2111.15323v3 [math.GT] UPDATED)
86. NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation. (arXiv:2112.02721v2 [cs.CL] UPDATED)
87. Frontiers in Collective Intelligence: A Workshop Report. (arXiv:2112.06864v2 [cs.AI] UPDATED)
88. Multi-Instance Training for Question Answering Across Table and Linked Text. (arXiv:2112.07337v2 [cs.CL] UPDATED)
89. AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees. (arXiv:2201.07984v4 [cs.AI] UPDATED)
90. Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v5 [cs.CL] UPDATED)
91. Towards Safe Reinforcement Learning with a Safety Editor Policy. (arXiv:2201.12427v3 [cs.LG] UPDATED)
92. Efficient Reinforcement Learning in Block MDPs: A Model-free Representation Learning Approach. (arXiv:2202.00063v3 [cs.LG] UPDATED)
93. PFGE: Parsimonious Fast Geometric Ensembling of DNNs. (arXiv:2202.06658v6 [cs.LG] UPDATED)
94. Efficient Policy Generation in Multi-Agent Systems via Hypergraph Neural Network. (arXiv:2203.03265v2 [cs.AI] UPDATED)
95. ZIN: When and How to Learn Invariance Without Environment Partition?. (arXiv:2203.05818v2 [cs.LG] UPDATED)
96. Automating Code Review Activities by Large-Scale Pre-training. (arXiv:2203.09095v2 [cs.SE] UPDATED)
97. Graph Neural Networks are Dynamic Programmers. (arXiv:2203.15544v3 [cs.LG] UPDATED)
98. SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution. (arXiv:2204.02227v2 [cs.CV] UPDATED)
99. Learning Generalized Policy Automata for Relational Stochastic Shortest Path Problems. (arXiv:2204.04301v3 [cs.AI] UPDATED)
100. Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v3 [cs.CL] UPDATED)
101. TropeTwist: Trope-based Narrative Structure Generation. (arXiv:2204.09672v2 [cs.AI] UPDATED)
102. RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning. (arXiv:2204.12581v3 [cs.LG] UPDATED)
103. Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v5 [cs.LG] UPDATED)
104. Unsupervised Learning of Depth, Camera Pose and Optical Flow from Monocular Video. (arXiv:2205.09821v2 [cs.CV] UPDATED)
105. Set-based Meta-Interpolation for Few-Task Meta-Learning. (arXiv:2205.09990v2 [cs.LG] UPDATED)
106. TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v3 [cs.CV] UPDATED)
107. Elucidating the Design Space of Diffusion-Based Generative Models. (arXiv:2206.00364v2 [cs.CV] UPDATED)
108. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v5 [cs.AI] UPDATED)
109. ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret. (arXiv:2206.04122v2 [cs.GT] UPDATED)
110. TwiBot-22: Towards Graph-Based Twitter Bot Detection. (arXiv:2206.04564v5 [cs.SI] UPDATED)
111. How to talk so AI will learn: Instructions, descriptions, and autonomy. (arXiv:2206.07870v3 [cs.AI] UPDATED)
112. Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning. (arXiv:2206.08686v2 [cs.RO] UPDATED)
113. EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. (arXiv:2206.09674v3 [cs.CL] UPDATED)
114. GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks. (arXiv:2206.09677v4 [cs.LG] UPDATED)
115. Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v2 [cs.LG] UPDATED)
116. pyKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models. (arXiv:2206.11460v2 [cs.LG] UPDATED)
117. DialogID: A Dialogic Instruction Dataset for Improving Teaching Effectiveness in Online Environments. (arXiv:2206.12034v2 [cs.CL] UPDATED)
118. Learning Semantics-Aware Locomotion Skills from Human Demonstration. (arXiv:2206.13631v2 [cs.RO] UPDATED)
119. Object Representations as Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation. (arXiv:2207.00787v2 [cs.LG] UPDATED)
120. Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v3 [cs.LG] UPDATED)
121. WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v2 [cs.CL] UPDATED)
122. Localized Sparse Incomplete Multi-view Clustering. (arXiv:2208.02998v2 [cs.CV] UPDATED)
123. Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim Verification with Pattern Exploiting Training. (arXiv:2208.08749v2 [cs.CL] UPDATED)
124. Differentiable Bilevel Programming for Stackelberg Congestion Games. (arXiv:2209.07618v2 [cs.GT] UPDATED)
125. Incorporating Causal Analysis into Diversified and Logical Response Generation. (arXiv:2209.09482v2 [cs.CL] UPDATED)
126. Macro-Action-Based Multi-Agent/Robot Deep Reinforcement Learning under Partial Observability. (arXiv:2209.10003v2 [cs.AI] UPDATED)
127. Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning. (arXiv:2209.10113v2 [cs.LG] UPDATED)
128. DeepVARwT: Deep Learning for a VAR Model with Trend. (arXiv:2209.10587v2 [stat.ME] UPDATED)
129. Concordance based Survival Cobra with regression type weak learners. (arXiv:2209.11919v3 [stat.ML] UPDATED)
130. GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks. (arXiv:2210.02040v3 [cs.LG] UPDATED)
131. Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data. (arXiv:2210.02974v2 [cs.AI] UPDATED)
132. Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization. (arXiv:2210.03029v2 [cs.CL] UPDATED)
133. Understanding Practices, Challenges, and Opportunities for User-Driven Algorithm Auditing in Industry Practice. (arXiv:2210.03709v2 [cs.HC] UPDATED)
134. Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT. (arXiv:2210.04186v2 [cs.CL] UPDATED)
135. Event-Driven Tactile Learning with Various Location Spiking Neurons. (arXiv:2210.04277v2 [cs.NE] UPDATED)
136. The Small Solution Hypothesis for MAPF on Directed Graphs Is True. (arXiv:2210.04590v2 [cs.AI] UPDATED)
137. Red-Teaming the Stable Diffusion Safety Filter. (arXiv:2210.04610v2 [cs.AI] UPDATED)
138. Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks. (arXiv:2210.04834v2 [cs.CL] UPDATED)
139. AutoMap: Automatic Medical Code Mapping for Clinical Prediction Model Deployment. (arXiv:2203.02446v1 [cs.AI] CROSS LISTED)
140. Motion Planning on Visual Manifolds. (arXiv:2210.04047v1 [cs.RO] CROSS LISTED)

