# Your interest papers
---
## cs.CV
---
### Object-Guided Day-**Night** Visual Localization in Urban Scenes. (arXiv:2202.04445v1 [cs.CV])
- Authors : Assia Benbihi, dric Pradalier, ej Chum
- Link : [http://arxiv.org/abs/2202.04445](http://arxiv.org/abs/2202.04445)
> ABSTRACT  :  We introduce Object-Guided Localization (OGuL) based on a novel method of local-feature matching. Direct matching of local features is sensitive to significant changes in illumination. In contrast, object detection often survives severe changes in lighting conditions. The proposed method first detects semantic objects and establishes correspondences of those objects between images. Object correspondences provide local coarse alignment of the images in the form of a planar homography. These homographies are consequently used to guide the matching of local features. Experiments on standard urban localization datasets (Aachen, Extended-CMU-Season, RobotCar-Season) show that OGuL significantly improves localization results with as simple local features as SIFT, and its performance competes with the state-of-the-art CNN-based methods trained for day-to-**night** localization.  
### Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains. (arXiv:2105.14391v2 [cs.CV] UPDATED)
- Authors : Bowen Wen, Chaitanya Mitash, Kostas Bekris
- Link : [http://arxiv.org/abs/2105.14391](http://arxiv.org/abs/2105.14391)
> ABSTRACT  :  Tracking the 6D pose of objects in video sequences is important for robot manipulation. This work presents se(3)-TrackNet, a data-driven optimization approach for long term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained solely with synthetic data can work effectively over real images. Comprehensive experiments over multiple benchmarks show se(3)-TrackNet achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach runs in **real time** at 90.9Hz. Code, data and supplementary video for this project are available at https://github.com/wenbowen123/iros20-6d-pose-tracking  
### PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v5 [cs.CV] UPDATED)
- Authors : Wenhai Wang, Enze Xie, Xiang Li, Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao
- Link : [http://arxiv.org/abs/2106.13797](http://arxiv.org/abs/2106.13797)
> ABSTRACT  :  Transformer recently has presented encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVTv1) by adding three designs, including (1) linear complexity attention layer, (2) overlapping patch embedding, and (3) convolutional feed-forward network. With these modifications, PVTv2 reduces the computational complexity of PVTv1 to linear and achieves significant improvements on fundamental vision tasks such as classification, detection, and segmentation. Notably, the proposed PVTv2 achieves comparable or better performances than recent works such as **Swin** Transformer. We hope this work will facilitate state-of-the-art Transformer researches in computer vision. Code is available at https://github.com/whai362/PVT.  
### Wavelet-Based Network For **High Dynamic Range** Imaging. (arXiv:2108.01434v2 [eess.IV] UPDATED)
- Authors : Tianhong Dai, Wei Li, Xilei Cao, Jianzhuang Liu, Xu Jia, Ales Leonardis, Youliang Yan, Shanxin Yuan
- Link : [http://arxiv.org/abs/2108.01434](http://arxiv.org/abs/2108.01434)
> ABSTRACT  :  **High dynamic range** (**HDR**) imaging from multiple low dynamic range (LDR) images has been suffering from ghosting artifacts caused by scene and objects motion. Existing methods, such as optical flow based and end-to-end deep learning based solutions, are error-prone either in detail **restoration** or ghosting artifacts removal. Comprehensive empirical evidence shows that ghosting artifacts caused by large foreground motion are mainly low-frequency signals and the details are mainly high-frequency signals. In this work, we propose a novel frequency-guided end-to-end deep neural network (F**HDR**Net) to conduct **HDR** fusion in the frequency domain, and Discrete Wavelet Transform (DWT) is used to decompose inputs into different frequency bands. The low-frequency signals are used to avoid specific ghosting artifacts, while the high-frequency signals are used for preserving details. Using a U-Net as the backbone, we propose two novel modules: merging module and frequency-guided upsampling module. The merging module applies the attention mechanism to the low-frequency components to deal with the ghost caused by large foreground motion. The frequency-guided upsampling module reconstructs details from multiple frequency-specific components with rich details. In addition, a new RAW dataset is created for training and evaluating multi-frame **HDR** imaging algorithms in the RAW domain. Extensive experiments are conducted on public datasets and our RAW dataset, showing that the proposed F**HDR**Net achieves state-of-the-art performance.  
### FBSNet: A Fast **Bilateral** Symmetrical Network for Real-Time Semantic Segmentation. (arXiv:2109.00699v2 [cs.CV] UPDATED)
- Authors : Guangwei Gao, Guoan Xu, Juncheng Li, Yi Yu, Huimin Lu, Jian Yang
- Link : [http://arxiv.org/abs/2109.00699](http://arxiv.org/abs/2109.00699)
> ABSTRACT  :  **Real-time** semantic segmentation, which can be visually understood as the pixel-level classification task on the input image, currently has broad application prospects, especially in the fast-developing fields of autonomous driving and drone navigation. However, the huge burden of calculation together with redundant parameters are still the obstacles to its technological development. In this paper, we propose a Fast **Bilateral** Symmetrical Network (FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a symmetrical encoder-decoder structure with two branches, semantic information branch, and spatial detail branch. The semantic information branch is the main branch with deep network architecture to acquire the contextual information of the input image and meanwhile acquire sufficient receptive field. While spatial detail branch is a shallow and simple network used to establish local dependencies of each pixel for preserving details, which is essential for restoring the original resolution during the decoding phase. Meanwhile, a feature aggregation module (FAM) is designed to effectively combine the output features of the two branches. The experimental results of Cityscapes and CamVid show that the proposed FBSNet can strike a good balance between accuracy and efficiency. Specifically, it obtains 70.9\% and 68.9\% mIoU along with the inference speed of 90 fps and 120 fps on these two test datasets, respectively, with only 0.62 million parameters on a single RTX 2080Ti GPU.  
### Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v2 [eess.IV] UPDATED)
- Authors : Amir Atapour
- Link : [http://arxiv.org/abs/2202.02832](http://arxiv.org/abs/2202.02832)
> ABSTRACT  :  Convolutional Neural Networks have demonstrated human-level performance in the classification of melanoma and other skin lesions, but evident performance disparities between differing skin tones should be addressed before widespread deployment. In this work, we utilise a modified variational autoencoder to uncover skin tone bias in datasets commonly used as benchmarks. We propose an efficient yet effective algorithm for automatically labelling the skin tone of lesion images, and use this to annotate the benchmark ISIC dataset. We subsequently use two leading bias unlearning techniques to mitigate skin tone bias. Our experimental results provide evidence that our skin tone detection algorithm outperforms existing solutions and that unlearning skin tone improves generalisation and can reduce the performance disparity between melanoma detection in lighter and **dark**er skin tones.  
### Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v2 [cs.CV] UPDATED)
- Authors : Xavier Suau, Nivedha Sivakumar, Luca Zappella, Nicholas Apostoloff
- Link : [http://arxiv.org/abs/2202.03586](http://arxiv.org/abs/2202.03586)
> ABSTRACT  :  As the use of deep learning in high impact domains becomes ubiquitous, it is increasingly important to assess the resilience of models. One such high impact domain is that of face recognition, with real world applications involving images affected by various degradations, such as motion blur or high **exposure**. Moreover, images captured across different attributes, such as gender and race, can also challenge the robustness of a face recognition algorithm. While traditional summary statistics suggest that the aggregate performance of face recognition models has continued to improve, these metrics do not directly measure the robustness or fairness of the models. Visual Psychophysics Sensitivity Analysis (VPSA) [1] provides a way to pinpoint the individual causes of failure by way of introducing incremental perturbations in the data. However, perturbations may affect subgroups differently. In this paper, we propose a new fairness evaluation based on robustness in the form of a generic framework that extends VPSA. With this framework, we can analyze the ability of a model to perform fairly for different subgroups of a population affected by perturbations, and pinpoint the exact failure modes for a subgroup by measuring targeted robustness. With the increasing focus on the fairness of models, we use face recognition as an example application of our framework and propose to compactly visualize the fairness analysis of a model via AUC matrices. We analyze the performance of common face recognition models and empirically show that certain subgroups are at a disadvantage when images are perturbed, thereby uncovering trends that were not visible using the model's performance on subgroups without perturbations.  
## eess.IV
---
### Wavelet-Based Network For **High Dynamic Range** Imaging. (arXiv:2108.01434v2 [eess.IV] UPDATED)
- Authors : Tianhong Dai, Wei Li, Xilei Cao, Jianzhuang Liu, Xu Jia, Ales Leonardis, Youliang Yan, Shanxin Yuan
- Link : [http://arxiv.org/abs/2108.01434](http://arxiv.org/abs/2108.01434)
> ABSTRACT  :  **High dynamic range** (**HDR**) imaging from multiple low dynamic range (LDR) images has been suffering from ghosting artifacts caused by scene and objects motion. Existing methods, such as optical flow based and end-to-end deep learning based solutions, are error-prone either in detail **restoration** or ghosting artifacts removal. Comprehensive empirical evidence shows that ghosting artifacts caused by large foreground motion are mainly low-frequency signals and the details are mainly high-frequency signals. In this work, we propose a novel frequency-guided end-to-end deep neural network (F**HDR**Net) to conduct **HDR** fusion in the frequency domain, and Discrete Wavelet Transform (DWT) is used to decompose inputs into different frequency bands. The low-frequency signals are used to avoid specific ghosting artifacts, while the high-frequency signals are used for preserving details. Using a U-Net as the backbone, we propose two novel modules: merging module and frequency-guided upsampling module. The merging module applies the attention mechanism to the low-frequency components to deal with the ghost caused by large foreground motion. The frequency-guided upsampling module reconstructs details from multiple frequency-specific components with rich details. In addition, a new RAW dataset is created for training and evaluating multi-frame **HDR** imaging algorithms in the RAW domain. Extensive experiments are conducted on public datasets and our RAW dataset, showing that the proposed F**HDR**Net achieves state-of-the-art performance.  
### Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v2 [eess.IV] UPDATED)
- Authors : Amir Atapour
- Link : [http://arxiv.org/abs/2202.02832](http://arxiv.org/abs/2202.02832)
> ABSTRACT  :  Convolutional Neural Networks have demonstrated human-level performance in the classification of melanoma and other skin lesions, but evident performance disparities between differing skin tones should be addressed before widespread deployment. In this work, we utilise a modified variational autoencoder to uncover skin tone bias in datasets commonly used as benchmarks. We propose an efficient yet effective algorithm for automatically labelling the skin tone of lesion images, and use this to annotate the benchmark ISIC dataset. We subsequently use two leading bias unlearning techniques to mitigate skin tone bias. Our experimental results provide evidence that our skin tone detection algorithm outperforms existing solutions and that unlearning skin tone improves generalisation and can reduce the performance disparity between melanoma detection in lighter and **dark**er skin tones.  
## cs.LG
---
### Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models. (arXiv:2202.04173v1 [cs.CL])
- Authors : Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, Bryan Catanzaro
- Link : [http://arxiv.org/abs/2202.04173](http://arxiv.org/abs/2202.04173)
> ABSTRACT  :  Pre-trained language models (LMs) are shown to easily generate toxic language. In this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. We conduct this study on three dimensions: training corpus, model size, and parameter efficiency. For the training corpus, we propose to leverage the generative power of LMs and generate nontoxic datasets for domain-adaptive training, which mitigates the **exposure** bias and is shown to be more data-efficient than using a curated pre-training corpus. We demonstrate that the self-generation method consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 1/3 smaller training corpus. We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before. We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify. We also explore parameter-efficient training methods for detoxification. We demonstrate that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.  
### A Multimodal Canonical-Correlated Graph Neural Network for Energy-Efficient Speech **Enhancement**. (arXiv:2202.04528v1 [cs.SD])
- Authors : Leandro Aparecido, Paulo Papa, Amir Hussain, Ahsan Adeel
- Link : [http://arxiv.org/abs/2202.04528](http://arxiv.org/abs/2202.04528)
> ABSTRACT  :  This paper proposes a novel multimodal self-supervised architecture for energy-efficient AV speech **enhancement** by integrating graph neural networks with canonical correlation analysis (CCA-GNN). This builds on a state-of-the-art CCA-GNN that aims to learn representative embeddings by maximizing the correlation between pairs of augmented views of the same input while decorrelating disconnected features. The key idea of the conventional CCA-GNN involves discarding augmentation-variant information and preserving augmentation-invariant information whilst preventing capturing of redundant information. Our proposed AV CCA-GNN model is designed to deal with the challenging multimodal representation learning context. Specifically, our model improves contextual AV speech processing by maximizing canonical correlation from augmented views of the same channel, as well as canonical correlation from audio and visual embeddings. In addition, we propose a positional encoding of the nodes that considers a prior-frame sequence distance instead of a feature-space representation while computing the node's nearest neighbors. This serves to introduce temporal information in the embeddings through the neighborhood's connectivity. Experiments conducted with the benchmark ChiME3 dataset show that our proposed prior frame-based AV CCA-GNN reinforces better feature learning in the temporal context, leading to more energy-efficient speech reconstruction compared to state-of-the-art CCA-GNN and multi-layer perceptron models. The results demonstrate the potential of our proposed approach for exploitation in future assistive technology and energy-efficient multimodal devices.  
### Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v2 [cs.CV] UPDATED)
- Authors : Xavier Suau, Nivedha Sivakumar, Luca Zappella, Nicholas Apostoloff
- Link : [http://arxiv.org/abs/2202.03586](http://arxiv.org/abs/2202.03586)
> ABSTRACT  :  As the use of deep learning in high impact domains becomes ubiquitous, it is increasingly important to assess the resilience of models. One such high impact domain is that of face recognition, with real world applications involving images affected by various degradations, such as motion blur or high **exposure**. Moreover, images captured across different attributes, such as gender and race, can also challenge the robustness of a face recognition algorithm. While traditional summary statistics suggest that the aggregate performance of face recognition models has continued to improve, these metrics do not directly measure the robustness or fairness of the models. Visual Psychophysics Sensitivity Analysis (VPSA) [1] provides a way to pinpoint the individual causes of failure by way of introducing incremental perturbations in the data. However, perturbations may affect subgroups differently. In this paper, we propose a new fairness evaluation based on robustness in the form of a generic framework that extends VPSA. With this framework, we can analyze the ability of a model to perform fairly for different subgroups of a population affected by perturbations, and pinpoint the exact failure modes for a subgroup by measuring targeted robustness. With the increasing focus on the fairness of models, we use face recognition as an example application of our framework and propose to compactly visualize the fairness analysis of a model via AUC matrices. We analyze the performance of common face recognition models and empirically show that certain subgroups are at a disadvantage when images are perturbed, thereby uncovering trends that were not visible using the model's performance on subgroups without perturbations.  
## cs.AI
---
### Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models. (arXiv:2202.04173v1 [cs.CL])
- Authors : Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, Bryan Catanzaro
- Link : [http://arxiv.org/abs/2202.04173](http://arxiv.org/abs/2202.04173)
> ABSTRACT  :  Pre-trained language models (LMs) are shown to easily generate toxic language. In this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. We conduct this study on three dimensions: training corpus, model size, and parameter efficiency. For the training corpus, we propose to leverage the generative power of LMs and generate nontoxic datasets for domain-adaptive training, which mitigates the **exposure** bias and is shown to be more data-efficient than using a curated pre-training corpus. We demonstrate that the self-generation method consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 1/3 smaller training corpus. We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before. We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify. We also explore parameter-efficient training methods for detoxification. We demonstrate that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.  
### Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v2 [cs.CV] UPDATED)
- Authors : Xavier Suau, Nivedha Sivakumar, Luca Zappella, Nicholas Apostoloff
- Link : [http://arxiv.org/abs/2202.03586](http://arxiv.org/abs/2202.03586)
> ABSTRACT  :  As the use of deep learning in high impact domains becomes ubiquitous, it is increasingly important to assess the resilience of models. One such high impact domain is that of face recognition, with real world applications involving images affected by various degradations, such as motion blur or high **exposure**. Moreover, images captured across different attributes, such as gender and race, can also challenge the robustness of a face recognition algorithm. While traditional summary statistics suggest that the aggregate performance of face recognition models has continued to improve, these metrics do not directly measure the robustness or fairness of the models. Visual Psychophysics Sensitivity Analysis (VPSA) [1] provides a way to pinpoint the individual causes of failure by way of introducing incremental perturbations in the data. However, perturbations may affect subgroups differently. In this paper, we propose a new fairness evaluation based on robustness in the form of a generic framework that extends VPSA. With this framework, we can analyze the ability of a model to perform fairly for different subgroups of a population affected by perturbations, and pinpoint the exact failure modes for a subgroup by measuring targeted robustness. With the increasing focus on the fairness of models, we use face recognition as an example application of our framework and propose to compactly visualize the fairness analysis of a model via AUC matrices. We analyze the performance of common face recognition models and empirically show that certain subgroups are at a disadvantage when images are perturbed, thereby uncovering trends that were not visible using the model's performance on subgroups without perturbations.  
# Paper List
---
## cs.CV
---
**66** new papers in cs.CV:-) 
1. Detecting and Localizing Copy-Move and Image-Splicing Forgery. (arXiv:2202.04069v1 [cs.CV])
2. Latent gaze information in highly dynamic decision-tasks. (arXiv:2202.04072v1 [cs.HC])
3. The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms. (arXiv:2202.04073v1 [eess.IV])
4. Cross-level Contrastive Learning and Consistency Constraint for Semi-supervised Medical Image Segmentation. (arXiv:2202.04074v1 [eess.IV])
5. Joint-bone Fusion Graph Convolutional Network for Semi-supervised Skeleton Action Recognition. (arXiv:2202.04075v1 [cs.CV])
6. Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces. (arXiv:2202.04101v1 [cs.CV])
7. Disentangle Saliency Detection into Cascaded Detail Modeling and Body Filling. (arXiv:2202.04112v1 [cs.CV])
8. Untrimmed Action Anticipation. (arXiv:2202.04132v1 [cs.CV])
9. Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v1 [eess.IV])
10. TransformNet: Self-supervised representation learning through predicting geometric transformations. (arXiv:2202.04181v1 [cs.CV])
11. MaskGIT: Masked Generative Image Transformer. (arXiv:2202.04200v1 [cs.CV])
12. Real-Time Event-Based Tracking and Detection for Maritime Environments. (arXiv:2202.04231v1 [cs.CV])
13. Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations. (arXiv:2202.04235v1 [cs.CV])
14. Learning Robust Convolutional Neural Networks with Relevant Feature Focusing via Explanations. (arXiv:2202.04237v1 [cs.CV])
15. A multiscale spatiotemporal approach for smallholder irrigation detection. (arXiv:2202.04239v1 [cs.CV])
16. Distillation with Contrast is All You Need for Self-Supervised Point Cloud Representation Learning. (arXiv:2202.04241v1 [cs.CV])
17. Motion-Aware Transformer For Occluded Person Re-identification. (arXiv:2202.04243v1 [cs.CV])
18. GiraffeDet: A Heavy-Neck Paradigm for Object Detection. (arXiv:2202.04256v1 [cs.CV])
19. Adversarial Detection without Model Information. (arXiv:2202.04271v1 [cs.CV])
20. Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation. (arXiv:2202.04287v1 [cs.CV])
21. Learning to Bootstrap for Combating Label Noise. (arXiv:2202.04291v1 [cs.CV])
22. Image Difference Captioning with Pre-training and Contrastive Learning. (arXiv:2202.04298v1 [cs.MM])
23. Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?. (arXiv:2202.04306v1 [cs.AI])
24. Conditional Motion In-betweening. (arXiv:2202.04307v1 [cs.CV])
25. Anchor Graph Structure Fusion Hashing for Cross-Modal Similarity Search. (arXiv:2202.04327v1 [cs.CV])
26. Deep Feature Rotation for Multimodal Image Style Transfer. (arXiv:2202.04426v1 [cs.CV])
27. Object-Guided Day-**Night** Visual Localization in Urban Scenes. (arXiv:2202.04445v1 [cs.CV])
28. Predicting the intended action using internal simulation of perception. (arXiv:2202.04466v1 [cs.AI])
29. CRAT-Pred: Vehicle Trajectory Prediction with Crystal Graph Convolutional Neural Networks and Multi-Head Self-Attention. (arXiv:2202.04488v1 [cs.CV])
30. End-to-End Blind Quality Assessment for Laparoscopic Videos using Neural Networks. (arXiv:2202.04517v1 [eess.IV])
31. NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v1 [cs.CV])
32. Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v1 [eess.IV])
33. Distance Estimation and Animal Tracking for Wildlife Camera Trapping. (arXiv:2202.04613v1 [cs.CV])
34. Reducing Redundancy in the Bottleneck Representation of the Autoencoders. (arXiv:2202.04629v1 [cs.CV])
35. Point-Level Region Contrast for Object Detection Pre-Training. (arXiv:2202.04639v1 [cs.CV])
36. Distributionally Robust Deep Learning using Hardness Weighted Sampling. (arXiv:2001.02658v3 [cs.LG] UPDATED)
37. Sampling possible reconstructions of undersampled acquisitions in MR imaging. (arXiv:2010.00042v3 [eess.IV] UPDATED)
38. SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural Networks. (arXiv:2010.09343v3 [cs.CV] UPDATED)
39. Tensor Composition Net for Visual Relationship Prediction. (arXiv:2012.05473v2 [cs.CV] UPDATED)
40. Balanced softmax cross-entropy for incremental learning with and without memory. (arXiv:2103.12532v4 [cs.LG] UPDATED)
41. Leaning Compact and Representative Features for Cross-Modality Person Re-Identification. (arXiv:2103.14210v2 [cs.CV] UPDATED)
42. MODS -- A USV-oriented object detection and obstacle segmentation benchmark. (arXiv:2105.02359v2 [cs.CV] UPDATED)
43. AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v3 [cs.NI] UPDATED)
44. Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains. (arXiv:2105.14391v2 [cs.CV] UPDATED)
45. LENAS: Learning-based Neural Architecture Search and Ensemble for 3D Radiotherapy Dose Prediction. (arXiv:2106.06733v3 [eess.IV] UPDATED)
46. PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v5 [cs.CV] UPDATED)
47. PU-Flow: a Point Cloud Upsampling Networkwith Normalizing Flows. (arXiv:2107.05893v2 [cs.CV] UPDATED)
48. Human Pose Transfer with Augmented Disentangled Feature Consistency. (arXiv:2107.10984v3 [cs.CV] UPDATED)
49. Wavelet-Based Network For **High Dynamic Range** Imaging. (arXiv:2108.01434v2 [eess.IV] UPDATED)
50. FBSNet: A Fast **Bilateral** Symmetrical Network for Real-Time Semantic Segmentation. (arXiv:2109.00699v2 [cs.CV] UPDATED)
51. Trident Pyramid Networks: The importance of processing at the feature pyramid level for better object detection. (arXiv:2110.04004v2 [cs.CV] UPDATED)
52. ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v3 [cs.IT] UPDATED)
53. InfAnFace: Bridging the infant-adult domain gap in facial landmark estimation in the wild. (arXiv:2110.08935v2 [cs.CV] UPDATED)
54. Residual Quantity in Percentage of Factory Machines Using Computer Vision and Mathematical Methods. (arXiv:2111.05080v2 [cs.CV] UPDATED)
55. SLOSH: Set LOcality Sensitive Hashing via Sliced-Wasserstein Embeddings. (arXiv:2112.05872v2 [cs.LG] UPDATED)
56. Persistent Object Identification Leveraging Non-Visual Markers. (arXiv:2112.06809v5 [cs.CV] UPDATED)
57. Conditional generative data-free knowledge distillation. (arXiv:2112.15358v2 [cs.CV] UPDATED)
58. ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v2 [eess.IV] UPDATED)
59. Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network. (arXiv:2201.08954v2 [cs.CV] UPDATED)
60. Background Invariant Classification on Infrared Imagery by Data Efficient Training and Reducing Bias in CNNs. (arXiv:2201.09144v2 [cs.CV] UPDATED)
61. Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v2 [eess.IV] UPDATED)
62. Reasoning for Complex Data through Ensemble-based Self-Supervised Learning. (arXiv:2202.03126v2 [cs.CV] UPDATED)
63. PSSNet: Planarity-sensible Semantic Segmentation of Large-scale Urban Meshes. (arXiv:2202.03209v2 [cs.CV] UPDATED)
64. Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v2 [cs.CV] UPDATED)
65. How to Understand Masked Autoencoders. (arXiv:2202.03670v2 [cs.CV] UPDATED)
66. Graph-Relational Domain Adaptation. (arXiv:2202.03628v1 [cs.LG] CROSS LISTED)
## eess.IV
---
**16** new papers in eess.IV:-) 
1. Detecting and Localizing Copy-Move and Image-Splicing Forgery. (arXiv:2202.04069v1 [cs.CV])
2. The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms. (arXiv:2202.04073v1 [eess.IV])
3. Cross-level Contrastive Learning and Consistency Constraint for Semi-supervised Medical Image Segmentation. (arXiv:2202.04074v1 [eess.IV])
4. The Rate-Distortion-Perception Tradeoff: The Role of Common Randomness. (arXiv:2202.04147v1 [cs.IT])
5. Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v1 [eess.IV])
6. Real-Time Event-Based Tracking and Detection for Maritime Environments. (arXiv:2202.04231v1 [cs.CV])
7. A hypothesis-driven method based on machine learning for neuroimaging data analysis. (arXiv:2202.04397v1 [stat.ML])
8. End-to-End Blind Quality Assessment for Laparoscopic Videos using Neural Networks. (arXiv:2202.04517v1 [eess.IV])
9. Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v1 [eess.IV])
10. Reducing Redundancy in the Bottleneck Representation of the Autoencoders. (arXiv:2202.04629v1 [cs.CV])
11. Sampling possible reconstructions of undersampled acquisitions in MR imaging. (arXiv:2010.00042v3 [eess.IV] UPDATED)
12. LENAS: Learning-based Neural Architecture Search and Ensemble for 3D Radiotherapy Dose Prediction. (arXiv:2106.06733v3 [eess.IV] UPDATED)
13. Wavelet-Based Network For **High Dynamic Range** Imaging. (arXiv:2108.01434v2 [eess.IV] UPDATED)
14. ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v2 [eess.IV] UPDATED)
15. Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network. (arXiv:2201.08954v2 [cs.CV] UPDATED)
16. Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v2 [eess.IV] UPDATED)
## cs.LG
---
**176** new papers in cs.LG:-) 
1. Detecting and Localizing Copy-Move and Image-Splicing Forgery. (arXiv:2202.04069v1 [cs.CV])
2. The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms. (arXiv:2202.04073v1 [eess.IV])
3. Teaching Networks to Solve Optimization Problems. (arXiv:2202.04104v1 [cs.LG])
4. Hierarchical Dependency Constrained Tree Augmented Naive Bayes Classifiers for Hierarchical Feature Spaces. (arXiv:2202.04105v1 [cs.LG])
5. A Lagrangian Duality Approach to Active Learning. (arXiv:2202.04108v1 [cs.LG])
6. Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs. (arXiv:2202.04109v1 [cs.LG])
7. PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v1 [cs.LG])
8. Financial Vision Based Reinforcement Learning Trading Strategy. (arXiv:2202.04115v1 [cs.AI])
9. A Mini-Block Natural Gradient Method for Deep Neural Networks. (arXiv:2202.04124v1 [cs.LG])
10. Independent Policy Gradient for Large-Scale Markov Potential Games: Sharper Rates, Function Approximation, and Game-Agnostic Convergence. (arXiv:2202.04129v1 [cs.LG])
11. A Novel Ontology-guided Attribute Partitioning Ensemble Learning Model for Early Prediction of Cognitive Deficits using Quantitative Structural MRI in Very Preterm Infants. (arXiv:2202.04134v1 [cs.LG])
12. Generative multitask learning mitigates target-causing confounding. (arXiv:2202.04136v1 [cs.LG])
13. Machine Learning in Heterogeneous Porous Materials. (arXiv:2202.04137v1 [cs.LG])
14. Simplified Graph Convolution with Heterophily. (arXiv:2202.04139v1 [cs.LG])
15. The Rate-Distortion-Perception Tradeoff: The Role of Common Randomness. (arXiv:2202.04147v1 [cs.IT])
16. Understanding the bias-variance tradeoff of Bregman divergences. (arXiv:2202.04167v1 [stat.ML])
17. SwiftAgg: Communication-Efficient and Dropout-Resistant Secure Aggregation for Federated Learning with Worst-Case Security Guarantees. (arXiv:2202.04169v1 [cs.IT])
18. Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models. (arXiv:2202.04173v1 [cs.CL])
19. Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v1 [eess.IV])
20. Police Text Analysis: Topic Modeling and Spatial Relative Density Estimation. (arXiv:2202.04176v1 [cs.LG])
21. VAEL: Bridging Variational Autoencoders and Probabilistic Logic Programming. (arXiv:2202.04178v1 [cs.PL])
22. TransformNet: Self-supervised representation learning through predicting geometric transformations. (arXiv:2202.04181v1 [cs.CV])
23. FMP: Toward Fair Graph Message Passing against Topology Bias. (arXiv:2202.04187v1 [cs.LG])
24. RECOVER: sequential model optimization platform for combination drug repurposing identifies novel synergistic compounds in vitro. (arXiv:2202.04202v1 [q-bio.QM])
25. Covariate-informed Representation Learning with Samplewise Optimal Identifiable Variational Autoencoders. (arXiv:2202.04206v1 [stat.ML])
26. Evaluating Causal Inference Methods. (arXiv:2202.04208v1 [stat.ME])
27. Fault Detection and Diagnosis with Imbalanced and Noisy Data: A Hybrid Framework for Rotating Machinery. (arXiv:2202.04212v1 [cs.LG])
28. Managers versus Machines: Do Algorithms Replicate Human Intuition in Credit Ratings?. (arXiv:2202.04218v1 [econ.EM])
29. Improving Computational Complexity in Statistical Models with Second-Order Information. (arXiv:2202.04219v1 [stat.ML])
30. Intelligent Autonomous Intersection Management. (arXiv:2202.04224v1 [cs.MA])
31. Deep Neural Networks to Correct Sub-Precision Errors in CFD. (arXiv:2202.04233v1 [physics.flu-dyn])
32. Learning Robust Convolutional Neural Networks with Relevant Feature Focusing via Explanations. (arXiv:2202.04237v1 [cs.CV])
33. Parametric t-Stochastic Neighbor Embedding With Quantum Neural Network. (arXiv:2202.04238v1 [quant-ph])
34. A multiscale spatiotemporal approach for smallholder irrigation detection. (arXiv:2202.04239v1 [cs.CV])
35. Regulatory Instruments for Fair Personalized Pricing. (arXiv:2202.04245v1 [cs.CY])
36. Improving greedy core-set configurations for active learning with uncertainty-scaled distances. (arXiv:2202.04251v1 [cs.LG])
37. A Data-Driven Approach to Robust Hypothesis Testing Using Sinkhorn Uncertainty Sets. (arXiv:2202.04258v1 [stat.ML])
38. Parsimonious Learning-Augmented Caching. (arXiv:2202.04262v1 [cs.DS])
39. MMLN: Leveraging Domain Knowledge for Multimodal Diagnosis. (arXiv:2202.04266v1 [cs.LG])
40. A decision-tree framework to select optimal box-sizes for product shipments. (arXiv:2202.04277v1 [cs.LG])
41. Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation. (arXiv:2202.04287v1 [cs.CV])
42. Optimal Clustering with Bandit Feedback. (arXiv:2202.04294v1 [cs.LG])
43. On Almost Sure Convergence Rates of Stochastic Gradient Methods. (arXiv:2202.04295v1 [cs.LG])
44. Log-based Anomaly Detection with Deep Learning: How Far Are We?. (arXiv:2202.04301v1 [cs.SE])
45. On the Implicit Bias of Gradient Descent for Temporal Extrapolation. (arXiv:2202.04302v1 [cs.LG])
46. TinyM$^2$Net: A Flexible System Algorithm Co-designed Multimodal Learning Framework for Tiny Devices. (arXiv:2202.04303v1 [cs.LG])
47. Vertical Federated Learning: Challenges, Methodologies and Experiments. (arXiv:2202.04309v1 [cs.LG])
48. Imitation Learning by State-Only Distribution Matching. (arXiv:2202.04332v1 [cs.LG])
49. Scenario-Assisted Deep Reinforcement Learning. (arXiv:2202.04337v1 [cs.LG])
50. Gradient Methods Provably Converge to Non-Robust Networks. (arXiv:2202.04347v1 [cs.LG])
51. MBCT: Tree-Based Feature-Aware Binning for Individual Uncertainty Calibration. (arXiv:2202.04348v1 [cs.LG])
52. pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v1 [cs.CL])
53. Generalized Strategic Classification and the Case of Aligned Incentives. (arXiv:2202.04357v1 [cs.LG])
54. House Price Valuation Model Based on Geographically Neural Network Weighted Regression: The Case Study of Shenzhen, China. (arXiv:2202.04358v1 [stat.AP])
55. Cost-effective Framework for Gradual Domain Adaptation with Multifidelity. (arXiv:2202.04359v1 [stat.ML])
56. A Reinforcement Learning Approach to Domain-Knowledge Inclusion Using Grammar Guided Symbolic Regression. (arXiv:2202.04367v1 [cs.LG])
57. A new perspective on classification: optimally allocating limited resources to uncertain tasks. (arXiv:2202.04369v1 [cs.LG])
58. Empirical Risk Minimization with Relative Entropy Regularization: Optimality and Sensitivity Analysis. (arXiv:2202.04385v1 [cs.LG])
59. Model Architecture Adaption for Bayesian Neural Networks. (arXiv:2202.04392v1 [cs.LG])
60. A hypothesis-driven method based on machine learning for neuroimaging data analysis. (arXiv:2202.04397v1 [stat.ML])
61. Agree to Disagree: Diversity through Disagreement for Better Transferability. (arXiv:2202.04414v1 [cs.LG])
62. Adapting to Mixing Time in Stochastic Optimization with Markovian Data. (arXiv:2202.04428v1 [cs.LG])
63. Conditional Drums Generation using Compound Word Representations. (arXiv:2202.04464v1 [cs.SD])
64. Predicting the intended action using internal simulation of perception. (arXiv:2202.04466v1 [cs.AI])
65. Rethinking Goal-conditioned Supervised Learning and Its Connection to Offline RL. (arXiv:2202.04478v1 [cs.LG])
66. False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger. (arXiv:2202.04479v1 [cs.LG])
67. Finding Optimal Arms in Non-stochastic Combinatorial Bandits with Semi-bandit Feedback and Finite Budget. (arXiv:2202.04487v1 [cs.LG])
68. CRAT-Pred: Vehicle Trajectory Prediction with Crystal Graph Convolutional Neural Networks and Multi-Head Self-Attention. (arXiv:2202.04488v1 [cs.CV])
69. Recurrent Spectral Network (RSN): shaping the basin of attraction of a discrete map to reach automated classification. (arXiv:2202.04497v1 [cond-mat.dis-nn])
70. Lightweight Jet Reconstruction and Identification as an Object Detection Task. (arXiv:2202.04499v1 [hep-ex])
71. Contextualize Me -- The Case for Context in Reinforcement Learning. (arXiv:2202.04500v1 [cs.LG])
72. Prediction Sensitivity: Continual Audit of Counterfactual Fairness in Deployed Classifiers. (arXiv:2202.04504v1 [cs.LG])
73. Optimising hadronic collider simulations using amplitude neural networks. (arXiv:2202.04506v1 [hep-ph])
74. Optimal learning rate schedules in high-dimensional non-convex optimization problems. (arXiv:2202.04509v1 [cs.LG])
75. The no-free-lunch theorems of supervised learning. (arXiv:2202.04513v1 [cs.LG])
76. Leverage Score Sampling for Tensor Product Matrices in Input Sparsity Time. (arXiv:2202.04515v1 [cs.LG])
77. End-to-End Blind Quality Assessment for Laparoscopic Videos using Neural Networks. (arXiv:2202.04517v1 [eess.IV])
78. Obtaining Dyadic Fairness by Optimal Transport. (arXiv:2202.04520v1 [cs.LG])
79. Explainable Predictive Modeling for Limited Spectral Data. (arXiv:2202.04527v1 [cs.LG])
80. A Multimodal Canonical-Correlated Graph Neural Network for Energy-Efficient Speech **Enhancement**. (arXiv:2202.04528v1 [cs.SD])
81. An Exploration of Multicalibration Uniform Convergence Bounds. (arXiv:2202.04530v1 [cs.LG])
82. Generating Training Data with Language Models: Towards Zero-Shot Language Understanding. (arXiv:2202.04538v1 [cs.CL])
83. Universal Hopfield Networks: A General Framework for Single-Shot Associative Memory Models. (arXiv:2202.04557v1 [cs.NE])
84. Precision Radiotherapy via Information Integration of Expert Human Knowledge and AI Recommendation to Optimize Clinical Decision Making. (arXiv:2202.04565v1 [stat.ML])
85. Optimal Hyperparameters and Structure Setting of Multi-Objective Robust CNN Systems via Generalized Taguchi Method and Objective Vector Norm. (arXiv:2202.04567v1 [cs.LG])
86. Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs. (arXiv:2202.04579v1 [cs.LG])
87. Noise fingerprints in quantum computers: Machine learning software tools. (arXiv:2202.04581v1 [quant-ph])
88. Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations. (arXiv:2202.04582v1 [cs.CL])
89. Adjoint-aided inference of Gaussian process driven differential equations. (arXiv:2202.04589v1 [stat.ML])
90. Stability Analysis of Recurrent Neural Networks by IQC with Copositive Mutipliers. (arXiv:2202.04592v1 [math.OC])
91. Stochastic Contextual Dueling Bandits under Linear Stochastic Transitivity Models. (arXiv:2202.04593v1 [cs.LG])
92. Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v1 [eess.IV])
93. Reproducibility in Optimization: Theoretical Framework and Limits. (arXiv:2202.04598v1 [math.OC])
94. Missing Data Imputation and Acquisition with Deep Hierarchical Models and Hamiltonian Monte Carlo. (arXiv:2202.04599v1 [cs.LG])
95. Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration. (arXiv:2202.04628v1 [cs.LG])
96. A Local Geometric Interpretation of Feature Extraction in Deep Feedforward Neural Networks. (arXiv:2202.04632v1 [cs.LG])
97. Offline Reinforcement Learning with Realizability and Single-policy Concentrability. (arXiv:2202.04634v1 [cs.LG])
98. Sharper Rates for Separable Minimax and Finite Sum Optimization via Primal-Dual Extragradient Methods. (arXiv:2202.04640v1 [math.OC])
99. A Probabilistic Generative Grammar for Semantic Parsing. (arXiv:1606.06361v2 [cs.CL] UPDATED)
100. A Practical Approach to Proper Inference with Linked Data. (arXiv:1810.01538v2 [stat.ME] UPDATED)
101. When can unlabeled data improve the learning rate?. (arXiv:1905.11866v2 [cs.LG] UPDATED)
102. Policy Optimization with Stochastic Mirror Descent. (arXiv:1906.10462v5 [cs.LG] UPDATED)
103. Defeating Misclassification Attacks Against Transfer Learning. (arXiv:1908.11230v4 [cs.LG] UPDATED)
104. Polynomial-Time Exact MAP Inference on Discrete Models with Global Dependencies. (arXiv:1912.12090v3 [cs.DM] UPDATED)
105. Distributionally Robust Deep Learning using Hardness Weighted Sampling. (arXiv:2001.02658v3 [cs.LG] UPDATED)
106. Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning. (arXiv:2004.10888v5 [cs.LG] UPDATED)
107. Deep Layer-wise Networks Have Closed-Form Weights. (arXiv:2006.08539v6 [stat.ML] UPDATED)
108. Sampling possible reconstructions of undersampled acquisitions in MR imaging. (arXiv:2010.00042v3 [eess.IV] UPDATED)
109. Frank-Wolfe with a Nearest Extreme Point Oracle. (arXiv:2102.02029v2 [math.OC] UPDATED)
110. Are deep learning models superior for missing data imputation in large surveys? Evidence from an empirical comparison. (arXiv:2103.09316v2 [cs.LG] UPDATED)
111. AutoTune: Controller Tuning for High-Speed Flight. (arXiv:2103.10698v2 [cs.RO] UPDATED)
112. Balanced softmax cross-entropy for incremental learning with and without memory. (arXiv:2103.12532v4 [cs.LG] UPDATED)
113. A Hybrid Inference System for Improved Curvature Estimation in the Level-Set Method Using Machine Learning. (arXiv:2104.02951v3 [cs.LG] UPDATED)
114. MODS -- A USV-oriented object detection and obstacle segmentation benchmark. (arXiv:2105.02359v2 [cs.CV] UPDATED)
115. AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v3 [cs.NI] UPDATED)
116. Independent mechanism analysis, a new concept?. (arXiv:2106.05200v3 [stat.ML] UPDATED)
117. Rare event estimation using stochastic spectral embedding. (arXiv:2106.05824v2 [cs.LG] UPDATED)
118. FedBABU: Towards Enhanced Representation for Federated Image Classification. (arXiv:2106.06042v2 [cs.LG] UPDATED)
119. LENAS: Learning-based Neural Architecture Search and Ensemble for 3D Radiotherapy Dose Prediction. (arXiv:2106.06733v3 [eess.IV] UPDATED)
120. Rethinking Adam: A Twofold Exponential Moving Average Approach. (arXiv:2106.11514v3 [cs.LG] UPDATED)
121. Self-Contrastive Learning: An Efficient Supervised Contrastive Framework with Single-view and Sub-network. (arXiv:2106.15499v5 [cs.LG] UPDATED)
122. Policy gradient methods for distortion risk measures. (arXiv:2107.04422v4 [cs.LG] UPDATED)
123. Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration. (arXiv:2107.08966v3 [cs.LG] UPDATED)
124. Automatic Unstructured Handwashing Recognition using Smartwatch to Reduce Contact Transmission of Pathogens. (arXiv:2107.13405v3 [cs.LG] UPDATED)
125. Learning more skills through optimistic exploration. (arXiv:2107.14226v3 [cs.LG] UPDATED)
126. Augmenting Decision Making via Interactive What-If Analysis. (arXiv:2109.06160v4 [cs.DB] UPDATED)
127. Classification with Nearest Disjoint Centroids. (arXiv:2109.10436v2 [cs.LG] UPDATED)
128. Deep Augmented MUSIC Algorithm for Data-Driven DoA Estimation. (arXiv:2109.10581v3 [eess.SP] UPDATED)
129. On Margin Maximization in Linear and ReLU Networks. (arXiv:2110.02732v3 [cs.LG] UPDATED)
130. Universal Approximation Under Constraints is Possible with Transformers. (arXiv:2110.03303v2 [cs.LG] UPDATED)
131. Unifying Likelihood-free Inference with Black-box Optimization and Beyond. (arXiv:2110.03372v2 [cs.LG] UPDATED)
132. A Comparison of Neural Network Architectures for Data-Driven Reduced-Order Modeling. (arXiv:2110.03442v3 [cs.LG] UPDATED)
133. Uncertainty in Data-Driven Kalman Filtering for Partially Known State-Space Models. (arXiv:2110.04738v2 [eess.SP] UPDATED)
134. Recurrent Model-Free RL can be a Strong Baseline for Many POMDPs. (arXiv:2110.05038v2 [cs.LG] UPDATED)
135. Learning Temporally Causal Latent Processes from General Temporal Data. (arXiv:2110.05428v4 [stat.ML] UPDATED)
136. ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v3 [cs.IT] UPDATED)
137. A Dual Approach to Constrained Markov Decision Processes with Entropy Regularization. (arXiv:2110.08923v2 [cs.LG] UPDATED)
138. Collaboration Challenges in Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process. (arXiv:2110.10234v3 [cs.SE] UPDATED)
139. Improving Fairness via Federated Learning. (arXiv:2110.15545v2 [cs.LG] UPDATED)
140. GBK-GNN: Gated Bi-Kernel Graph Neural Networks for Modeling Both Homophily and Heterophily. (arXiv:2110.15777v2 [cs.LG] UPDATED)
141. High-dimensional multi-trait GWAS by reverse prediction of genotypes. (arXiv:2111.00108v2 [q-bio.GN] UPDATED)
142. Implicit Acoustic Echo Cancellation for Keyword Spotting and Device-Directed Speech Detection. (arXiv:2111.10639v2 [cs.SD] UPDATED)
143. Characteristic Neural Ordinary Differential Equations. (arXiv:2111.13207v2 [cs.LG] UPDATED)
144. Active Sensing for Communications by Learning. (arXiv:2112.04075v3 [cs.IT] UPDATED)
145. Proximal Iteration for Deep Reinforcement Learning. (arXiv:2112.05848v2 [cs.LG] UPDATED)
146. SLOSH: Set LOcality Sensitive Hashing via Sliced-Wasserstein Embeddings. (arXiv:2112.05872v2 [cs.LG] UPDATED)
147. BayesFlow Can Reliably Detect Model Misspecification and Posterior Errors in Amortized Bayesian Inference. (arXiv:2112.08866v2 [stat.ME] UPDATED)
148. Multitask Network for Respiration Rate Estimation -- A Practical Perspective. (arXiv:2112.09071v3 [eess.SP] UPDATED)
149. Supervised Multivariate Learning with Simultaneous Feature Auto-grouping and Dimension Reduction. (arXiv:2112.09746v2 [stat.ML] UPDATED)
150. ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v2 [eess.IV] UPDATED)
151. Machine Learning-enhanced Efficient Spectroscopic Ellipsometry Modeling. (arXiv:2201.04933v2 [cond-mat.mtrl-sci] UPDATED)
152. Sketch-and-Lift: Scalable Subsampled Semidefinite Program for $K$-means Clustering. (arXiv:2201.08226v2 [stat.ML] UPDATED)
153. Predictive Inference with Weak Supervision. (arXiv:2201.08315v2 [stat.ML] UPDATED)
154. On the Convergence of Heterogeneous Federated Learning with Arbitrary Adaptive Online Model Pruning. (arXiv:2201.11803v2 [cs.LG] UPDATED)
155. Toward Training at ImageNet Scale with Differential Privacy. (arXiv:2201.12328v2 [cs.LG] UPDATED)
156. Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning. (arXiv:2201.13425v2 [cs.LG] UPDATED)
157. Unified Scaling Laws for Routed Language Models. (arXiv:2202.01169v2 [cs.CL] UPDATED)
158. Machine Learning Solar Wind Driving Magnetospheric Convection in Tail Lobes. (arXiv:2202.01383v2 [astro-ph.SR] UPDATED)
159. Theoretical Exploration of Solutions of Feedforward ReLU networks. (arXiv:2202.01919v3 [cs.LG] UPDATED)
160. Comprehensive survey of computational learning methods for analysis of gene expression data in genomics. (arXiv:2202.02958v2 [q-bio.GN] UPDATED)
161. Approximation error of single hidden layer neural networks with fixed weights. (arXiv:2202.03289v2 [cs.LG] UPDATED)
162. Conditional Gradients for the Approximately Vanishing Ideal. (arXiv:2202.03349v3 [cs.LG] UPDATED)
163. Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v2 [cs.LG] UPDATED)
164. Locally Random P-adic Alloy Codes with Channel Coding Theorems for Distributed Coded Tensors. (arXiv:2202.03469v2 [cs.IT] UPDATED)
165. Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v2 [cs.CV] UPDATED)
166. How to Understand Masked Autoencoders. (arXiv:2202.03670v2 [cs.CV] UPDATED)
167. Backtrack Tie-Breaking for Decision Trees: A Note on Deodata Predictors. (arXiv:2202.03865v2 [cs.LG] UPDATED)
168. Robust Hybrid Learning With Expert Augmentation. (arXiv:2202.03881v2 [cs.LG] UPDATED)
169. GraphDCA -- a Framework for Node Distribution Comparison in Real and Synthetic Graphs. (arXiv:2202.03884v2 [cs.LG] UPDATED)
170. KENN: Enhancing Deep Neural Networks by Leveraging Knowledge for Time Series Forecasting. (arXiv:2202.03903v2 [cs.LG] UPDATED)
171. CausPref: Causal Preference Learning for Out-of-Distribution Recommendation. (arXiv:2202.03984v2 [cs.LG] UPDATED)
172. Learning Sinkhorn divergences for supervised change point detection. (arXiv:2202.04000v2 [cs.LG] UPDATED)
173. PrivFair: a Library for Privacy-Preserving Fairness Auditing. (arXiv:2202.04058v2 [cs.LG] UPDATED)
174. Graph-Relational Domain Adaptation. (arXiv:2202.03628v1 [cs.LG] CROSS LISTED)
175. InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training. (arXiv:2202.03751v1 [eess.AS] CROSS LISTED)
176. Rainbow Differential Privacy. (arXiv:2202.03974v1 [cs.CR] CROSS LISTED)
## cs.AI
---
**67** new papers in cs.AI:-) 
1. Latent gaze information in highly dynamic decision-tasks. (arXiv:2202.04072v1 [cs.HC])
2. Machine Explanations and Human Understanding. (arXiv:2202.04092v1 [cs.AI])
3. PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v1 [cs.LG])
4. Financial Vision Based Reinforcement Learning Trading Strategy. (arXiv:2202.04115v1 [cs.AI])
5. IoT Malware Detection Architecture using a Novel Channel Boosted and Squeezed CNN. (arXiv:2202.04121v1 [cs.CR])
6. A Mini-Block Natural Gradient Method for Deep Neural Networks. (arXiv:2202.04124v1 [cs.LG])
7. Generative multitask learning mitigates target-causing confounding. (arXiv:2202.04136v1 [cs.LG])
8. Using a Language Model in a Kiosk Recommender System at Fast-Food Restaurants. (arXiv:2202.04145v1 [cs.AI])
9. Logical Reasoning for Task Oriented Dialogue Systems. (arXiv:2202.04161v1 [cs.CL])
10. Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models. (arXiv:2202.04173v1 [cs.CL])
11. Covariate-informed Representation Learning with Samplewise Optimal Identifiable Variational Autoencoders. (arXiv:2202.04206v1 [stat.ML])
12. Data-Driven Online Interactive Bidding Strategy for Demand Response. (arXiv:2202.04236v1 [cs.AI])
13. Learning Robust Convolutional Neural Networks with Relevant Feature Focusing via Explanations. (arXiv:2202.04237v1 [cs.CV])
14. The volcspeech system for the icassp 2022 multi-channel multi-party meeting transcription challenge. (arXiv:2202.04261v1 [cs.SD])
15. Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?. (arXiv:2202.04306v1 [cs.AI])
16. Conditional Motion In-betweening. (arXiv:2202.04307v1 [cs.CV])
17. ARIBA: Towards Accurate and Robust Identification of Backdoor Attacks in Federated Learning. (arXiv:2202.04311v1 [cs.AI])
18. pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v1 [cs.CL])
19. Generalized Strategic Classification and the Case of Aligned Incentives. (arXiv:2202.04357v1 [cs.LG])
20. A Reinforcement Learning Approach to Domain-Knowledge Inclusion Using Grammar Guided Symbolic Regression. (arXiv:2202.04367v1 [cs.LG])
21. Improving short-term bike sharing demand forecast through an irregular convolutional neural network. (arXiv:2202.04376v1 [cs.AI])
22. Leveraging Experience in Lifelong Multi-Agent Pathfinding. (arXiv:2202.04382v1 [cs.MA])
23. Model Architecture Adaption for Bayesian Neural Networks. (arXiv:2202.04392v1 [cs.LG])
24. A.I. and Data-Driven Mobility at Volkswagen Financial Services AG. (arXiv:2202.04411v1 [cs.AI])
25. Revisiting QMIX: Discriminative Credit Assignment by Gradient Entropy Regularization. (arXiv:2202.04427v1 [cs.AI])
26. Predicting the intended action using internal simulation of perception. (arXiv:2202.04466v1 [cs.AI])
27. Rethinking Goal-conditioned Supervised Learning and Its Connection to Offline RL. (arXiv:2202.04478v1 [cs.LG])
28. Prediction Sensitivity: Continual Audit of Counterfactual Fairness in Deployed Classifiers. (arXiv:2202.04504v1 [cs.LG])
29. Optimising hadronic collider simulations using amplitude neural networks. (arXiv:2202.04506v1 [hep-ph])
30. Obtaining Dyadic Fairness by Optimal Transport. (arXiv:2202.04520v1 [cs.LG])
31. Universal Hopfield Networks: A General Framework for Single-Shot Associative Memory Models. (arXiv:2202.04557v1 [cs.NE])
32. Precision Radiotherapy via Information Integration of Expert Human Knowledge and AI Recommendation to Optimize Clinical Decision Making. (arXiv:2202.04565v1 [stat.ML])
33. Task Modifiers for HTN Planning and Acting. (arXiv:2202.04611v1 [cs.AI])
34. Distance Estimation and Animal Tracking for Wildlife Camera Trapping. (arXiv:2202.04613v1 [cs.CV])
35. Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration. (arXiv:2202.04628v1 [cs.LG])
36. Joint Inference of Reward Machines and Policies for Reinforcement Learning. (arXiv:1909.05912v2 [cs.AI] UPDATED)
37. Beyond Pairwise Comparisons in Social Choice: A Setwise Kemeny Aggregation Problem. (arXiv:1911.06226v2 [cs.AI] UPDATED)
38. Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning. (arXiv:2004.10888v5 [cs.LG] UPDATED)
39. Long-Term Resource Allocation Fairness in Average Markov Decision Process (AMDP) Environment. (arXiv:2102.07120v3 [cs.AI] UPDATED)
40. Independent mechanism analysis, a new concept?. (arXiv:2106.05200v3 [stat.ML] UPDATED)
41. Rethinking Adam: A Twofold Exponential Moving Average Approach. (arXiv:2106.11514v3 [cs.LG] UPDATED)
42. Self-Contrastive Learning: An Efficient Supervised Contrastive Framework with Single-view and Sub-network. (arXiv:2106.15499v5 [cs.LG] UPDATED)
43. Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration. (arXiv:2107.08966v3 [cs.LG] UPDATED)
44. Learning more skills through optimistic exploration. (arXiv:2107.14226v3 [cs.LG] UPDATED)
45. Universal Approximation Under Constraints is Possible with Transformers. (arXiv:2110.03303v2 [cs.LG] UPDATED)
46. Unifying Likelihood-free Inference with Black-box Optimization and Beyond. (arXiv:2110.03372v2 [cs.LG] UPDATED)
47. Recurrent Model-Free RL can be a Strong Baseline for Many POMDPs. (arXiv:2110.05038v2 [cs.LG] UPDATED)
48. Learning Temporally Causal Latent Processes from General Temporal Data. (arXiv:2110.05428v4 [stat.ML] UPDATED)
49. GraphSearchNet: Enhancing GNNs via Capturing Global Dependency for Semantic Code Search. (arXiv:2111.02671v3 [cs.SE] UPDATED)
50. Conversational Recommendation: Theoretical Model and Complexity Analysis. (arXiv:2111.05578v3 [cs.AI] UPDATED)
51. Calculating Question Similarity is Enough: A New Method for KBQA Tasks. (arXiv:2111.07658v3 [cs.CL] UPDATED)
52. Proximal Iteration for Deep Reinforcement Learning. (arXiv:2112.05848v2 [cs.LG] UPDATED)
53. Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning. (arXiv:2201.13425v2 [cs.LG] UPDATED)
54. Efficient Policy Space Response Oracles. (arXiv:2202.00633v2 [cs.GT] UPDATED)
55. AI-as-a-Service Toolkit for Human-Centered Intelligence in Autonomous Driving. (arXiv:2202.01645v2 [cs.AI] UPDATED)
56. Theoretical Exploration of Solutions of Feedforward ReLU networks. (arXiv:2202.01919v3 [cs.LG] UPDATED)
57. Comprehensive survey of computational learning methods for analysis of gene expression data in genomics. (arXiv:2202.02958v2 [q-bio.GN] UPDATED)
58. OPP-Miner: Order-preserving sequential pattern mining. (arXiv:2202.03140v2 [cs.DB] UPDATED)
59. Reward is not enough: can we liberate AI from the reinforcement learning paradigm?. (arXiv:2202.03192v2 [cs.AI] UPDATED)
60. Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v2 [cs.LG] UPDATED)
61. Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v2 [cs.CV] UPDATED)
62. Counterfactual Multi-Token Fairness in Text Classification. (arXiv:2202.03792v2 [cs.CL] UPDATED)
63. GraphDCA -- a Framework for Node Distribution Comparison in Real and Synthetic Graphs. (arXiv:2202.03884v2 [cs.LG] UPDATED)
64. KENN: Enhancing Deep Neural Networks by Leveraging Knowledge for Time Series Forecasting. (arXiv:2202.03903v2 [cs.LG] UPDATED)
65. Existence, Stability and Scalability of Orthogonal Convolutional Neural Networks. (arXiv:2108.05623v2 [math.ST] CROSS LISTED)
66. Graph-Relational Domain Adaptation. (arXiv:2202.03628v1 [cs.LG] CROSS LISTED)
67. InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training. (arXiv:2202.03751v1 [eess.AS] CROSS LISTED)

