# Your interest papers
---
## cs.CV
---
### Phase Recognition in Contrast-Enhanced CT Scans based on Deep Learning and Random Sampling. (arXiv:2203.11206v1 [eess.IV])
- Authors : 
- Link : [http://arxiv.org/abs/2203.11206](http://arxiv.org/abs/2203.11206)
> ABSTRACT  :  A fully automated system for interpreting abdominal computed tomography (CT) scans with multiple phases of contrast **enhancement** requires an accurate classification of the phases. This work aims at developing and validating a precise, fast multi-phase classifier to recognize three main types of contrast phases in abdominal CT scans. We propose in this study a novel method that uses a random sampling mechanism on top of deep CNNs for the phase recognition of abdominal CT scans of four different phases: non-contrast, arterial, venous, and others. The CNNs work as a slice-wise phase prediction, while the random sampling selects input slices for the CNN models. Afterward, majority voting synthesizes the slice-wise results of the CNNs, to provide the final prediction at scan level. Our classifier was trained on 271,426 slices from 830 phase-annotated CT scans, and when combined with majority voting on 30% of slices randomly chosen from each scan, achieved a mean F1-score of 92.09% on our internal test set of 358 scans. The proposed method was also evaluated on 2 external test sets: CTPAC-CCRCC (N = 242) and LiTS (N = 131), which were annotated by our experts. Although a drop in performance has been observed, the model performance remained at a high level of accuracy with a mean F1-score of 76.79% and 86.94% on CTPAC-CCRCC and LiTS datasets, respectively. Our experimental results also showed that the proposed method significantly outperformed the state-of-the-art 3D approaches while requiring less computation time for inference.  
### Unsupervised Deraining: Where Contrastive Learning Meets Self-similarity. (arXiv:2203.11509v1 [cs.CV])
- Authors : Ye Yuntong, Yu Changfeng, Chang Yi, Zhu Lin, Zhao Xile, Yan Luxin, Tian Yonghong
- Link : [http://arxiv.org/abs/2203.11509](http://arxiv.org/abs/2203.11509)
> ABSTRACT  :  Image deraining is a typical low-level image **restoration** task, which aims at decomposing the rainy image into two distinguishable layers: the clean image layer and the rain layer. Most of the existing learning-based deraining methods are supervisedly trained on synthetic rainy-clean pairs. The domain gap between the synthetic and real rains makes them less generalized to different real rainy scenes. Moreover, the existing methods mainly utilize the property of the two layers independently, while few of them have considered the mutually exclusive relationship between the two layers. In this work, we propose a novel non-local contrastive learning (NLCL) method for unsupervised image deraining. Consequently, we not only utilize the intrinsic self-similarity property within samples but also the mutually exclusive property between the two layers, so as to better differ the rain layer from the clean image. Specifically, the non-local self-similarity image layer patches as the positives are pulled together and similar rain layer patches as the negatives are pushed away. Thus the similar positive/negative samples that are close in the original space benefit us to enrich more discriminative representation. Apart from the self-similarity sampling strategy, we analyze how to choose an appropriate feature encoder in NLCL. Extensive experiments on different real rainy datasets demonstrate that the proposed method obtains state-of-the-art performance in real deraining.  
### Adaptive Patch Exiting for Scalable Single Image Super-Resolution. (arXiv:2203.11589v1 [cs.CV])
- Authors : Shizun Wang, Ming Lu, Kaixin Chen, Xiaoqi Li, Jiaming Liu, Yandong Guo
- Link : [http://arxiv.org/abs/2203.11589](http://arxiv.org/abs/2203.11589)
> ABSTRACT  :  Since the future of computing is heterogeneous, scalability is a crucial problem for single image super-resolution. Recent works try to train one network, which can be deployed on platforms with different capacities. However, they rely on the pixel-wise sparse convolution, which is not hardware-friendly and achieves limited practical speedup. As image can be divided into patches, which have various **restoration** difficulties, we present a scalable method based on Adaptive Patch Exiting (APE) to achieve more practical speedup. Specifically, we propose to train a regressor to predict the incremental capacity of each layer for the patch. Once the incremental capacity is below the threshold, the patch can exit at the specific layer. Our method can easily adjust the trade-off between performance and efficiency by changing the threshold of incremental capacity. Furthermore, we propose a novel strategy to enable the network training of our method. We conduct extensive experiments across various backbones, datasets and scaling factors to demonstrate the advantages of our method. Code will be released.  
### Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain. (arXiv:2203.11722v1 [eess.IV])
- Authors : Rodrigo de, Barros Vimieiro, Chuang Niu, Hongming Shan, Lucas Rodrigues, Ge Wang, Marcelo Andrade, da Costa
- Link : [http://arxiv.org/abs/2203.11722](http://arxiv.org/abs/2203.11722)
> ABSTRACT  :  Digital breast tomosynthesis (DBT) exams should utilize the lowest possible radiation dose while maintaining sufficiently good image quality for accurate medical diagnosis. In this work, we propose a convolution neural network (CNN) to restore low-dose (LD) DBT projections to achieve an image quality equivalent to a standard full-dose (FD) acquisition. The proposed network architecture benefits from priors in terms of layers that were inspired by traditional model-based (MB) **restoration** methods, considering a model-based deep learning approach, where the network is trained to operate in the variance stabilization transformation (VST) domain. To accurately control the network operation point, in terms of noise and blur of the restored image, we propose a loss function that minimizes the bias and matches residual noise between the input and the output. The training dataset was composed of clinical data acquired at the standard FD and low-dose pairs obtained by the injection of quantum noise. The network was tested using real DBT projections acquired with a physical anthropomorphic breast phantom. The proposed network achieved superior results in terms of the mean normalized squared error (MNSE), training time and noise spatial correlation compared with networks trained with traditional data-driven methods. The proposed approach can be extended for other medical imaging application that requires LD acquisitions.  
### ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based Motion Segmentation. (arXiv:2203.11732v1 [cs.CV])
- Authors : Jinze Chen, Yang Wang, Yang Cao, Feng Wu, Jun Zha
- Link : [http://arxiv.org/abs/2203.11732](http://arxiv.org/abs/2203.11732)
> ABSTRACT  :  Dynamic Vision Sensor (DVS) can asynchronously output the events reflecting apparent motion of objects with microsecond resolution, and shows great application potential in monitoring and other fields. However, the output event stream of existing DVS inevitably contains background activity noise (BA noise) due to **dark** current and junction leakage current, which will affect the temporal correlation of objects, resulting in deteriorated motion estimation performance. Particularly, the existing filter-based denoising methods cannot be directly applied to suppress the noise in event stream, since there is no spatial correlation. To address this issue, this paper presents a novel progressive framework, in which a Motion Estimation (ME) module and an Event Denoising (ED) module are jointly optimized in a mutually reinforced manner. Specifically, based on the maximum sharpness criterion, ME module divides the input event into several segments by adaptive clustering in a motion compensating warp field, and captures the temporal correlation of event stream according to the clustered motion parameters. Taking temporal correlation as guidance, ED module calculates the confidence that each event belongs to real activity events, and transmits it to ME module to update energy function of motion segmentation for noise suppression. The two steps are iteratively updated until stable motion segmentation results are obtained. Extensive experimental results on both synthetic and real datasets demonstrate the superiority of our proposed approaches against the State-Of-The-Art (SOTA) methods.  
### Exploring and Evaluating Image **Restoration** Potential in Dynamic Scenes. (arXiv:2203.11754v1 [cs.CV])
- Authors : Cheng Zhang, Shaolin Su, Yu Zhu, Qingsen Yan, Jinqiu Sun, Yanning Zhang
- Link : [http://arxiv.org/abs/2203.11754](http://arxiv.org/abs/2203.11754)
> ABSTRACT  :  In dynamic scenes, images often suffer from dynamic blur due to superposition of motions or low signal-noise ratio resulted from quick shutter speed when avoiding motions. Recovering sharp and clean results from the captured images heavily depends on the ability of **restoration** methods and the quality of the input. Although existing research on image **restoration** focuses on developing models for obtaining better restored results, fewer have studied to evaluate how and which input image leads to superior restored quality. In this paper, to better study an image's potential value that can be explored for **restoration**, we propose a novel concept, referring to image **restoration** potential (IRP). Specifically, We first establish a dynamic scene imaging dataset containing composite distortions and applied image **restoration** processes to validate the rationality of the existence to IRP. Based on this dataset, we investigate several properties of IRP and propose a novel deep model to accurately predict IRP values. By gradually distilling and selective fusing the degradation features, the proposed model shows its superiority in IRP prediction. Thanks to the proposed model, we are then able to validate how various image **restoration** related applications are benefited from IRP prediction. We show the potential usages of IRP as a filtering principle to select valuable frames, an auxiliary guidance to improve **restoration** models, and even an indicator to optimize camera settings for capturing better images under dynamic scenarios.  
### A Novel Framework for Assessment of Learning-based Detectors in Realistic Conditions with Application to Deepfake Detection. (arXiv:2203.11797v1 [cs.CV])
- Authors : Yuhang Lu, Ruizhi Luo, Touradj Ebrahimi
- Link : [http://arxiv.org/abs/2203.11797](http://arxiv.org/abs/2203.11797)
> ABSTRACT  :  Deep convolutional neural networks have shown remarkable results on multiple detection tasks. Despite the significant progress, the performance of such detectors are often assessed in public benchmarks under non-realistic conditions. Specifically, impact of conventional distortions and processing operations such as compression, noise, and **enhancement** are not sufficiently studied. This paper proposes a rigorous framework to assess performance of learning-based detectors in more realistic situations. An illustrative example is shown under deepfake detection context. Inspired by the assessment results, a data augmentation strategy based on natural image degradation process is designed, which significantly improves the generalization ability of two deepfake detectors.  
### A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions. (arXiv:2203.11807v1 [cs.CV])
- Authors : Yuhang Lu, Touradj Ebrahimi
- Link : [http://arxiv.org/abs/2203.11807](http://arxiv.org/abs/2203.11807)
> ABSTRACT  :  Deep convolutional neural networks have achieved exceptional results on multiple detection and recognition tasks. However, the performance of such detectors are often evaluated in public benchmarks under constrained and non-realistic situations. The impact of conventional distortions and processing operations found in imaging workflows such as compression, noise, and **enhancement** are not sufficiently studied. Currently, only a few researches have been done to improve the detector robustness to unseen perturbations. This paper proposes a more effective data augmentation scheme based on real-world image degradation process. This novel technique is deployed for deepfake detection tasks and has been evaluated by a more realistic assessment framework. Extensive experiments show that the proposed data augmentation scheme improves generalization ability to unpredictable data distortions and unseen datasets.  
### A **Real-time** Junk Food Recognition System based on Machine Learning. (arXiv:2203.11836v1 [cs.CV])
- Authors : Sirajum Munira, Takitazwar Parthib, Sabikunnahar Talukder, Nila Maitra, Niloy Kumar, Kishor Morol
- Link : [http://arxiv.org/abs/2203.11836](http://arxiv.org/abs/2203.11836)
> ABSTRACT  :  $ $As a result of bad eating habits, humanity may be destroyed. People are constantly on the lookout for tasty foods, with junk foods being the most common source. As a consequence, our eating patterns are shifting, and we're gravitating toward junk food more than ever, which is bad for our health and increases our risk of acquiring health problems. Machine learning principles are applied in every aspect of our lives, and one of them is object recognition via image processing. However, because foods vary in nature, this procedure is crucial, and traditional methods like ANN, SVM, KNN, PLS etc., will result in a low accuracy rate. All of these issues were defeated by the Deep Neural Network. In this work, we created a fresh dataset of 10,000 data points from 20 junk food classifications to try to recognize junk foods. All of the data in the data set was gathered using the Google search engine, which is thought to be one-of-a-kind in every way. The goal was achieved using Convolution Neural Network (CNN) technology, which is well-known for image processing. We achieved a 98.05\% accuracy rate throughout the research, which was satisfactory. In addition, we conducted a test based on a real-life event, and the outcome was extraordinary. Our goal is to advance this research to the next level, so that it may be applied to a future study. Our ultimate goal is to create a system that would encourage people to avoid eating junk food and to be health-conscious. \keywords{ Machine Learning \and junk food \and object detection \and YOLOv3 \and custom food dataset.}  
### Focal Modulation Networks. (arXiv:2203.11926v1 [cs.CV])
- Authors : Jianwei Yang, Chunyuan Li, Jianfeng Gao
- Link : [http://arxiv.org/abs/2203.11926](http://arxiv.org/abs/2203.11926)
> ABSTRACT  :  In this work, we propose focal modulation network (FocalNet in short), where self-attention (SA) is completely replaced by a focal modulation module that is more effective and efficient for modeling token interactions. Focal modulation comprises three components: $(i)$ hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges at different granularity levels, $(ii)$ gated aggregation to selectively aggregate context features for each visual token (query) based on its content, and $(iii)$ modulation or element-wise affine transformation to fuse the aggregated features into the query vector. Extensive experiments show that FocalNets outperform the state-of-the-art SA counterparts (e.g., **Swin** Transformers) with similar time and memory cost on the tasks of image classification, object detection, and semantic segmentation. Specifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\times$224 and 384$\times$384, respectively. FocalNets exhibit remarkable superiority when transferred to downstream tasks. For object detection with Mask R-CNN, our FocalNet base trained with 1$\times$ already surpasses **Swin** trained with 3$\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet, FocalNet base evaluated at single-scale outperforms **Swin** evaluated at multi-scale (50.5 v.s. 49.7). These results render focal modulation a favorable alternative to SA for effective and efficient visual modeling in real-world applications. Code is available at https://github.com/microsoft/FocalNet.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v5 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
### Learning Efficient Multi-Agent Cooperative Visual Exploration. (arXiv:2110.05734v2 [cs.CV] UPDATED)
- Authors : Chao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, Yi Wu
- Link : [http://arxiv.org/abs/2110.05734](http://arxiv.org/abs/2110.05734)
> ABSTRACT  :  We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent **enhancement**s to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.  
### DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v3 [cs.CV] UPDATED)
- Authors : Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, **Lei Zhang**
- Link : [http://arxiv.org/abs/2201.12329](http://arxiv.org/abs/2201.12329)
> ABSTRACT  :  We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \url{https://github.com/SlongLiu/DAB-DETR}.  
### Abandoning the Bayer-Filter to See in the **Dark**. (arXiv:2203.04042v2 [eess.IV] UPDATED)
- Authors : Xingbo Dong, Wanyan Xu, Zhihui Miao, Lan Ma, Chao Zhang, Jiewen Yang, Zhe Jin, Andrew Beng, Jin Teoh, Jiajun Shen
- Link : [http://arxiv.org/abs/2203.04042](http://arxiv.org/abs/2203.04042)
> ABSTRACT  :  **Low-light** image **enhancement** - a pervasive but challenging problem, plays a central role in enhancing the visibility of an image captured in a poor illumination environment. Due to the fact that not all photons can pass the Bayer-Filter on the sensor of the color camera, in this work, we first present a De-Bayer-Filter simulator based on deep neural networks to generate a monochrome raw image from the colored raw image. Next, a fully convolutional network is proposed to achieve the **low-light** image **enhancement** by fusing colored raw data with synthesized monochrome raw data. Channel-wise attention is also introduced to the fusion process to establish a complementary interaction between features from colored and monochrome raw images. To train the convolutional networks, we propose a dataset with monochrome and color raw pairs named Mono-Colored Raw paired dataset (MCR) collected by using a monochrome camera without Bayer-Filter and a color camera with Bayer-Filter. The proposed pipeline take advantages of the fusion of the virtual monochrome and the color raw images and our extensive experiments indicate that significant improvement can be achieved by leveraging raw sensor data and data-driven learning.  
### CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v2 [cs.CV] UPDATED)
- Authors : Matheus Souza, Wolfgang Heidrich
- Link : [http://arxiv.org/abs/2203.10562](http://arxiv.org/abs/2203.10562)
> ABSTRACT  :  Image signal processors (ISPs) are historically grown legacy software systems for reconstructing color images from noisy raw sensor measurements. They are usually composited of many heuristic blocks for denoising, demosaicking, and color **restoration**. Color reproduction in this context is of particular importance, since the raw colors are often severely distorted, and each smart phone manufacturer has developed their own characteristic heuristics for improving the color rendition, for example of skin tones and other visually important colors.    In recent years there has been strong interest in replacing the historically grown ISP systems with deep learned pipelines. Much progress has been made in approximating legacy ISPs with such learned models. However, so far the focus of these efforts has been on reproducing the structural features of the images, with less attention paid to color rendition.    Here we present CRISPnet, the first learned ISP model to specifically target color rendition accuracy relative to a complex, legacy smart phone ISP. We achieve this by utilizing both image metadata (like a legacy ISP would), as well as by learning simple global semantics based on image classification -- similar to what a legacy ISP does to determine the scene type. We also contribute a new ISP image dataset consisting of both **high dynamic range** monitor data, as well as real-world data, both captured with an actual cell phone ISP pipeline under a variety of lighting conditions, **exposure** times, and gain settings.  
## eess.IV
---
### Phase Recognition in Contrast-Enhanced CT Scans based on Deep Learning and Random Sampling. (arXiv:2203.11206v1 [eess.IV])
- Authors : 
- Link : [http://arxiv.org/abs/2203.11206](http://arxiv.org/abs/2203.11206)
> ABSTRACT  :  A fully automated system for interpreting abdominal computed tomography (CT) scans with multiple phases of contrast **enhancement** requires an accurate classification of the phases. This work aims at developing and validating a precise, fast multi-phase classifier to recognize three main types of contrast phases in abdominal CT scans. We propose in this study a novel method that uses a random sampling mechanism on top of deep CNNs for the phase recognition of abdominal CT scans of four different phases: non-contrast, arterial, venous, and others. The CNNs work as a slice-wise phase prediction, while the random sampling selects input slices for the CNN models. Afterward, majority voting synthesizes the slice-wise results of the CNNs, to provide the final prediction at scan level. Our classifier was trained on 271,426 slices from 830 phase-annotated CT scans, and when combined with majority voting on 30% of slices randomly chosen from each scan, achieved a mean F1-score of 92.09% on our internal test set of 358 scans. The proposed method was also evaluated on 2 external test sets: CTPAC-CCRCC (N = 242) and LiTS (N = 131), which were annotated by our experts. Although a drop in performance has been observed, the model performance remained at a high level of accuracy with a mean F1-score of 76.79% and 86.94% on CTPAC-CCRCC and LiTS datasets, respectively. Our experimental results also showed that the proposed method significantly outperformed the state-of-the-art 3D approaches while requiring less computation time for inference.  
### Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain. (arXiv:2203.11722v1 [eess.IV])
- Authors : Rodrigo de, Barros Vimieiro, Chuang Niu, Hongming Shan, Lucas Rodrigues, Ge Wang, Marcelo Andrade, da Costa
- Link : [http://arxiv.org/abs/2203.11722](http://arxiv.org/abs/2203.11722)
> ABSTRACT  :  Digital breast tomosynthesis (DBT) exams should utilize the lowest possible radiation dose while maintaining sufficiently good image quality for accurate medical diagnosis. In this work, we propose a convolution neural network (CNN) to restore low-dose (LD) DBT projections to achieve an image quality equivalent to a standard full-dose (FD) acquisition. The proposed network architecture benefits from priors in terms of layers that were inspired by traditional model-based (MB) **restoration** methods, considering a model-based deep learning approach, where the network is trained to operate in the variance stabilization transformation (VST) domain. To accurately control the network operation point, in terms of noise and blur of the restored image, we propose a loss function that minimizes the bias and matches residual noise between the input and the output. The training dataset was composed of clinical data acquired at the standard FD and low-dose pairs obtained by the injection of quantum noise. The network was tested using real DBT projections acquired with a physical anthropomorphic breast phantom. The proposed network achieved superior results in terms of the mean normalized squared error (MNSE), training time and noise spatial correlation compared with networks trained with traditional data-driven methods. The proposed approach can be extended for other medical imaging application that requires LD acquisitions.  
### A Novel Framework for Assessment of Learning-based Detectors in Realistic Conditions with Application to Deepfake Detection. (arXiv:2203.11797v1 [cs.CV])
- Authors : Yuhang Lu, Ruizhi Luo, Touradj Ebrahimi
- Link : [http://arxiv.org/abs/2203.11797](http://arxiv.org/abs/2203.11797)
> ABSTRACT  :  Deep convolutional neural networks have shown remarkable results on multiple detection tasks. Despite the significant progress, the performance of such detectors are often assessed in public benchmarks under non-realistic conditions. Specifically, impact of conventional distortions and processing operations such as compression, noise, and **enhancement** are not sufficiently studied. This paper proposes a rigorous framework to assess performance of learning-based detectors in more realistic situations. An illustrative example is shown under deepfake detection context. Inspired by the assessment results, a data augmentation strategy based on natural image degradation process is designed, which significantly improves the generalization ability of two deepfake detectors.  
### A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions. (arXiv:2203.11807v1 [cs.CV])
- Authors : Yuhang Lu, Touradj Ebrahimi
- Link : [http://arxiv.org/abs/2203.11807](http://arxiv.org/abs/2203.11807)
> ABSTRACT  :  Deep convolutional neural networks have achieved exceptional results on multiple detection and recognition tasks. However, the performance of such detectors are often evaluated in public benchmarks under constrained and non-realistic situations. The impact of conventional distortions and processing operations found in imaging workflows such as compression, noise, and **enhancement** are not sufficiently studied. Currently, only a few researches have been done to improve the detector robustness to unseen perturbations. This paper proposes a more effective data augmentation scheme based on real-world image degradation process. This novel technique is deployed for deepfake detection tasks and has been evaluated by a more realistic assessment framework. Extensive experiments show that the proposed data augmentation scheme improves generalization ability to unpredictable data distortions and unseen datasets.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v5 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
### Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral Images using a Hybrid Deep Neural Network. (arXiv:2107.04631v3 [eess.IV] UPDATED)
- Authors : Fangcao Xu, Jian Sun, Guido Cervone, Mark Salvador
- Link : [http://arxiv.org/abs/2107.04631](http://arxiv.org/abs/2107.04631)
> ABSTRACT  :  Atmospheric correction is a fundamental task in remote sensing because observations are taken either of the atmosphere or looking through the atmosphere. Atmospheric correction errors can significantly alter the spectral signature of the observations, and lead to invalid classifications or target detection. This is even more crucial when working with hyperspectral data, where a precise measurement of spectral properties is required. State-of-the-art physics-based atmospheric correction approaches require extensive prior knowledge about sensor characteristics, collection geometry, and environmental characteristics of the scene being collected. These approaches are computationally expensive, prone to inaccuracy due to lack of sufficient environmental and collection information, and often impossible for real-time applications. In this paper, a geometry-dependent hybrid neural network is proposed for automatic atmospheric correction using multi-scan hyperspectral data collected from different geometries. The proposed network can characterize the atmosphere without any additional meteorological data. A grid-search method is also proposed to solve the temperature emissivity separation problem. Results show that the proposed network has the capacity to accurately characterize the atmosphere and estimate target emissivity spectra with a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This solution can lead to accurate atmospheric correction to improve target detection for **real time** applications.  
### Abandoning the Bayer-Filter to See in the **Dark**. (arXiv:2203.04042v2 [eess.IV] UPDATED)
- Authors : Xingbo Dong, Wanyan Xu, Zhihui Miao, Lan Ma, Chao Zhang, Jiewen Yang, Zhe Jin, Andrew Beng, Jin Teoh, Jiajun Shen
- Link : [http://arxiv.org/abs/2203.04042](http://arxiv.org/abs/2203.04042)
> ABSTRACT  :  **Low-light** image **enhancement** - a pervasive but challenging problem, plays a central role in enhancing the visibility of an image captured in a poor illumination environment. Due to the fact that not all photons can pass the Bayer-Filter on the sensor of the color camera, in this work, we first present a De-Bayer-Filter simulator based on deep neural networks to generate a monochrome raw image from the colored raw image. Next, a fully convolutional network is proposed to achieve the **low-light** image **enhancement** by fusing colored raw data with synthesized monochrome raw data. Channel-wise attention is also introduced to the fusion process to establish a complementary interaction between features from colored and monochrome raw images. To train the convolutional networks, we propose a dataset with monochrome and color raw pairs named Mono-Colored Raw paired dataset (MCR) collected by using a monochrome camera without Bayer-Filter and a color camera with Bayer-Filter. The proposed pipeline take advantages of the fusion of the virtual monochrome and the color raw images and our extensive experiments indicate that significant improvement can be achieved by leveraging raw sensor data and data-driven learning.  
### CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v2 [cs.CV] UPDATED)
- Authors : Matheus Souza, Wolfgang Heidrich
- Link : [http://arxiv.org/abs/2203.10562](http://arxiv.org/abs/2203.10562)
> ABSTRACT  :  Image signal processors (ISPs) are historically grown legacy software systems for reconstructing color images from noisy raw sensor measurements. They are usually composited of many heuristic blocks for denoising, demosaicking, and color **restoration**. Color reproduction in this context is of particular importance, since the raw colors are often severely distorted, and each smart phone manufacturer has developed their own characteristic heuristics for improving the color rendition, for example of skin tones and other visually important colors.    In recent years there has been strong interest in replacing the historically grown ISP systems with deep learned pipelines. Much progress has been made in approximating legacy ISPs with such learned models. However, so far the focus of these efforts has been on reproducing the structural features of the images, with less attention paid to color rendition.    Here we present CRISPnet, the first learned ISP model to specifically target color rendition accuracy relative to a complex, legacy smart phone ISP. We achieve this by utilizing both image metadata (like a legacy ISP would), as well as by learning simple global semantics based on image classification -- similar to what a legacy ISP does to determine the scene type. We also contribute a new ISP image dataset consisting of both **high dynamic range** monitor data, as well as real-world data, both captured with an actual cell phone ISP pipeline under a variety of lighting conditions, **exposure** times, and gain settings.  
## cs.LG
---
### DIANES: A DEI Audit Toolkit for News Sources. (arXiv:2203.11383v1 [cs.IR])
- Authors : Xiaoxiao Shang, Zhiyuan Peng, Qiming Yuan, Sabiq Khan, Lauren Xie, Yi Fang, Subramaniam Vincent
- Link : [http://arxiv.org/abs/2203.11383](http://arxiv.org/abs/2203.11383)
> ABSTRACT  :  Professional news media organizations have always touted the importance that they give to multiple perspectives. However, in practice the traditional approach to all-sides has favored people in the dominant culture. Hence it has come under ethical critique under the new norms of diversity, equity, and inclusion (DEI). When DEI is applied to journalism, it goes beyond conventional notions of impartiality and bias and instead democratizes the journalistic practice of sourcing -- who is quoted or interviewed, who is not, how often, from which demographic group, gender, and so forth. There is currently no real-time or on-demand tool in the hands of reporters to analyze the persons they quote. In this paper, we present DIANES, a DEI Audit Toolkit for News Sources. It consists of a natural language processing pipeline on the backend to extract quotes, speakers, titles, and organizations from news articles in **real time**. On the frontend, DIANES offers the WordPress plugins, a Web monitor, and a DEI annotation API service, to help news media monitor their own quoting patterns and push themselves towards DEI norms.  
### Residual-Guided Non-Intrusive Speech Quality Assessment. (arXiv:2203.11499v1 [cs.SD])
- Authors : Zhe Ye, Jiahao Chen, Diqun Yan
- Link : [http://arxiv.org/abs/2203.11499](http://arxiv.org/abs/2203.11499)
> ABSTRACT  :  This paper proposes an approach to improve Non-Intrusive speech quality assessment(NI-SQA) based on the residuals between impaired speech and enhanced speech. The difficulty in our task is particularly lack of information, for which the corresponding reference speech is absent. We generate an enhanced speech on the impaired speech to compensate for the absence of the reference audio, then pair the information of residuals with the impaired speech. Compared to feeding the impaired speech directly into the model, residuals could bring some extra helpful information from the contrast in **enhancement**. The human ear is sensitive to certain noises but different to deep learning model. Causing the Mean Opinion Score(MOS) the model predicted is not enough to fit our subjective sensitive well and causes deviation. These residuals have a close relationship to reference speech and then improve the ability of the deep learning models to predict MOS. During the training phase, experimental results demonstrate that paired with residuals can quickly obtain better evaluation indicators under the same conditions. Furthermore, our final results improved 31.3 percent and 14.1 percent, respectively, in PLCC and RMSE.  
### Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain. (arXiv:2203.11722v1 [eess.IV])
- Authors : Rodrigo de, Barros Vimieiro, Chuang Niu, Hongming Shan, Lucas Rodrigues, Ge Wang, Marcelo Andrade, da Costa
- Link : [http://arxiv.org/abs/2203.11722](http://arxiv.org/abs/2203.11722)
> ABSTRACT  :  Digital breast tomosynthesis (DBT) exams should utilize the lowest possible radiation dose while maintaining sufficiently good image quality for accurate medical diagnosis. In this work, we propose a convolution neural network (CNN) to restore low-dose (LD) DBT projections to achieve an image quality equivalent to a standard full-dose (FD) acquisition. The proposed network architecture benefits from priors in terms of layers that were inspired by traditional model-based (MB) **restoration** methods, considering a model-based deep learning approach, where the network is trained to operate in the variance stabilization transformation (VST) domain. To accurately control the network operation point, in terms of noise and blur of the restored image, we propose a loss function that minimizes the bias and matches residual noise between the input and the output. The training dataset was composed of clinical data acquired at the standard FD and low-dose pairs obtained by the injection of quantum noise. The network was tested using real DBT projections acquired with a physical anthropomorphic breast phantom. The proposed network achieved superior results in terms of the mean normalized squared error (MNSE), training time and noise spatial correlation compared with networks trained with traditional data-driven methods. The proposed approach can be extended for other medical imaging application that requires LD acquisitions.  
### Machine Learning based Data Driven Diagnostic and Prognostic Approach for Laser Reliability **Enhancement**. (arXiv:2203.11728v1 [cs.LG])
- Authors : khouloud Abdelli, Helmut Griesser, Stephan Pachnicke
- Link : [http://arxiv.org/abs/2203.11728](http://arxiv.org/abs/2203.11728)
> ABSTRACT  :  In this paper, a data-driven diagnostic and prognostic approach based on machine learning is proposed to detect laser failure modes and to predict the remaining useful life (RUL) of a laser during its operation. We present an architecture of the proposed cognitive predictive maintenance framework and demonstrate its effectiveness using synthetic data.  
### Machine Learning based Laser Failure Mode Detection. (arXiv:2203.11729v1 [cs.LG])
- Authors : khouloud Abdelli, Danish Rafique, Stephan Pachnicke
- Link : [http://arxiv.org/abs/2203.11729](http://arxiv.org/abs/2203.11729)
> ABSTRACT  :  Laser degradation analysis is a crucial process for the **enhancement** of laser reliability. Here, we propose a data-driven fault detection approach based on Long Short-Term Memory (LSTM) recurrent neural networks to detect the different laser degradation modes based on synthetic historical failure data. In comparison to typical threshold-based systems, attaining 24.41% classification accuracy, the LSTM-based model achieves 95.52% accuracy, and also outperforms classical machine learning (ML) models namely Random Forest (RF), K-Nearest Neighbours (KNN) and Logistic Regression (LR).  
### A **Real-time** Junk Food Recognition System based on Machine Learning. (arXiv:2203.11836v1 [cs.CV])
- Authors : Sirajum Munira, Takitazwar Parthib, Sabikunnahar Talukder, Nila Maitra, Niloy Kumar, Kishor Morol
- Link : [http://arxiv.org/abs/2203.11836](http://arxiv.org/abs/2203.11836)
> ABSTRACT  :  $ $As a result of bad eating habits, humanity may be destroyed. People are constantly on the lookout for tasty foods, with junk foods being the most common source. As a consequence, our eating patterns are shifting, and we're gravitating toward junk food more than ever, which is bad for our health and increases our risk of acquiring health problems. Machine learning principles are applied in every aspect of our lives, and one of them is object recognition via image processing. However, because foods vary in nature, this procedure is crucial, and traditional methods like ANN, SVM, KNN, PLS etc., will result in a low accuracy rate. All of these issues were defeated by the Deep Neural Network. In this work, we created a fresh dataset of 10,000 data points from 20 junk food classifications to try to recognize junk foods. All of the data in the data set was gathered using the Google search engine, which is thought to be one-of-a-kind in every way. The goal was achieved using Convolution Neural Network (CNN) technology, which is well-known for image processing. We achieved a 98.05\% accuracy rate throughout the research, which was satisfactory. In addition, we conducted a test based on a real-life event, and the outcome was extraordinary. Our goal is to advance this research to the next level, so that it may be applied to a future study. Our ultimate goal is to create a system that would encourage people to avoid eating junk food and to be health-conscious. \keywords{ Machine Learning \and junk food \and object detection \and YOLOv3 \and custom food dataset.}  
### Focal Modulation Networks. (arXiv:2203.11926v1 [cs.CV])
- Authors : Jianwei Yang, Chunyuan Li, Jianfeng Gao
- Link : [http://arxiv.org/abs/2203.11926](http://arxiv.org/abs/2203.11926)
> ABSTRACT  :  In this work, we propose focal modulation network (FocalNet in short), where self-attention (SA) is completely replaced by a focal modulation module that is more effective and efficient for modeling token interactions. Focal modulation comprises three components: $(i)$ hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges at different granularity levels, $(ii)$ gated aggregation to selectively aggregate context features for each visual token (query) based on its content, and $(iii)$ modulation or element-wise affine transformation to fuse the aggregated features into the query vector. Extensive experiments show that FocalNets outperform the state-of-the-art SA counterparts (e.g., **Swin** Transformers) with similar time and memory cost on the tasks of image classification, object detection, and semantic segmentation. Specifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\times$224 and 384$\times$384, respectively. FocalNets exhibit remarkable superiority when transferred to downstream tasks. For object detection with Mask R-CNN, our FocalNet base trained with 1$\times$ already surpasses **Swin** trained with 3$\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet, FocalNet base evaluated at single-scale outperforms **Swin** evaluated at multi-scale (50.5 v.s. 49.7). These results render focal modulation a favorable alternative to SA for effective and efficient visual modeling in real-world applications. Code is available at https://github.com/microsoft/FocalNet.  
### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v5 [cs.CV] UPDATED)
- Authors : Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, **Peyman Milanfar**, Feng Yang
- Link : [http://arxiv.org/abs/2104.13450](http://arxiv.org/abs/2104.13450)
> ABSTRACT  :  Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.  
### Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral Images using a Hybrid Deep Neural Network. (arXiv:2107.04631v3 [eess.IV] UPDATED)
- Authors : Fangcao Xu, Jian Sun, Guido Cervone, Mark Salvador
- Link : [http://arxiv.org/abs/2107.04631](http://arxiv.org/abs/2107.04631)
> ABSTRACT  :  Atmospheric correction is a fundamental task in remote sensing because observations are taken either of the atmosphere or looking through the atmosphere. Atmospheric correction errors can significantly alter the spectral signature of the observations, and lead to invalid classifications or target detection. This is even more crucial when working with hyperspectral data, where a precise measurement of spectral properties is required. State-of-the-art physics-based atmospheric correction approaches require extensive prior knowledge about sensor characteristics, collection geometry, and environmental characteristics of the scene being collected. These approaches are computationally expensive, prone to inaccuracy due to lack of sufficient environmental and collection information, and often impossible for real-time applications. In this paper, a geometry-dependent hybrid neural network is proposed for automatic atmospheric correction using multi-scan hyperspectral data collected from different geometries. The proposed network can characterize the atmosphere without any additional meteorological data. A grid-search method is also proposed to solve the temperature emissivity separation problem. Results show that the proposed network has the capacity to accurately characterize the atmosphere and estimate target emissivity spectra with a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This solution can lead to accurate atmospheric correction to improve target detection for **real time** applications.  
### Learning Efficient Multi-Agent Cooperative Visual Exploration. (arXiv:2110.05734v2 [cs.CV] UPDATED)
- Authors : Chao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, Yi Wu
- Link : [http://arxiv.org/abs/2110.05734](http://arxiv.org/abs/2110.05734)
> ABSTRACT  :  We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent **enhancement**s to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.  
## cs.AI
---
### Focal Modulation Networks. (arXiv:2203.11926v1 [cs.CV])
- Authors : Jianwei Yang, Chunyuan Li, Jianfeng Gao
- Link : [http://arxiv.org/abs/2203.11926](http://arxiv.org/abs/2203.11926)
> ABSTRACT  :  In this work, we propose focal modulation network (FocalNet in short), where self-attention (SA) is completely replaced by a focal modulation module that is more effective and efficient for modeling token interactions. Focal modulation comprises three components: $(i)$ hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges at different granularity levels, $(ii)$ gated aggregation to selectively aggregate context features for each visual token (query) based on its content, and $(iii)$ modulation or element-wise affine transformation to fuse the aggregated features into the query vector. Extensive experiments show that FocalNets outperform the state-of-the-art SA counterparts (e.g., **Swin** Transformers) with similar time and memory cost on the tasks of image classification, object detection, and semantic segmentation. Specifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\times$224 and 384$\times$384, respectively. FocalNets exhibit remarkable superiority when transferred to downstream tasks. For object detection with Mask R-CNN, our FocalNet base trained with 1$\times$ already surpasses **Swin** trained with 3$\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet, FocalNet base evaluated at single-scale outperforms **Swin** evaluated at multi-scale (50.5 v.s. 49.7). These results render focal modulation a favorable alternative to SA for effective and efficient visual modeling in real-world applications. Code is available at https://github.com/microsoft/FocalNet.  
# Paper List
---
## cs.CV
---
**147** new papers in cs.CV:-) 
1. VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. (arXiv:2203.11205v1 [eess.IV])
2. Phase Recognition in Contrast-Enhanced CT Scans based on Deep Learning and Random Sampling. (arXiv:2203.11206v1 [eess.IV])
3. On the Effect of Pre-Processing and Model Complexity for Plastic Analysis Using Short-Wave-Infrared Hyper-Spectral Imaging. (arXiv:2203.11209v1 [cs.CV])
4. Disentangling Patterns and Transformations from One Sequence of Images with Shape-invariant Lie Group Transformer. (arXiv:2203.11210v1 [cs.CV])
5. ME-Net: Multi-Encoder Net Framework for Brain Tumor Segmentation. (arXiv:2203.11213v1 [eess.IV])
6. A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v1 [cs.LG])
7. Contribution of Different Handwriting Modalities to Differential Diagnosis of Parkinson's Disease. (arXiv:2203.11269v1 [eess.SP])
8. NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction. (arXiv:2203.11283v1 [cs.CV])
9. A Contrastive Objective for Learning Disentangled Representations. (arXiv:2203.11284v1 [cs.CV])
10. Generative Adversarial Network for Future Hand Segmentation from Egocentric Video. (arXiv:2203.11305v1 [cs.CV])
11. Training Quantised Neural Networks with STE Variants: the Additive Noise Annealing Algorithm. (arXiv:2203.11323v1 [cs.LG])
12. Global Matching with Overlapping Attention for Optical Flow Estimation. (arXiv:2203.11335v1 [cs.CV])
13. Segmenting Medical Instruments in Minimally Invasive Surgeries using AttentionMask. (arXiv:2203.11358v1 [cs.CV])
14. Audio visual character profiles for detecting background characters in entertainment media. (arXiv:2203.11368v1 [cs.CV])
15. HyperShot: Few-Shot Learning by Kernel HyperNetworks. (arXiv:2203.11378v1 [cs.LG])
16. Survival Analysis for Idiopathic Pulmonary Fibrosis using CT Images and Incomplete Clinical Data. (arXiv:2203.11391v1 [eess.IV])
17. Multiple Convex Objects Image Segmentation via Proximal Alternating Direction Method of Multipliers. (arXiv:2203.11395v1 [cs.CV])
18. A Real World Dataset for Multi-view 3D Reconstruction. (arXiv:2203.11397v1 [cs.CV])
19. Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception. (arXiv:2203.11405v1 [cs.CV])
20. Gated Domain-Invariant Feature Disentanglement for Domain Generalizable Object Detection. (arXiv:2203.11432v1 [cs.CV])
21. Making DeepFakes more spurious: evading deep face forgery detection via trace removal attack. (arXiv:2203.11433v1 [cs.CV])
22. Self-Supervised Representation Learning as Multimodal Variational Inference. (arXiv:2203.11437v1 [cs.CV])
23. Multi-Modal Learning for AU Detection Based on Multi-Head Fused Transformers. (arXiv:2203.11441v1 [cs.CV])
24. Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v1 [cs.CV])
25. Manipulating UAV Imagery for Satellite Model Training, Calibration and Testing. (arXiv:2203.11447v1 [cs.CV])
26. Leveraging Textures in Zero-shot Understanding of Fine-Grained Domains. (arXiv:2203.11449v1 [cs.CV])
27. DepthGAN: GAN-based Depth Generation of Indoor Scenes from Semantic Layouts. (arXiv:2203.11453v1 [cs.CV])
28. Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization. (arXiv:2203.11471v1 [cs.CV])
29. Remember Intentions: Retrospective-Memory-based Trajectory Prediction. (arXiv:2203.11474v1 [cs.CV])
30. WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v1 [cs.CV])
31. Mixed Differential Privacy in Computer Vision. (arXiv:2203.11481v1 [cs.CV])
32. Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation. (arXiv:2203.11483v1 [cs.CV])
33. SSD-KD: A Self-supervised Diverse Knowledge Distillation Method for Lightweight Skin Lesion Classification Using Dermoscopic Images. (arXiv:2203.11490v1 [cs.CV])
34. FrameHopper: Selective Processing of Video Frames in Detection-driven Real-Time Video Analytics. (arXiv:2203.11493v1 [cs.CV])
35. TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers. (arXiv:2203.11496v1 [cs.CV])
36. Rebalanced Siamese Contrastive Mining for Long-Tailed Recognition. (arXiv:2203.11506v1 [cs.CV])
37. Unsupervised Deraining: Where Contrastive Learning Meets Self-similarity. (arXiv:2203.11509v1 [cs.CV])
38. Convolutional Neural Network-based Efficient Dense Point Cloud Generation using Unsigned Distance Fields. (arXiv:2203.11537v1 [cs.CV])
39. Mask Usage Recognition using Vision Transformer with Transfer Learning and Data Augmentation. (arXiv:2203.11542v1 [cs.CV])
40. Frugal Learning of Virtual Exemplars for Label-Efficient Satellite Image Change Detection. (arXiv:2203.11559v1 [cs.CV])
41. Reinforcement-based frugal learning for satellite image change detection. (arXiv:2203.11564v1 [cs.CV])
42. Multi-layer Clustering-based Residual Sparsifying Transform for Low-dose CT Image Reconstruction. (arXiv:2203.11565v1 [eess.IV])
43. Adaptive Patch Exiting for Scalable Single Image Super-Resolution. (arXiv:2203.11589v1 [cs.CV])
44. IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment. (arXiv:2203.11590v1 [cs.CV])
45. HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation. (arXiv:2203.11591v1 [cs.CV])
46. Unified Negative Pair Generation toward Well-discriminative Feature Space for Face Recognition. (arXiv:2203.11593v1 [cs.CV])
47. Dense Residual Networks for Gaze Mapping on Indian Roads. (arXiv:2203.11611v1 [cs.CV])
48. High-resolution Iterative Feedback Network for Camouflaged Object Detection. (arXiv:2203.11624v1 [cs.CV])
49. QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human Motion Animation. (arXiv:2203.11632v1 [cs.CV])
50. Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos. (arXiv:2203.11637v1 [cs.CV])
51. Semantic State Estimation in Cloth Manipulation Tasks. (arXiv:2203.11647v1 [cs.RO])
52. Weakly-Supervised Salient Object Detection Using Point Supervison. (arXiv:2203.11652v1 [cs.CV])
53. Fine-Grained Scene Graph Generation with Data Transfer. (arXiv:2203.11654v1 [cs.CV])
54. Channel Self-Supervision for Online Knowledge Distillation. (arXiv:2203.11660v1 [cs.CV])
55. CNNs and Transformers Perceive Hybrid Images Similar to Humans. (arXiv:2203.11678v1 [cs.CV])
56. Meta-attention for ViT-backed Continual Learning. (arXiv:2203.11684v1 [cs.CV])
57. End-to-End Learned Block-Based Image Compression with Block-Level Masked Convolutions and Asymptotic Closed Loop Training. (arXiv:2203.11686v1 [eess.IV])
58. Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v1 [eess.IV])
59. Optical Flow Based Motion Detection for Autonomous Driving. (arXiv:2203.11693v1 [cs.CV])
60. CP2: Copy-Paste Contrastive Pretraining for Semantic Segmentation. (arXiv:2203.11709v1 [cs.CV])
61. Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain. (arXiv:2203.11722v1 [eess.IV])
62. Unsupervised Anomaly Detection in Medical Images with a Memory-augmented Multi-level Cross-attentional Masked Autoencoder. (arXiv:2203.11725v1 [eess.IV])
63. AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography. (arXiv:2203.11726v1 [physics.med-ph])
64. ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based Motion Segmentation. (arXiv:2203.11732v1 [cs.CV])
65. Exploring and Evaluating Image **Restoration** Potential in Dynamic Scenes. (arXiv:2203.11754v1 [cs.CV])
66. A Novel Framework for Assessment of Learning-based Detectors in Realistic Conditions with Application to Deepfake Detection. (arXiv:2203.11797v1 [cs.CV])
67. AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. (arXiv:2203.11799v1 [cs.CV])
68. A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions. (arXiv:2203.11807v1 [cs.CV])
69. A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v1 [cs.CV])
70. Was that so hard? Estimating human classification difficulty. (arXiv:2203.11824v1 [cs.CV])
71. Cross-View Panorama Image Synthesis. (arXiv:2203.11832v1 [cs.CV])
72. Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v1 [cs.LG])
73. A **Real-time** Junk Food Recognition System based on Machine Learning. (arXiv:2203.11836v1 [cs.CV])
74. ImageNet Challenging Classification with the Raspberry Pi: An Incremental Local Stochastic Gradient Descent Algorithm. (arXiv:2203.11853v1 [cs.CV])
75. Generating natural images with direct Patch Distributions Matching. (arXiv:2203.11862v1 [cs.CV])
76. Open-Vocabulary DETR with Conditional Matching. (arXiv:2203.11876v1 [cs.CV])
77. Under the Hood of Transformer Networks for Trajectory Forecasting. (arXiv:2203.11878v1 [cs.CV])
78. GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v1 [cs.CV])
79. Detection, Recognition, and Tracking: A Survey. (arXiv:2203.11900v1 [cs.CV])
80. Enabling faster and more reliable sonographic assessment of gestational age through machine learning. (arXiv:2203.11903v1 [cs.LG])
81. Improving Neural Predictivity in the Visual Cortex with Gated Recurrent Connections. (arXiv:2203.11910v1 [cs.CV])
82. Focal Modulation Networks. (arXiv:2203.11926v1 [cs.CV])
83. Dataset Distillation by Matching Training Trajectories. (arXiv:2203.11932v1 [cs.CV])
84. A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v1 [cs.LG])
85. Learning from All Vehicles. (arXiv:2203.11934v1 [cs.RO])
86. 4D-OR: Semantic Scene Graphs for OR Domain Modeling. (arXiv:2203.11937v1 [cs.CV])
87. {\phi}-SfT: Shape-from-Template with a Physics-Based Deformation Model. (arXiv:2203.11938v1 [cs.CV])
88. Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v7 [eess.SP] UPDATED)
89. Sparsely Activated Networks. (arXiv:1907.06592v8 [cs.LG] UPDATED)
90. kDecay: Just adding k-decay items on Learning-Rate Schedule to improve Neural Networks. (arXiv:2004.05909v5 [cs.LG] UPDATED)
91. Statistical Shape Analysis of Brain Arterial Networks (BAN). (arXiv:2007.04793v2 [cs.CV] UPDATED)
92. A range characterization of the single-quadrant ADRT. (arXiv:2010.05360v2 [math.NA] UPDATED)
93. DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v3 [cs.CV] UPDATED)
94. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v5 [cs.CV] UPDATED)
95. Oriented RepPoints for Aerial Object Detection. (arXiv:2105.11111v3 [cs.CV] UPDATED)
96. Pho(SC)-CTC -- A Hybrid Approach Towards Zero-shot Word Image Recognition. (arXiv:2105.15093v2 [cs.CV] UPDATED)
97. More Than Meets the Eye: Self-Supervised Depth Reconstruction From Brain Activity. (arXiv:2106.05113v3 [cs.CV] UPDATED)
98. On Anytime Learning at Macroscale. (arXiv:2106.09563v3 [cs.LG] UPDATED)
99. Orthonormal Product Quantization Network for Scalable Face Image Retrieval. (arXiv:2107.00327v3 [cs.CV] UPDATED)
100. Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters. (arXiv:2107.09783v3 [cs.CV] UPDATED)
101. Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v5 [cs.CV] UPDATED)
102. Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP. (arXiv:2109.02748v3 [cs.CV] UPDATED)
103. Vision Transformer Hashing for Image Retrieval. (arXiv:2109.12564v2 [cs.CV] UPDATED)
104. Learning Efficient Multi-Agent Cooperative Visual Exploration. (arXiv:2110.05734v2 [cs.CV] UPDATED)
105. Are we ready for a new paradigm shift? A Survey on Visual Deep MLP. (arXiv:2111.04060v5 [cs.CV] UPDATED)
106. DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet. (arXiv:2111.04739v2 [eess.IV] UPDATED)
107. Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v3 [cs.CV] UPDATED)
108. DeepCurrents: Learning Implicit Representations of Shapes with Boundaries. (arXiv:2111.09383v2 [cs.CV] UPDATED)
109. CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation. (arXiv:2111.10502v3 [cs.CV] UPDATED)
110. L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v9 [cs.CV] UPDATED)
111. 3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces. (arXiv:2111.12448v4 [cs.CV] UPDATED)
112. ACPL: Anti-curriculum Pseudo-labelling for Semi-supervised Medical Image Classification. (arXiv:2111.12918v3 [cs.CV] UPDATED)
113. Efficient Self-Ensemble for Semantic Segmentation. (arXiv:2111.13280v2 [cs.CV] UPDATED)
114. Modeling Annotator Preference and Stochastic Annotation Error for Medical Image Segmentation. (arXiv:2111.13410v3 [cs.CV] UPDATED)
115. IDR: Self-Supervised Image Denoising via Iterative Data Refinement. (arXiv:2111.14358v2 [cs.CV] UPDATED)
116. PTTR: Relational 3D Point Cloud Object Tracking with Transformer. (arXiv:2112.02857v4 [cs.CV] UPDATED)
117. E$^2$(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition. (arXiv:2112.03596v2 [cs.CV] UPDATED)
118. Shaping Visual Representations with Attributes for Few-Shot Learning. (arXiv:2112.06398v2 [cs.CV] UPDATED)
119. Learning to Prompt for Continual Learning. (arXiv:2112.08654v2 [cs.LG] UPDATED)
120. DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v3 [cs.CV] UPDATED)
121. HCSC: Hierarchical Contrastive Selective Coding. (arXiv:2202.00455v3 [cs.CV] UPDATED)
122. A hybrid 2-stage vision transformer for artificial intelligence-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic tool for guiding gastric cancer treatment. (arXiv:2202.08510v2 [eess.IV] UPDATED)
123. "If you could see me through my eyes": Predicting Pedestrian Perception. (arXiv:2202.13981v2 [cs.CV] UPDATED)
124. Colar: Effective and Efficient Online Action Detection by Consulting Exemplars. (arXiv:2203.01057v2 [cs.CV] UPDATED)
125. PanFormer: a Transformer Based Model for Pan-sharpening. (arXiv:2203.02916v2 [cs.CV] UPDATED)
126. Audio-Visual MLP for Scoring Sport. (arXiv:2203.03990v2 [cs.CV] UPDATED)
127. Shape-invariant 3D Adversarial Point Clouds. (arXiv:2203.04041v2 [cs.CV] UPDATED)
128. Abandoning the Bayer-Filter to See in the **Dark**. (arXiv:2203.04042v2 [eess.IV] UPDATED)
129. Point Density-Aware Voxels for LiDAR 3D Object Detection. (arXiv:2203.05662v2 [cs.CV] UPDATED)
130. AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v2 [cs.CV] UPDATED)
131. Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v3 [cs.CV] UPDATED)
132. Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding. (arXiv:2203.08481v2 [cs.CV] UPDATED)
133. Topology-Preserving Shape Reconstruction and Registration via Neural Diffeomorphic Flow. (arXiv:2203.08652v2 [cs.CV] UPDATED)
134. Occlusion Fields: An Implicit Representation for Non-Line-of-Sight Surface Reconstruction. (arXiv:2203.08657v2 [cs.CV] UPDATED)
135. Know your sensORs -- A Modality Study For Surgical Action Classification. (arXiv:2203.08674v2 [cs.CV] UPDATED)
136. Bi-directional Object-context Prioritization Learning for Saliency Ranking. (arXiv:2203.09416v2 [cs.CV] UPDATED)
137. Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v2 [cs.CV] UPDATED)
138. MatchFormer: Interleaving Attention in Transformers for Feature Matching. (arXiv:2203.09645v2 [cs.CV] UPDATED)
139. TVConv: Efficient Translation Variant Convolution for Layout-aware Visual Processing. (arXiv:2203.10489v2 [cs.CV] UPDATED)
140. SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization. (arXiv:2203.10492v2 [cs.CV] UPDATED)
141. CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v2 [cs.CV] UPDATED)
142. Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild. (arXiv:2203.10636v2 [cs.CV] UPDATED)
143. Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation. (arXiv:2203.10739v2 [cs.CV] UPDATED)
144. Hyperbolic Vision Transformers: Combining Improvements in Metric Learning. (arXiv:2203.10833v2 [cs.CV] UPDATED)
145. Computational ergonomics for task delegation in Human-Robot Collaboration: spatiotemporal adaptation of the robot to the human through contactless gesture recognition. (arXiv:2203.11007v2 [cs.RO] UPDATED)
146. Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v2 [cs.CV] UPDATED)
147. Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation. (arXiv:2010.06255v5 [cs.CV] CROSS LISTED)
## eess.IV
---
**29** new papers in eess.IV:-) 
1. VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. (arXiv:2203.11205v1 [eess.IV])
2. Phase Recognition in Contrast-Enhanced CT Scans based on Deep Learning and Random Sampling. (arXiv:2203.11206v1 [eess.IV])
3. Hybrid training of optical neural networks. (arXiv:2203.11207v1 [cs.LG])
4. On the Effect of Pre-Processing and Model Complexity for Plastic Analysis Using Short-Wave-Infrared Hyper-Spectral Imaging. (arXiv:2203.11209v1 [cs.CV])
5. ME-Net: Multi-Encoder Net Framework for Brain Tumor Segmentation. (arXiv:2203.11213v1 [eess.IV])
6. Survival Analysis for Idiopathic Pulmonary Fibrosis using CT Images and Incomplete Clinical Data. (arXiv:2203.11391v1 [eess.IV])
7. FrameHopper: Selective Processing of Video Frames in Detection-driven Real-Time Video Analytics. (arXiv:2203.11493v1 [cs.CV])
8. Multi-layer Clustering-based Residual Sparsifying Transform for Low-dose CT Image Reconstruction. (arXiv:2203.11565v1 [eess.IV])
9. End-to-End Learned Block-Based Image Compression with Block-Level Masked Convolutions and Asymptotic Closed Loop Training. (arXiv:2203.11686v1 [eess.IV])
10. Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v1 [eess.IV])
11. Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain. (arXiv:2203.11722v1 [eess.IV])
12. Unsupervised Anomaly Detection in Medical Images with a Memory-augmented Multi-level Cross-attentional Masked Autoencoder. (arXiv:2203.11725v1 [eess.IV])
13. AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography. (arXiv:2203.11726v1 [physics.med-ph])
14. A Novel Framework for Assessment of Learning-based Detectors in Realistic Conditions with Application to Deepfake Detection. (arXiv:2203.11797v1 [cs.CV])
15. AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. (arXiv:2203.11799v1 [cs.CV])
16. A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions. (arXiv:2203.11807v1 [cs.CV])
17. Enabling faster and more reliable sonographic assessment of gestational age through machine learning. (arXiv:2203.11903v1 [cs.LG])
18. Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v7 [eess.SP] UPDATED)
19. Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans. (arXiv:2009.06412v5 [eess.IV] UPDATED)
20. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v5 [cs.CV] UPDATED)
21. Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral Images using a Hybrid Deep Neural Network. (arXiv:2107.04631v3 [eess.IV] UPDATED)
22. DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet. (arXiv:2111.04739v2 [eess.IV] UPDATED)
23. Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v3 [cs.CV] UPDATED)
24. A hybrid 2-stage vision transformer for artificial intelligence-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic tool for guiding gastric cancer treatment. (arXiv:2202.08510v2 [eess.IV] UPDATED)
25. Abandoning the Bayer-Filter to See in the **Dark**. (arXiv:2203.04042v2 [eess.IV] UPDATED)
26. MatchFormer: Interleaving Attention in Transformers for Feature Matching. (arXiv:2203.09645v2 [cs.CV] UPDATED)
27. Learning Nonlocal Sparse and Low-Rank Models for Image Compressive Sensing. (arXiv:2203.09656v2 [eess.IV] UPDATED)
28. CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v2 [cs.CV] UPDATED)
29. Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v2 [cs.CV] UPDATED)
## cs.LG
---
**182** new papers in cs.LG:-) 
1. Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series. (arXiv:2203.11196v1 [cs.LG])
2. Teachable Reinforcement Learning via Advice Distillation. (arXiv:2203.11197v1 [cs.LG])
3. Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model. (arXiv:2203.11199v1 [cs.LG])
4. Exploiting Neighbor Effect: Conv-Agnostic GNNs Framework for Graphs with Heterophily. (arXiv:2203.11200v1 [cs.LG])
5. Efficient Neural Network Analysis with Sum-of-Infeasibilities. (arXiv:2203.11201v1 [cs.LG])
6. Reinforcement learning for automatic quadrilateral mesh generation: a soft actor-critic approach. (arXiv:2203.11203v1 [cs.LG])
7. Hybrid training of optical neural networks. (arXiv:2203.11207v1 [cs.LG])
8. On the Effect of Pre-Processing and Model Complexity for Plastic Analysis Using Short-Wave-Infrared Hyper-Spectral Imaging. (arXiv:2203.11209v1 [cs.CV])
9. ReCCoVER: Detecting Causal Confusion for Explainable Reinforcement Learning. (arXiv:2203.11211v1 [cs.LG])
10. The Conceptual VAE. (arXiv:2203.11216v1 [cs.LG])
11. A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v1 [cs.LG])
12. Healthy Twitter discussions? Time will tell. (arXiv:2203.11261v1 [cs.SI])
13. Domain Knowledge Aids in Signal Disaggregation; the Example of the Cumulative Water Heater. (arXiv:2203.11268v1 [eess.SP])
14. Model Comparison in Approximate Bayesian Computation. (arXiv:2203.11276v1 [stat.ML])
15. One-Bit Compressive Sensing: Can We Go Deep and Blind?. (arXiv:2203.11278v1 [eess.SP])
16. EEG based Emotion Recognition: A Tutorial and Review. (arXiv:2203.11279v1 [eess.SP])
17. A Contrastive Objective for Learning Disentangled Representations. (arXiv:2203.11284v1 [cs.CV])
18. PCA-RF: An Efficient Parkinson's Disease Prediction Model based on Random Forest Classification. (arXiv:2203.11287v1 [cs.LG])
19. Benchmarking Test-Time Unsupervised Deep Neural Network Adaptation on Edge Devices. (arXiv:2203.11295v1 [cs.LG])
20. Landscape Analysis for Surrogate Models in the Evolutionary Black-Box Context. (arXiv:2203.11315v1 [cs.NE])
21. Random vector functional link network: recent developments, applications, and future directions. (arXiv:2203.11316v1 [cs.NE])
22. The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error. (arXiv:2203.11317v1 [cs.CL])
23. Deep Reinforcement Learning and Convex Mean-Variance Optimisation for Portfolio Management. (arXiv:2203.11318v1 [q-fin.PM])
24. Alarm-Based Root Cause Analysis in Industrial Processes Using Deep Learning. (arXiv:2203.11321v1 [eess.SY])
25. Training Quantised Neural Networks with STE Variants: the Additive Noise Annealing Algorithm. (arXiv:2203.11323v1 [cs.LG])
26. Learning robot motor skills with mixed reality. (arXiv:2203.11324v1 [cs.RO])
27. On The Robustness of Offensive Language Classifiers. (arXiv:2203.11331v1 [cs.CL])
28. Origami in N dimensions: How feed-forward networks manufacture linear separability. (arXiv:2203.11355v1 [cs.LG])
29. PI-VAE: Physics-Informed Variational Auto-Encoder for stochastic differential equations. (arXiv:2203.11363v1 [stat.ML])
30. An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels. (arXiv:2203.11364v1 [cs.CL])
31. Temporal Abstractions-Augmented Temporally Contrastive Learning: An Alternative to the Laplacian in RL. (arXiv:2203.11369v1 [cs.LG])
32. Language modeling via stochastic processes. (arXiv:2203.11370v1 [cs.CL])
33. Two methods for Jamming Identification in UAVs Networks using New Synthetic Dataset. (arXiv:2203.11373v1 [cs.CR])
34. Sequential algorithmic modification with test data reuse. (arXiv:2203.11377v1 [stat.ML])
35. HyperShot: Few-Shot Learning by Kernel HyperNetworks. (arXiv:2203.11378v1 [cs.LG])
36. A Bayesian Deep Learning Technique for Multi-Step Ahead Solar Generation Forecasting. (arXiv:2203.11379v1 [cs.LG])
37. Preference Exploration for Efficient Bayesian Optimization with Multiple Outcomes. (arXiv:2203.11382v1 [cs.LG])
38. DIANES: A DEI Audit Toolkit for News Sources. (arXiv:2203.11383v1 [cs.IR])
39. Optimizing Binary Decision Diagrams with MaxSAT for classification. (arXiv:2203.11386v1 [cs.AI])
40. Survival Analysis for Idiopathic Pulmonary Fibrosis using CT Images and Incomplete Clinical Data. (arXiv:2203.11391v1 [eess.IV])
41. Towards Textual Out-of-Domain Detection without In-Domain Labels. (arXiv:2203.11396v1 [cs.CL])
42. Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective. (arXiv:2203.11401v1 [cs.CL])
43. A Primer on Maximum Causal Entropy Inverse Reinforcement Learning. (arXiv:2203.11409v1 [cs.LG])
44. Dazzle: Using Optimized Generative Adversarial Networks to Address Security Data Class Imbalance Issue. (arXiv:2203.11410v1 [cs.CR])
45. Non-linear Embeddings in Hilbert Simplex Geometry. (arXiv:2203.11434v1 [cs.LG])
46. Root-aligned SMILES for Molecular Retrosynthesis Prediction. (arXiv:2203.11444v1 [cs.LG])
47. Hierarchical Graph Representation Learning for the Prediction of Drug-Target Binding Affinity. (arXiv:2203.11458v1 [cs.LG])
48. BigBird: Big Data Storage and Analytics at Scale in Hybrid Cloud. (arXiv:2203.11472v1 [cs.DC])
49. Federated Class-Incremental Learning. (arXiv:2203.11473v1 [cs.LG])
50. Approaches for Improving the Performance of Fake News Detection in Bangla: Imbalance Handling and Model Stacking. (arXiv:2203.11486v1 [cs.LG])
51. A Note on Target Q-learning For Solving Finite MDPs with A Generative Oracle. (arXiv:2203.11489v1 [cs.LG])
52. Making Recommender Systems Forget: Learning and Unlearning for Erasable Recommendation. (arXiv:2203.11491v1 [cs.IR])
53. Exploring High-Order Structure for Robust Graph Structure Learning. (arXiv:2203.11492v1 [cs.LG])
54. Residual-Guided Non-Intrusive Speech Quality Assessment. (arXiv:2203.11499v1 [cs.SD])
55. Action Candidate Driven Clipped Double Q-learning for Discrete and Continuous Action Tasks. (arXiv:2203.11526v1 [cs.LG])
56. Out-of-distribution Generalization with Causal Invariant Transformations. (arXiv:2203.11528v1 [stat.ML])
57. Scale-out Systolic Arrays. (arXiv:2203.11540v1 [cs.AR])
58. Mask Usage Recognition using Vision Transformer with Transfer Learning and Data Augmentation. (arXiv:2203.11542v1 [cs.CV])
59. Factual Consistency of Multilingual Pretrained Language Models. (arXiv:2203.11552v1 [cs.CL])
60. Gradient flows and randomised thresholding: sparse inversion and classification. (arXiv:2203.11555v1 [math.NA])
61. VQ-Flows: Vector Quantized Local Normalizing Flows. (arXiv:2203.11556v1 [cs.LG])
62. Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals. (arXiv:2203.11560v1 [q-bio.NC])
63. Conditional Generative Data Augmentation for Clinical Audio Datasets. (arXiv:2203.11570v1 [cs.SD])
64. Fast Multi-view Clustering via Ensembles: Towards Scalability, Superiority, and Simplicity. (arXiv:2203.11572v1 [cs.LG])
65. Local Stochastic Factored Gradient Descent for Distributed Quantum State Tomography. (arXiv:2203.11579v1 [quant-ph])
66. Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue. (arXiv:2203.11587v1 [cs.CL])
67. Diagnosis of Schizophrenia: A comprehensive evaluation. (arXiv:2203.11610v1 [cs.LG])
68. On Neural Network Equivalence Checking using SMT Solvers. (arXiv:2203.11629v1 [cs.AI])
69. Semi-Targeted Model Poisoning Attack on Federated Learning via Backward Error Analysis. (arXiv:2203.11633v1 [cs.LG])
70. Multi-Source Domain Adaptation Based on Federated Knowledge Alignment. (arXiv:2203.11635v1 [cs.LG])
71. Performance Evaluation of Machine Learning-based Algorithm and Taguchi Algorithm for the Determination of the Hardness Value of the Friction Stir Welded AA 6262 Joints at a Nugget Zone. (arXiv:2203.11649v1 [cs.LG])
72. Is Vanilla Policy Gradient Overlooked? Analyzing Deep Reinforcement Learning for Hanabi. (arXiv:2203.11656v1 [cs.LG])
73. Are You Misinformed? A Study of Covid-Related Fake News in Bengali on Facebook. (arXiv:2203.11669v1 [cs.CL])
74. Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation. (arXiv:2203.11670v1 [cs.CL])
75. Robust Action Gap Increasing with Clipped Advantage Learning. (arXiv:2203.11677v1 [cs.LG])
76. Twin Weisfeiler-Lehman: High Expressive GNNs for Graph Classification. (arXiv:2203.11683v1 [cs.LG])
77. BEFANA: A Tool for Biodiversity-Ecosystem Functioning Assessment by Network Analysis. (arXiv:2203.11687v1 [q-bio.QM])
78. GAM(L)A: An econometric model for interpretable Machine Learning. (arXiv:2203.11691v1 [stat.ML])
79. A Machine Learning Generative Method for Automating Antenna Design and Optimization. (arXiv:2203.11698v1 [cs.LG])
80. Exploring Linear Feature Disentanglement For Neural Networks. (arXiv:2203.11700v1 [cs.LG])
81. BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning in Sentiment Analysis. (arXiv:2203.11702v1 [cs.CL])
82. A Bayesian Approach for Shaft Centre Localisation in Journal Bearings. (arXiv:2203.11719v1 [cs.LG])
83. Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain. (arXiv:2203.11722v1 [eess.IV])
84. Explainable Misinformation Detection Across Multiple Social Media Platforms. (arXiv:2203.11724v1 [cs.LG])
85. Gated Recurrent Unit based Autoencoder for Optical Link Fault Diagnosis in Passive Optical Networks. (arXiv:2203.11727v1 [eess.SP])
86. Machine Learning based Data Driven Diagnostic and Prognostic Approach for Laser Reliability **Enhancement**. (arXiv:2203.11728v1 [cs.LG])
87. Machine Learning based Laser Failure Mode Detection. (arXiv:2203.11729v1 [cs.LG])
88. Plasticity Neural Network Based on Astrocytic Influence at Critical Periods, Synaptic Competition and Compensation by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v1 [cs.NE])
89. FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction. (arXiv:2203.11751v1 [cs.LG])
90. Linear convergence of a policy gradient method for finite horizon continuous time stochastic control problems. (arXiv:2203.11758v1 [math.OC])
91. Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model. (arXiv:2203.11774v1 [cs.SD])
92. Learning Program Semantics with Code Representations: An Empirical Study. (arXiv:2203.11790v1 [cs.SE])
93. A Perspective on Neural Capacity Estimation: Viability and Reliability. (arXiv:2203.11793v1 [cs.IT])
94. On Robust Classification using Contractive Hamiltonian Neural ODEs. (arXiv:2203.11805v1 [cs.LG])
95. Neural System Level Synthesis: Learning over All Stabilizing Policies for Nonlinear Systems. (arXiv:2203.11812v1 [eess.SY])
96. Clustering units in neural networks: upstream vs downstream information. (arXiv:2203.11815v1 [cs.LG])
97. A Novel Exploration of Diffusion Process based on Multi-types Galton-Watson Forests. (arXiv:2203.11816v1 [cs.SI])
98. Was that so hard? Estimating human classification difficulty. (arXiv:2203.11824v1 [cs.CV])
99. Explainable Landscape Analysis in Automated Algorithm Performance Prediction. (arXiv:2203.11828v1 [cs.LG])
100. Provable Constrained Stochastic Convex Optimization with XOR-Projected Gradient Descent. (arXiv:2203.11829v1 [math.OC])
101. Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v1 [cs.LG])
102. A **Real-time** Junk Food Recognition System based on Machine Learning. (arXiv:2203.11836v1 [cs.CV])
103. X-MEN: Guaranteed XOR-Maximum Entropy Constrained Inverse Reinforcement Learning. (arXiv:2203.11842v1 [cs.LG])
104. A Girl Has A Name, And It's ... Adversarial Authorship Attribution for Deobfuscation. (arXiv:2203.11849v1 [cs.CL])
105. A Survey on Techniques for Identifying and Resolving Representation Bias in Data. (arXiv:2203.11852v1 [cs.DB])
106. ImageNet Challenging Classification with the Raspberry Pi: An Incremental Local Stochastic Gradient Descent Algorithm. (arXiv:2203.11853v1 [cs.CV])
107. Sionna: An Open-Source Library for Next-Generation Physical Layer Research. (arXiv:2203.11854v1 [cs.IT])
108. Practical tradeoffs between memory, compute, and performance in learned optimizers. (arXiv:2203.11860v1 [cs.LG])
109. On the (Non-)Robustness of Two-Layer Neural Networks in Different Learning Regimes. (arXiv:2203.11864v1 [stat.ML])
110. Performance of long short-term memory artificial neural networks in nowcasting during the COVID-19 crisis. (arXiv:2203.11872v1 [stat.ML])
111. Insights From the NeurIPS 2021 NetHack Challenge. (arXiv:2203.11889v1 [cs.LG])
112. GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v1 [cs.CV])
113. Enabling faster and more reliable sonographic assessment of gestational age through machine learning. (arXiv:2203.11903v1 [cs.LG])
114. SPRITE: A Scalable Privacy-Preserving and Verifiable Collaborative Learning for Industrial IoT. (arXiv:2203.11914v1 [cs.CR])
115. On Supervised Feature Selection from High Dimensional Feature Spaces. (arXiv:2203.11924v1 [cs.LG])
116. Focal Modulation Networks. (arXiv:2203.11926v1 [cs.CV])
117. MetaMorph: Learning Universal Controllers with Transformers. (arXiv:2203.11931v1 [cs.LG])
118. Dataset Distillation by Matching Training Trajectories. (arXiv:2203.11932v1 [cs.CV])
119. A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v1 [cs.LG])
120. Learning from All Vehicles. (arXiv:2203.11934v1 [cs.RO])
121. Sparsely Activated Networks. (arXiv:1907.06592v8 [cs.LG] UPDATED)
122. kDecay: Just adding k-decay items on Learning-Rate Schedule to improve Neural Networks. (arXiv:2004.05909v5 [cs.LG] UPDATED)
123. Inductive logic programming at 30: a new introduction. (arXiv:2008.07912v5 [cs.AI] UPDATED)
124. Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans. (arXiv:2009.06412v5 [eess.IV] UPDATED)
125. ATCN: Resource-Efficient Processing of Time Series on Edge. (arXiv:2011.05260v4 [cs.LG] UPDATED)
126. Unbox the Blackbox: Predict and Interpret YouTube Viewership Using Deep Learning. (arXiv:2101.01076v5 [cs.LG] UPDATED)
127. Neural Termination Analysis. (arXiv:2102.03824v3 [cs.LG] UPDATED)
128. EigenGame Unloaded: When playing games is better than optimizing. (arXiv:2102.04152v2 [stat.ML] UPDATED)
129. KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural Networks with Non-Zero Training Loss. (arXiv:2102.11923v2 [math.DS] UPDATED)
130. Functional Collection Programming with Semi-Ring Dictionaries. (arXiv:2103.06376v3 [cs.PL] UPDATED)
131. A Splicing Approach to Best Subset of Groups Selection. (arXiv:2104.12576v2 [cs.LG] UPDATED)
132. Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v5 [cs.CV] UPDATED)
133. DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism. (arXiv:2105.02446v6 [eess.AS] UPDATED)
134. FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v5 [cs.CL] UPDATED)
135. More Than Meets the Eye: Self-Supervised Depth Reconstruction From Brain Activity. (arXiv:2106.05113v3 [cs.CV] UPDATED)
136. Automated Self-Supervised Learning for Graphs. (arXiv:2106.05470v3 [cs.LG] UPDATED)
137. Masked Training of Neural Networks with Partial Gradients. (arXiv:2106.08895v3 [cs.LG] UPDATED)
138. On Anytime Learning at Macroscale. (arXiv:2106.09563v3 [cs.LG] UPDATED)
139. DPPIN: A Biological Repository of Dynamic Protein-Protein Interaction Network Data. (arXiv:2107.02168v4 [cs.LG] UPDATED)
140. Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral Images using a Hybrid Deep Neural Network. (arXiv:2107.04631v3 [eess.IV] UPDATED)
141. Direct speech-to-speech translation with discrete units. (arXiv:2107.05604v2 [cs.CL] UPDATED)
142. Meta-Reinforcement Learning in Broad and Non-Parametric Environments. (arXiv:2108.03718v2 [cs.LG] UPDATED)
143. Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP. (arXiv:2109.02748v3 [cs.CV] UPDATED)
144. Spline-PINN: Approaching PDEs without Data using Fast, Physics-Informed Hermite-Spline CNNs. (arXiv:2109.07143v2 [cs.LG] UPDATED)
145. Recommender systems based on graph embedding techniques: A comprehensive review. (arXiv:2109.09587v2 [cs.IR] UPDATED)
146. Adaptive Control of SE(3) Hamiltonian Dynamics with Learned Disturbance Features. (arXiv:2109.09974v2 [cs.RO] UPDATED)
147. Learning Efficient Multi-Agent Cooperative Visual Exploration. (arXiv:2110.05734v2 [cs.CV] UPDATED)
148. Graph Condensation for Graph Neural Networks. (arXiv:2110.07580v2 [cs.LG] UPDATED)
149. The CoRa Tensor Compiler: Compilation for Ragged Tensors with Minimal Padding. (arXiv:2110.10221v3 [cs.LG] UPDATED)
150. Dynamic Review-based Recommenders. (arXiv:2110.14747v2 [cs.IR] UPDATED)
151. OneFlow: Redesign the Distributed Deep Learning Framework from Scratch. (arXiv:2110.15032v5 [cs.DC] UPDATED)
152. Fair-SSL: Building fair ML Software with less data. (arXiv:2111.02038v5 [cs.SE] UPDATED)
153. Data Selection for Efficient Model Update in Federated Learning. (arXiv:2111.03512v2 [cs.LG] UPDATED)
154. Towards noise robust trigger-word detection with contrastive learning pre-task for fast on-boarding of new trigger-words. (arXiv:2111.03971v2 [cs.SD] UPDATED)
155. Model-assisted deep learning of rare extreme events from partial observations. (arXiv:2111.04857v2 [cs.LG] UPDATED)
156. A Comparative Study on Basic Elements of Deep Learning Models for Spatial-Temporal Traffic Forecasting. (arXiv:2111.07513v3 [cs.LG] UPDATED)
157. Non-separable Spatio-temporal Graph Kernels via SPDEs. (arXiv:2111.08524v2 [cs.LG] UPDATED)
158. L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v9 [cs.CV] UPDATED)
159. Subspace Adversarial Training. (arXiv:2111.12229v2 [cs.LG] UPDATED)
160. 3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces. (arXiv:2111.12448v4 [cs.CV] UPDATED)
161. Non-Intrusive Binaural Speech Intelligibility Prediction from Discrete Latent Representations. (arXiv:2111.12531v2 [cs.SD] UPDATED)
162. Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach. (arXiv:2112.04571v2 [cs.LG] UPDATED)
163. Learning to Prompt for Continual Learning. (arXiv:2112.08654v2 [cs.LG] UPDATED)
164. Ditch the Gold Standard: Re-evaluating Conversational Question Answering. (arXiv:2112.08812v2 [cs.CL] UPDATED)
165. The Universal $\ell^p$-Metric on Merge Trees. (arXiv:2112.12165v2 [cs.CG] UPDATED)
166. Solving time dependent Fokker-Planck equations via temporal normalizing flow. (arXiv:2112.14012v2 [cs.LG] UPDATED)
167. A causal model of safety assurance for machine learning. (arXiv:2201.05451v2 [cs.SE] UPDATED)
168. Unified theory of atom-centered representations and message-passing machine-learning schemes. (arXiv:2202.01566v2 [stat.ML] UPDATED)
169. A Graph Neural Network Framework for Grid-Based Simulation. (arXiv:2202.02652v2 [cs.LG] UPDATED)
170. Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. (arXiv:2202.05928v2 [cs.LG] UPDATED)
171. Tensor Moments of Gaussian Mixture Models: Theory and Applications. (arXiv:2202.06930v2 [stat.ML] UPDATED)
172. A hybrid 2-stage vision transformer for artificial intelligence-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic tool for guiding gastric cancer treatment. (arXiv:2202.08510v2 [eess.IV] UPDATED)
173. Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v3 [eess.AS] UPDATED)
174. Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v3 [cs.CV] UPDATED)
175. Occlusion Fields: An Implicit Representation for Non-Line-of-Sight Surface Reconstruction. (arXiv:2203.08657v2 [cs.CV] UPDATED)
176. Investigation of Physics-Informed Deep Learning for the Prediction of Parametric, Three-Dimensional Flow Based on Boundary Data. (arXiv:2203.09204v2 [cs.LG] UPDATED)
177. Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v2 [cs.CV] UPDATED)
178. The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v2 [cs.LG] UPDATED)
179. ASE: Anomaly Scoring Based Ensemble Learning for Imbalanced Datasets. (arXiv:2203.10769v2 [cs.LG] UPDATED)
180. Hyperbolic Vision Transformers: Combining Improvements in Metric Learning. (arXiv:2203.10833v2 [cs.CV] UPDATED)
181. Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v2 [cs.CV] UPDATED)
182. Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance. (arXiv:2203.11178v2 [cs.LG] UPDATED)
## cs.AI
---
**76** new papers in cs.AI:-) 
1. Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series. (arXiv:2203.11196v1 [cs.LG])
2. Teachable Reinforcement Learning via Advice Distillation. (arXiv:2203.11197v1 [cs.LG])
3. Exploiting Neighbor Effect: Conv-Agnostic GNNs Framework for Graphs with Heterophily. (arXiv:2203.11200v1 [cs.LG])
4. Efficient Neural Network Analysis with Sum-of-Infeasibilities. (arXiv:2203.11201v1 [cs.LG])
5. Reinforcement learning for automatic quadrilateral mesh generation: a soft actor-critic approach. (arXiv:2203.11203v1 [cs.LG])
6. An efficient heuristic approach combining maximal itemsets and area measure for compressing voluminous table constraints. (arXiv:2203.11208v1 [cs.DB])
7. On the Effect of Pre-Processing and Model Complexity for Plastic Analysis Using Short-Wave-Infrared Hyper-Spectral Imaging. (arXiv:2203.11209v1 [cs.CV])
8. Disentangling Patterns and Transformations from One Sequence of Images with Shape-invariant Lie Group Transformer. (arXiv:2203.11210v1 [cs.CV])
9. ReCCoVER: Detecting Causal Confusion for Explainable Reinforcement Learning. (arXiv:2203.11211v1 [cs.LG])
10. ME-Net: Multi-Encoder Net Framework for Brain Tumor Segmentation. (arXiv:2203.11213v1 [eess.IV])
11. The Conceptual VAE. (arXiv:2203.11216v1 [cs.LG])
12. A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v1 [cs.LG])
13. EEG based Emotion Recognition: A Tutorial and Review. (arXiv:2203.11279v1 [eess.SP])
14. Towards a Change Taxonomy for Machine Learning Systems. (arXiv:2203.11365v1 [cs.SE])
15. HyperShot: Few-Shot Learning by Kernel HyperNetworks. (arXiv:2203.11378v1 [cs.LG])
16. Optimizing Binary Decision Diagrams with MaxSAT for classification. (arXiv:2203.11386v1 [cs.AI])
17. Towards Textual Out-of-Domain Detection without In-Domain Labels. (arXiv:2203.11396v1 [cs.CL])
18. A Real World Dataset for Multi-view 3D Reconstruction. (arXiv:2203.11397v1 [cs.CV])
19. A Primer on Maximum Causal Entropy Inverse Reinforcement Learning. (arXiv:2203.11409v1 [cs.LG])
20. Robust Pivoting: Exploiting Frictional Stability Using Bilevel Optimization. (arXiv:2203.11412v1 [cs.RO])
21. Learning Confidence for Transformer-based Neural Machine Translation. (arXiv:2203.11413v1 [cs.CL])
22. Consent as a Foundation for Responsible Autonomy. (arXiv:2203.11420v1 [cs.AI])
23. Towards Abstractive Grounded Summarization of Podcast Transcripts. (arXiv:2203.11425v1 [cs.CL])
24. Hierarchical Graph Representation Learning for the Prediction of Drug-Target Binding Affinity. (arXiv:2203.11458v1 [cs.LG])
25. Exploring High-Order Structure for Robust Graph Structure Learning. (arXiv:2203.11492v1 [cs.LG])
26. Action Candidate Driven Clipped Double Q-learning for Discrete and Continuous Action Tasks. (arXiv:2203.11526v1 [cs.LG])
27. Visuo-Haptic Object Perception for Robots: An Overview. (arXiv:2203.11544v1 [cs.RO])
28. Explainability in reinforcement learning: perspective and position. (arXiv:2203.11547v1 [cs.AI])
29. VQ-Flows: Vector Quantized Local Normalizing Flows. (arXiv:2203.11556v1 [cs.LG])
30. Collective motion emerging from evolving swarm controllers in different environments using gradient following task. (arXiv:2203.11585v1 [cs.RO])
31. IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment. (arXiv:2203.11590v1 [cs.CV])
32. Distributing Collaborative Multi-Robot Planning with Gaussian Belief Propagation. (arXiv:2203.11618v1 [cs.RO])
33. On Neural Network Equivalence Checking using SMT Solvers. (arXiv:2203.11629v1 [cs.AI])
34. Learning Relation-Specific Representations for Few-shot Knowledge Graph Completion. (arXiv:2203.11639v1 [cs.CL])
35. Weakly-Supervised Salient Object Detection Using Point Supervison. (arXiv:2203.11652v1 [cs.CV])
36. Fine-Grained Scene Graph Generation with Data Transfer. (arXiv:2203.11654v1 [cs.CV])
37. Is Vanilla Policy Gradient Overlooked? Analyzing Deep Reinforcement Learning for Hanabi. (arXiv:2203.11656v1 [cs.LG])
38. Robust Action Gap Increasing with Clipped Advantage Learning. (arXiv:2203.11677v1 [cs.LG])
39. CNNs and Transformers Perceive Hybrid Images Similar to Humans. (arXiv:2203.11678v1 [cs.CV])
40. Meta-attention for ViT-backed Continual Learning. (arXiv:2203.11684v1 [cs.CV])
41. Exploring Linear Feature Disentanglement For Neural Networks. (arXiv:2203.11700v1 [cs.LG])
42. BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning in Sentiment Analysis. (arXiv:2203.11702v1 [cs.CL])
43. CP2: Copy-Paste Contrastive Pretraining for Semantic Segmentation. (arXiv:2203.11709v1 [cs.CV])
44. Explainable Misinformation Detection Across Multiple Social Media Platforms. (arXiv:2203.11724v1 [cs.LG])
45. The Stanford Drone Dataset is More Complex than We Think: An Analysis of Key Characteristics. (arXiv:2203.11743v1 [cs.AI])
46. FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction. (arXiv:2203.11751v1 [cs.LG])
47. Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments. (arXiv:2203.11764v1 [cs.CL])
48. Sionna: An Open-Source Library for Next-Generation Physical Layer Research. (arXiv:2203.11854v1 [cs.IT])
49. A Computational Approach to Understand Mental Health from Reddit: Knowledge-aware Multitask Learning Framework. (arXiv:2203.11856v1 [cs.CL])
50. Open-Vocabulary DETR with Conditional Matching. (arXiv:2203.11876v1 [cs.CV])
51. Insights From the NeurIPS 2021 NetHack Challenge. (arXiv:2203.11889v1 [cs.LG])
52. GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v1 [cs.CV])
53. What can we Learn Even From the Weakest? Learning Sketches for Programmatic Strategies. (arXiv:2203.11912v1 [cs.AI])
54. Focal Modulation Networks. (arXiv:2203.11926v1 [cs.CV])
55. Dataset Distillation by Matching Training Trajectories. (arXiv:2203.11932v1 [cs.CV])
56. Residual Learning from Demonstration: Adapting DMPs for Contact-rich Manipulation. (arXiv:2008.07682v5 [cs.RO] UPDATED)
57. Inductive logic programming at 30: a new introduction. (arXiv:2008.07912v5 [cs.AI] UPDATED)
58. Predicting Events in MOBA Games: Prediction, Attribution, and Evaluation. (arXiv:2012.09424v4 [cs.AI] UPDATED)
59. Unbox the Blackbox: Predict and Interpret YouTube Viewership Using Deep Learning. (arXiv:2101.01076v5 [cs.LG] UPDATED)
60. EigenGame Unloaded: When playing games is better than optimizing. (arXiv:2102.04152v2 [stat.ML] UPDATED)
61. Coordinate descent heuristics for the irregular strip packing problem of rasterized shapes. (arXiv:2104.04525v2 [cs.CG] UPDATED)
62. Randomized Algorithms for Scientific Computing (RASC). (arXiv:2104.11079v3 [cs.AI] UPDATED)
63. Automated Self-Supervised Learning for Graphs. (arXiv:2106.05470v3 [cs.LG] UPDATED)
64. Meta-Reinforcement Learning in Broad and Non-Parametric Environments. (arXiv:2108.03718v2 [cs.LG] UPDATED)
65. Half a Dozen Real-World Applications of Evolutionary Multitasking, and More. (arXiv:2109.13101v4 [cs.NE] UPDATED)
66. Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding. (arXiv:2110.07476v2 [cs.CL] UPDATED)
67. Graph Condensation for Graph Neural Networks. (arXiv:2110.07580v2 [cs.LG] UPDATED)
68. OneFlow: Redesign the Distributed Deep Learning Framework from Scratch. (arXiv:2110.15032v5 [cs.DC] UPDATED)
69. Internal language model estimation through explicit context vector learning for attention-based encoder-decoder ASR. (arXiv:2201.11627v2 [eess.AS] UPDATED)
70. Potential destination prediction for low predictability individuals based on knowledge graph. (arXiv:2201.12845v2 [cs.AI] UPDATED)
71. A Graph Neural Network Framework for Grid-Based Simulation. (arXiv:2202.02652v2 [cs.LG] UPDATED)
72. Modeling Dual Read/Write Paths for Simultaneous Machine Translation. (arXiv:2203.09163v2 [cs.CL] UPDATED)
73. The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v2 [cs.LG] UPDATED)
74. ASE: Anomaly Scoring Based Ensemble Learning for Imbalanced Datasets. (arXiv:2203.10769v2 [cs.LG] UPDATED)
75. Fictitious Play with Maximin Initialization. (arXiv:2203.10774v2 [cs.GT] UPDATED)
76. Computational ergonomics for task delegation in Human-Robot Collaboration: spatiotemporal adaptation of the robot to the human through contactless gesture recognition. (arXiv:2203.11007v2 [cs.RO] UPDATED)

