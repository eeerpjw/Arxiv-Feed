# Your interest papers
---
## cs.CV
---
### DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction. (arXiv:2201.10776v1 [eess.IV])
- Authors : Bo Zhou, Jo Schlemper, Neel Dey, Seyed Sadegh, Mohseni Salehi, Chi Liu, Michal Sofka
- Link : [http://arxiv.org/abs/2201.10776](http://arxiv.org/abs/2201.10776)
> ABSTRACT  :  Multi-contrast MRI (MC-MRI) captures multiple complementary imaging modalities to aid in radiological decision-making. Given the need for lowering the time cost of multiple acquisitions, current deep accelerated MRI reconstruction networks focus on exploiting the redundancy between multiple contrasts. However, existing works are largely supervised with paired data and/or prohibitively expensive fully-sampled MRI sequences. Further, reconstruction networks typically rely on convolutional architectures which are limited in their capacity to model long-range interactions and may lead to suboptimal recovery of fine anatomical detail. To these ends, we present a dual-domain self-supervised transformer (DSFormer) for accelerated MC-MRI reconstruction. DSFormer develops a deep conditional cascade transformer (DCCT) consisting of several cascaded **Swin** transformer reconstruction networks (**Swin**RN) trained under two deep conditioning strategies to enable MC-MRI information sharing. We further present a dual-domain (image and k-space) self-supervised learning strategy for DCCT to alleviate the costs of acquiring fully sampled training data. DSFormer generates high-fidelity reconstructions which experimentally outperform current fully-supervised baselines. Moreover, we find that DSFormer achieves nearly the same performance when trained either with full supervision or with our proposed dual-domain self-supervision.  
### ASFD: Automatic and Scalable Face Detector. (arXiv:2201.10781v1 [cs.CV])
- Authors : Jian Li, Bin Zhang, Yabiao Wang, Ying Tai, ZhenYu Zhang, Chengjie Wang, Jilin Li, Xiaoming Huang, Yili Xia
- Link : [http://arxiv.org/abs/2201.10781](http://arxiv.org/abs/2201.10781)
> ABSTRACT  :  Along with current multi-scale based detectors, Feature Aggregation and **Enhancement** (FAE) modules have shown superior performance gains for cutting-edge object detection. However, these hand-crafted FAE modules show inconsistent improvements on face detection, which is mainly due to the significant distribution difference between its training and applying corpus, COCO vs. WIDER Face. To tackle this problem, we essentially analyse the effect of data distribution, and consequently propose to search an effective FAE architecture, termed AutoFAE by a differentiable architecture search, which outperforms all existing FAE modules in face detection with a considerable margin. Upon the found AutoFAE and existing backbones, a supernet is further built and trained, which automatically obtains a family of detectors under the different complexity constraints. Extensive experiments conducted on popular benchmarks, WIDER Face and FDDB, demonstrate the state-of-the-art performance-efficiency trade-off for the proposed automatic and scalable face detector (ASFD) family. In particular, our strong ASFD-D6 outperforms the best competitor with AP 96.7/96.2/92.1 on WIDER Face test, and the lightweight ASFD-D0 costs about 3.1 ms, more than 320 FPS, on the V100 GPU with VGA-resolution images.  
### When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism. (arXiv:2201.10801v1 [cs.CV])
- Authors : Guangting Wang, Yucheng Zhao, Chuanxin Tang, Chong Luo, Wenjun Zeng
- Link : [http://arxiv.org/abs/2201.10801](http://arxiv.org/abs/2201.10801)
> ABSTRACT  :  Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline **Swin** Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.  
### Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v1 [cs.CV])
- Authors : Lin Zhu, Xiao Wang, Yi Chang, Jianing Li, Tiejun Huang, Yonghong Tian
- Link : [http://arxiv.org/abs/2201.10943](http://arxiv.org/abs/2201.10943)
> ABSTRACT  :  Neuromorphic vision sensor is a new bio-inspired imaging paradigm that reports asynchronous, continuously per-pixel brightness changes called `events' with high temporal resolution and **high dynamic range**. So far, the event-based image reconstruction methods are based on artificial neural networks (ANN) or hand-crafted spatiotemporal smoothing techniques. In this paper, we first implement the image reconstruction work via fully spiking neural network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. We propose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Membrane Potential (MP) neuron. We find that the spiking neurons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Furthermore, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AMP) neuron, which adaptively updates the membrane potential according to the input spikes. The experimental results demonstrate that our models achieve comparable performance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are 19.36$\times$ and 7.75$\times$ more computationally efficient than their ANN architectures, respectively.  
### Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer. (arXiv:2201.10947v1 [cs.LG])
- Authors : Kaiqi Zhao, Yitao Chen, Ming Zhao
- Link : [http://arxiv.org/abs/2201.10947](http://arxiv.org/abs/2201.10947)
> ABSTRACT  :  Deep learning models have introduced various intelligent applications to edge devices, such as image classification, speech recognition, and augmented reality. There is an increasing need of training such models on the devices in order to deliver personalized, responsive, and private learning. To address this need, this paper presents a new solution for deploying and training state-of-the-art models on the resource-constrained devices. First, the paper proposes a novel filter-pruning-based model compression method to create lightweight trainable models from large models trained in the cloud, without much loss of accuracy. Second, it proposes a novel knowledge transfer method to enable the on-device model to update incrementally in **real time** or near **real time** using incremental learning on new data and enable the on-device model to learn the unseen categories with the help of the in-cloud model in an unsupervised fashion. The results show that 1) our model compression method can remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy of over 90% on CIFAR-10; 2) our knowledge transfer method enables the compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good accuracy on old categories; 3) it allows the compressed models to converge within **real time** (three to six minutes) on the edge for incremental learning tasks; 4) it enables the model to classify unseen categories of data (78.92% Top-1 accuracy) that it is never trained with.  
### How Robust are Discriminatively Trained Zero-Shot Learning Models?. (arXiv:2201.10972v1 [cs.CV])
- Authors : Mehmet Kerim, Ramazan Gokberk, Pinar Duygulu
- Link : [http://arxiv.org/abs/2201.10972](http://arxiv.org/abs/2201.10972)
> ABSTRACT  :  Data shift robustness is an active research topic, however, it has been primarily investigated from a fully supervised perspective, and robustness of zero-shot learning (ZSL) models have been largely neglected. In this paper, we present a novel analysis on the robustness of discriminative ZSL to image corruptions. We leverage the well-known label embedding model and subject it to a large set of common corruptions and defenses. In order to realize the corruption analysis, we curate and release the first ZSL corruption robustness datasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account the dataset characteristics, class imbalance, class transition trends between seen and unseen classes and the discrepancies between ZSL and GZSL performances. Our results show that discriminative ZSL suffer from corruptions and this trend is further exacerbated by the severe class imbalance and model weakness inherent in ZSL methods. We then combine our findings with those based on adversarial attacks in ZSL, and highlight the different effects of corruptions and adversarial examples, such as the pseudo-robustness effect present under adversarial attacks. We also obtain new strong baselines for the label embedding model with certain corruption robustness **enhancement** methods. Finally, our experiments show that although existing methods to improve robustness somewhat work for ZSL models, they do not produce a tangible effect.  
### CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v2 [eess.IV] UPDATED)
- Authors : Dimitris Perdios, Manuel Vonlanthen, Florian Martinez, Marcel Arditi, Philippe Thiran
- Link : [http://arxiv.org/abs/2008.12750](http://arxiv.org/abs/2008.12750)
> ABSTRACT  :  Ultrafast ultrasound (US) revolutionized biomedical imaging with its capability of acquiring full-view frames at over 1 kHz, unlocking breakthrough modalities such as shear-wave elastography and functional US neuroimaging. Yet, it suffers from strong diffraction artifacts, mainly caused by grating lobes, side lobes, or edge waves. Multiple acquisitions are typically required to obtain a sufficient image quality, at the cost of a reduced frame rate. To answer the increasing demand for high-quality imaging from single unfocused acquisitions, we propose a two-step convolutional neural network (CNN)-based image reconstruction method, compatible with real-time imaging. A low-quality estimate is obtained by means of a backprojection-based operation, akin to conventional delay-and-sum beamforming, from which a high-quality image is restored using a residual CNN with multi-scale and multi-channel filtering properties, trained specifically to remove the diffraction artifacts inherent to ultrafast US imaging. To account for both the **high dynamic range** and the oscillating properties of radio frequency US images, we introduce the mean signed logarithmic absolute error (MSLAE) as training loss function. Experiments were conducted with a linear transducer array, in single plane wave (PW) imaging. Trainings were performed on a simulated dataset, crafted to contain a wide diversity of structures and echogenicities. Extensive numerical evaluations demonstrate that the proposed approach can reconstruct images from single PWs with a quality similar to that of gold-standard synthetic aperture imaging, on a dynamic range in excess of 60 dB. In vitro and in vivo experiments show that trainings carried out on simulated data perform well in experimental settings.  
### Attention-based Proposals Refinement for 3D Object Detection. (arXiv:2201.07070v2 [cs.CV] UPDATED)
- Authors : Quan Dao, Vincent Fr
- Link : [http://arxiv.org/abs/2201.07070](http://arxiv.org/abs/2201.07070)
> ABSTRACT  :  Recent advances in 3D object detection is made by developing the refinement stage for voxel-based Region Proposal Networks (RPN) to better strike the balance between accuracy and efficiency. A popular approach among state-of-the-art frameworks is to divide proposals, or Regions of Interest (ROI), into grids and extract feature for each grid location before synthesizing them to form ROI feature. While achieving impressive performances, such an approach involves a number of hand crafted components (e.g. grid sampling, set abstraction) which requires expert knowledge to be tuned correctly. This paper proposes a data-driven approach to ROI feature computing named APRO3D-Net which consists of a voxel-based RPN and a refinement stage made of Vector Attention. Unlike the original multi-head attention, Vector Attention assigns different weights to different channels within a point feature, thus being able to capture a more sophisticated relation between pooled points and ROI. Experiments on KITTI \textit{validation} set show that our method achieves competitive performance of 84.84 AP for class Car at Moderate difficulty while having the least parameters compared to closely related methods and attaining a quasi-**real time** inference speed at 15 FPS on NVIDIA V100 GPU. The code is released in https://github.com/quan-dao/APRO3D-Net.  
### **Real-time** Rendering for Integral Imaging Light Field Displays Based on a Voxel-Pixel Lookup Table. (arXiv:2201.08266v2 [cs.GR] UPDATED)
- Authors : Quanzhen Wan
- Link : [http://arxiv.org/abs/2201.08266](http://arxiv.org/abs/2201.08266)
> ABSTRACT  :  A real-time elemental image array (EIA) generation method which does not sacrifice accuracy nor rely on high-performance hardware is developed, through raytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both offline and online working flow, experiments verified the effectiveness.  
### S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image **Enhancement**. (arXiv:2201.10294v2 [eess.IV] UPDATED)
- Authors : Chaoyang Zhang, Shaojie Chang, Ti Bai, Xi Chen
- Link : [http://arxiv.org/abs/2201.10294](http://arxiv.org/abs/2201.10294)
> ABSTRACT  :  Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps in different energy channels, reflecting energy properties of the scanned object. Due to the limited photon numbers and the non-ideal detector response of each energy channel, the reconstructed images usually contain much noise. With the development of Deep Learning (DL) technique, different kinds of DL-based models have been proposed for noise reduction. However, most of the models require clean data set as the training labels, which are not always available in medical imaging field. Inspiring by the similarities of each channel's reconstructed image, we proposed a self-supervised learning based PCCT image **enhancement** framework via multi-spectral channels (S2MS). In S2MS framework, both the input and output labels are noisy images. Specifically, one single channel image was used as output while images of other single channels and channel-sum image were used as input to train the network, which can fully use the spectral data information without extra cost. The simulation results based on the AAPM Low-dose CT Challenge database showed that the proposed S2MS model can suppress the noise and preserve details more effectively in comparison with the traditional DL models, which has potential to improve the image quality of PCCT in clinical applications.  
## eess.IV
---
### DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction. (arXiv:2201.10776v1 [eess.IV])
- Authors : Bo Zhou, Jo Schlemper, Neel Dey, Seyed Sadegh, Mohseni Salehi, Chi Liu, Michal Sofka
- Link : [http://arxiv.org/abs/2201.10776](http://arxiv.org/abs/2201.10776)
> ABSTRACT  :  Multi-contrast MRI (MC-MRI) captures multiple complementary imaging modalities to aid in radiological decision-making. Given the need for lowering the time cost of multiple acquisitions, current deep accelerated MRI reconstruction networks focus on exploiting the redundancy between multiple contrasts. However, existing works are largely supervised with paired data and/or prohibitively expensive fully-sampled MRI sequences. Further, reconstruction networks typically rely on convolutional architectures which are limited in their capacity to model long-range interactions and may lead to suboptimal recovery of fine anatomical detail. To these ends, we present a dual-domain self-supervised transformer (DSFormer) for accelerated MC-MRI reconstruction. DSFormer develops a deep conditional cascade transformer (DCCT) consisting of several cascaded **Swin** transformer reconstruction networks (**Swin**RN) trained under two deep conditioning strategies to enable MC-MRI information sharing. We further present a dual-domain (image and k-space) self-supervised learning strategy for DCCT to alleviate the costs of acquiring fully sampled training data. DSFormer generates high-fidelity reconstructions which experimentally outperform current fully-supervised baselines. Moreover, we find that DSFormer achieves nearly the same performance when trained either with full supervision or with our proposed dual-domain self-supervision.  
### CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v2 [eess.IV] UPDATED)
- Authors : Dimitris Perdios, Manuel Vonlanthen, Florian Martinez, Marcel Arditi, Philippe Thiran
- Link : [http://arxiv.org/abs/2008.12750](http://arxiv.org/abs/2008.12750)
> ABSTRACT  :  Ultrafast ultrasound (US) revolutionized biomedical imaging with its capability of acquiring full-view frames at over 1 kHz, unlocking breakthrough modalities such as shear-wave elastography and functional US neuroimaging. Yet, it suffers from strong diffraction artifacts, mainly caused by grating lobes, side lobes, or edge waves. Multiple acquisitions are typically required to obtain a sufficient image quality, at the cost of a reduced frame rate. To answer the increasing demand for high-quality imaging from single unfocused acquisitions, we propose a two-step convolutional neural network (CNN)-based image reconstruction method, compatible with real-time imaging. A low-quality estimate is obtained by means of a backprojection-based operation, akin to conventional delay-and-sum beamforming, from which a high-quality image is restored using a residual CNN with multi-scale and multi-channel filtering properties, trained specifically to remove the diffraction artifacts inherent to ultrafast US imaging. To account for both the **high dynamic range** and the oscillating properties of radio frequency US images, we introduce the mean signed logarithmic absolute error (MSLAE) as training loss function. Experiments were conducted with a linear transducer array, in single plane wave (PW) imaging. Trainings were performed on a simulated dataset, crafted to contain a wide diversity of structures and echogenicities. Extensive numerical evaluations demonstrate that the proposed approach can reconstruct images from single PWs with a quality similar to that of gold-standard synthetic aperture imaging, on a dynamic range in excess of 60 dB. In vitro and in vivo experiments show that trainings carried out on simulated data perform well in experimental settings.  
### S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image **Enhancement**. (arXiv:2201.10294v2 [eess.IV] UPDATED)
- Authors : Chaoyang Zhang, Shaojie Chang, Ti Bai, Xi Chen
- Link : [http://arxiv.org/abs/2201.10294](http://arxiv.org/abs/2201.10294)
> ABSTRACT  :  Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps in different energy channels, reflecting energy properties of the scanned object. Due to the limited photon numbers and the non-ideal detector response of each energy channel, the reconstructed images usually contain much noise. With the development of Deep Learning (DL) technique, different kinds of DL-based models have been proposed for noise reduction. However, most of the models require clean data set as the training labels, which are not always available in medical imaging field. Inspiring by the similarities of each channel's reconstructed image, we proposed a self-supervised learning based PCCT image **enhancement** framework via multi-spectral channels (S2MS). In S2MS framework, both the input and output labels are noisy images. Specifically, one single channel image was used as output while images of other single channels and channel-sum image were used as input to train the network, which can fully use the spectral data information without extra cost. The simulation results based on the AAPM Low-dose CT Challenge database showed that the proposed S2MS model can suppress the noise and preserve details more effectively in comparison with the traditional DL models, which has potential to improve the image quality of PCCT in clinical applications.  
## cs.LG
---
### DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction. (arXiv:2201.10776v1 [eess.IV])
- Authors : Bo Zhou, Jo Schlemper, Neel Dey, Seyed Sadegh, Mohseni Salehi, Chi Liu, Michal Sofka
- Link : [http://arxiv.org/abs/2201.10776](http://arxiv.org/abs/2201.10776)
> ABSTRACT  :  Multi-contrast MRI (MC-MRI) captures multiple complementary imaging modalities to aid in radiological decision-making. Given the need for lowering the time cost of multiple acquisitions, current deep accelerated MRI reconstruction networks focus on exploiting the redundancy between multiple contrasts. However, existing works are largely supervised with paired data and/or prohibitively expensive fully-sampled MRI sequences. Further, reconstruction networks typically rely on convolutional architectures which are limited in their capacity to model long-range interactions and may lead to suboptimal recovery of fine anatomical detail. To these ends, we present a dual-domain self-supervised transformer (DSFormer) for accelerated MC-MRI reconstruction. DSFormer develops a deep conditional cascade transformer (DCCT) consisting of several cascaded **Swin** transformer reconstruction networks (**Swin**RN) trained under two deep conditioning strategies to enable MC-MRI information sharing. We further present a dual-domain (image and k-space) self-supervised learning strategy for DCCT to alleviate the costs of acquiring fully sampled training data. DSFormer generates high-fidelity reconstructions which experimentally outperform current fully-supervised baselines. Moreover, we find that DSFormer achieves nearly the same performance when trained either with full supervision or with our proposed dual-domain self-supervision.  
### Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer. (arXiv:2201.10947v1 [cs.LG])
- Authors : Kaiqi Zhao, Yitao Chen, Ming Zhao
- Link : [http://arxiv.org/abs/2201.10947](http://arxiv.org/abs/2201.10947)
> ABSTRACT  :  Deep learning models have introduced various intelligent applications to edge devices, such as image classification, speech recognition, and augmented reality. There is an increasing need of training such models on the devices in order to deliver personalized, responsive, and private learning. To address this need, this paper presents a new solution for deploying and training state-of-the-art models on the resource-constrained devices. First, the paper proposes a novel filter-pruning-based model compression method to create lightweight trainable models from large models trained in the cloud, without much loss of accuracy. Second, it proposes a novel knowledge transfer method to enable the on-device model to update incrementally in **real time** or near **real time** using incremental learning on new data and enable the on-device model to learn the unseen categories with the help of the in-cloud model in an unsupervised fashion. The results show that 1) our model compression method can remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy of over 90% on CIFAR-10; 2) our knowledge transfer method enables the compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good accuracy on old categories; 3) it allows the compressed models to converge within **real time** (three to six minutes) on the edge for incremental learning tasks; 4) it enables the model to classify unseen categories of data (78.92% Top-1 accuracy) that it is never trained with.  
### Server-Side Stepsizes and Sampling Without Replacement Provably Help in Federated Optimization. (arXiv:2201.11066v1 [cs.LG])
- Authors : Grigory Malinovsky, Konstantin Mishchenko, Peter Richt
- Link : [http://arxiv.org/abs/2201.11066](http://arxiv.org/abs/2201.11066)
> ABSTRACT  :  We present a theoretical study of server-side optimization in federated learning. Our results are the first to show that the widely popular heuristic of scaling the client updates with an extra parameter is very useful in the context of Federated Averaging (FedAvg) with local passes over the client data. Each local pass is performed without replacement using Random Reshuffling, which is a key reason we can show improved complexities. In particular, we prove that whenever the local stepsizes are small, and the update direction is given by FedAvg in conjunction with Random Reshuffling over all clients, one can take a big leap in the obtained direction and improve rates for convex, strongly convex, and non-convex objectives. In particular, in non-convex regime we get an **enhancement** of the rate of convergence from $\mathcal{O}\left(\varepsilon^{-3}\right)$ to $\mathcal{O}\left(\varepsilon^{-2}\right)$. This result is new even for Random Reshuffling performed on a single node. In contrast, if the local stepsizes are large, we prove that the noise of client sampling can be controlled by using a small server-side stepsize. To the best of our knowledge, this is the first time that local steps provably help to overcome the communication bottleneck. Together, our results on the advantage of large and small server-side stepsizes give a formal justification for the practice of adaptive server-side optimization in federated learning. Moreover, we consider a variant of our algorithm that supports partial client participation, which makes the method more practical.  
### Self-Directed Online Machine Learning for Topology Optimization. (arXiv:2002.01927v8 [cs.CE] UPDATED)
- Authors : Changyu Deng, Yizhou Wang, Can Qin, Yun Fu, Wei Lu
- Link : [http://arxiv.org/abs/2002.01927](http://arxiv.org/abs/2002.01927)
> ABSTRACT  :  Topology optimization by optimally distributing materials in a given domain requires non-gradient optimizers to solve highly complicated problems. However, with hundreds of design variables or more involved, solving such problems would require millions of Finite Element Method (FEM) calculations whose computational cost is huge and impractical. Here we report Self-directed Online Learning Optimization (SOLO) which integrates Deep Neural Network (DNN) with FEM calculations. A DNN learns and substitutes the objective as a function of design variables. A small number of training data is generated dynamically based on the DNN's prediction of the optimum. The DNN adapts to the new training data and gives better prediction in the region of interest until convergence. The optimum predicted by the DNN is proved to converge to the true global optimum through iterations. Our algorithm was tested by four types of problems including compliance minimization, fluid-structure optimization, heat transfer **enhancement** and truss optimization. It reduced the computational time by 2 ~ 5 orders of magnitude compared with directly using heuristic methods, and outperformed all state-of-the-art algorithms tested in our experiments. This approach enables solving large multi-dimensional optimization problems.  
### CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v2 [eess.IV] UPDATED)
- Authors : Dimitris Perdios, Manuel Vonlanthen, Florian Martinez, Marcel Arditi, Philippe Thiran
- Link : [http://arxiv.org/abs/2008.12750](http://arxiv.org/abs/2008.12750)
> ABSTRACT  :  Ultrafast ultrasound (US) revolutionized biomedical imaging with its capability of acquiring full-view frames at over 1 kHz, unlocking breakthrough modalities such as shear-wave elastography and functional US neuroimaging. Yet, it suffers from strong diffraction artifacts, mainly caused by grating lobes, side lobes, or edge waves. Multiple acquisitions are typically required to obtain a sufficient image quality, at the cost of a reduced frame rate. To answer the increasing demand for high-quality imaging from single unfocused acquisitions, we propose a two-step convolutional neural network (CNN)-based image reconstruction method, compatible with real-time imaging. A low-quality estimate is obtained by means of a backprojection-based operation, akin to conventional delay-and-sum beamforming, from which a high-quality image is restored using a residual CNN with multi-scale and multi-channel filtering properties, trained specifically to remove the diffraction artifacts inherent to ultrafast US imaging. To account for both the **high dynamic range** and the oscillating properties of radio frequency US images, we introduce the mean signed logarithmic absolute error (MSLAE) as training loss function. Experiments were conducted with a linear transducer array, in single plane wave (PW) imaging. Trainings were performed on a simulated dataset, crafted to contain a wide diversity of structures and echogenicities. Extensive numerical evaluations demonstrate that the proposed approach can reconstruct images from single PWs with a quality similar to that of gold-standard synthetic aperture imaging, on a dynamic range in excess of 60 dB. In vitro and in vivo experiments show that trainings carried out on simulated data perform well in experimental settings.  
### Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs for Robust Speech Recognition. (arXiv:2110.04934v2 [cs.CL] UPDATED)
- Authors : Yiming Wang, Jinyu Li, Heming Wang, Yao Qian, Chengyi Wang, Yu Wu
- Link : [http://arxiv.org/abs/2110.04934](http://arxiv.org/abs/2110.04934)
> ABSTRACT  :  The goal of self-supervised learning (SSL) for automatic speech recognition (ASR) is to learn good speech representations from a large amount of unlabeled speech for the downstream ASR task. However, most SSL frameworks do not consider noise robustness which is crucial for real-world applications. In this paper we propose wav2vec-Switch, a method to encode noise robustness into contextualized representations of speech via contrastive learning. Specifically, we feed original-noisy speech pairs simultaneously into the wav2vec 2.0 network. In addition to the existing contrastive learning task, we switch the quantized representations of the original and noisy speech as additional prediction targets of each other. By doing this, it enforces the network to have consistent predictions for the original and noisy speech, thus allows to learn contextualized representation with noise robustness. Our experiments on synthesized and real noisy data show the effectiveness of our method: it achieves 2.9--4.9% relative word error rate (WER) reduction on the synthesized noisy LibriSpeech data without deterioration on the original data, and 5.7% on CHiME-4 real 1-channel noisy data compared to a data augmentation baseline even with a strong language model for decoding. Our results on CHiME-4 can match or even surpass those with well-designed speech **enhancement** components.  
### Towards General Deep Leakage in Federated Learning. (arXiv:2110.09074v2 [cs.LG] UPDATED)
- Authors : Jiahui Geng, Yongli Mou, Feifei Li, Qing Li, Oya Beyan, Stefan Decker, Chunming Rong
- Link : [http://arxiv.org/abs/2110.09074](http://arxiv.org/abs/2110.09074)
> ABSTRACT  :  Unlike traditional central training, federated learning (FL) improves the performance of the global model by sharing and aggregating local models rather than local data to protect the users' privacy. Although this training approach appears secure, some research has demonstrated that an attacker can still recover private data based on the shared gradient information. This on-the-fly reconstruction attack deserves to be studied in depth because it can occur at any stage of training, whether at the beginning or at the end of model training; no relevant dataset is required and no additional models need to be trained. We break through some unrealistic assumptions and limitations to apply this reconstruction attack in a broader range of scenarios. We propose methods that can reconstruct the training data from shared gradients or weights, corresponding to the FedSGD and FedAvg usage scenarios, respectively. We propose a zero-shot approach to restore labels even if there are duplicate labels in the batch. We study the relationship between the label and image **restoration**. We find that image **restoration** fails even if there is only one incorrectly inferred label in the batch; we also find that when batch images have the same label, the corresponding image is restored as a fusion of that class of images. Our approaches are evaluated on classic image benchmarks, including CIFAR-10 and ImageNet. The batch size, image quality, and the adaptability of the label distribution of our approach exceed those of GradInversion, the state-of-the-art.  
### S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image **Enhancement**. (arXiv:2201.10294v2 [eess.IV] UPDATED)
- Authors : Chaoyang Zhang, Shaojie Chang, Ti Bai, Xi Chen
- Link : [http://arxiv.org/abs/2201.10294](http://arxiv.org/abs/2201.10294)
> ABSTRACT  :  Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps in different energy channels, reflecting energy properties of the scanned object. Due to the limited photon numbers and the non-ideal detector response of each energy channel, the reconstructed images usually contain much noise. With the development of Deep Learning (DL) technique, different kinds of DL-based models have been proposed for noise reduction. However, most of the models require clean data set as the training labels, which are not always available in medical imaging field. Inspiring by the similarities of each channel's reconstructed image, we proposed a self-supervised learning based PCCT image **enhancement** framework via multi-spectral channels (S2MS). In S2MS framework, both the input and output labels are noisy images. Specifically, one single channel image was used as output while images of other single channels and channel-sum image were used as input to train the network, which can fully use the spectral data information without extra cost. The simulation results based on the AAPM Low-dose CT Challenge database showed that the proposed S2MS model can suppress the noise and preserve details more effectively in comparison with the traditional DL models, which has potential to improve the image quality of PCCT in clinical applications.  
## cs.AI
---
### Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer. (arXiv:2201.10947v1 [cs.LG])
- Authors : Kaiqi Zhao, Yitao Chen, Ming Zhao
- Link : [http://arxiv.org/abs/2201.10947](http://arxiv.org/abs/2201.10947)
> ABSTRACT  :  Deep learning models have introduced various intelligent applications to edge devices, such as image classification, speech recognition, and augmented reality. There is an increasing need of training such models on the devices in order to deliver personalized, responsive, and private learning. To address this need, this paper presents a new solution for deploying and training state-of-the-art models on the resource-constrained devices. First, the paper proposes a novel filter-pruning-based model compression method to create lightweight trainable models from large models trained in the cloud, without much loss of accuracy. Second, it proposes a novel knowledge transfer method to enable the on-device model to update incrementally in **real time** or near **real time** using incremental learning on new data and enable the on-device model to learn the unseen categories with the help of the in-cloud model in an unsupervised fashion. The results show that 1) our model compression method can remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy of over 90% on CIFAR-10; 2) our knowledge transfer method enables the compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good accuracy on old categories; 3) it allows the compressed models to converge within **real time** (three to six minutes) on the edge for incremental learning tasks; 4) it enables the model to classify unseen categories of data (78.92% Top-1 accuracy) that it is never trained with.  
### How Robust are Discriminatively Trained Zero-Shot Learning Models?. (arXiv:2201.10972v1 [cs.CV])
- Authors : Mehmet Kerim, Ramazan Gokberk, Pinar Duygulu
- Link : [http://arxiv.org/abs/2201.10972](http://arxiv.org/abs/2201.10972)
> ABSTRACT  :  Data shift robustness is an active research topic, however, it has been primarily investigated from a fully supervised perspective, and robustness of zero-shot learning (ZSL) models have been largely neglected. In this paper, we present a novel analysis on the robustness of discriminative ZSL to image corruptions. We leverage the well-known label embedding model and subject it to a large set of common corruptions and defenses. In order to realize the corruption analysis, we curate and release the first ZSL corruption robustness datasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account the dataset characteristics, class imbalance, class transition trends between seen and unseen classes and the discrepancies between ZSL and GZSL performances. Our results show that discriminative ZSL suffer from corruptions and this trend is further exacerbated by the severe class imbalance and model weakness inherent in ZSL methods. We then combine our findings with those based on adversarial attacks in ZSL, and highlight the different effects of corruptions and adversarial examples, such as the pseudo-robustness effect present under adversarial attacks. We also obtain new strong baselines for the label embedding model with certain corruption robustness **enhancement** methods. Finally, our experiments show that although existing methods to improve robustness somewhat work for ZSL models, they do not produce a tangible effect.  
### Towards General Deep Leakage in Federated Learning. (arXiv:2110.09074v2 [cs.LG] UPDATED)
- Authors : Jiahui Geng, Yongli Mou, Feifei Li, Qing Li, Oya Beyan, Stefan Decker, Chunming Rong
- Link : [http://arxiv.org/abs/2110.09074](http://arxiv.org/abs/2110.09074)
> ABSTRACT  :  Unlike traditional central training, federated learning (FL) improves the performance of the global model by sharing and aggregating local models rather than local data to protect the users' privacy. Although this training approach appears secure, some research has demonstrated that an attacker can still recover private data based on the shared gradient information. This on-the-fly reconstruction attack deserves to be studied in depth because it can occur at any stage of training, whether at the beginning or at the end of model training; no relevant dataset is required and no additional models need to be trained. We break through some unrealistic assumptions and limitations to apply this reconstruction attack in a broader range of scenarios. We propose methods that can reconstruct the training data from shared gradients or weights, corresponding to the FedSGD and FedAvg usage scenarios, respectively. We propose a zero-shot approach to restore labels even if there are duplicate labels in the batch. We study the relationship between the label and image **restoration**. We find that image **restoration** fails even if there is only one incorrectly inferred label in the batch; we also find that when batch images have the same label, the corresponding image is restored as a fusion of that class of images. Our approaches are evaluated on classic image benchmarks, including CIFAR-10 and ImageNet. The batch size, image quality, and the adaptability of the label distribution of our approach exceed those of GradInversion, the state-of-the-art.  
# Paper List
---
## cs.CV
---
**79** new papers in cs.CV:-) 
1. Jacobian Computation for Cumulative B-splines on SE(3) and Application to Continuous-Time Object Tracking. (arXiv:2201.10602v1 [cs.CV])
2. Unsupervised Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation via Semi-supervised Learning and Label Fusion. (arXiv:2201.10647v1 [cs.CV])
3. Attentive Task Interaction Network for Multi-Task Learning. (arXiv:2201.10649v1 [cs.CV])
4. Beyond Visual Image: Automated Diagnosis of Pigmented Skin Lesions Combining Clinical Image Features with Patient Data. (arXiv:2201.10650v1 [cs.CV])
5. SA-VQA: Structured Alignment of Visual and Semantic Representations for Visual Question Answering. (arXiv:2201.10654v1 [cs.CV])
6. MGA-VQA: Multi-Granularity Alignment for Visual Question Answering. (arXiv:2201.10656v1 [cs.CV])
7. Do Neural Networks for Segmentation Understand Insideness?. (arXiv:2201.10664v1 [cs.CV])
8. Writer Recognition Using Off-line Handwritten Single Block Characters. (arXiv:2201.10665v1 [cs.CV])
9. Virtual Adversarial Training for Semi-supervised Breast Mass Classification. (arXiv:2201.10675v1 [cs.CV])
10. Estimation of Spectral Biophysical Skin Properties from Captured RGB Albedo. (arXiv:2201.10695v1 [cs.CV])
11. Deep Image Deblurring: A Survey. (arXiv:2201.10700v1 [cs.CV])
12. Anomaly Detection via Reverse Distillation from One-Class Embedding. (arXiv:2201.10703v1 [cs.CV])
13. Toward Data-Driven STAP Radar. (arXiv:2201.10712v1 [cs.CV])
14. Image Generation with Self Pixel-wise Normalization. (arXiv:2201.10725v1 [cs.CV])
15. Training Vision Transformers with Only 2040 Images. (arXiv:2201.10728v1 [cs.CV])
16. Mitigating the Mutual Error Amplification for Semi-Supervised Object Detection. (arXiv:2201.10734v1 [cs.CV])
17. A Joint Convolution Auto-encoder Network for Infrared and Visible Image Fusion. (arXiv:2201.10736v1 [cs.CV])
18. Class-Aware Generative Adversarial Transformers for Medical Image Segmentation. (arXiv:2201.10737v1 [cs.CV])
19. Infrared and visible image fusion based on Multi-State Contextual Hidden Markov Model. (arXiv:2201.10739v1 [cs.CV])
20. Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution. (arXiv:2201.10747v1 [eess.IV])
21. Interactive Image Inpainting Using Semantic Guidance. (arXiv:2201.10753v1 [cs.CV])
22. A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes. (arXiv:2201.10766v1 [cs.CV])
23. DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction. (arXiv:2201.10776v1 [eess.IV])
24. ASFD: Automatic and Scalable Face Detector. (arXiv:2201.10781v1 [cs.CV])
25. Self-supervised 3D Semantic Representation Learning for Vision-and-Language Navigation. (arXiv:2201.10788v1 [cs.CV])
26. When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism. (arXiv:2201.10801v1 [cs.CV])
27. MonoDistill: Learning Spatial Features for Monocular 3D Object Detection. (arXiv:2201.10830v1 [cs.CV])
28. PARS: Pseudo-Label Aware Robust Sample Selection for Learning with Noisy Labels. (arXiv:2201.10836v1 [cs.CV])
29. Comparison of Depth Estimation Setups from Stereo Endoscopy and Optical Tracking for Point Measurements. (arXiv:2201.10848v1 [cs.CV])
30. Predicting Knee Osteoarthritis Progression from Structural MRI using Deep Learning. (arXiv:2201.10849v1 [eess.IV])
31. Visualizing the diversity of representations learned by Bayesian neural networks. (arXiv:2201.10859v1 [cs.LG])
32. On the Issues of TrueDepth Sensor Data for Computer Vision Tasks Across Different iPad Generations. (arXiv:2201.10865v1 [cs.CV])
33. TransPPG: Two-stream Transformer for Remote Heart Rate Estimate. (arXiv:2201.10873v1 [cs.CV])
34. Hyperparameter Optimization for COVID-19 Chest X-Ray Classification. (arXiv:2201.10885v1 [eess.IV])
35. One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v1 [cs.LG])
36. Speeding up Heterogeneous Federated Learning with Sequentially Trained Superclients. (arXiv:2201.10899v1 [cs.LG])
37. A Bayesian Based Deep Unrolling Algorithm for Single-Photon Lidar Systems. (arXiv:2201.10910v1 [eess.IV])
38. Boosting 3D Adversarial Attacks with Attacking On Frequency. (arXiv:2201.10937v1 [cs.CV])
39. Projective Urban Texturing. (arXiv:2201.10938v1 [cs.CV])
40. Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v1 [cs.CV])
41. Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer. (arXiv:2201.10947v1 [cs.LG])
42. Dual-Tasks Siamese Transformer Framework for Building Damage Assessment. (arXiv:2201.10953v1 [cs.CV])
43. Learning to Compose Diversified Prompts for Image Emotion Classification. (arXiv:2201.10963v1 [cs.CV])
44. How Robust are Discriminatively Trained Zero-Shot Learning Models?. (arXiv:2201.10972v1 [cs.CV])
45. Joint Liver and Hepatic Lesion Segmentation using a Hybrid CNN with Transformer Layers. (arXiv:2201.10981v1 [eess.IV])
46. Jalisco's multiclass land cover analysis and classification using a novel lightweight convnet with real-world multispectral and relief data. (arXiv:2201.10985v1 [cs.CV])
47. Learning To Recognize Procedural Activities with Distant Supervision. (arXiv:2201.10990v1 [cs.CV])
48. One shot PACS: Patient specific Anatomic Context and Shape prior aware recurrent registration-segmentation of longitudinal thoracic cone beam CTs. (arXiv:2201.11000v1 [eess.IV])
49. A Multi-rater Comparative Study of Automatic Target Localization Methods for Epilepsy Deep Brain Stimulation Procedures. (arXiv:2201.11002v1 [eess.IV])
50. An Overview of Compressible and Learnable Image Transformation with Secret Key and Its Applications. (arXiv:2201.11006v1 [cs.CV])
51. Evaluating language-biased image classification based on semantic representations. (arXiv:2201.11014v1 [cs.CV])
52. RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation. (arXiv:2201.11037v1 [eess.IV])
53. Momentum Capsule Networks. (arXiv:2201.11091v1 [cs.CV])
54. Self-Attention Neural Bag-of-Features. (arXiv:2201.11092v1 [cs.CV])
55. Self-attention fusion for audiovisual emotion recognition with incomplete data. (arXiv:2201.11095v1 [cs.CV])
56. Adaptive Instance Distillation for Object Detection in Autonomous Driving. (arXiv:2201.11097v1 [cs.CV])
57. Auto-Compressing Subset Pruning for Semantic Image Segmentation. (arXiv:2201.11103v1 [cs.CV])
58. Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v1 [cs.CV])
59. Generating a Fusion Image: One's Identity and Another's Shape. (arXiv:1804.07455v2 [cs.CV] UPDATED)
60. A Data-driven Adversarial Examples Recognition Framework via Adversarial Feature Genome. (arXiv:1812.10085v3 [cs.CV] UPDATED)
61. Graph Neural Network for Video Relocalization. (arXiv:2007.09877v2 [cs.CV] UPDATED)
62. CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v2 [eess.IV] UPDATED)
63. ES Attack: Model Stealing against Deep Neural Networks without Data Hurdles. (arXiv:2009.09560v2 [cs.CV] UPDATED)
64. Open-World Semi-Supervised Learning. (arXiv:2102.03526v3 [cs.LG] UPDATED)
65. Generative Transformer for Accurate and Reliable Salient Object Detection. (arXiv:2104.10127v3 [cs.CV] UPDATED)
66. Pruning Ternary Quantization. (arXiv:2107.10998v2 [cs.CV] UPDATED)
67. Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v3 [cs.CV] UPDATED)
68. Deep Kernel Representation for Image Reconstruction in PET. (arXiv:2110.01174v3 [eess.IV] UPDATED)
69. Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v2 [eess.IV] UPDATED)
70. TAda! Temporally-Adaptive Convolutions for Video Understanding. (arXiv:2110.06178v3 [cs.CV] UPDATED)
71. Three approaches to facilitate DNN generalization to objects in out-of-distribution orientations and illuminations. (arXiv:2111.00131v2 [cs.CV] UPDATED)
72. Does a Face Mask Protect my Privacy?: Deep Learning to Predict Protected Attributes from Masked Face Images. (arXiv:2112.07879v2 [cs.CV] UPDATED)
73. iSegFormer: Interactive Image Segmentation with Transformers. (arXiv:2112.11325v2 [cs.CV] UPDATED)
74. A Saliency based Feature Fusion Model for EEG Emotion Estimation. (arXiv:2201.03891v2 [cs.CV] UPDATED)
75. Attention-based Proposals Refinement for 3D Object Detection. (arXiv:2201.07070v2 [cs.CV] UPDATED)
76. **Real-time** Rendering for Integral Imaging Light Field Displays Based on a Voxel-Pixel Lookup Table. (arXiv:2201.08266v2 [cs.GR] UPDATED)
77. SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v2 [eess.IV] UPDATED)
78. S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image **Enhancement**. (arXiv:2201.10294v2 [eess.IV] UPDATED)
79. Plaque segmentation via masking of the artery wall. (arXiv:2201.10424v2 [eess.IV] UPDATED)
## eess.IV
---
**22** new papers in eess.IV:-) 
1. Virtual Adversarial Training for Semi-supervised Breast Mass Classification. (arXiv:2201.10675v1 [cs.CV])
2. Class-Aware Generative Adversarial Transformers for Medical Image Segmentation. (arXiv:2201.10737v1 [cs.CV])
3. Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution. (arXiv:2201.10747v1 [eess.IV])
4. DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction. (arXiv:2201.10776v1 [eess.IV])
5. Predicting Knee Osteoarthritis Progression from Structural MRI using Deep Learning. (arXiv:2201.10849v1 [eess.IV])
6. Hyperparameter Optimization for COVID-19 Chest X-Ray Classification. (arXiv:2201.10885v1 [eess.IV])
7. A Bayesian Based Deep Unrolling Algorithm for Single-Photon Lidar Systems. (arXiv:2201.10910v1 [eess.IV])
8. Dual-Tasks Siamese Transformer Framework for Building Damage Assessment. (arXiv:2201.10953v1 [cs.CV])
9. Joint Liver and Hepatic Lesion Segmentation using a Hybrid CNN with Transformer Layers. (arXiv:2201.10981v1 [eess.IV])
10. One shot PACS: Patient specific Anatomic Context and Shape prior aware recurrent registration-segmentation of longitudinal thoracic cone beam CTs. (arXiv:2201.11000v1 [eess.IV])
11. A Multi-rater Comparative Study of Automatic Target Localization Methods for Epilepsy Deep Brain Stimulation Procedures. (arXiv:2201.11002v1 [eess.IV])
12. Meta-optic Accelerators for Object Classifiers. (arXiv:2201.11034v1 [physics.optics])
13. RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation. (arXiv:2201.11037v1 [eess.IV])
14. Graph Neural Network for Video Relocalization. (arXiv:2007.09877v2 [cs.CV] UPDATED)
15. CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v2 [eess.IV] UPDATED)
16. Deep Kernel Representation for Image Reconstruction in PET. (arXiv:2110.01174v3 [eess.IV] UPDATED)
17. Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v2 [eess.IV] UPDATED)
18. SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v2 [eess.IV] UPDATED)
19. The risk of bias in denoising methods. (arXiv:2201.09351v2 [stat.AP] UPDATED)
20. Deep Unrolling for Magnetic Resonance Fingerprinting. (arXiv:2201.09375v2 [eess.IV] UPDATED)
21. S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image **Enhancement**. (arXiv:2201.10294v2 [eess.IV] UPDATED)
22. Plaque segmentation via masking of the artery wall. (arXiv:2201.10424v2 [eess.IV] UPDATED)
## cs.LG
---
**117** new papers in cs.LG:-) 
1. A Kernel Learning Method for Backward SDE Filter. (arXiv:2201.10600v1 [math.NA])
2. Exploiting Hybrid Models of Tensor-Train Networks for Spoken Command Recognition. (arXiv:2201.10609v1 [cs.SD])
3. Extending compositional data analysis from a graph signal processing perspective. (arXiv:2201.10610v1 [stat.ME])
4. Attentive Task Interaction Network for Multi-Task Learning. (arXiv:2201.10649v1 [cs.CV])
5. Do Neural Networks for Segmentation Understand Insideness?. (arXiv:2201.10664v1 [cs.CV])
6. Promises and Challenges of Causality for Ethical Machine Learning. (arXiv:2201.10683v1 [cs.LG])
7. Invertible Voice Conversion. (arXiv:2201.10687v1 [eess.AS])
8. A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model. (arXiv:2201.10707v1 [cs.CL])
9. Sparsity Regularization For Cold-Start Recommendation. (arXiv:2201.10711v1 [cs.IR])
10. Adaptive Resonance Theory-based Topological Clustering with a Divisive Hierarchical Structure Capable of Continual Learning. (arXiv:2201.10713v1 [cs.LG])
11. Image Generation with Self Pixel-wise Normalization. (arXiv:2201.10725v1 [cs.CV])
12. Class-Aware Generative Adversarial Transformers for Medical Image Segmentation. (arXiv:2201.10737v1 [cs.CV])
13. Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution. (arXiv:2201.10747v1 [eess.IV])
14. Graph Neural Networks with Dynamic and Static Representations for Social Recommendation. (arXiv:2201.10751v1 [cs.IR])
15. Phishing Attacks Detection -- A Machine Learning-Based Approach. (arXiv:2201.10752v1 [cs.CR])
16. An Efficient and Robust System for Vertically Federated Random Forest. (arXiv:2201.10761v1 [cs.LG])
17. Competition over data: how does data purchase affect users?. (arXiv:2201.10774v1 [cs.LG])
18. DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction. (arXiv:2201.10776v1 [eess.IV])
19. Meta-learning Spiking Neural Networks with Surrogate Gradient Descent. (arXiv:2201.10777v1 [cs.NE])
20. Towards Sharp Stochastic Zeroth Order Hessian Estimators over Riemannian Manifolds. (arXiv:2201.10780v1 [stat.ML])
21. Variational Model Inversion Attacks. (arXiv:2201.10787v1 [cs.LG])
22. An Automated Question-Answering Framework Based on Evolution Algorithm. (arXiv:2201.10797v1 [cs.CL])
23. Exploiting Semantic Epsilon Greedy Exploration Strategy in Multi-Agent Reinforcement Learning. (arXiv:2201.10803v1 [cs.LG])
24. An Explainable Artificial Intelligence Framework for Quality-Aware IoE Service Delivery. (arXiv:2201.10822v1 [cs.AI])
25. Privacy-Preserving Logistic Regression Training with a Faster Gradient Variant. (arXiv:2201.10838v1 [cs.CR])
26. Visualizing the diversity of representations learned by Bayesian neural networks. (arXiv:2201.10859v1 [cs.LG])
27. A deep learning method based on patchwise training for reconstructing temperature field. (arXiv:2201.10860v1 [cs.LG])
28. S$^2$NN: Time Step Reduction of Spiking Surrogate Gradients for Training Energy Efficient Single-Step Neural Networks. (arXiv:2201.10879v1 [cs.LG])
29. Hyperparameter Optimization for COVID-19 Chest X-Ray Classification. (arXiv:2201.10885v1 [eess.IV])
30. One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v1 [cs.LG])
31. Speeding up Heterogeneous Federated Learning with Sequentially Trained Superclients. (arXiv:2201.10899v1 [cs.LG])
32. Improving robustness and calibration in ensembles with diversity regularization. (arXiv:2201.10908v1 [cs.LG])
33. FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control. (arXiv:2201.10936v1 [cs.SD])
34. On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v1 [cs.SI])
35. Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer. (arXiv:2201.10947v1 [cs.LG])
36. Dual-Tasks Siamese Transformer Framework for Building Damage Assessment. (arXiv:2201.10953v1 [cs.CV])
37. Physics-informed ConvNet: Learning Physical Field from a Shallow Neural Network. (arXiv:2201.10967v1 [cs.LG])
38. Machine Learning for Food Review and Recommendation. (arXiv:2201.10978v1 [cs.IR])
39. Alleviating Cold-start Problem in CTR Prediction with A Variational Embedding Learning Framework. (arXiv:2201.10980v1 [cs.IR])
40. Joint Liver and Hepatic Lesion Segmentation using a Hybrid CNN with Transformer Layers. (arXiv:2201.10981v1 [eess.IV])
41. Online POI Recommendation: Learning Dynamic Geo-Human Interactions in Streams. (arXiv:2201.10983v1 [cs.IR])
42. Uphill Roads to Variational Tightness: Monotonicity and Monte Carlo Objectives. (arXiv:2201.10989v1 [stat.ML])
43. Evaluating language-biased image classification based on semantic representations. (arXiv:2201.11014v1 [cs.CV])
44. Recency Dropout for Recurrent Recommender Systems. (arXiv:2201.11016v1 [cs.IR])
45. Fast Server Learning Rate Tuning for Coded Federated Dropout. (arXiv:2201.11036v1 [cs.LG])
46. Generalization Error Bounds on Deep Learning with Markov Datasets. (arXiv:2201.11059v1 [stat.ML])
47. Server-Side Stepsizes and Sampling Without Replacement Provably Help in Federated Optimization. (arXiv:2201.11066v1 [cs.LG])
48. Understanding and Compressing Music with Maximal Transformable Patterns. (arXiv:2201.11085v1 [cs.LG])
49. Momentum Capsule Networks. (arXiv:2201.11091v1 [cs.CV])
50. Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v1 [cs.LG])
51. A probabilistic latent variable model for detecting structure in binary data. (arXiv:2201.11108v1 [stat.ML])
52. Using a Novel COVID-19 Calculator to Measure Positive U.S. Socio-Economic Impact of a COVID-19 Pre-Screening Solution (AI/ML). (arXiv:2201.11109v1 [cs.AI])
53. Post-training Quantization for Neural Networks with Provable Guarantees. (arXiv:2201.11113v1 [cs.LG])
54. Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v1 [cs.CV])
55. CsFEVER and CTKFacts: Czech Datasets for Fact Verification. (arXiv:2201.11115v1 [cs.CL])
56. Finite-Sample Analysis of Nonlinear Stochastic Approximation with Applications in Reinforcement Learning. (arXiv:1905.11425v7 [math.OC] UPDATED)
57. BRIDGE: Byzantine-resilient Decentralized Gradient Descent. (arXiv:1908.08098v2 [stat.ML] UPDATED)
58. Probabilistic Surrogate Networks for Simulators with Unbounded Randomness. (arXiv:1910.11950v2 [cs.LG] UPDATED)
59. Self-Directed Online Machine Learning for Topology Optimization. (arXiv:2002.01927v8 [cs.CE] UPDATED)
60. Personalized Federated Learning with Moreau Envelopes. (arXiv:2006.08848v3 [cs.LG] UPDATED)
61. Splintering with distributions: A stochastic decoy scheme for private computation. (arXiv:2007.02719v3 [cs.LG] UPDATED)
62. Graph Neural Network for Video Relocalization. (arXiv:2007.09877v2 [cs.CV] UPDATED)
63. CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v2 [eess.IV] UPDATED)
64. A Deeper Look at Discounting Mismatch in Actor-Critic Algorithms. (arXiv:2010.01069v4 [cs.LG] UPDATED)
65. DONE: Distributed Approximate Newton-type Method for Federated Edge Learning. (arXiv:2012.05625v4 [cs.LG] UPDATED)
66. Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL. (arXiv:2012.09737v2 [cs.LG] UPDATED)
67. Complexity of zigzag sampling algorithm for strongly log-concave distributions. (arXiv:2012.11094v2 [stat.ML] UPDATED)
68. A SOM-based Gradient-Free Deep Learning Method with Convergence Analysis. (arXiv:2101.05612v2 [cs.LG] UPDATED)
69. Open-World Semi-Supervised Learning. (arXiv:2102.03526v3 [cs.LG] UPDATED)
70. Scalable Hypergraph Embedding System. (arXiv:2103.09660v5 [cs.SI] UPDATED)
71. Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective. (arXiv:2104.03413v4 [cs.LG] UPDATED)
72. SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. (arXiv:2105.11043v3 [cs.LG] UPDATED)
73. Robust Implicit Networks via Non-Euclidean Contractions. (arXiv:2106.03194v6 [cs.LG] UPDATED)
74. Variational multiple shooting for Bayesian ODEs with Gaussian processes. (arXiv:2106.10905v2 [cs.LG] UPDATED)
75. Multiaccurate Proxies for Downstream Fairness. (arXiv:2107.04423v2 [cs.LG] UPDATED)
76. Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v7 [q-bio.OT] UPDATED)
77. Mis-spoke or mis-lead: Achieving Robustness in Multi-Agent Communicative Reinforcement Learning. (arXiv:2108.03803v2 [cs.LG] UPDATED)
78. Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v3 [cs.CV] UPDATED)
79. Enel: Context-Aware Dynamic Scaling of Distributed Dataflow Jobs using Graph Propagation. (arXiv:2108.12211v3 [cs.DC] UPDATED)
80. Explicit construction of the minimum error variance estimator for stochastic LTI state-space systems. (arXiv:2109.02384v2 [math.OC] UPDATED)
81. ReLU Regression with Massart Noise. (arXiv:2109.04623v2 [cs.LG] UPDATED)
82. Autoregressive neural-network wavefunctions for ab initio quantum chemistry. (arXiv:2109.12606v2 [physics.chem-ph] UPDATED)
83. The edge of chaos: quantum field theory and deep neural networks. (arXiv:2109.13247v2 [hep-th] UPDATED)
84. On the Provable Generalization of Recurrent Neural Networks. (arXiv:2109.14142v4 [cs.LG] UPDATED)
85. Discovering Boundary Values of Feature-based Machine Learning Classifiers through Exploratory Datamorphic Testing. (arXiv:2110.00330v2 [cs.LG] UPDATED)
86. Information-Theoretic Characterization of the Generalization Error for Iterative Semi-Supervised Learning. (arXiv:2110.00926v2 [cs.LG] UPDATED)
87. Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v2 [eess.IV] UPDATED)
88. Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs for Robust Speech Recognition. (arXiv:2110.04934v2 [cs.CL] UPDATED)
89. LaughNet: synthesizing laughter utterances from waveform silhouettes and a single laughter example. (arXiv:2110.04946v2 [cs.SD] UPDATED)
90. Towards General Deep Leakage in Federated Learning. (arXiv:2110.09074v2 [cs.LG] UPDATED)
91. Confidence-Aware Imitation Learning from Demonstrations with Varying Optimality. (arXiv:2110.14754v2 [cs.LG] UPDATED)
92. Generating 3D Molecules Conditional on Receptor Binding Sites with Deep Generative Models. (arXiv:2110.15200v2 [q-bio.QM] UPDATED)
93. Physics-informed linear regression is competitive with two Machine Learning methods in residential building MPC. (arXiv:2110.15911v2 [cs.LG] UPDATED)
94. Three approaches to facilitate DNN generalization to objects in out-of-distribution orientations and illuminations. (arXiv:2111.00131v2 [cs.CV] UPDATED)
95. Dynamic Data Augmentation with Gating Networks. (arXiv:2111.03253v2 [cs.LG] UPDATED)
96. Explaining Hyperparameter Optimization via Partial Dependence Plots. (arXiv:2111.04820v2 [cs.LG] UPDATED)
97. Federated Learning for Internet of Things: Applications, Challenges, and Opportunities. (arXiv:2111.07494v2 [cs.LG] UPDATED)
98. PySINDy: A comprehensive Python package for robust sparse system identification. (arXiv:2111.08481v2 [eess.SY] UPDATED)
99. Self-Learning Tuning for Post-Silicon Validation. (arXiv:2111.08995v3 [cs.LG] UPDATED)
100. Bayesian Learning via Neural Schr\"odinger-F\"ollmer Flows. (arXiv:2111.10510v6 [stat.ML] UPDATED)
101. KML: Using Machine Learning to Improve Storage Systems. (arXiv:2111.11554v2 [cs.OS] UPDATED)
102. LightSAFT: Lightweight Latent Source Aware Frequency Transform for Source Separation. (arXiv:2111.12516v2 [eess.AS] UPDATED)
103. Graph Neural Networks for Charged Particle Tracking on FPGAs. (arXiv:2112.02048v2 [physics.ins-det] UPDATED)
104. Equivariant Quantum Graph Circuits. (arXiv:2112.05261v2 [cs.LG] UPDATED)
105. Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v2 [cs.CL] UPDATED)
106. Bridging Adversarial and Nonstationary Multi-armed Bandit. (arXiv:2201.01628v2 [cs.LG] UPDATED)
107. Opportunities of Hybrid Model-based Reinforcement Learning for Cell Therapy Manufacturing Process Control. (arXiv:2201.03116v2 [eess.SY] UPDATED)
108. Deep Unified Representation for Heterogeneous Recommendation. (arXiv:2201.05861v2 [cs.IR] UPDATED)
109. Generalization in Supervised Learning Through Riemannian Contraction. (arXiv:2201.06656v2 [cs.LG] UPDATED)
110. Graph Neural Network-based Android Malware Classification with Jumping Knowledge. (arXiv:2201.07537v3 [cs.CR] UPDATED)
111. Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows. (arXiv:2201.07677v3 [cs.LG] UPDATED)
112. Debiased Graph Neural Networks with Agnostic Label Selection Bias. (arXiv:2201.07708v2 [cs.LG] UPDATED)
113. Dissipative Hamiltonian Neural Networks: Learning Dissipative and Conservative Dynamics Separately. (arXiv:2201.10085v2 [cs.LG] UPDATED)
114. Stochastic Coded Federated Learning with Convergence and Privacy Guarantees. (arXiv:2201.10092v2 [cs.LG] UPDATED)
115. S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image **Enhancement**. (arXiv:2201.10294v2 [eess.IV] UPDATED)
116. Zero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention. (arXiv:2201.10375v2 [eess.AS] UPDATED)
117. Beyond the Frontier: Fairness Without Accuracy Loss. (arXiv:2201.10408v2 [cs.LG] UPDATED)
## cs.AI
---
**46** new papers in cs.AI:-) 
1. Learning Norms via Natural Language Teachings. (arXiv:2201.10556v1 [cs.AI])
2. DebtFree: Minimizing Labeling Cost in Self-Admitted Technical Debt Identification using Semi-Supervised Learning. (arXiv:2201.10592v1 [cs.SE])
3. The Price of Strategyproofing Peer Assessment. (arXiv:2201.10631v1 [cs.GT])
4. Intersectionality Goes Analytical: Taming Combinatorial Explosion Through Type Abstraction. (arXiv:2201.10643v1 [cs.HC])
5. Beyond Visual Image: Automated Diagnosis of Pigmented Skin Lesions Combining Clinical Image Features with Patient Data. (arXiv:2201.10650v1 [cs.CV])
6. Virtual Adversarial Training for Semi-supervised Breast Mass Classification. (arXiv:2201.10675v1 [cs.CV])
7. Training Vision Transformers with Only 2040 Images. (arXiv:2201.10728v1 [cs.CV])
8. Class-Aware Generative Adversarial Transformers for Medical Image Segmentation. (arXiv:2201.10737v1 [cs.CV])
9. A Cooperation-Aware Lane Change Method for Autonomous Vehicles. (arXiv:2201.10746v1 [cs.RO])
10. Interactive Image Inpainting Using Semantic Guidance. (arXiv:2201.10753v1 [cs.CV])
11. Exploiting Semantic Epsilon Greedy Exploration Strategy in Multi-Agent Reinforcement Learning. (arXiv:2201.10803v1 [cs.LG])
12. Speed, Quality, and the Optimal Timing of Complex Decisions: Field Evidence. (arXiv:2201.10808v1 [econ.GN])
13. An Explainable Artificial Intelligence Framework for Quality-Aware IoE Service Delivery. (arXiv:2201.10822v1 [cs.AI])
14. Visualizing the diversity of representations learned by Bayesian neural networks. (arXiv:2201.10859v1 [cs.LG])
15. A deep learning method based on patchwise training for reconstructing temperature field. (arXiv:2201.10860v1 [cs.LG])
16. One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v1 [cs.LG])
17. Improving robustness and calibration in ensembles with diversity regularization. (arXiv:2201.10908v1 [cs.LG])
18. Behavior Tree-Based Asynchronous Task Planning for Multiple Mobile Robots using a Data Distribution Service. (arXiv:2201.10918v1 [cs.RO])
19. On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v1 [cs.SI])
20. Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer. (arXiv:2201.10947v1 [cs.LG])
21. How Robust are Discriminatively Trained Zero-Shot Learning Models?. (arXiv:2201.10972v1 [cs.CV])
22. Online POI Recommendation: Learning Dynamic Geo-Human Interactions in Streams. (arXiv:2201.10983v1 [cs.IR])
23. Jalisco's multiclass land cover analysis and classification using a novel lightweight convnet with real-world multispectral and relief data. (arXiv:2201.10985v1 [cs.CV])
24. Uphill Roads to Variational Tightness: Monotonicity and Monte Carlo Objectives. (arXiv:2201.10989v1 [stat.ML])
25. Momentum Capsule Networks. (arXiv:2201.11091v1 [cs.CV])
26. Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v1 [cs.LG])
27. Do You See What I See? Capabilities and Limits of Automated Multimedia Content Analysis. (arXiv:2201.11105v1 [cs.MM])
28. Using a Novel COVID-19 Calculator to Measure Positive U.S. Socio-Economic Impact of a COVID-19 Pre-Screening Solution (AI/ML). (arXiv:2201.11109v1 [cs.AI])
29. Post-training Quantization for Neural Networks with Provable Guarantees. (arXiv:2201.11113v1 [cs.LG])
30. Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v1 [cs.CV])
31. Cybertrust: From Explainable to Actionable and Interpretable AI (AI2). (arXiv:2201.11117v1 [cs.AI])
32. A Deeper Look at Discounting Mismatch in Actor-Critic Algorithms. (arXiv:2010.01069v4 [cs.LG] UPDATED)
33. Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL. (arXiv:2012.09737v2 [cs.LG] UPDATED)
34. Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v4 [cs.AI] UPDATED)
35. Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v7 [q-bio.OT] UPDATED)
36. Pruning Ternary Quantization. (arXiv:2107.10998v2 [cs.CV] UPDATED)
37. Towards General Deep Leakage in Federated Learning. (arXiv:2110.09074v2 [cs.LG] UPDATED)
38. Confidence-Aware Imitation Learning from Demonstrations with Varying Optimality. (arXiv:2110.14754v2 [cs.LG] UPDATED)
39. Natural Language Processing for Smart Healthcare. (arXiv:2110.15803v2 [cs.CL] UPDATED)
40. Three approaches to facilitate DNN generalization to objects in out-of-distribution orientations and illuminations. (arXiv:2111.00131v2 [cs.CV] UPDATED)
41. Self-Learning Tuning for Post-Silicon Validation. (arXiv:2111.08995v3 [cs.LG] UPDATED)
42. Intersection focused Situation Coverage-based Verification and Validation Framework for Autonomous Vehicles Implemented in CARLA. (arXiv:2112.14706v2 [cs.RO] UPDATED)
43. A Saliency based Feature Fusion Model for EEG Emotion Estimation. (arXiv:2201.03891v2 [cs.CV] UPDATED)
44. Neural Network Compression of ACAS Xu is Unsafe: Closed-Loop Verification through Quantized State Backreachability. (arXiv:2201.06626v2 [math.NA] UPDATED)
45. Debiased Graph Neural Networks with Agnostic Label Selection Bias. (arXiv:2201.07708v2 [cs.LG] UPDATED)
46. Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection. (arXiv:2201.10474v2 [cs.CL] UPDATED)
