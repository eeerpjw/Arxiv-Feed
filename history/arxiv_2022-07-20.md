# Your interest papers
---
## cs.CV
---
### Global-Local Stepwise Generative Network for Ultra High-Resolution Image **Restoration**. (arXiv:2207.08808v1 [cs.CV])
- Authors : Xin Feng, Haobo Ji, Wenjie Pei, Fanglin Chen, David Zhang, Guangming Lu
- Link : [http://arxiv.org/abs/2207.08808](http://arxiv.org/abs/2207.08808)
> ABSTRACT  :  While the research on image background **restoration** from regular size of degraded images has achieved remarkable progress, restoring ultra high-resolution (e.g., 4K) images remains an extremely challenging task due to the explosion of computational complexity and memory usage, as well as the deficiency of annotated data. In this paper we present a novel model for ultra high-resolution image **restoration**, referred to as the Global-Local Stepwise Generative Network (GLSGN), which employs a stepwise restoring strategy involving four restoring pathways: three local pathways and one global pathway. The local pathways focus on conducting image **restoration** in a fine-grained manner over local but high-resolution image patches, while the global pathway performs image **restoration** coarsely on the scale-down but intact image to provide cues for the local pathways in a global view including semantics and noise patterns. To smooth the mutual collaboration between these four pathways, our GLSGN is designed to ensure the inter-pathway consistency in four aspects in terms of low-level content, perceptual attention, restoring intensity and high-level semantics, respectively. As another major contribution of this work, we also introduce the first ultra high-resolution dataset to date for both reflection removal and rain streak removal, comprising 4,670 real-world and synthetic images. Extensive experiments across three typical tasks for image background **restoration**, including image reflection removal, image rain streak removal and image dehazing, show that our GLSGN consistently outperforms state-of-the-art methods.  
### Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video. (arXiv:2207.08920v1 [cs.CV])
- Authors : Fen Tsai
- Link : [http://arxiv.org/abs/2207.08920](http://arxiv.org/abs/2207.08920)
> ABSTRACT  :  Introduction: Hand function is a central determinant of independence after stroke. Measuring hand use in the home environment is necessary to evaluate the impact of new interventions, and calls for novel wearable technologies. Egocentric video can capture hand-object interactions in context, as well as show how more-affected hands are used during **bilateral** tasks (for stabilization or manipulation). Automated methods are required to extract this information. Objective: To use artificial intelligence-based computer vision to classify hand use and hand role from egocentric videos recorded at home after stroke. Methods: Twenty-one stroke survivors participated in the study. A random forest classifier, a SlowFast neural network, and the Hand Object Detector neural network were applied to identify hand use and hand role at home. Leave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the performance of the three models. Between-group differences of the models were calculated based on the Mathews correlation coefficient (MCC). Results: For hand use detection, the Hand Object Detector had significantly higher performance than the other models. The macro average MCCs using this model in the LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for the less-affected hands. Hand role classification had macro average MCCs in the LOSOCV that were close to zero for all models. Conclusion: Using egocentric video to capture the hand use of stroke survivors at home is feasible. Pose estimation to track finger movements may be beneficial to classifying hand roles in the future.  
### Box-supervised Instance Segmentation with Level Set Evolution. (arXiv:2207.09055v1 [cs.CV])
- Authors : Wentong Li, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Xiansheng Hua, **Lei Zhang**
- Link : [http://arxiv.org/abs/2207.09055](http://arxiv.org/abs/2207.09055)
> ABSTRACT  :  In contrast to the fully supervised methods using pixel-wise mask labels, box-supervised instance segmentation takes advantage of the simple box annotations, which has recently attracted a lot of research attentions. In this paper, we propose a novel single-shot box-supervised instance segmentation approach, which integrates the classical level set model with deep neural network delicately. Specifically, our proposed method iteratively learns a series of level sets through a continuous Chan-Vese energy-based function in an end-to-end fashion. A simple mask supervised SOLOv2 model is adapted to predict the instance-aware mask map as the level set for each instance. Both the input image and its deep features are employed as the input data to evolve the level set curves, where a box projection function is employed to obtain the initial boundary. By minimizing the fully differentiable energy function, the level set for each instance is iteratively optimized within its corresponding bounding box annotation. The experimental results on four challenging benchmarks demonstrate the leading performance of our proposed approach to robust instance segmentation in various scenarios. The code is available at: https://github.com/LiWentomng/boxlevelset.  
### KinD-LCE Curve Estimation And Retinex Fusion On **Low-Light** Image. (arXiv:2207.09210v1 [cs.CV])
- Authors : Xiaochun Lei, Junlin Xie, Zetao Jiang, Weiliang Mai, Zhaoting Gong, Chang Lu, Linjun Lu, Ziqi Shan
- Link : [http://arxiv.org/abs/2207.09210](http://arxiv.org/abs/2207.09210)
> ABSTRACT  :  The problems of **low light** image noise and chromatic aberration is a challenging problem for tasks such as object detection, semantic segmentation, instance segmentation, etc. In this paper, we propose the algorithm for low illumination **enhancement**. KinD-LCE uses the light curve estimation module in the network structure to enhance the illumination map in the Retinex decomposed image, which improves the image brightness; we proposed the illumination map and reflection map fusion module to restore the restored image details and reduce the detail loss. Finally, we included a total variation loss function to eliminate noise. Our method uses the GladNet dataset as the training set, and the LOL dataset as the test set and is validated using Ex**Dark** as the dataset for downstream tasks. Extensive Experiments on the benchmarks demonstrate the advantages of our method and are close to the state-of-the-art results, which achieve a PSNR of 19.7216 and SSIM of 0.8213 in terms of metrics.  
### IDET: Iterative Difference-Enhanced Transformers for High-Quality Change Detection. (arXiv:2207.09240v1 [cs.CV])
- Authors : Rui Huang, Ruofei Wang, Qing Guo, Yuxiang Zhang, Wei Fan
- Link : [http://arxiv.org/abs/2207.09240](http://arxiv.org/abs/2207.09240)
> ABSTRACT  :  Change detection (CD) aims to detect change regions within an image pair captured at different times, playing a significant role for diverse real-world applications. Nevertheless, most of existing works focus on designing advanced network architectures to map the feature difference to the final change map while ignoring the influence of the quality of the feature difference. In this paper, we study the CD from a new perspective, i.e., how to optimize the feature difference to highlight changes and suppress unchanged regions, and propose a novel module denoted as iterative difference-enhanced transformers (IDET). IDET contains three transformers: two transformers for extracting the long-range information of the two images and one transformer for enhancing the feature difference. In contrast to the previous transformers, the third transformer takes the outputs of the first two transformers to guide the **enhancement** of the feature difference iteratively. To achieve more effective refinement, we further propose the multi-scale IDET-based change detection that uses multi-scale representations of the images for multiple feature difference refinements and proposes a coarse-to-fine fusion strategy to combine all refinements. Our final CD method outperforms seven state-of-the-art methods on six large-scale datasets under diverse application scenarios, which demonstrates the importance of feature difference **enhancement**s and the effectiveness of IDET.  
### Deep Semantic Statistics Matching (D2SM) Denoising Network. (arXiv:2207.09302v1 [cs.CV])
- Authors : Kangfu Mei, Rui Huang
- Link : [http://arxiv.org/abs/2207.09302](http://arxiv.org/abs/2207.09302)
> ABSTRACT  :  The ultimate aim of image **restoration** like denoising is to find an exact correlation between the noisy and clear image domains. But the optimization of end-to-end denoising learning like pixel-wise losses is performed in a sample-to-sample manner, which ignores the intrinsic correlation of images, especially semantics. In this paper, we introduce the Deep Semantic Statistics Matching (D2SM) Denoising Network. It exploits semantic features of pretrained classification networks, then it implicitly matches the probabilistic distribution of clear images at the semantic feature space. By learning to preserve the semantic distribution of denoised images, we empirically find our method significantly improves the denoising capabilities of networks, and the denoised results can be better understood by high-level vision tasks. Comprehensive experiments conducted on the noisy Cityscapes dataset demonstrate the superiority of our method on both the denoising performance and semantic segmentation accuracy. Moreover, the performance improvement observed on our extended tasks including super-resolution and dehazing experiments shows its potentiality as a new general plug-and-play component.  
### Content-aware Scalable Deep Compressed Sensing. (arXiv:2207.09313v1 [cs.CV])
- Authors : Bin Chen, Jian Zhang
- Link : [http://arxiv.org/abs/2207.09313](http://arxiv.org/abs/2207.09313)
> ABSTRACT  :  To more efficiently address image compressed sensing (CS) problems, we present a novel content-aware scalable network dubbed CASNet which collectively achieves adaptive sampling rate allocation, fine granular scalability and high-quality reconstruction. We first adopt a data-driven saliency detector to evaluate the importances of different image regions and propose a saliency-based block ratio aggregation (BRA) strategy for sampling rate allocation. A unified learnable generating matrix is then developed to produce sampling matrix of any CS ratio with an ordered structure. Being equipped with the optimization-inspired recovery subnet guided by saliency information and a multi-block training scheme preventing blocking artifacts, CASNet jointly reconstructs the image blocks sampled at various sampling rates with one single model. To accelerate training convergence and improve network robustness, we propose an SVD-based initialization scheme and a random transformation **enhancement** (RTE) strategy, which are extensible without introducing extra parameters. All the CASNet components can be combined and learned end-to-end. We further provide a four-stage implementation for evaluation and practical deployments. Experiments demonstrate that CASNet outperforms other CS networks by a large margin, validating the collaboration and mutual supports among its components and strategies. Codes are available at https://github.com/Guaishou74851/CASNet.  
### Computer Vision to the Rescue: Infant Postural Symmetry Estimation from Incongruent Annotations. (arXiv:2207.09352v1 [cs.CV])
- Authors : Xiaofei Huang, Michael Wan, Lingfei Luan, Bethany Tunik, Sarah Ostadabbas
- Link : [http://arxiv.org/abs/2207.09352](http://arxiv.org/abs/2207.09352)
> ABSTRACT  :  **Bilateral** postural symmetry plays a key role as a potential risk marker for autism spectrum disorder (ASD) and as a symptom of congenital muscular torticollis (CMT) in infants, but current methods of assessing symmetry require laborious clinical expert assessments. In this paper, we develop a computer vision based infant symmetry assessment system, leveraging 3D human pose estimation for infants. Evaluation and calibration of our system against ground truth assessments is complicated by our findings from a survey of human ratings of angle and symmetry, that such ratings exhibit low inter-rater reliability. To rectify this, we develop a Bayesian estimator of the ground truth derived from a probabilistic graphical model of fallible human raters. We show that the 3D infant pose estimation model can achieve 68% area under the receiver operating characteristic curve performance in predicting the Bayesian aggregate labels, compared to only 61% from a 2D infant pose estimation model and 60% from a 3D adult pose estimation model, highlighting the importance of 3D poses and infant domain knowledge in assessing infant body symmetry. Our survey analysis also suggests that human ratings are susceptible to higher levels of bias and inconsistency, and hence our final 3D pose-based symmetry assessment system is calibrated but not directly supervised by Bayesian aggregate human ratings, yielding higher levels of consistency and lower levels of inter-limb assessment bias.  
### On Learning the Right Attention Point for Feature **Enhancement**. (arXiv:2012.06257v3 [cs.CV] UPDATED)
- Authors : Liqiang Lin, Pengdi Huang, Wing Fu, Kai Xu, Hao Zhang, Hui Huang
- Link : [http://arxiv.org/abs/2012.06257](http://arxiv.org/abs/2012.06257)
> ABSTRACT  :  We present a novel attention-based mechanism to learn enhanced point features for point cloud processing tasks, e.g., classification and segmentation. Unlike prior works, which were trained to optimize the weights of a pre-selected set of attention points, our approach learns to locate the best attention points to maximize the performance of a specific task, e.g., point cloud classification. Importantly, we advocate the use of single attention point to facilitate semantic understanding in point feature learning. Specifically, we formulate a new and simple convolution, which combines convolutional features from an input point and its corresponding learned attention point, or LAP, for short. Our attention mechanism can be easily incorporated into state-of-the-art point cloud classification and segmentation networks. Extensive experiments on common benchmarks such as ModelNet40, ShapeNetPart, and S3DIS all demonstrate that our LAP-enabled networks consistently outperform the respective original networks, as well as other competitive alternatives, which employ multiple attention points, either pre-selected or learned under our LAP framework.  
### Physics-based Learning of Parameterized Thermodynamics from **Real-time** Thermography. (arXiv:2203.13148v2 [cs.CV] UPDATED)
- Authors : Hamza El, Yongseok Lee, Joseph Bentsman
- Link : [http://arxiv.org/abs/2203.13148](http://arxiv.org/abs/2203.13148)
> ABSTRACT  :  Progress in automatic control of thermal processes and real-time estimation of heat penetration into live tissue has long been limited by the difficulty of obtaining high-fidelity thermodynamic models. Traditionally, in complex thermodynamic systems, it is often infeasible to estimate the thermophysical parameters of spatiotemporally varying processes, forcing the adoption of model-free control architectures. This comes at the cost of losing any robustness guarantees, and implies a need for extensive real-life testing. In recent years, however, infrared cameras and other thermographic equipment have become readily applicable to these processes, allowing for a real-time, non-invasive means of sensing the thermal state of a process. In this work, we present a novel physics-based approach to learning a thermal process's dynamics directly from such real-time thermographic data, while focusing attention on regions with high thermal activity. We call this process, which applies to any higher-dimensional scalar field, attention-based noise robust averaging (ANRA). Given a partial-differential equation model structure, we show that our approach is robust against noise, and can be used to initialize optimization routines to further refine parameter estimates. We demonstrate our method on several simulation examples, as well as by applying it to electrosurgical thermal response data on in vivo porcine skin tissue.  
### Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-ahead Forward Ones. (arXiv:2204.05532v2 [cs.CV] UPDATED)
- Authors : Junyi Li, Xiaohe Wu, Zhenxin Niu, Wangmeng Zuo
- Link : [http://arxiv.org/abs/2204.05532](http://arxiv.org/abs/2204.05532)
> ABSTRACT  :  While significant progress has been made in deep video denoising, it remains very challenging for exploiting historical and future frames. Bidirectional recurrent networks (BiRNN) have exhibited appealing performance in several video **restoration** tasks. However, BiRNN is intrinsically offline because it uses backward recurrent modules to propagate from the last to current frames, which causes high latency and large memory consumption. To address the offline issue of BiRNN, we present a novel recurrent network consisting of forward and look-ahead recurrent modules for unidirectional video denoising. Particularly, look-ahead module is an elaborate forward module for leveraging information from near-future frames. When denoising the current frame, the hidden features by forward and look-ahead recurrent modules are combined, thereby making it feasible to exploit both historical and near-future frames. Due to the scene motion between non-neighboring frames, border pixels missing may occur when warping look-ahead feature from near-future frame to current frame, which can be largely alleviated by incorporating forward warping and proposed border enlargement. Experiments show that our method achieves state-of-the-art performance with constant latency and memory consumption. Code is avaliable at https://github.com/nagejacob/FloRNN.  
### Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR). (arXiv:2204.09815v2 [math.NA] UPDATED)
- Authors : Ege Ozsar, Misha Kilmer, Eric Miller, Eric de, Arvind Saibaba
- Link : [http://arxiv.org/abs/2204.09815](http://arxiv.org/abs/2204.09815)
> ABSTRACT  :  In this paper, we consider the **restoration** and reconstruction of piecewise constant objects in two and three dimensions using PaLEnTIR, a significantly enhanced Parametric level set (PaLS) model relative to the current state-of-the-art. The primary contribution of this paper is a new PaLS formulation which requires only a single level set function to recover a scene with piecewise constant objects possessing multiple unknown contrasts. Our model offers distinct advantages over current approaches to the multi-contrast, multi-object problem, all of which require multiple level sets and explicit estimation of the contrast magnitudes. Given upper and lower bounds on the contrast, our approach is able to recover objects with any distribution of contrasts and eliminates the need to know either the number of contrasts in a given scene or their values. We provide an iterative process for finding these space-varying contrast limits. Relative to most PaLS methods which employ radial basis functions (RBFs), our model makes use of non-isotropic basis functions, thereby expanding the class of shapes that a PaLS model of a given complexity can approximate. Finally, PaLEnTIR improves the conditioning of the Jacobian matrix required as part of the parameter identification process and consequently accelerates the optimization methods by controlling the magnitude of the PaLS expansion coefficients, fixing the centers of the basis functions, and the uniqueness of parametric to image mappings provided by the new parameterization. We demonstrate the performance of the new approach using both 2D and 3D variants of X-ray computed tomography, diffuse optical tomography (DOT), denoising, deconvolution problems. Application to experimental sparse CT data and simulated data with different types of noise are performed to further validate the proposed method.  
### LaTeRF: Label and Text Driven Object Radiance Fields. (arXiv:2207.01583v3 [cs.CV] UPDATED)
- Authors : Ashkan Mirzaei, Yash Kant, Jonathan Kelly, Igor Gilitschenski
- Link : [http://arxiv.org/abs/2207.01583](http://arxiv.org/abs/2207.01583)
> ABSTRACT  :  Obtaining 3D object representations is important for creating photo-realistic simulations and for collecting AR and VR assets. Neural fields have shown their effectiveness in learning a continuous volumetric representation of a scene from 2D images, but acquiring object representations from these models with weak supervision remains an open challenge. In this paper we introduce LaTeRF, a method for extracting an object of interest from a scene given 2D images of the entire scene, known camera poses, a natural language description of the object, and a set of point-labels of object and non-object points in the input images. To faithfully extract the object from the scene, LaTeRF extends the **NeRF** formulation with an additional `objectness' probability at each 3D point. Additionally, we leverage the rich latent space of a pre-trained CLIP model combined with our differentiable object renderer, to inpaint the occluded parts of the object. We demonstrate high-fidelity object extraction on both synthetic and real-world datasets and justify our design choices through an extensive ablation study.  
### RCRN: Real-world Character Image **Restoration** Network via Skeleton Extraction. (arXiv:2207.07795v2 [cs.CV] UPDATED)
- Authors : Daqian Shi, Xiaolei Diao, Hao Tang, Xiaomin Li, Hao Xing, Hao Xu
- Link : [http://arxiv.org/abs/2207.07795](http://arxiv.org/abs/2207.07795)
> ABSTRACT  :  Constructing high-quality character image datasets is challenging because real-world images are often affected by image degradation. There are limitations when applying current image **restoration** methods to such real-world character images, since (i) the categories of noise in character images are different from those in general images; (ii) real-world character images usually contain more complex image degradation, e.g., mixed noise at different noise levels. To address these problems, we propose a real-world character **restoration** network (RCRN) to effectively restore degraded character images, where character skeleton information and scale-ensemble feature extraction are utilized to obtain better **restoration** performance. The proposed method consists of a skeleton extractor (SENet) and a character image restorer (CiRNet). SENet aims to preserve the structural consistency of the character and normalize complex noise. Then, CiRNet reconstructs clean images from degraded character images and their skeletons. Due to the lack of benchmarks for real-world character image **restoration**, we constructed a dataset containing 1,606 character images with real-world degradation to evaluate the validity of the proposed method. The experimental results demonstrate that RCRN outperforms state-of-the-art methods quantitatively and qualitatively.  
### Structural Prior Guided Generative Adversarial Transformers for **Low-Light** Image **Enhancement**. (arXiv:2207.07828v2 [cs.CV] UPDATED)
- Authors : Cong Wang, Jinshan Pan, Ming Wu
- Link : [http://arxiv.org/abs/2207.07828](http://arxiv.org/abs/2207.07828)
> ABSTRACT  :  We propose an effective Structural Prior guided Generative Adversarial Transformer (SPGAT) to solve **low-light** image **enhancement**. Our SPGAT mainly contains a generator with two discriminators and a structural prior estimator (SPE). The generator is based on a U-shaped Transformer which is used to explore non-local information for better clear image **restoration**. The SPE is used to explore useful structures from images to guide the generator for better structural detail estimation. To generate more realistic images, we develop a new structural prior guided adversarial learning method by building the skip connections between the generator and discriminators so that the discriminators can better discriminate between real and fake features. Finally, we propose a parallel windows-based **Swin** Transformer block to aggregate different level hierarchical features for high-quality image **restoration**. Experimental results demonstrate that the proposed SPGAT performs favorably against recent state-of-the-art methods on both synthetic and real-world datasets.  
## eess.IV
---
### Content-aware Scalable Deep Compressed Sensing. (arXiv:2207.09313v1 [cs.CV])
- Authors : Bin Chen, Jian Zhang
- Link : [http://arxiv.org/abs/2207.09313](http://arxiv.org/abs/2207.09313)
> ABSTRACT  :  To more efficiently address image compressed sensing (CS) problems, we present a novel content-aware scalable network dubbed CASNet which collectively achieves adaptive sampling rate allocation, fine granular scalability and high-quality reconstruction. We first adopt a data-driven saliency detector to evaluate the importances of different image regions and propose a saliency-based block ratio aggregation (BRA) strategy for sampling rate allocation. A unified learnable generating matrix is then developed to produce sampling matrix of any CS ratio with an ordered structure. Being equipped with the optimization-inspired recovery subnet guided by saliency information and a multi-block training scheme preventing blocking artifacts, CASNet jointly reconstructs the image blocks sampled at various sampling rates with one single model. To accelerate training convergence and improve network robustness, we propose an SVD-based initialization scheme and a random transformation **enhancement** (RTE) strategy, which are extensible without introducing extra parameters. All the CASNet components can be combined and learned end-to-end. We further provide a four-stage implementation for evaluation and practical deployments. Experiments demonstrate that CASNet outperforms other CS networks by a large margin, validating the collaboration and mutual supports among its components and strategies. Codes are available at https://github.com/Guaishou74851/CASNet.  
### Computer Vision to the Rescue: Infant Postural Symmetry Estimation from Incongruent Annotations. (arXiv:2207.09352v1 [cs.CV])
- Authors : Xiaofei Huang, Michael Wan, Lingfei Luan, Bethany Tunik, Sarah Ostadabbas
- Link : [http://arxiv.org/abs/2207.09352](http://arxiv.org/abs/2207.09352)
> ABSTRACT  :  **Bilateral** postural symmetry plays a key role as a potential risk marker for autism spectrum disorder (ASD) and as a symptom of congenital muscular torticollis (CMT) in infants, but current methods of assessing symmetry require laborious clinical expert assessments. In this paper, we develop a computer vision based infant symmetry assessment system, leveraging 3D human pose estimation for infants. Evaluation and calibration of our system against ground truth assessments is complicated by our findings from a survey of human ratings of angle and symmetry, that such ratings exhibit low inter-rater reliability. To rectify this, we develop a Bayesian estimator of the ground truth derived from a probabilistic graphical model of fallible human raters. We show that the 3D infant pose estimation model can achieve 68% area under the receiver operating characteristic curve performance in predicting the Bayesian aggregate labels, compared to only 61% from a 2D infant pose estimation model and 60% from a 3D adult pose estimation model, highlighting the importance of 3D poses and infant domain knowledge in assessing infant body symmetry. Our survey analysis also suggests that human ratings are susceptible to higher levels of bias and inconsistency, and hence our final 3D pose-based symmetry assessment system is calibrated but not directly supervised by Bayesian aggregate human ratings, yielding higher levels of consistency and lower levels of inter-limb assessment bias.  
### Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR). (arXiv:2204.09815v2 [math.NA] UPDATED)
- Authors : Ege Ozsar, Misha Kilmer, Eric Miller, Eric de, Arvind Saibaba
- Link : [http://arxiv.org/abs/2204.09815](http://arxiv.org/abs/2204.09815)
> ABSTRACT  :  In this paper, we consider the **restoration** and reconstruction of piecewise constant objects in two and three dimensions using PaLEnTIR, a significantly enhanced Parametric level set (PaLS) model relative to the current state-of-the-art. The primary contribution of this paper is a new PaLS formulation which requires only a single level set function to recover a scene with piecewise constant objects possessing multiple unknown contrasts. Our model offers distinct advantages over current approaches to the multi-contrast, multi-object problem, all of which require multiple level sets and explicit estimation of the contrast magnitudes. Given upper and lower bounds on the contrast, our approach is able to recover objects with any distribution of contrasts and eliminates the need to know either the number of contrasts in a given scene or their values. We provide an iterative process for finding these space-varying contrast limits. Relative to most PaLS methods which employ radial basis functions (RBFs), our model makes use of non-isotropic basis functions, thereby expanding the class of shapes that a PaLS model of a given complexity can approximate. Finally, PaLEnTIR improves the conditioning of the Jacobian matrix required as part of the parameter identification process and consequently accelerates the optimization methods by controlling the magnitude of the PaLS expansion coefficients, fixing the centers of the basis functions, and the uniqueness of parametric to image mappings provided by the new parameterization. We demonstrate the performance of the new approach using both 2D and 3D variants of X-ray computed tomography, diffuse optical tomography (DOT), denoising, deconvolution problems. Application to experimental sparse CT data and simulated data with different types of noise are performed to further validate the proposed method.  
### Tilt-then-Blur or Blur-then-Tilt? Clarifying the Atmospheric Turbulence Model. (arXiv:2207.06377v2 [eess.IV] UPDATED)
- Authors : 
- Link : [http://arxiv.org/abs/2207.06377](http://arxiv.org/abs/2207.06377)
> ABSTRACT  :  Imaging at a long distance often requires advanced image **restoration** algorithms to compensate for the distortions caused by atmospheric turbulence. However, unlike many standard **restoration** problems such as deconvolution, the forward image formation model of the atmospheric turbulence does not have a simple expression. Thanks to the Zernike representation of the phase, one can show that the forward model is a combination of tilt (pixel shifting due to the linear phase terms) and blur (image smoothing due to the high order aberrations).    Confusions then arise between the ordering of the two operators. Should the model be tilt-then-blur, or blur-then-tilt? Some papers in the literature say that the model is tilt-then-blur, whereas more papers say that it is blur-then-tilt. This paper clarifies the differences between the two and discusses why the tilt-then-blur is the correct model. Recommendations are given to the research community.  
### Structural Prior Guided Generative Adversarial Transformers for **Low-Light** Image **Enhancement**. (arXiv:2207.07828v2 [cs.CV] UPDATED)
- Authors : Cong Wang, Jinshan Pan, Ming Wu
- Link : [http://arxiv.org/abs/2207.07828](http://arxiv.org/abs/2207.07828)
> ABSTRACT  :  We propose an effective Structural Prior guided Generative Adversarial Transformer (SPGAT) to solve **low-light** image **enhancement**. Our SPGAT mainly contains a generator with two discriminators and a structural prior estimator (SPE). The generator is based on a U-shaped Transformer which is used to explore non-local information for better clear image **restoration**. The SPE is used to explore useful structures from images to guide the generator for better structural detail estimation. To generate more realistic images, we develop a new structural prior guided adversarial learning method by building the skip connections between the generator and discriminators so that the discriminators can better discriminate between real and fake features. Finally, we propose a parallel windows-based **Swin** Transformer block to aggregate different level hierarchical features for high-quality image **restoration**. Experimental results demonstrate that the proposed SPGAT performs favorably against recent state-of-the-art methods on both synthetic and real-world datasets.  
## cs.LG
---
### A Convolutional Neural Network Approach to Supernova Time-Series Classification. (arXiv:2207.09440v1 [astro-ph.IM])
- Authors : Helen Qu, Masao Sako, Anais Moller, Cyrille Doux
- Link : [http://arxiv.org/abs/2207.09440](http://arxiv.org/abs/2207.09440)
> ABSTRACT  :  One of the brightest objects in the universe, supernovae (SNe) are powerful explosions marking the end of a star's lifetime. Supernova (SN) type is defined by spectroscopic emission lines, but obtaining spectroscopy is often logistically unfeasible. Thus, the ability to identify SNe by type using time-series image data alone is crucial, especially in light of the increasing breadth and depth of upcoming telescopes. We present a convolutional neural network method for fast supernova time-series classification, with observed brightness data smoothed in both the wavelength and time directions with Gaussian process regression. We apply this method to full duration and truncated SN time-series, to simulate retrospective as well as real-time classification performance. Retrospective classification is used to differentiate cosmologically useful Type Ia SNe from other SN types, and this method achieves &gt;99% accuracy on this task. We are also able to differentiate between 6 SN types with 60% accuracy given only two **night**s of data and 98% accuracy retrospectively.  
## cs.AI
---
# Paper List
---
## cs.CV
---
**148** new papers in cs.CV:-) 
1. Global-Local Stepwise Generative Network for Ultra High-Resolution Image **Restoration**. (arXiv:2207.08808v1 [cs.CV])
2. Audio Input Generates Continuous Frames to Synthesize Facial Video Using Generative Adiversarial Networks. (arXiv:2207.08813v1 [cs.SD])
3. Prior-Guided Adversarial Initialization for Fast Adversarial Training. (arXiv:2207.08859v1 [cs.CV])
4. Romanus: Robust Task Offloading in Modular Multi-Sensor Autonomous Driving Systems. (arXiv:2207.08865v1 [cs.DC])
5. FLAIR: Federated Learning Annotated Image Repository. (arXiv:2207.08869v1 [cs.LG])
6. Prior Knowledge Guided Unsupervised Domain Adaptation. (arXiv:2207.08877v1 [cs.LG])
7. A hierarchical semantic segmentation framework for computer vision-based bridge damage detection. (arXiv:2207.08878v1 [cs.CV])
8. NeuForm: Adaptive Overfitting for Neural Shape Editing. (arXiv:2207.08890v1 [cs.CV])
9. Conditional DETR V2: Efficient Detection Transformer with Box Queries. (arXiv:2207.08914v1 [cs.CV])
10. Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video. (arXiv:2207.08920v1 [cs.CV])
11. I2I: Image to Icosahedral Projection for $\mathrm{SO}(3)$ Object Reasoning from Single-View Images. (arXiv:2207.08925v1 [cs.CV])
12. Easy Batch Normalization. (arXiv:2207.08940v1 [cs.LG])
13. Robustar: Interactive Toolbox Supporting Precise Data Annotation for Robust Vision Learning. (arXiv:2207.08944v1 [cs.CV])
14. Multi-step domain adaptation by adversarial attack to $\mathcal{H} \Delta \mathcal{H}$-divergence. (arXiv:2207.08948v1 [cs.LG])
15. MonoIndoor++:Towards Better Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments. (arXiv:2207.08951v1 [cs.CV])
16. Exploiting Unlabeled Data with Vision and Language Models for Object Detection. (arXiv:2207.08954v1 [cs.CV])
17. Enhancing Space-time Video Super-resolution via Spatial-temporal Feature Interaction. (arXiv:2207.08960v1 [cs.CV])
18. Superficial White Matter Analysis: An Efficient Point-cloud-based Deep Learning Framework with Supervised Contrastive Learning for Consistent Tractography Parcellation across Populations and dMRI Acquisitions. (arXiv:2207.08975v1 [eess.IV])
19. SelectionConv: Convolutional Neural Networks for Non-rectilinear Image Data. (arXiv:2207.08979v1 [cs.CV])
20. DeformIrisNet: An Identity-Preserving Model of Iris Texture Deformation. (arXiv:2207.08980v1 [cs.CV])
21. Capabilities, Limitations and Challenges of Style Transfer with CycleGANs: A Study on Automatic Ring Design Generation. (arXiv:2207.08989v1 [cs.CV])
22. Structure from Action: Learning Interactions for Articulated Object 3D Structure Discovery. (arXiv:2207.08997v1 [cs.CV])
23. Discovering novel systemic biomarkers in photos of the external eye. (arXiv:2207.08998v1 [eess.IV])
24. SS-MFAR : Semi-supervised Multi-task Facial Affect Recognition. (arXiv:2207.09012v1 [cs.CV])
25. Structure-aware Editable Morphable Model for 3D Facial Detail Animation and Manipulation. (arXiv:2207.09019v1 [cs.CV])
26. Indoor Localization for Personalized Ambient Assisted Living of Multiple Users in Multi-Floor Smart Environments. (arXiv:2207.09025v1 [cs.AI])
27. ML-BPM: Multi-teacher Learning with Bidirectional Photometric Mixing for Open Compound Domain Adaptation in Semantic Segmentation. (arXiv:2207.09045v1 [cs.CV])
28. Dynamic Prototype Mask for Occluded Person Re-Identification. (arXiv:2207.09046v1 [cs.CV])
29. TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation. (arXiv:2207.09048v1 [cs.CV])
30. RepBNN: towards a precise Binary Neural Network with Enhanced Feature Map via Repeating. (arXiv:2207.09049v1 [cs.CV])
31. Balanced Contrastive Learning for Long-Tailed Visual Recognition. (arXiv:2207.09052v1 [cs.CV])
32. Box-supervised Instance Segmentation with Level Set Evolution. (arXiv:2207.09055v1 [cs.CV])
33. Few-shot Open-set Recognition Using Background as Unknowns. (arXiv:2207.09059v1 [cs.CV])
34. Moment Centralization based Gradient Descent Optimizers for Convolutional Neural Networks. (arXiv:2207.09066v1 [cs.CV])
35. Time Is MattEr: Temporal Self-supervision for Video Transformers. (arXiv:2207.09067v1 [cs.CV])
36. Context Unaware Knowledge Distillation for Image Retrieval. (arXiv:2207.09070v1 [cs.CV])
37. Incremental Task Learning with Incremental Rank Updates. (arXiv:2207.09074v1 [cs.CV])
38. Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks. (arXiv:2207.09083v1 [cs.RO])
39. Dual Adaptive Transformations for Weakly Supervised Point Cloud Segmentation. (arXiv:2207.09084v1 [cs.CV])
40. MHR-Net: Multiple-Hypothesis Reconstruction of Non-Rigid Shapes from 2D Views. (arXiv:2207.09086v1 [cs.CV])
41. MONet: Multi-scale Overlap Network for Duplication Detection in Biomedical Images. (arXiv:2207.09107v1 [cs.CV])
42. eCDT: Event Clustering for Simultaneous Feature Detection and Tracking-. (arXiv:2207.09108v1 [cs.CV])
43. Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios. (arXiv:2207.09120v1 [cs.CV])
44. Shrinking the Semantic Gap: Spatial Pooling of Local Moment Invariants for Copy-Move Forgery Detection. (arXiv:2207.09135v1 [cs.CV])
45. ParticleSfM: Exploiting Dense Point Trajectories for Localizing Moving Cameras in the Wild. (arXiv:2207.09137v1 [cs.CV])
46. What Matters for 3D Scene Flow Network. (arXiv:2207.09143v1 [cs.CV])
47. Learning Mutual Modulation for Self-Supervised Cross-Modal Super-Resolution. (arXiv:2207.09156v1 [cs.CV])
48. FedX: Unsupervised Federated Learning with Cross Knowledge Distillation. (arXiv:2207.09158v1 [cs.CV])
49. Single Stage Virtual Try-on via Deformable Attention Flows. (arXiv:2207.09161v1 [cs.CV])
50. Global and Local Features through Gaussian Mixture Models on Image Semantic Segmentation. (arXiv:2207.09162v1 [cs.CV])
51. A Multi-Stage Framework for the 2022 Multi-Structure Segmentation for Renal Cancer Treatment. (arXiv:2207.09165v1 [eess.IV])
52. Self-Supervision Can Be a Good Few-Shot Learner. (arXiv:2207.09176v1 [cs.CV])
53. NDF: Neural Deformable Fields for Dynamic Human Modelling. (arXiv:2207.09193v1 [cs.CV])
54. Exploring Disentangled Content Information for Face Forgery Detection. (arXiv:2207.09202v1 [cs.CV])
55. VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data. (arXiv:2207.09204v1 [cs.CV])
56. KinD-LCE Curve Estimation And Retinex Fusion On **Low-Light** Image. (arXiv:2207.09210v1 [cs.CV])
57. Image Super-Resolution with Deep Dictionary. (arXiv:2207.09228v1 [cs.CV])
58. IDET: Iterative Difference-Enhanced Transformers for High-Quality Change Detection. (arXiv:2207.09240v1 [cs.CV])
59. Don't Stop Learning: Towards Continual Learning for the CLIP Model. (arXiv:2207.09248v1 [cs.CV])
60. Action Quality Assessment with Temporal Parsing Transformer. (arXiv:2207.09270v1 [cs.CV])
61. Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain Adaptation. (arXiv:2207.09280v1 [cs.CV])
62. 3D Room Layout Estimation from a Cubemap of Panorama Image via Deep Manhattan Hough Transform. (arXiv:2207.09291v1 [cs.CV])
63. The Caltech Fish Counting Dataset: A Benchmark for Multiple-Object Tracking and Counting. (arXiv:2207.09295v1 [cs.CV])
64. Deep Semantic Statistics Matching (D2SM) Denoising Network. (arXiv:2207.09302v1 [cs.CV])
65. DH-AUG: DH Forward Kinematics Model Driven Augmentation for 3D Human Pose Estimation. (arXiv:2207.09303v1 [cs.CV])
66. Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for COVID-19 Screening With Chest Radiography. (arXiv:2207.09312v1 [eess.IV])
67. Content-aware Scalable Deep Compressed Sensing. (arXiv:2207.09313v1 [cs.CV])
68. Self-Supervised Interactive Object Segmentation Through a Singulation-and-Grasping Approach. (arXiv:2207.09314v1 [cs.RO])
69. Rethinking IoU-based Optimization for Single-stage 3D Object Detection. (arXiv:2207.09332v1 [cs.CV])
70. Uncertainty in Contrastive Learning: On the Predictability of Downstream Performance. (arXiv:2207.09336v1 [cs.LG])
71. Visual Representation Learning with Transformer: A Sequence-to-Sequence Perspective. (arXiv:2207.09339v1 [cs.CV])
72. Computer Vision to the Rescue: Infant Postural Symmetry Estimation from Incongruent Annotations. (arXiv:2207.09352v1 [cs.CV])
73. Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and Editability. (arXiv:2207.09367v1 [cs.CV])
74. Emotion Recognition based on Multi-Task Learning Framework in the ABAW4 Challenge. (arXiv:2207.09373v1 [cs.CV])
75. Image Synthesis with Disentangled Attributes for Chest X-Ray Nodule Augmentation and Detection. (arXiv:2207.09389v1 [eess.IV])
76. RCLane: Relay Chain Prediction for Lane Detection. (arXiv:2207.09399v1 [cs.CV])
77. Det6D: A Ground-Aware Full-Pose 3D Object Detector for Improving Terrain Robustness. (arXiv:2207.09412v1 [cs.CV])
78. SphereFed: Hyperspherical Federated Learning. (arXiv:2207.09413v1 [cs.LG])
79. Geometric Features Informed Multi-person Human-object Interaction Recognition in Videos. (arXiv:2207.09425v1 [cs.CV])
80. Theseus: A Library for Differentiable Nonlinear Optimization. (arXiv:2207.09442v1 [cs.RO])
81. PoserNet: Refining Relative Camera Poses Exploiting Object Detections. (arXiv:2207.09445v1 [cs.CV])
82. ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model. (arXiv:2207.09446v1 [cs.CV])
83. Human-to-Robot Imitation in the Wild. (arXiv:2207.09450v1 [cs.RO])
84. A New Perspective on Stabilizing GANs training: Direct Adversarial Training. (arXiv:2008.09041v5 [eess.IV] UPDATED)
85. On Learning the Right Attention Point for Feature **Enhancement**. (arXiv:2012.06257v3 [cs.CV] UPDATED)
86. Explainability of deep vision-based autonomous driving systems: Review and challenges. (arXiv:2101.05307v2 [cs.CV] UPDATED)
87. MOTR: End-to-End Multiple-Object Tracking with Transformer. (arXiv:2105.03247v4 [cs.CV] UPDATED)
88. When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics. (arXiv:2105.08997v2 [cs.LG] UPDATED)
89. Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v2 [cs.CV] UPDATED)
90. Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset. (arXiv:2107.04286v3 [cs.CV] UPDATED)
91. Robust outlier detection by de-biasing VAE likelihoods. (arXiv:2108.08760v3 [cs.LG] UPDATED)
92. GaitPrivacyON: Privacy-Preserving Mobile Gait Biometrics using Unsupervised Learning. (arXiv:2110.03967v2 [cs.CV] UPDATED)
93. Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization. (arXiv:2110.04538v2 [cs.CV] UPDATED)
94. Content Preserving Image Translation with Texture Co-occurrence and Spatial Self-Similarity for Texture Debiasing and Domain Adaptation. (arXiv:2110.07920v4 [cs.CV] UPDATED)
95. Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation. (arXiv:2111.08557v4 [cs.CV] UPDATED)
96. STEEX: Steering Counterfactual Explanations with Semantics. (arXiv:2111.09094v3 [cs.CV] UPDATED)
97. Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v6 [cs.CV] UPDATED)
98. Facial Depth and Normal Estimation using Single Dual-Pixel Camera. (arXiv:2111.12928v2 [cs.CV] UPDATED)
99. VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition. (arXiv:2111.13579v4 [cs.CV] UPDATED)
100. $\ell_\infty$-Robustness and Beyond: Unleashing Efficient Adversarial Training. (arXiv:2112.00378v2 [cs.LG] UPDATED)
101. Object-Centric Unsupervised Image Captioning. (arXiv:2112.00969v2 [cs.CV] UPDATED)
102. TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation. (arXiv:2112.01398v2 [cs.CV] UPDATED)
103. CA-SSL: Class-Agnostic Semi-Supervised Learning for Detection and Segmentation. (arXiv:2112.04966v2 [cs.CV] UPDATED)
104. FEAR: Fast, Efficient, Accurate and Robust Visual Tracker. (arXiv:2112.07957v2 [cs.CV] UPDATED)
105. Comprehensive Analysis of the Object Detection Pipeline on UAVs. (arXiv:2203.00306v2 [cs.RO] UPDATED)
106. Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification. (arXiv:2203.01386v2 [cs.CV] UPDATED)
107. DiT: Self-supervised Pre-training for Document Image Transformer. (arXiv:2203.02378v3 [cs.CV] UPDATED)
108. CF-ViT: A General Coarse-to-Fine Method for Vision Transformer. (arXiv:2203.03821v4 [cs.CV] UPDATED)
109. Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point Clouds for Closing Domain Gap. (arXiv:2203.03833v2 [cs.CV] UPDATED)
110. VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer. (arXiv:2203.04099v2 [cs.SD] UPDATED)
111. PETR: Position Embedding Transformation for Multi-View 3D Object Detection. (arXiv:2203.05625v3 [cs.CV] UPDATED)
112. CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving. (arXiv:2203.07724v2 [cs.CV] UPDATED)
113. REALY: Rethinking the Evaluation of 3D Face Reconstruction. (arXiv:2203.09729v2 [cs.CV] UPDATED)
114. ARM: Any-Time Super-Resolution Method. (arXiv:2203.10812v2 [cs.CV] UPDATED)
115. PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark. (arXiv:2203.11089v3 [cs.CV] UPDATED)
116. Physics-based Learning of Parameterized Thermodynamics from **Real-time** Thermography. (arXiv:2203.13148v2 [cs.CV] UPDATED)
117. Primitive-based Shape Abstraction via Nonparametric Bayesian Inference. (arXiv:2203.14714v2 [cs.CV] UPDATED)
118. Quantized GAN for Complex Music Generation from Dance Videos. (arXiv:2204.00604v2 [cs.CV] UPDATED)
119. Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation. (arXiv:2204.02548v2 [cs.CV] UPDATED)
120. Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-ahead Forward Ones. (arXiv:2204.05532v2 [cs.CV] UPDATED)
121. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v3 [cs.CL] UPDATED)
122. GIMO: Gaze-Informed Human Motion Prediction in Context. (arXiv:2204.09443v2 [cs.CV] UPDATED)
123. Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR). (arXiv:2204.09815v2 [math.NA] UPDATED)
124. MMRotate: A Rotated Object Detection Benchmark using PyTorch. (arXiv:2204.13317v4 [cs.CV] UPDATED)
125. An Overview of Color Transfer and Style Transfer for Images and Videos. (arXiv:2204.13339v3 [cs.CV] UPDATED)
126. Reliable Label Correction is a Good Booster When Learning with Extremely Noisy Labels. (arXiv:2205.00186v2 [cs.CV] UPDATED)
127. 3D Object Detection with a Self-supervised Lidar Scene Flow Backbone. (arXiv:2205.00705v2 [cs.CV] UPDATED)
128. Causal Balancing for Domain Generalization. (arXiv:2206.05263v2 [cs.LG] UPDATED)
129. Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v3 [cs.CV] UPDATED)
130. End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology. (arXiv:2207.00095v2 [cs.CV] UPDATED)
131. LaTeRF: Label and Text Driven Object Radiance Fields. (arXiv:2207.01583v3 [cs.CV] UPDATED)
132. Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling. (arXiv:2207.02196v3 [cs.CV] UPDATED)
133. Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection. (arXiv:2207.02541v2 [cs.CV] UPDATED)
134. CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer. (arXiv:2207.04808v4 [cs.CV] UPDATED)
135. Compound Prototype Matching for Few-shot Action Recognition. (arXiv:2207.05515v3 [cs.CV] UPDATED)
136. Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras. (arXiv:2207.06058v2 [cs.CV] UPDATED)
137. Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain. (arXiv:2207.07316v3 [cs.CV] UPDATED)
138. Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection. (arXiv:2207.07783v2 [cs.CV] UPDATED)
139. RCRN: Real-world Character Image **Restoration** Network via Skeleton Extraction. (arXiv:2207.07795v2 [cs.CV] UPDATED)
140. CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising. (arXiv:2207.07798v2 [cs.CV] UPDATED)
141. Structural Prior Guided Generative Adversarial Transformers for **Low-Light** Image **Enhancement**. (arXiv:2207.07828v2 [cs.CV] UPDATED)
142. Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches. (arXiv:2207.08220v2 [cs.CV] UPDATED)
143. Show Me What I Like: Detecting User-Specific Video Highlights Using Content-Based Multi-Head Attention. (arXiv:2207.08352v2 [cs.CV] UPDATED)
144. Open-world Semantic Segmentation via Contrasting and Clustering Vision-Language Embedding. (arXiv:2207.08455v2 [cs.CV] UPDATED)
145. Segmenting white matter hyperintensities on isotropic three-dimensional Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison of Deep learning tools on a Norwegian national imaging database. (arXiv:2207.08467v2 [eess.IV] UPDATED)
146. Hierarchical Feature Alignment Network for Unsupervised Video Object Segmentation. (arXiv:2207.08485v2 [cs.CV] UPDATED)
147. Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v2 [cs.LG] UPDATED)
148. FakeCLR: Exploring Contrastive Learning for Solving Latent Discontinuity in Data-Efficient GANs. (arXiv:2207.08630v2 [cs.CV] UPDATED)
## eess.IV
---
**20** new papers in eess.IV:-) 
1. Superficial White Matter Analysis: An Efficient Point-cloud-based Deep Learning Framework with Supervised Contrastive Learning for Consistent Tractography Parcellation across Populations and dMRI Acquisitions. (arXiv:2207.08975v1 [eess.IV])
2. Capabilities, Limitations and Challenges of Style Transfer with CycleGANs: A Study on Automatic Ring Design Generation. (arXiv:2207.08989v1 [cs.CV])
3. Discovering novel systemic biomarkers in photos of the external eye. (arXiv:2207.08998v1 [eess.IV])
4. A Multi-Stage Framework for the 2022 Multi-Structure Segmentation for Renal Cancer Treatment. (arXiv:2207.09165v1 [eess.IV])
5. VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data. (arXiv:2207.09204v1 [cs.CV])
6. Image Super-Resolution with Deep Dictionary. (arXiv:2207.09228v1 [cs.CV])
7. Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for COVID-19 Screening With Chest Radiography. (arXiv:2207.09312v1 [eess.IV])
8. Content-aware Scalable Deep Compressed Sensing. (arXiv:2207.09313v1 [cs.CV])
9. Uncertainty in Contrastive Learning: On the Predictability of Downstream Performance. (arXiv:2207.09336v1 [cs.LG])
10. Computer Vision to the Rescue: Infant Postural Symmetry Estimation from Incongruent Annotations. (arXiv:2207.09352v1 [cs.CV])
11. Image Synthesis with Disentangled Attributes for Chest X-Ray Nodule Augmentation and Detection. (arXiv:2207.09389v1 [eess.IV])
12. A New Perspective on Stabilizing GANs training: Direct Adversarial Training. (arXiv:2008.09041v5 [eess.IV] UPDATED)
13. Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR). (arXiv:2204.09815v2 [math.NA] UPDATED)
14. End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology. (arXiv:2207.00095v2 [cs.CV] UPDATED)
15. Tilt-then-Blur or Blur-then-Tilt? Clarifying the Atmospheric Turbulence Model. (arXiv:2207.06377v2 [eess.IV] UPDATED)
16. Structural Prior Guided Generative Adversarial Transformers for **Low-Light** Image **Enhancement**. (arXiv:2207.07828v2 [cs.CV] UPDATED)
17. Learnable Mixed-precision and Dimension Reduction Co-design for Low-storage Activation. (arXiv:2207.07931v2 [eess.IV] UPDATED)
18. Accelerating Magnetic Resonance Parametric Mapping Using Simultaneously Spatial Patch-based and Parametric Group-based Low-rank Tensors (SMART). (arXiv:2207.08117v2 [eess.IV] UPDATED)
19. Segmenting white matter hyperintensities on isotropic three-dimensional Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison of Deep learning tools on a Norwegian national imaging database. (arXiv:2207.08467v2 [eess.IV] UPDATED)
20. Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v2 [cs.LG] UPDATED)
## cs.LG
---
**162** new papers in cs.LG:-) 
1. Unified 2D and 3D Pre-Training of Molecular Representations. (arXiv:2207.08806v1 [cs.LG])
2. A Study of Deep CNN Model with Labeling Noise Based on Granular-ball Computing. (arXiv:2207.08810v1 [cs.LG])
3. Fusion of Physiological and Behavioural Signals on SPD Manifolds with Application to Stress and Pain Detection. (arXiv:2207.08811v1 [cs.LG])
4. Audio Input Generates Continuous Frames to Synthesize Facial Video Using Generative Adiversarial Networks. (arXiv:2207.08813v1 [cs.SD])
5. Why do tree-based models still outperform deep learning on tabular data?. (arXiv:2207.08815v1 [cs.LG])
6. Discovering Behavioral Predispositions in Data to Improve Human Activity Recognition. (arXiv:2207.08816v1 [cs.LG])
7. Research Trends and Applications of Data Augmentation Algorithms. (arXiv:2207.08817v1 [cs.LG])
8. SeLoC-ML: Semantic Low-Code Engineering for Machine Learning Applications in Industrial IoT. (arXiv:2207.08818v1 [cs.SE])
9. Accelerating Deep Learning Model Inference on Arm CPUs with Ultra-Low Bit Quantization and Runtime. (arXiv:2207.08820v1 [cs.LG])
10. The Multiple Subnetwork Hypothesis: Enabling Multidomain Learning by Isolating Task-Specific Subnetworks in Feedforward Neural Networks. (arXiv:2207.08821v1 [cs.LG])
11. Is Integer Arithmetic Enough for Deep Learning Training?. (arXiv:2207.08822v1 [cs.LG])
12. Using attention methods to predict judicial outcomes. (arXiv:2207.08823v1 [cs.LG])
13. 3D Equivariant Molecular Graph Pretraining. (arXiv:2207.08824v1 [q-bio.QM])
14. Contrastive Environmental Sound Representation Learning. (arXiv:2207.08825v1 [cs.SD])
15. Romanus: Robust Task Offloading in Modular Multi-Sensor Autonomous Driving Systems. (arXiv:2207.08865v1 [cs.DC])
16. MCTensor: A High-Precision Deep Learning Library with Multi-Component Floating-Point. (arXiv:2207.08867v1 [cs.LG])
17. FLAIR: Federated Learning Annotated Image Repository. (arXiv:2207.08869v1 [cs.LG])
18. Consistent Polyhedral Surrogates for Top-$k$ Classification and Variants. (arXiv:2207.08873v1 [cs.LG])
19. Prior Knowledge Guided Unsupervised Domain Adaptation. (arXiv:2207.08877v1 [cs.LG])
20. Deep Sequence Models for Text Classification Tasks. (arXiv:2207.08880v1 [cs.CL])
21. NeuForm: Adaptive Overfitting for Neural Shape Editing. (arXiv:2207.08890v1 [cs.CV])
22. Learning multi-robot coordination from demonstrations. (arXiv:2207.08892v1 [cs.RO])
23. A Deep Reinforcement Learning Approach for Finding Non-Exploitable Strategies in Two-Player Atari Games. (arXiv:2207.08894v1 [cs.LG])
24. On the Study of Sample Complexity for Polynomial Neural Networks. (arXiv:2207.08896v1 [cs.LG])
25. Benchmarking Machine Learning Robustness in Covid-19 Genome Sequence Classification. (arXiv:2207.08898v1 [q-bio.GN])
26. Deeply-Learned Generalized Linear Models with Missing Data. (arXiv:2207.08911v1 [stat.ML])
27. I2I: Image to Icosahedral Projection for $\mathrm{SO}(3)$ Object Reasoning from Single-View Images. (arXiv:2207.08925v1 [cs.CV])
28. Towards Learning Self-Organized Criticality of Rydberg Atoms using Graph Neural Networks. (arXiv:2207.08927v1 [physics.atom-ph])
29. Learning Sparsity-Promoting Regularizers using Bilevel Optimization. (arXiv:2207.08939v1 [cs.LG])
30. Easy Batch Normalization. (arXiv:2207.08940v1 [cs.LG])
31. Implicit Regularization with Polynomial Growth in Deep Tensor Factorization. (arXiv:2207.08942v1 [cs.LG])
32. MRCLens: an MRC Dataset Bias Detection Toolkit. (arXiv:2207.08943v1 [cs.CL])
33. Robustar: Interactive Toolbox Supporting Precise Data Annotation for Robust Vision Learning. (arXiv:2207.08944v1 [cs.CV])
34. Gauge-equivariant flow models for sampling in lattice field theories with pseudofermions. (arXiv:2207.08945v1 [hep-lat])
35. Multi-step domain adaptation by adversarial attack to $\mathcal{H} \Delta \mathcal{H}$-divergence. (arXiv:2207.08948v1 [cs.LG])
36. Adversarial Training Improves Joint Energy-Based Generative Modelling. (arXiv:2207.08950v1 [cs.LG])
37. Residual and Attentional Architectures for Vector-Symbols. (arXiv:2207.08953v1 [cs.LG])
38. Online Learning with Off-Policy Feedback. (arXiv:2207.08956v1 [cs.LG])
39. The m-connecting imset and factorization for ADMG models. (arXiv:2207.08963v1 [stat.ML])
40. Superficial White Matter Analysis: An Efficient Point-cloud-based Deep Learning Framework with Supervised Contrastive Learning for Consistent Tractography Parcellation across Populations and dMRI Acquisitions. (arXiv:2207.08975v1 [eess.IV])
41. Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift. (arXiv:2207.08977v1 [cs.LG])
42. Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices. (arXiv:2207.08988v1 [cs.LG])
43. Capabilities, Limitations and Challenges of Style Transfer with CycleGANs: A Study on Automatic Ring Design Generation. (arXiv:2207.08989v1 [cs.CV])
44. Machine Learning in Orbit Estimation: a Survey. (arXiv:2207.08993v1 [astro-ph.EP])
45. Discovering novel systemic biomarkers in photos of the external eye. (arXiv:2207.08998v1 [eess.IV])
46. Indoor Localization for Personalized Ambient Assisted Living of Multiple Users in Multi-Floor Smart Environments. (arXiv:2207.09025v1 [cs.AI])
47. Decorrelative Network Architecture for Robust Electrocardiogram Classification. (arXiv:2207.09031v1 [cs.LG])
48. RepBNN: towards a precise Binary Neural Network with Enhanced Feature Map via Repeating. (arXiv:2207.09049v1 [cs.CV])
49. Don't Forget to Buy Milk: Contextually Aware Grocery Reminder Household Robot. (arXiv:2207.09050v1 [cs.RO])
50. Balanced Contrastive Learning for Long-Tailed Visual Recognition. (arXiv:2207.09052v1 [cs.CV])
51. Data Science and Machine Learning in Education. (arXiv:2207.09060v1 [physics.ed-ph])
52. A-SFS: Semi-supervised Feature Selection based on Multi-task Self-supervision. (arXiv:2207.09061v1 [cs.LG])
53. Time Is MattEr: Temporal Self-supervision for Video Transformers. (arXiv:2207.09067v1 [cs.CV])
54. Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks. (arXiv:2207.09071v1 [cs.LG])
55. Incremental Task Learning with Incremental Rank Updates. (arXiv:2207.09074v1 [cs.CV])
56. Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v1 [cs.LG])
57. Can You Fool AI by Doing a 180? $\unicode{x2013}$ A Case Study on Authorship Analysis of Texts by Arata Osada. (arXiv:2207.09085v1 [cs.CL])
58. Is Vertical Logistic Regression Privacy-Preserving? A Comprehensive Privacy Analysis and Beyond. (arXiv:2207.09087v1 [cs.CR])
59. XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection and Forensics. (arXiv:2207.09088v1 [cs.CR])
60. Actor-Critic based Improper Reinforcement Learning. (arXiv:2207.09090v1 [cs.LG])
61. MoEC: Mixture of Expert Clusters. (arXiv:2207.09094v1 [cs.CL])
62. Lazy Estimation of Variable Importance for Large Neural Networks. (arXiv:2207.09097v1 [stat.ML])
63. Analyzing Bagging Methods for Language Models. (arXiv:2207.09099v1 [cs.CL])
64. Identity Testing for High-Dimensional Distributions via Entropy Tensorization. (arXiv:2207.09102v1 [cs.DS])
65. Active-Learning-as-a-Service: An Efficient MLOps System for Data-Centric AI. (arXiv:2207.09109v1 [cs.LG])
66. Heterogeneous Treatment Effect with Trained Kernels of the Nadaraya-Watson Regression. (arXiv:2207.09139v1 [cs.LG])
67. Using Neural Networks by Modelling Semi-Active Shock Absorber. (arXiv:2207.09141v1 [eess.SY])
68. On the development of a Bayesian optimisation framework for complex unknown systems. (arXiv:2207.09154v1 [cs.LG])
69. FedX: Unsupervised Federated Learning with Cross Knowledge Distillation. (arXiv:2207.09158v1 [cs.CV])
70. SCARA: Scalable Graph Neural Networks with Feature-Oriented Optimization. (arXiv:2207.09179v1 [cs.LG])
71. Multi-view hierarchical Variational AutoEncoders with Factor Analysis latent space. (arXiv:2207.09185v1 [cs.LG])
72. Adaptive Learning for the Resource-Constrained Classification Problem. (arXiv:2207.09196v1 [cs.LG])
73. VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data. (arXiv:2207.09204v1 [cs.CV])
74. Similarity of Pre-trained and Fine-tuned Representations. (arXiv:2207.09225v1 [cs.LG])
75. Over-the-Air Federated Edge Learning with Hierarchical Clustering. (arXiv:2207.09232v1 [cs.LG])
76. Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification. (arXiv:2207.09237v1 [cs.LG])
77. Formal Algorithms for Transformers. (arXiv:2207.09238v1 [cs.LG])
78. Assaying Out-Of-Distribution Generalization in Transfer Learning. (arXiv:2207.09239v1 [cs.LG])
79. Abstract Demonstrations and Adaptive Exploration for Efficient and Stable Multi-step Sparse Reward Reinforcement Learning. (arXiv:2207.09243v1 [cs.RO])
80. EVE: Environmental Adaptive Neural Network Models for Low-power Energy Harvesting System. (arXiv:2207.09258v1 [cs.LG])
81. Quantum Feature Extraction for THz Multi-Layer Imaging. (arXiv:2207.09285v1 [quant-ph])
82. The Caltech Fish Counting Dataset: A Benchmark for Multiple-Object Tracking and Counting. (arXiv:2207.09295v1 [cs.CV])
83. A sharp uniform-in-time error estimate for Stochastic Gradient Langevin Dynamics. (arXiv:2207.09304v1 [math.PR])
84. Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for COVID-19 Screening With Chest Radiography. (arXiv:2207.09312v1 [eess.IV])
85. Metadata Representations for Queryable ML Model Zoos. (arXiv:2207.09315v1 [cs.LG])
86. Signed Network Embedding with Application to Simultaneous Detection of Communities and Anomalies. (arXiv:2207.09324v1 [cs.SI])
87. Multi-parametric Analysis for Mixed Integer Linear Programming: An Application to Transmission Planning and Congestion Control. (arXiv:2207.09325v1 [math.OC])
88. Uncertainty in Contrastive Learning: On the Predictability of Downstream Performance. (arXiv:2207.09336v1 [cs.LG])
89. A coherence parameter characterizing generative compressed sensing with Fourier measurements. (arXiv:2207.09340v1 [cs.IT])
90. Online Dynamics Learning for Predictive Control with an Application to Aerial Robots. (arXiv:2207.09344v1 [cs.RO])
91. Riemannian Stochastic Gradient Method for Nested Composition Optimization. (arXiv:2207.09350v1 [math.OC])
92. Beyond Transmitting Bits: Context, Semantics, and Task-Oriented Communications. (arXiv:2207.09353v1 [cs.IT])
93. Data-Centric Epidemic Forecasting: A Survey. (arXiv:2207.09370v1 [cs.LG])
94. Green, Quantized Federated Learning over Wireless Networks: An Energy-Efficient Design. (arXiv:2207.09387v1 [cs.LG])
95. Neural Greedy Pursuit for Feature Selection. (arXiv:2207.09390v1 [cs.LG])
96. Bayesian Generational Population-Based Training. (arXiv:2207.09405v1 [cs.LG])
97. Bounding generalization error with input compression: An empirical study with infinite-width networks. (arXiv:2207.09408v1 [cs.LG])
98. SphereFed: Hyperspherical Federated Learning. (arXiv:2207.09413v1 [cs.LG])
99. Unrolled algorithms for group synchronization. (arXiv:2207.09418v1 [eess.SP])
100. Deep equilibrium networks are sensitive to initialization statistics. (arXiv:2207.09432v1 [cs.LG])
101. Regret Minimization with Noisy Observations. (arXiv:2207.09435v1 [cs.DS])
102. A Convolutional Neural Network Approach to Supernova Time-Series Classification. (arXiv:2207.09440v1 [astro-ph.IM])
103. Theseus: A Library for Differentiable Nonlinear Optimization. (arXiv:2207.09442v1 [cs.RO])
104. Human-to-Robot Imitation in the Wild. (arXiv:2207.09450v1 [cs.RO])
105. The Implicit Bias of Gradient Descent on Separable Data. (arXiv:1710.10345v5 [stat.ML] UPDATED)
106. Adversarial Bandits with Knapsacks. (arXiv:1811.11881v8 [cs.DS] UPDATED)
107. A Unifying Causal Framework for Analyzing Dataset Shift-stable Learning Algorithms. (arXiv:1905.11374v5 [stat.ML] UPDATED)
108. Deep learning generates custom-made logistic regression models for explaining how breast cancer subtypes are classified. (arXiv:2001.06988v2 [cs.LG] UPDATED)
109. A Unified Single-loop Alternating Gradient Projection Algorithm for Nonconvex-Concave and Convex-Nonconcave Minimax Problems. (arXiv:2006.02032v3 [math.OC] UPDATED)
110. A New Perspective on Stabilizing GANs training: Direct Adversarial Training. (arXiv:2008.09041v5 [eess.IV] UPDATED)
111. Implicit Gradient Regularization. (arXiv:2009.11162v3 [cs.LG] UPDATED)
112. Explainability of deep vision-based autonomous driving systems: Review and challenges. (arXiv:2101.05307v2 [cs.CV] UPDATED)
113. How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation. (arXiv:2102.02805v4 [cs.LG] UPDATED)
114. Unsupervised Ground Metric Learning using Wasserstein Singular Vectors. (arXiv:2102.06278v3 [stat.ML] UPDATED)
115. SafeDrug: Dual Molecular Graph Encoders for Recommending Effective and Safe Drug Combinations. (arXiv:2105.02711v2 [cs.LG] UPDATED)
116. When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics. (arXiv:2105.08997v2 [cs.LG] UPDATED)
117. A Hybrid Recommender System for Recommending Smartphones to Prospective Customers. (arXiv:2105.12876v2 [cs.IR] UPDATED)
118. Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v2 [cs.CV] UPDATED)
119. Robust outlier detection by de-biasing VAE likelihoods. (arXiv:2108.08760v3 [cs.LG] UPDATED)
120. Quantum Algorithms and Lower Bounds for Linear Regression with Norm Constraints. (arXiv:2110.13086v2 [quant-ph] UPDATED)
121. A label-efficient two-sample test. (arXiv:2111.08861v5 [cs.LG] UPDATED)
122. $\ell_\infty$-Robustness and Beyond: Unleashing Efficient Adversarial Training. (arXiv:2112.00378v2 [cs.LG] UPDATED)
123. Sufficient Statistic Memory AMP. (arXiv:2112.15327v3 [cs.IT] UPDATED)
124. GNNRank: Learning Global Rankings from Pairwise Comparisons via Directed Graph Neural Networks. (arXiv:2202.00211v3 [cs.LG] UPDATED)
125. Robust Training of Neural Networks Using Scale Invariant Architectures. (arXiv:2202.00980v2 [cs.LG] UPDATED)
126. Generalization Bounds via Convex Analysis. (arXiv:2202.04985v3 [stat.ML] UPDATED)
127. Flexible learning of quantum states with generative query neural networks. (arXiv:2202.06804v2 [quant-ph] UPDATED)
128. A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation. (arXiv:2202.08146v3 [cs.LG] UPDATED)
129. Algorithm and System Co-design for Efficient Subgraph-based Graph Representation Learning. (arXiv:2202.13538v2 [cs.LG] UPDATED)
130. GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation. (arXiv:2203.00949v2 [cs.LG] UPDATED)
131. Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion. (arXiv:2203.03279v3 [cs.LG] UPDATED)
132. VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer. (arXiv:2203.04099v2 [cs.SD] UPDATED)
133. Survival Prediction of Brain Cancer with Incomplete Radiology, Pathology, Genomics, and Demographic Data. (arXiv:2203.04419v2 [cs.LG] UPDATED)
134. PACTran: PAC-Bayesian Metrics for Estimating the Transferability of Pretrained Models to Classification Tasks. (arXiv:2203.05126v2 [cs.LG] UPDATED)
135. CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving. (arXiv:2203.07724v2 [cs.CV] UPDATED)
136. A Comparative Survey of Deep Active Learning. (arXiv:2203.13450v3 [cs.LG] UPDATED)
137. Revealing the CO2 emission reduction of ridesplitting and its determinants based on real-world data. (arXiv:2204.00777v2 [cs.LG] UPDATED)
138. FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations. (arXiv:2204.06508v2 [cs.CL] UPDATED)
139. Harnessing Interpretable Machine Learning for Holistic Inverse Design of Origami. (arXiv:2204.07235v2 [cond-mat.soft] UPDATED)
140. Uncertainty Minimization for Personalized Federated Semi-Supervised Learning. (arXiv:2205.02438v2 [cs.LG] UPDATED)
141. Utterance Weighted Multi-Dilation Temporal Convolutional Networks for Monaural Speech Dereverberation. (arXiv:2205.08455v2 [cs.SD] UPDATED)
142. A Classification of $G$-invariant Shallow Neural Networks. (arXiv:2205.09219v3 [cs.LG] UPDATED)
143. Federated Learning Aggregation: New Robust Algorithms with Guarantees. (arXiv:2205.10864v2 [stat.ML] UPDATED)
144. Finite-Sample Maximum Likelihood Estimation of Location. (arXiv:2206.02348v2 [math.ST] UPDATED)
145. Causal Balancing for Domain Generalization. (arXiv:2206.05263v2 [cs.LG] UPDATED)
146. Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v3 [cs.CV] UPDATED)
147. CEN : Cooperatively Evolving Networks. (arXiv:2207.02192v2 [cs.LG] UPDATED)
148. Not All Models Are Equal: Predicting Model Transferability in a Self-challenging Fisher Space. (arXiv:2207.03036v2 [cs.LG] UPDATED)
149. Safe reinforcement learning for multi-energy management systems with known constraint functions. (arXiv:2207.03830v3 [eess.SY] UPDATED)
150. Long-term Reproducibility for Neural Architecture Search. (arXiv:2207.04821v2 [cs.LG] UPDATED)
151. Non-Myopic Multifidelity Bayesian Optimization. (arXiv:2207.06325v2 [cs.LG] UPDATED)
152. Interference-Limited Ultra-Reliable and Low-Latency Communications: Graph Neural Networks or Stochastic Geometry?. (arXiv:2207.06918v2 [eess.SP] UPDATED)
153. Learning inducing points and uncertainty on molecular data. (arXiv:2207.07654v2 [physics.chem-ph] UPDATED)
154. Do Not Sleep on Linear Models: Simple and Interpretable Techniques Outperform Deep Learning for Sleep Scoring. (arXiv:2207.07753v2 [stat.ML] UPDATED)
155. Learnable Mixed-precision and Dimension Reduction Co-design for Low-storage Activation. (arXiv:2207.07931v2 [eess.IV] UPDATED)
156. Minimum Description Length Control. (arXiv:2207.08258v2 [cs.LG] UPDATED)
157. MLGOPerf: An ML Guided Inliner to Optimize Performance. (arXiv:2207.08389v2 [cs.PL] UPDATED)
158. GATE: Gated Additive Tree Ensemble for Tabular Classification and Regression. (arXiv:2207.08548v2 [cs.LG] UPDATED)
159. Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v2 [cs.LG] UPDATED)
160. Comprehensive Graph Gradual Pruning for Sparse Training in Graph Neural Networks. (arXiv:2207.08629v2 [cs.LG] UPDATED)
161. FakeCLR: Exploring Contrastive Learning for Solving Latent Discontinuity in Data-Efficient GANs. (arXiv:2207.08630v2 [cs.CV] UPDATED)
162. Lightweight Automated Feature Monitoring for Data Streams. (arXiv:2207.08640v2 [cs.LG] UPDATED)
## cs.AI
---
**89** new papers in cs.AI:-) 
1. Unified 2D and 3D Pre-Training of Molecular Representations. (arXiv:2207.08806v1 [cs.LG])
2. A Study of Deep CNN Model with Labeling Noise Based on Granular-ball Computing. (arXiv:2207.08810v1 [cs.LG])
3. Fusion of Physiological and Behavioural Signals on SPD Manifolds with Application to Stress and Pain Detection. (arXiv:2207.08811v1 [cs.LG])
4. PBRE: A Rule Extraction Method from Trained Neural Networks Designed for Smart Home Services. (arXiv:2207.08814v1 [cs.AI])
5. Why do tree-based models still outperform deep learning on tabular data?. (arXiv:2207.08815v1 [cs.LG])
6. Discovering Behavioral Predispositions in Data to Improve Human Activity Recognition. (arXiv:2207.08816v1 [cs.LG])
7. SeLoC-ML: Semantic Low-Code Engineering for Machine Learning Applications in Industrial IoT. (arXiv:2207.08818v1 [cs.SE])
8. Accelerating Deep Learning Model Inference on Arm CPUs with Ultra-Low Bit Quantization and Runtime. (arXiv:2207.08820v1 [cs.LG])
9. The Multiple Subnetwork Hypothesis: Enabling Multidomain Learning by Isolating Task-Specific Subnetworks in Feedforward Neural Networks. (arXiv:2207.08821v1 [cs.LG])
10. Using attention methods to predict judicial outcomes. (arXiv:2207.08823v1 [cs.LG])
11. Contrastive Environmental Sound Representation Learning. (arXiv:2207.08825v1 [cs.SD])
12. RESAM: Requirements Elicitation and Specification for Deep-Learning Anomaly Models with Applications to UAV Flight Controllers. (arXiv:2207.08857v1 [cs.SE])
13. Optimizing Indoor Navigation Policies For Spatial Distancing. (arXiv:2207.08860v1 [cs.MA])
14. Deep Sequence Models for Text Classification Tasks. (arXiv:2207.08880v1 [cs.CL])
15. A Deep Reinforcement Learning Approach for Finding Non-Exploitable Strategies in Two-Player Atari Games. (arXiv:2207.08894v1 [cs.LG])
16. A Community-Aware Framework for Social Influence Maximization. (arXiv:2207.08937v1 [cs.SI])
17. Implicit Regularization with Polynomial Growth in Deep Tensor Factorization. (arXiv:2207.08942v1 [cs.LG])
18. Selection Bias Induced Spurious Correlations in Large Language Models. (arXiv:2207.08982v1 [cs.CL])
19. Indoor Localization for Personalized Ambient Assisted Living of Multiple Users in Multi-Floor Smart Environments. (arXiv:2207.09025v1 [cs.AI])
20. Decorrelative Network Architecture for Robust Electrocardiogram Classification. (arXiv:2207.09031v1 [cs.LG])
21. RepBNN: towards a precise Binary Neural Network with Enhanced Feature Map via Repeating. (arXiv:2207.09049v1 [cs.CV])
22. Don't Forget to Buy Milk: Contextually Aware Grocery Reminder Household Robot. (arXiv:2207.09050v1 [cs.RO])
23. HICF: Hyperbolic Informative Collaborative Filtering. (arXiv:2207.09051v1 [cs.IR])
24. An Intelligent Trust Cloud Management Method for Secure Clustering in 5G enabled Internet of Medical Things. (arXiv:2207.09057v1 [cs.NI])
25. A-SFS: Semi-supervised Feature Selection based on Multi-task Self-supervision. (arXiv:2207.09061v1 [cs.LG])
26. Time Is MattEr: Temporal Self-supervision for Video Transformers. (arXiv:2207.09067v1 [cs.CV])
27. PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search. (arXiv:2207.09068v1 [cs.CL])
28. Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks. (arXiv:2207.09071v1 [cs.LG])
29. ILASR: Privacy-Preserving Incremental Learning for AutomaticSpeech Recognition at Production Scale. (arXiv:2207.09078v1 [cs.CL])
30. Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v1 [cs.LG])
31. Can You Fool AI by Doing a 180? $\unicode{x2013}$ A Case Study on Authorship Analysis of Texts by Arata Osada. (arXiv:2207.09085v1 [cs.CL])
32. Actor-Critic based Improper Reinforcement Learning. (arXiv:2207.09090v1 [cs.LG])
33. Explainable Human-in-the-loop Dynamic Data-Driven Digital Twins. (arXiv:2207.09106v1 [cs.AI])
34. MONet: Multi-scale Overlap Network for Duplication Detection in Biomedical Images. (arXiv:2207.09107v1 [cs.CV])
35. ParticleSfM: Exploiting Dense Point Trajectories for Localizing Moving Cameras in the Wild. (arXiv:2207.09137v1 [cs.CV])
36. GAFX: A General Audio Feature eXtractor. (arXiv:2207.09145v1 [eess.AS])
37. On the Usability of Transformers-based models for a French Question-Answering task. (arXiv:2207.09150v1 [cs.CL])
38. Benchmarking Transformers-based models on French Spoken Language Understanding tasks. (arXiv:2207.09152v1 [cs.CL])
39. Multi-view hierarchical Variational AutoEncoders with Factor Analysis latent space. (arXiv:2207.09185v1 [cs.LG])
40. NDF: Neural Deformable Fields for Dynamic Human Modelling. (arXiv:2207.09193v1 [cs.CV])
41. FLDetector: Detecting Malicious Clients in Model Poisoning Attacks to Federated Learning. (arXiv:2207.09209v1 [cs.CR])
42. Similarity of Pre-trained and Fine-tuned Representations. (arXiv:2207.09225v1 [cs.LG])
43. Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification. (arXiv:2207.09237v1 [cs.LG])
44. Formal Algorithms for Transformers. (arXiv:2207.09238v1 [cs.LG])
45. Abstract Demonstrations and Adaptive Exploration for Efficient and Stable Multi-step Sparse Reward Reinforcement Learning. (arXiv:2207.09243v1 [cs.RO])
46. Magpie: Automatically Tuning Static Parameters for Distributed File Systems using Deep Reinforcement Learning. (arXiv:2207.09298v1 [cs.DC])
47. Few-Shot Teamwork. (arXiv:2207.09300v1 [cs.MA])
48. Uncertainty in Contrastive Learning: On the Predictability of Downstream Performance. (arXiv:2207.09336v1 [cs.LG])
49. Beyond Transmitting Bits: Context, Semantics, and Task-Oriented Communications. (arXiv:2207.09353v1 [cs.IT])
50. On Decentralizing Federated Reinforcement Learning in Multi-Robot Scenarios. (arXiv:2207.09372v1 [cs.RO])
51. Alterfactual Explanations -- The Relevance of Irrelevance for Explaining AI Systems. (arXiv:2207.09374v1 [cs.AI])
52. Mimetic Models: Ethical Implications of AI that Acts Like You. (arXiv:2207.09394v1 [cs.AI])
53. Bayesian Generational Population-Based Training. (arXiv:2207.09405v1 [cs.LG])
54. Bounding generalization error with input compression: An empirical study with infinite-width networks. (arXiv:2207.09408v1 [cs.LG])
55. SphereFed: Hyperspherical Federated Learning. (arXiv:2207.09413v1 [cs.LG])
56. ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model. (arXiv:2207.09446v1 [cs.CV])
57. Human-to-Robot Imitation in the Wild. (arXiv:2207.09450v1 [cs.RO])
58. A Unifying Causal Framework for Analyzing Dataset Shift-stable Learning Algorithms. (arXiv:1905.11374v5 [stat.ML] UPDATED)
59. Deep learning generates custom-made logistic regression models for explaining how breast cancer subtypes are classified. (arXiv:2001.06988v2 [cs.LG] UPDATED)
60. Modeling emotion for human-like behavior in future intelligent robots. (arXiv:2009.14810v2 [cs.AI] UPDATED)
61. Achilles Heels for AGI/ASI via Decision Theoretic Adversaries. (arXiv:2010.05418v5 [cs.AI] UPDATED)
62. Explainability of deep vision-based autonomous driving systems: Review and challenges. (arXiv:2101.05307v2 [cs.CV] UPDATED)
63. IoTDevID: A Behavior-Based Device Identification Method for the IoT. (arXiv:2102.08866v3 [cs.CR] UPDATED)
64. Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization. (arXiv:2110.04538v2 [cs.CV] UPDATED)
65. Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation. (arXiv:2111.08557v4 [cs.CV] UPDATED)
66. Object-Centric Unsupervised Image Captioning. (arXiv:2112.00969v2 [cs.CV] UPDATED)
67. Dynamic programming with incomplete information to overcome navigational uncertainty in a nautical environment. (arXiv:2112.14657v2 [math.OC] UPDATED)
68. Learning nonlinear dynamics in synchronization of knowledge-based leader-following networks. (arXiv:2112.14676v2 [eess.SY] UPDATED)
69. Diagnosing AI Explanation Methods with Folk Concepts of Behavior. (arXiv:2201.11239v2 [cs.AI] UPDATED)
70. A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation. (arXiv:2202.08146v3 [cs.LG] UPDATED)
71. Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification. (arXiv:2203.01386v2 [cs.CV] UPDATED)
72. Boosting human decision-making with AI-generated decision aids. (arXiv:2203.02776v2 [cs.AI] UPDATED)
73. Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion. (arXiv:2203.03279v3 [cs.LG] UPDATED)
74. Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point Clouds for Closing Domain Gap. (arXiv:2203.03833v2 [cs.CV] UPDATED)
75. On the link between conscious function and general intelligence in humans and machines. (arXiv:2204.05133v2 [cs.AI] UPDATED)
76. FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations. (arXiv:2204.06508v2 [cs.CL] UPDATED)
77. Automation of Radiation Treatment Planning for Rectal Cancer. (arXiv:2204.12539v2 [physics.med-ph] UPDATED)
78. MMRotate: A Rotated Object Detection Benchmark using PyTorch. (arXiv:2204.13317v4 [cs.CV] UPDATED)
79. Causal Balancing for Domain Generalization. (arXiv:2206.05263v2 [cs.LG] UPDATED)
80. Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v3 [cs.CV] UPDATED)
81. Not All Models Are Equal: Predicting Model Transferability in a Self-challenging Fisher Space. (arXiv:2207.03036v2 [cs.LG] UPDATED)
82. Safe reinforcement learning for multi-energy management systems with known constraint functions. (arXiv:2207.03830v3 [eess.SY] UPDATED)
83. Modeling Multi-interest News Sequence for News Recommendation. (arXiv:2207.07331v2 [cs.IR] UPDATED)
84. Stochastic Market Games. (arXiv:2207.07388v3 [cs.MA] UPDATED)
85. Do Not Sleep on Linear Models: Simple and Interpretable Techniques Outperform Deep Learning for Sleep Scoring. (arXiv:2207.07753v2 [stat.ML] UPDATED)
86. MLGOPerf: An ML Guided Inliner to Optimize Performance. (arXiv:2207.08389v2 [cs.PL] UPDATED)
87. Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v2 [cs.LG] UPDATED)
88. Comprehensive Graph Gradual Pruning for Sparse Training in Graph Neural Networks. (arXiv:2207.08629v2 [cs.LG] UPDATED)
89. FakeCLR: Exploring Contrastive Learning for Solving Latent Discontinuity in Data-Efficient GANs. (arXiv:2207.08630v2 [cs.CV] UPDATED)

