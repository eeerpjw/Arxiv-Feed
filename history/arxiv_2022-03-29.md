# Your interest papers
---
## cs.CV
---
### A Cross-Domain Approach for Continuous Impression Recognition from Dyadic Audio-Visual-Physio Signals. (arXiv:2203.13932v1 [cs.MM])
- Authors : Yuanchao Li, Catherine Lai
- Link : [http://arxiv.org/abs/2203.13932](http://arxiv.org/abs/2203.13932)
> ABSTRACT  :  The impression we make on others depends not only on what we say, but also, to a large extent, on how we say it. As a sub-branch of affective computing and social signal processing, impression recognition has proven critical in both human-human conversations and spoken dialogue systems. However, most research has studied impressions only from the signals expressed by the emitter, ignoring the response from the receiver. In this paper, we perform impression recognition using a proposed cross-domain architecture on the dyadic IMPRESSION dataset. This improved architecture makes use of cross-domain attention and regularization. The cross-domain attention consists of intra- and inter-attention mechanisms, which capture intra- and inter-domain relatedness, respectively. The cross-domain regularization includes knowledge distillation and similarity **enhancement** losses, which strengthen the feature connections between the emitter and receiver. The experimental evaluation verified the effectiveness of our approach. Our approach achieved a concordance correlation coefficient of 0.770 in competence dimension and 0.748 in warmth dimension.  
### Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])
- Authors : Pratik Katte, Siva Teja, Geetha Manjunath
- Link : [http://arxiv.org/abs/2203.14128](http://arxiv.org/abs/2203.14128)
> ABSTRACT  :  In the last two years, millions of lives have been lost due to COVID-19. Despite the vaccination programmes for a year, hospitalization rates and deaths are still high due to the new variants of COVID-19. Stringent guidelines and COVID-19 screening measures such as temperature check and mask check at all public places are helping reduce the spread of COVID-19. Visual inspections to ensure these screening measures can be taxing and erroneous. Automated inspection ensures an effective and accurate screening. Traditional approaches involve identification of faces and masks from visual camera images followed by extraction of temperature values from thermal imaging cameras. Use of visual imaging as a primary modality limits these applications only for good-lighting conditions. The use of thermal imaging alone for these screening measures makes the system invariant to illumination. However, lack of open source datasets is an issue to develop such systems. In this paper, we discuss our work on using machine learning over thermal video streams for face and mask detection and subsequent temperature screening in a passive non-invasive way that enables an effective automated COVID-19 screening method in public places. We open source our NTIC dataset that was used for training our models and was collected at 8 different locations. Our results show that the use of thermal imaging is as effective as visual imaging in the presence of high illumination. This performance stays the same for thermal images even under **low-light**ing conditions, whereas the performance with visual trained classifiers show more than 50% degradation.  
### ThunderNet: Towards **Real-time** Generic Object Detection. (arXiv:1903.11752v3 [cs.CV] UPDATED)
- Authors : Zheng Qin, Zeming Li, Zhaoning Zhang, Yiping Bao, Gang Yu, Yuxing Peng, Jian Sun
- Link : [http://arxiv.org/abs/1903.11752](http://arxiv.org/abs/1903.11752)
> ABSTRACT  :  **Real-time** generic object detection on mobile platforms is a crucial but challenging computer vision task. However, previous CNN-based detectors suffer from enormous computational cost, which hinders them from real-time inference in computation-constrained scenarios. In this paper, we investigate the effectiveness of two-stage detectors in real-time generic detection and propose a lightweight two-stage detector named ThunderNet. In the backbone part, we analyze the drawbacks in previous lightweight backbones and present a lightweight backbone designed for object detection. In the detection part, we exploit an extremely efficient RPN and detection head design. To generate more discriminative feature representation, we design two efficient architecture blocks, Context **Enhancement** Module and Spatial Attention Module. At last, we investigate the balance between the input resolution, the backbone, and the detection head. Compared with lightweight one-stage detectors, ThunderNet achieves superior performance with only 40% of the computational cost on PASCAL VOC and COCO benchmarks. Without bells and whistles, our model runs at 24.1 fps on an ARM-based device. To the best of our knowledge, this is the first real-time detector reported on ARM platforms. Our code and models are available at \url{https://github.com/qinzheng93/ThunderNet}.  
### Unsupervised **Low-light** Image **Enhancement** with Decoupled Networks. (arXiv:2005.02818v2 [eess.IV] UPDATED)
- Authors : Wei Xiong, Ding Liu, Xiaohui Shen, Chen Fang, Jiebo Luo
- Link : [http://arxiv.org/abs/2005.02818](http://arxiv.org/abs/2005.02818)
> ABSTRACT  :  In this paper, we tackle the problem of enhancing real-world **low-light** images with significant noise in an unsupervised fashion. Conventional unsupervised learning-based approaches usually tackle the **low-light** image **enhancement** problem using an image-to-image translation model. They focus primarily on illumination or contrast **enhancement** but fail to suppress the noise that ubiquitously exists in images taken under real-world **low-light** conditions. To address this issue, we explicitly decouple this task into two sub-tasks: illumination **enhancement** and noise suppression. We propose to learn a two-stage GAN-based framework to enhance the real-world **low-light** images in a fully unsupervised fashion. To facilitate the unsupervised training of our model, we construct samples with pseudo labels. Furthermore, we propose an adaptive content loss to suppress real image noise in different regions based on illumination intensity. In addition to conventional benchmark datasets, a new unpaired **low-light** image **enhancement** dataset is built and used to thoroughly evaluate the performance of our model. Extensive experiments show that our proposed method outperforms the state-of-the-art unsupervised image **enhancement** methods in terms of both illumination **enhancement** and noise reduction.  
### Unsupervised Segmentation for Terracotta Warrior Point Cloud (SRG-Net). (arXiv:2012.00433v2 [cs.CV] UPDATED)
- Authors : Yao Hu, Guohua Geng, Kang Li, Wei Zhou
- Link : [http://arxiv.org/abs/2012.00433](http://arxiv.org/abs/2012.00433)
> ABSTRACT  :  The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum Site Museum is handcrafted by experts, and the increasing amounts of unearthed pieces of terracotta warriors make the archaeologists too challenging to conduct the **restoration** of terracotta warriors efficiently. We hope to segment the 3D point cloud data of the terracotta warriors automatically and store the fragment data in the database to assist the archaeologists in matching the actual fragments with the ones in the database, which could result in higher repairing efficiency of terracotta warriors. Moreover, the existing 3D neural network research is mainly focusing on supervised classification, clustering, unsupervised representation, and reconstruction. There are few pieces of researches concentrating on unsupervised point cloud part segmentation. In this paper, we present SRG-Net for 3D point clouds of terracotta warriors to address these problems. Firstly, we adopt a customized seed-region-growing algorithm to segment the point cloud coarsely. Then we present a supervised segmentation and unsupervised reconstruction networks to learn the characteristics of 3D point clouds. Finally, we combine the SRG algorithm with our improved CNN(convolution neural network) using a refinement method. This pipeline is called SRG-Net, which aims at conducting segmentation tasks on the terracotta warriors. Our proposed SRG-Net is evaluated on the terracotta warrior data and ShapeNet dataset by measuring the accuracy and the latency. The experimental results show that our SRG-Net outperforms the state-of-the-art methods. Our code is available at https://github.com/hyoau/SRG-Net.  
### Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v3 [cs.CV] UPDATED)
- Authors : Alessandro Sebastianelli, Artur Nowakowski, Erika Puglisi, Maria Pia, Del Rosso, Jamila Mifdal, Fiora Pirri, Pierre Philippe, Silvia Liberata
- Link : [http://arxiv.org/abs/2106.12226](http://arxiv.org/abs/2106.12226)
> ABSTRACT  :  Cloud removal is a relevant topic in Remote Sensing as it fosters the usability of high-resolution optical images for Earth monitoring and study. Related techniques have been analyzed for years with a progressively clearer view of the appropriate methods to adopt, from multi-spectral to inpainting methods. Recent applications of deep generative models and sequence-to-sequence-based models have proved their capability to advance the field significantly. Nevertheless, there are still some gaps, mostly related to the amount of cloud coverage, the density and thickness of clouds, and the occurred temporal landscape changes. In this work, we fill some of these gaps by introducing a novel multi-modal method that uses different sources of information, both spatial and temporal, to restore the whole optical scene of interest. The proposed method introduces an innovative deep model, using the outcomes of both temporal-sequence blending and direct translation from Synthetic Aperture Radar (SAR) to optical images to obtain a pixel-wise **restoration** of the whole scene. The advantage of our approach is demonstrated across a variety of atmospheric conditions tested on a dataset we have generated and made available. Quantitative and qualitative results prove that the proposed method obtains cloud-free images, preserving scene details without resorting to a huge portion of a clean image and coping with landscape changes.  
### **Dark**Lighter: Light Up the **Dark**ness for UAV Tracking. (arXiv:2107.14389v2 [cs.CV] UPDATED)
- Authors : Junjie Ye, Changhong Fu, Guangze Zheng, Ziang Cao, Bowen Li
- Link : [http://arxiv.org/abs/2107.14389](http://arxiv.org/abs/2107.14389)
> ABSTRACT  :  Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to **low-light** scenes that are commonly lacked in the existing training set. In indistinguishable **night** scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the **dark** through a general fashion, this work proposes a **low-light** image enhancer namely **Dark**Lighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, i.e., ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV **dark** tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of **Dark**Lighter, with high efficiency. Moreover, **Dark**Lighter has further been implemented on a typical UAV system. Real-world tests at **night** scenes have verified its practicability and dependability.  
### Learning Co-segmentation by Segment Swapping for Retrieval and Discovery. (arXiv:2110.15904v2 [cs.CV] UPDATED)
- Authors : Xi Shen, Armand Joulin, Mathieu Aubry
- Link : [http://arxiv.org/abs/2110.15904](http://arxiv.org/abs/2110.15904)
> ABSTRACT  :  The goal of this work is to efficiently identify visually similar patterns in images, e.g. identifying an artwork detail copied between an engraving and an oil painting, or recognizing parts of a **night**-time photograph visible in its daytime counterpart. Lack of training data is a key challenge for this co-segmentation task. We present a simple yet surprisingly effective approach to overcome this difficulty: we generate synthetic training pairs by selecting segments in an image and copy-pasting them into another image. We then learn to predict the repeated region masks. We find that it is crucial to predict the correspondences as an auxiliary task and to use Poisson blending and style transfer on the training pairs to generalize on real data. We analyse results with two deep architectures relevant to our joint image analysis task: a transformer-based architecture and Sparse Nc-Net, a recent network designed to predict coarse correspondences using 4D convolutions. We show our approach provides clear improvements for artwork details retrieval on the Brueghel dataset and achieves competitive performance on two place recognition benchmarks, Tokyo247 and Pitts30K. We also demonstrate the potential of our approach for unsupervised image collection analysis by introducing a spectral graph clustering approach to object discovery and demonstrating it on the object discovery dataset of \cite{rubinstein2013unsupervised} and the Brueghel dataset. Our code and data are available at <a href="http://imagine.enpc.fr/~shenx/SegSwap/.">this http URL</a>  
### Mip-**NeRF** 360: Unbounded Anti-Aliased Neural Radiance Fields. (arXiv:2111.12077v3 [cs.CV] UPDATED)
- Authors : Ben Mildenhall, Dor Verbin, Peter Hedman
- Link : [http://arxiv.org/abs/2111.12077](http://arxiv.org/abs/2111.12077)
> ABSTRACT  :  Though neural radiance fields (**NeRF**) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on "unbounded" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing **NeRF**-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-**NeRF** (a **NeRF** variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub "mip-**NeRF** 360" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-**NeRF**, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.  
### **Swin**BERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v2 [cs.CV] UPDATED)
- Authors : Kevin Lin, Linjie Li, Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, Lijuan Wang
- Link : [http://arxiv.org/abs/2111.13196](http://arxiv.org/abs/2111.13196)
> ABSTRACT  :  The canonical approach to video captioning dictates a caption generation model to learn from offline-extracted dense video features. These feature extractors usually operate on video frames sampled at a fixed frame rate and are often trained on image/video understanding tasks, without adaption to video captioning data. In this work, we present **Swin**BERT, an end-to-end transformer-based model for video captioning, which takes video frame patches directly as inputs, and outputs a natural language description. Instead of leveraging multiple 2D/3D feature extractors, our method adopts a video transformer to encode spatial-temporal representations that can adapt to variable lengths of video input without dedicated design for different frame rates. Based on this model architecture, we show that video captioning can benefit significantly from more densely sampled video frames as opposed to previous successes with sparsely sampled video frames for video-and-language understanding tasks (e.g., video question answering). Moreover, to avoid the inherent redundancy in consecutive video frames, we propose adaptively learning a sparse attention mask and optimizing it for task-specific performance improvement through better long-range video sequence modeling. Through extensive experiments on 5 video captioning datasets, we show that **Swin**BERT achieves across-the-board performance improvements over previous methods, often by a large margin. The learned sparse attention masks in addition push the limit to new state of the arts, and can be transferred between different video lengths and between different datasets. Code is available at https://github.com/microsoft/**Swin**BERT  
### GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v2 [cs.CV] UPDATED)
- Authors : Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Dacheng Tao
- Link : [http://arxiv.org/abs/2111.13680](http://arxiv.org/abs/2111.13680)
> ABSTRACT  :  Learning-based optical flow estimation has been dominated with the pipeline of cost volume with convolutions for flow regression, which is inherently limited to local correlations and thus is hard to address the long-standing challenge of large displacements. To alleviate this, the state-of-the-art framework RAFT gradually improves its prediction quality by using a large number of iterative refinements, achieving remarkable performance but introducing linearly increasing inference time. To enable both high accuracy and efficiency, we completely revamp the dominant flow regression pipeline by reformulating optical flow as a global matching problem, which identifies the correspondences by directly comparing feature similarities. Specifically, we propose a GMFlow framework, which consists of three main components: a customized Transformer for feature **enhancement**, a correlation and softmax layer for global feature matching, and a self-attention layer for flow propagation. We further introduce a refinement step that reuses GMFlow at higher feature resolution for residual flow prediction. Our new framework outperforms 31-refinements RAFT on the challenging Sintel benchmark, while using only one refinement and running faster, suggesting a new paradigm for accurate and efficient optical flow estimation. Code is available at https://github.com/haofeixu/gmflow.  
### Deblur-**NeRF**: Neural Radiance Fields from Blurry Images. (arXiv:2111.14292v2 [cs.CV] UPDATED)
- Authors : Li Ma, Xiaoyu Li, Jing Liao, Qi Zhang, Xuan Wang, Jue Wang
- Link : [http://arxiv.org/abs/2111.14292](http://arxiv.org/abs/2111.14292)
> ABSTRACT  :  Neural Radiance Field (**NeRF**) has gained considerable attention recently for 3D scene reconstruction and novel view synthesis due to its remarkable synthesis quality. However, image blurriness caused by defocus or motion, which often occurs when capturing scenes in the wild, significantly degrades its reconstruction quality. To address this problem, We propose Deblur-**NeRF**, the first method that can recover a sharp **NeRF** from blurry input. We adopt an analysis-by-synthesis approach that reconstructs blurry views by simulating the blurring process, thus making **NeRF** robust to blurry inputs. The core of this simulation is a novel Deformable Sparse Kernel (DSK) module that models spatially-varying blur kernels by deforming a canonical sparse kernel at each spatial location. The ray origin of each kernel point is jointly optimized, inspired by the physical blurring process. This module is parameterized as an MLP that has the ability to be generalized to various blur types. Jointly optimizing the **NeRF** and the DSK module allows us to restore a sharp **NeRF**. We demonstrate that our method can be used on both camera motion blur and defocus blur: the two most common types of blur in real scenes. Evaluation results on both synthetic and real-world data show that our method outperforms several baselines. The synthetic and real datasets along with the source code is publicly available at https://limacv.github.io/deblurnerf/  
### Self-Supervised Pre-Training of **Swin** Transformers for 3D Medical Image Analysis. (arXiv:2111.14791v2 [cs.CV] UPDATED)
- Authors : Yucheng Tang, Dong Yang, Wenqi Li, Holger Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, Ali Hatamizadeh
- Link : [http://arxiv.org/abs/2111.14791](http://arxiv.org/abs/2111.14791)
> ABSTRACT  :  Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed **Swin** UNEt TRansformers (**Swin** UNETR), with a hierarchical encoder for self-supervised pre-training; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art (i.e. ranked 1st) on the public test leaderboards of both MSD and BTCV datasets. Code: https://monai.io/research/swin-unetr  
### Multi-domain Integrative **Swin** Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v6 [eess.IV] UPDATED)
- Authors : Jiayi Pan, Weiwen Wu, Zhifan Gao, Heye Zhang
- Link : [http://arxiv.org/abs/2111.14831](http://arxiv.org/abs/2111.14831)
> ABSTRACT  :  Decreasing projection views to lower X-ray radiation dose usually leads to severe streak artifacts. To improve image quality from sparse-view data, a Multi-domain Integrative **Swin** Transformer network (MIST-net) was developed in this article. First, MIST-net incorporated lavish domain features from data, residual-data, image, and residual-image using flexible network architectures, where residual-data and residual-image sub-network was considered as data consistency module to eliminate interpolation and reconstruction errors. Second, a trainable edge **enhancement** filter was incorporated to detect and protect image edges. Third, a high-quality reconstruction **Swin** transformer (i.e., Recformer) was designed to capture image global features. The experiment results on numerical and real cardiac clinical datasets with 48-views demonstrated that our proposed MIST-net provided better image quality with more small features and sharp edges than other competitors.  
### Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v3 [cs.CV] UPDATED)
- Authors : Xiuwei Xu, Yifan Wang, Yu Zheng, Yongming Rao, Jie Zhou, Jiwen Lu
- Link : [http://arxiv.org/abs/2203.05238](http://arxiv.org/abs/2203.05238)
> ABSTRACT  :  In this paper, we propose a weakly-supervised approach for 3D object detection, which makes it possible to train a strong 3D detector with position-level annotations (i.e. annotations of object centers). In order to remedy the information loss from box annotations to centers, our method, namely Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak labels into fully-annotated virtual scenes as stronger supervision, and in turn utilizes the perfect virtual labels to complement and refine the real labels. Specifically, we first assemble 3D shapes into physically reasonable virtual scenes according to the coarse scene layout extracted from position-level annotations. Then we go back to reality by applying a virtual-to-real domain adaptation method, which refine the weak labels and additionally supervise the training of detector with the virtual scenes. Furthermore, we propose a more challenging benckmark for indoor 3D object detection with more diversity in object sizes to better show the potential of BR. With less than 5% of the labeling labor, we achieve comparable detection performance with some popular fully-supervised approaches on the widely used ScanNet dataset. Code is available at: https://github.com/wyf-ACCEPT/BackToReality  
### MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer. (arXiv:2203.10981v2 [cs.CV] UPDATED)
- Authors : Chih Huang, Han Wu, Ting Su
- Link : [http://arxiv.org/abs/2203.10981](http://arxiv.org/abs/2203.10981)
> ABSTRACT  :  Monocular 3D object detection is an important yet challenging task in autonomous driving. Some existing methods leverage depth information from an off-the-shelf depth estimator to assist 3D detection, but suffer from the additional computational burden and achieve limited performance caused by inaccurate depth priors. To alleviate this, we propose MonoDTR, a novel end-to-end depth-aware transformer network for monocular 3D object detection. It mainly consists of two components: (1) the Depth-Aware Feature **Enhancement** (DFE) module that implicitly learns depth-aware features with auxiliary supervision without requiring extra computation, and (2) the Depth-Aware Transformer (DTR) module that globally integrates context- and depth-aware features. Moreover, different from conventional pixel-wise positional encodings, we introduce a novel depth positional encoding (DPE) to inject depth positional hints into transformers. Our proposed depth-aware modules can be easily plugged into existing image-only monocular 3D object detectors to improve the performance. Extensive experiments on the KITTI dataset demonstrate that our approach outperforms previous state-of-the-art monocular-based methods and achieves real-time detection. Code is available at https://github.com/kuanchihhuang/MonoDTR  
## eess.IV
---
### Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])
- Authors : Pratik Katte, Siva Teja, Geetha Manjunath
- Link : [http://arxiv.org/abs/2203.14128](http://arxiv.org/abs/2203.14128)
> ABSTRACT  :  In the last two years, millions of lives have been lost due to COVID-19. Despite the vaccination programmes for a year, hospitalization rates and deaths are still high due to the new variants of COVID-19. Stringent guidelines and COVID-19 screening measures such as temperature check and mask check at all public places are helping reduce the spread of COVID-19. Visual inspections to ensure these screening measures can be taxing and erroneous. Automated inspection ensures an effective and accurate screening. Traditional approaches involve identification of faces and masks from visual camera images followed by extraction of temperature values from thermal imaging cameras. Use of visual imaging as a primary modality limits these applications only for good-lighting conditions. The use of thermal imaging alone for these screening measures makes the system invariant to illumination. However, lack of open source datasets is an issue to develop such systems. In this paper, we discuss our work on using machine learning over thermal video streams for face and mask detection and subsequent temperature screening in a passive non-invasive way that enables an effective automated COVID-19 screening method in public places. We open source our NTIC dataset that was used for training our models and was collected at 8 different locations. Our results show that the use of thermal imaging is as effective as visual imaging in the presence of high illumination. This performance stays the same for thermal images even under **low-light**ing conditions, whereas the performance with visual trained classifiers show more than 50% degradation.  
### Deep Polarimetric **HDR** Reconstruction. (arXiv:2203.14190v1 [eess.IV])
- Authors : Juiwen Ting, Moein Shakeri, Hong Zhang
- Link : [http://arxiv.org/abs/2203.14190](http://arxiv.org/abs/2203.14190)
> ABSTRACT  :  This paper proposes a novel learning based high-dynamic-range (**HDR**) reconstruction method using a polarization camera. We utilize a previous observation that polarization filters with different orientations can attenuate natural light differently, and we treat the multiple images acquired by the polarization camera as a set acquired under different **exposure** times, to introduce the development of solutions for the **HDR** reconstruction problem. We propose a deep **HDR** reconstruction framework with a feature masking mechanism that uses polarimetric cues available from the polarization camera, called Deep Polarimetric **HDR** Reconstruction (DPHR). The proposed DPHR obtains polarimetric information to propagate valid features through the network more effectively to regress the missing pixels. We demonstrate through both qualitative and quantitative evaluations that the proposed DPHR performs favorably than state-of-the-art **HDR** reconstruction algorithms.  
### A Survey of Super-Resolution in Iris Biometrics with Evaluation of Dictionary-Learning. (arXiv:2203.14203v1 [cs.CV])
- Authors : 
- Link : [http://arxiv.org/abs/2203.14203](http://arxiv.org/abs/2203.14203)
> ABSTRACT  :  The lack of resolution has a negative impact on the performance of image-based biometrics. While many generic super-resolution methods have been proposed to restore low-resolution images, they usually aim to enhance their visual appearance. However, a visual **enhancement** of biometric images does not necessarily correlate with a better recognition performance. Reconstruction approaches need thus to incorporate specific information from the target biometric modality to effectively improve recognition. This paper presents a comprehensive survey of iris super-resolution approaches proposed in the literature. We have also adapted an Eigen-patches reconstruction method based on PCA Eigen-transformation of local image patches. The structure of the iris is exploited by building a patch-position dependent dictionary. In addition, image patches are restored separately, having their own reconstruction weights. This allows the solution to be locally optimized, helping to preserve local information. To evaluate the algorithm, we degraded high-resolution images from the CASIA Interval V3 database. Different **restoration**s were considered, with 15x15 pixels being the smallest resolution. To the best of our knowledge, this is among the smallest resolutions employed in the literature. The framework is complemented with six public iris comparators, which were used to carry out biometric verification and identification experiments. Experimental results show that the proposed method significantly outperforms both bilinear and bicubic interpolation at very low-resolution. The performance of a number of comparators attains an impressive Equal Error Rate as low as 5%, and a Top-1 accuracy of 77-84% when considering iris images of only 15x15 pixels. These results clearly demonstrate the benefit of using trained super-resolution techniques to improve the quality of iris images prior to matching.  
### Efficient and Degradation-Adaptive Network for Real-World Image Super-Resolution. (arXiv:2203.14216v1 [cs.CV])
- Authors : Jie Liang, Hui Zeng, **Lei Zhang**
- Link : [http://arxiv.org/abs/2203.14216](http://arxiv.org/abs/2203.14216)
> ABSTRACT  :  Efficient and effective real-world image super-resolution (Real-ISR) is a challenging task due to the unknown complex degradation of real-world images and the limited computation resources in practical applications. Recent research on Real-ISR has achieved significant progress by modeling the image degradation space; however, these methods largely rely on heavy backbone networks and they are inflexible to handle images of different degradation levels. In this paper, we propose an efficient and effective degradation-adaptive super-resolution (DASR) network, whose parameters are adaptively specified by estimating the degradation of each input image. Specifically, a tiny regression network is employed to predict the degradation parameters of the input image, while several convolutional experts with the same topology are jointly optimized to specify the network parameters via a non-linear mixture of experts. The joint optimization of multiple experts and the degradation-adaptive pipeline significantly extend the model capacity to handle degradations of various levels, while the inference remains efficient since only one adaptively specified network is used for super-resolving the input image. Our extensive experiments demonstrate that the proposed DASR is not only much more effective than existing methods on handling real-world images with different degradation levels but also efficient for easy deployment. Codes, models and datasets are available at https://github.com/csjliang/DASR.  
### Limited Parameter Denoising for Low-dose X-ray Computed Tomography Using Deep Reinforcement Learning. (arXiv:2203.14794v1 [eess.IV])
- Authors : Mayank Patwari, Ralf Gutjahr, Rainer Raupach, Andreas Maier
- Link : [http://arxiv.org/abs/2203.14794](http://arxiv.org/abs/2203.14794)
> ABSTRACT  :  The use of deep learning has successfully solved several problems in the field of medical imaging. Deep learning has been applied to the CT denoising problem successfully. However, the use of deep learning requires large amounts of data to train deep convolutional networks (CNNs). Moreover, due to large parameter count, such deep CNNs may cause unexpected results. In this study, we introduce a novel CT denoising framework, which has interpretable behaviour, and provides useful results with limited data. We employ **bilateral** filtering in both the projection and volume domains to remove noise. To account for non-stationary noise, we tune the $\sigma$ parameters of the volume for every projection view, and for every volume pixel. The tuning is carried out by two deep CNNs. Due to impracticality of labelling, the two deep CNNs are trained via a Deep-Q reinforcement learning task. The reward for the task is generated by using a custom reward function represented by a neural network. Our experiments were carried out on abdominal scans for the Mayo Clinic TCIA dataset, and the AAPM Low Dose CT Grand Challenge. Our denoising framework has excellent denoising performance increasing the PSNR from 28.53 to 28.93, and increasing the SSIM from 0.8952 to 0.9204. We outperform several state-of-the-art deep CNNs, which have several orders of magnitude higher number of parameters (p-value (PSNR) = 0.000, p-value (SSIM) = 0.000). Our method does not introduce any blurring, which is introduced by MSE loss based methods, or any deep learning artifacts, which are introduced by WGAN based models. Our ablation studies show that parameter tuning and using our reward network results in the best possible results.  
### Unsupervised **Low-light** Image **Enhancement** with Decoupled Networks. (arXiv:2005.02818v2 [eess.IV] UPDATED)
- Authors : Wei Xiong, Ding Liu, Xiaohui Shen, Chen Fang, Jiebo Luo
- Link : [http://arxiv.org/abs/2005.02818](http://arxiv.org/abs/2005.02818)
> ABSTRACT  :  In this paper, we tackle the problem of enhancing real-world **low-light** images with significant noise in an unsupervised fashion. Conventional unsupervised learning-based approaches usually tackle the **low-light** image **enhancement** problem using an image-to-image translation model. They focus primarily on illumination or contrast **enhancement** but fail to suppress the noise that ubiquitously exists in images taken under real-world **low-light** conditions. To address this issue, we explicitly decouple this task into two sub-tasks: illumination **enhancement** and noise suppression. We propose to learn a two-stage GAN-based framework to enhance the real-world **low-light** images in a fully unsupervised fashion. To facilitate the unsupervised training of our model, we construct samples with pseudo labels. Furthermore, we propose an adaptive content loss to suppress real image noise in different regions based on illumination intensity. In addition to conventional benchmark datasets, a new unpaired **low-light** image **enhancement** dataset is built and used to thoroughly evaluate the performance of our model. Extensive experiments show that our proposed method outperforms the state-of-the-art unsupervised image **enhancement** methods in terms of both illumination **enhancement** and noise reduction.  
### Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v3 [cs.CV] UPDATED)
- Authors : Alessandro Sebastianelli, Artur Nowakowski, Erika Puglisi, Maria Pia, Del Rosso, Jamila Mifdal, Fiora Pirri, Pierre Philippe, Silvia Liberata
- Link : [http://arxiv.org/abs/2106.12226](http://arxiv.org/abs/2106.12226)
> ABSTRACT  :  Cloud removal is a relevant topic in Remote Sensing as it fosters the usability of high-resolution optical images for Earth monitoring and study. Related techniques have been analyzed for years with a progressively clearer view of the appropriate methods to adopt, from multi-spectral to inpainting methods. Recent applications of deep generative models and sequence-to-sequence-based models have proved their capability to advance the field significantly. Nevertheless, there are still some gaps, mostly related to the amount of cloud coverage, the density and thickness of clouds, and the occurred temporal landscape changes. In this work, we fill some of these gaps by introducing a novel multi-modal method that uses different sources of information, both spatial and temporal, to restore the whole optical scene of interest. The proposed method introduces an innovative deep model, using the outcomes of both temporal-sequence blending and direct translation from Synthetic Aperture Radar (SAR) to optical images to obtain a pixel-wise **restoration** of the whole scene. The advantage of our approach is demonstrated across a variety of atmospheric conditions tested on a dataset we have generated and made available. Quantitative and qualitative results prove that the proposed method obtains cloud-free images, preserving scene details without resorting to a huge portion of a clean image and coping with landscape changes.  
### Multi-domain Integrative **Swin** Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v6 [eess.IV] UPDATED)
- Authors : Jiayi Pan, Weiwen Wu, Zhifan Gao, Heye Zhang
- Link : [http://arxiv.org/abs/2111.14831](http://arxiv.org/abs/2111.14831)
> ABSTRACT  :  Decreasing projection views to lower X-ray radiation dose usually leads to severe streak artifacts. To improve image quality from sparse-view data, a Multi-domain Integrative **Swin** Transformer network (MIST-net) was developed in this article. First, MIST-net incorporated lavish domain features from data, residual-data, image, and residual-image using flexible network architectures, where residual-data and residual-image sub-network was considered as data consistency module to eliminate interpolation and reconstruction errors. Second, a trainable edge **enhancement** filter was incorporated to detect and protect image edges. Third, a high-quality reconstruction **Swin** transformer (i.e., Recformer) was designed to capture image global features. The experiment results on numerical and real cardiac clinical datasets with 48-views demonstrated that our proposed MIST-net provided better image quality with more small features and sharp edges than other competitors.  
## cs.LG
---
### Improving robustness of jet tagging algorithms with adversarial training. (arXiv:2203.13890v1 [physics.data-an])
- Authors : Annika Stein, Xavier Coubez, Spandan Mondal, Andrzej Novak, Alexander Schmidt
- Link : [http://arxiv.org/abs/2203.13890](http://arxiv.org/abs/2203.13890)
> ABSTRACT  :  Deep learning is a standard tool in the field of high-energy physics, facilitating considerable sensitivity **enhancement**s for numerous analysis strategies. In particular, in identification of physics objects, such as jet flavor tagging, complex neural network architectures play a major role. However, these methods are reliant on accurate simulations. Mismodeling can lead to non-negligible differences in performance in data that need to be measured and calibrated against. We investigate the classifier response to input data with injected mismodelings and probe the vulnerability of flavor tagging algorithms via application of adversarial attacks. Subsequently, we present an adversarial training strategy that mitigates the impact of such simulated attacks and improves the classifier robustness. We examine the relationship between performance and vulnerability and show that this method constitutes a promising approach to reduce the vulnerability to poor modeling.  
### Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])
- Authors : Pratik Katte, Siva Teja, Geetha Manjunath
- Link : [http://arxiv.org/abs/2203.14128](http://arxiv.org/abs/2203.14128)
> ABSTRACT  :  In the last two years, millions of lives have been lost due to COVID-19. Despite the vaccination programmes for a year, hospitalization rates and deaths are still high due to the new variants of COVID-19. Stringent guidelines and COVID-19 screening measures such as temperature check and mask check at all public places are helping reduce the spread of COVID-19. Visual inspections to ensure these screening measures can be taxing and erroneous. Automated inspection ensures an effective and accurate screening. Traditional approaches involve identification of faces and masks from visual camera images followed by extraction of temperature values from thermal imaging cameras. Use of visual imaging as a primary modality limits these applications only for good-lighting conditions. The use of thermal imaging alone for these screening measures makes the system invariant to illumination. However, lack of open source datasets is an issue to develop such systems. In this paper, we discuss our work on using machine learning over thermal video streams for face and mask detection and subsequent temperature screening in a passive non-invasive way that enables an effective automated COVID-19 screening method in public places. We open source our NTIC dataset that was used for training our models and was collected at 8 different locations. Our results show that the use of thermal imaging is as effective as visual imaging in the presence of high illumination. This performance stays the same for thermal images even under **low-light**ing conditions, whereas the performance with visual trained classifiers show more than 50% degradation.  
### Meta-Reinforcement Learning for Reliable Communication in THz/VLC Wireless VR Networks. (arXiv:2102.12277v2 [cs.NI] UPDATED)
- Authors : Yining Wang, Mingzhe Chen, Zhaohui Yang, Walid Saad, Tao luo, Shuguang Cui, Vincent Poor
- Link : [http://arxiv.org/abs/2102.12277](http://arxiv.org/abs/2102.12277)
> ABSTRACT  :  In this paper, the problem of enhancing the quality of virtual reality (VR) services is studied for an indoor terahertz (THz)/visible light communication (VLC) wireless network. In the studied model, small base stations (SBSs) transmit high-quality VR images to VR users over THz bands and light-emitting diodes (LEDs) provide accurate indoor positioning services for them using VLC. Here, VR users move in **real time** and their movement patterns change over time according to their applications, where both THz and VLC links can be blocked by the bodies of VR users. To control the energy consumption of the studied THz/VLC wireless VR network, VLC access points (VAPs) must be selectively turned on so as to ensure accurate and extensive positioning for VR users. Based on the user positions, each SBS must generate corresponding VR images and establish THz links without body blockage to transmit the VR content. The problem is formulated as an optimization problem whose goal is to maximize the reliability of the VR network by selecting the appropriate VAPs to be turned on and controlling the user association with SBSs. To solve this problem, a policy gradient-based reinforcement learning (RL) algorithm that adopts a meta-learning approach is proposed. The proposed meta policy gradient (MPG) algorithm enables the trained policy to quickly adapt to new user movement patterns. In order to solve the problem of maximizing the average number of successfully served users for VR scenarios with a large number of users, a dual method based MPG algorithm (D-MPG) with a low complexity is proposed. Simulation results demonstrate that, compared to the trust region policy optimization algorithm (TRPO), the proposed MPG and D-MPG algorithms yield up to 26.8% and 21.9% improvement in the reliability as well as 81.2% and 87.5% gains in the convergence speed, respectively.  
### A neural simulation-based inference approach for characterizing the Galactic Center $\gamma$-ray excess. (arXiv:2110.06931v2 [astro-ph.HE] UPDATED)
- Authors : Siddharth Mishra, Kyle Cranmer
- Link : [http://arxiv.org/abs/2110.06931](http://arxiv.org/abs/2110.06931)
> ABSTRACT  :  The nature of the Fermi gamma-ray Galactic Center Excess (GCE) has remained a persistent mystery for over a decade. Although the excess is broadly compatible with emission expected due to **dark** matter annihilation, an explanation in terms of a population of unresolved astrophysical point sources e.g., millisecond pulsars, remains viable. The effort to uncover the origin of the GCE is hampered in particular by an incomplete understanding of diffuse emission of Galactic origin. This can lead to spurious features that make it difficult to robustly differentiate smooth emission, as expected for a **dark** matter origin, from more "clumpy" emission expected for a population of relatively bright, unresolved point sources. We use recent advancements in the field of simulation-based inference, in particular density estimation techniques using normalizing flows, in order to characterize the contribution of modeled components, including unresolved point source populations, to the GCE. Compared to traditional techniques based on the statistical distribution of photon counts, our machine learning-based method is able to utilize more of the information contained in a given model of the Galactic Center emission, and in particular can perform posterior parameter estimation while accounting for pixel-to-pixel spatial correlations in the gamma-ray map. This makes the method demonstrably more resilient to certain forms of model misspecification. On application to Fermi data, the method generically attributes a smaller fraction of the GCE flux to unresolved point sources when compared to traditional approaches. We nevertheless infer such a contribution to make up a non-negligible fraction of the GCE across all analysis variations considered, with at least $38^{+9}_{-19}\%$ of the excess attributed to unresolved point sources in our baseline analysis.  
### Self-Supervised Pre-Training of **Swin** Transformers for 3D Medical Image Analysis. (arXiv:2111.14791v2 [cs.CV] UPDATED)
- Authors : Yucheng Tang, Dong Yang, Wenqi Li, Holger Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, Ali Hatamizadeh
- Link : [http://arxiv.org/abs/2111.14791](http://arxiv.org/abs/2111.14791)
> ABSTRACT  :  Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed **Swin** UNEt TRansformers (**Swin** UNETR), with a hierarchical encoder for self-supervised pre-training; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art (i.e. ranked 1st) on the public test leaderboards of both MSD and BTCV datasets. Code: https://monai.io/research/swin-unetr  
### Multi-domain Integrative **Swin** Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v6 [eess.IV] UPDATED)
- Authors : Jiayi Pan, Weiwen Wu, Zhifan Gao, Heye Zhang
- Link : [http://arxiv.org/abs/2111.14831](http://arxiv.org/abs/2111.14831)
> ABSTRACT  :  Decreasing projection views to lower X-ray radiation dose usually leads to severe streak artifacts. To improve image quality from sparse-view data, a Multi-domain Integrative **Swin** Transformer network (MIST-net) was developed in this article. First, MIST-net incorporated lavish domain features from data, residual-data, image, and residual-image using flexible network architectures, where residual-data and residual-image sub-network was considered as data consistency module to eliminate interpolation and reconstruction errors. Second, a trainable edge **enhancement** filter was incorporated to detect and protect image edges. Third, a high-quality reconstruction **Swin** transformer (i.e., Recformer) was designed to capture image global features. The experiment results on numerical and real cardiac clinical datasets with 48-views demonstrated that our proposed MIST-net provided better image quality with more small features and sharp edges than other competitors.  
### Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v3 [cs.CV] UPDATED)
- Authors : Xiuwei Xu, Yifan Wang, Yu Zheng, Yongming Rao, Jie Zhou, Jiwen Lu
- Link : [http://arxiv.org/abs/2203.05238](http://arxiv.org/abs/2203.05238)
> ABSTRACT  :  In this paper, we propose a weakly-supervised approach for 3D object detection, which makes it possible to train a strong 3D detector with position-level annotations (i.e. annotations of object centers). In order to remedy the information loss from box annotations to centers, our method, namely Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak labels into fully-annotated virtual scenes as stronger supervision, and in turn utilizes the perfect virtual labels to complement and refine the real labels. Specifically, we first assemble 3D shapes into physically reasonable virtual scenes according to the coarse scene layout extracted from position-level annotations. Then we go back to reality by applying a virtual-to-real domain adaptation method, which refine the weak labels and additionally supervise the training of detector with the virtual scenes. Furthermore, we propose a more challenging benckmark for indoor 3D object detection with more diversity in object sizes to better show the potential of BR. With less than 5% of the labeling labor, we achieve comparable detection performance with some popular fully-supervised approaches on the widely used ScanNet dataset. Code is available at: https://github.com/wyf-ACCEPT/BackToReality  
## cs.AI
---
### Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])
- Authors : Pratik Katte, Siva Teja, Geetha Manjunath
- Link : [http://arxiv.org/abs/2203.14128](http://arxiv.org/abs/2203.14128)
> ABSTRACT  :  In the last two years, millions of lives have been lost due to COVID-19. Despite the vaccination programmes for a year, hospitalization rates and deaths are still high due to the new variants of COVID-19. Stringent guidelines and COVID-19 screening measures such as temperature check and mask check at all public places are helping reduce the spread of COVID-19. Visual inspections to ensure these screening measures can be taxing and erroneous. Automated inspection ensures an effective and accurate screening. Traditional approaches involve identification of faces and masks from visual camera images followed by extraction of temperature values from thermal imaging cameras. Use of visual imaging as a primary modality limits these applications only for good-lighting conditions. The use of thermal imaging alone for these screening measures makes the system invariant to illumination. However, lack of open source datasets is an issue to develop such systems. In this paper, we discuss our work on using machine learning over thermal video streams for face and mask detection and subsequent temperature screening in a passive non-invasive way that enables an effective automated COVID-19 screening method in public places. We open source our NTIC dataset that was used for training our models and was collected at 8 different locations. Our results show that the use of thermal imaging is as effective as visual imaging in the presence of high illumination. This performance stays the same for thermal images even under **low-light**ing conditions, whereas the performance with visual trained classifiers show more than 50% degradation.  
### Mysteries of Visual Experience. (arXiv:1604.08612v6 [q-bio.NC] UPDATED)
- Authors : Jerome Feldman
- Link : [http://arxiv.org/abs/1604.08612](http://arxiv.org/abs/1604.08612)
> ABSTRACT  :  Science is a crowning glory of the human spirit and its applications remain our best hope for social progress. But there are limitations to current science and perhaps to any science. The general mind-body problem is known to be intractable and currently mysterious. This is one of many deep problems that are universally agreed to be beyond the current purview of Science, including quantum phenomena, etc. But all of these famous unsolved problems are either remote from everyday experience (entanglement, **dark** matter) or are hard to even define sharply (phenomenology, consciousness, etc.).    An updated summary of this work has been published as: Feldman, J. (2022). Computation, perception, and mind. Behavioral and Brain Sciences, 45, E48. doi:10.1017/S0140525X21001886 A more readable, open access, version is: https://escholarship.org/uc/item/6cs78450  
### Unsupervised Segmentation for Terracotta Warrior Point Cloud (SRG-Net). (arXiv:2012.00433v2 [cs.CV] UPDATED)
- Authors : Yao Hu, Guohua Geng, Kang Li, Wei Zhou
- Link : [http://arxiv.org/abs/2012.00433](http://arxiv.org/abs/2012.00433)
> ABSTRACT  :  The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum Site Museum is handcrafted by experts, and the increasing amounts of unearthed pieces of terracotta warriors make the archaeologists too challenging to conduct the **restoration** of terracotta warriors efficiently. We hope to segment the 3D point cloud data of the terracotta warriors automatically and store the fragment data in the database to assist the archaeologists in matching the actual fragments with the ones in the database, which could result in higher repairing efficiency of terracotta warriors. Moreover, the existing 3D neural network research is mainly focusing on supervised classification, clustering, unsupervised representation, and reconstruction. There are few pieces of researches concentrating on unsupervised point cloud part segmentation. In this paper, we present SRG-Net for 3D point clouds of terracotta warriors to address these problems. Firstly, we adopt a customized seed-region-growing algorithm to segment the point cloud coarsely. Then we present a supervised segmentation and unsupervised reconstruction networks to learn the characteristics of 3D point clouds. Finally, we combine the SRG algorithm with our improved CNN(convolution neural network) using a refinement method. This pipeline is called SRG-Net, which aims at conducting segmentation tasks on the terracotta warriors. Our proposed SRG-Net is evaluated on the terracotta warrior data and ShapeNet dataset by measuring the accuracy and the latency. The experimental results show that our SRG-Net outperforms the state-of-the-art methods. Our code is available at https://github.com/hyoau/SRG-Net.  
### Reference Knowledgeable Network for Machine Reading Comprehension. (arXiv:2012.03709v3 [cs.CL] UPDATED)
- Authors : Yilin Zhao, Zhuosheng Zhang, Hai Zhao
- Link : [http://arxiv.org/abs/2012.03709](http://arxiv.org/abs/2012.03709)
> ABSTRACT  :  Multi-choice Machine Reading Comprehension (MRC) as a challenge requires models to select the most appropriate answer from a set of candidates with a given passage and question. Most of the existing researches focus on the modeling of specific tasks or complex networks, without explicitly referring to relevant and credible external knowledge sources, which are supposed to greatly make up for the deficiency of the given passage. Thus we propose a novel reference-based knowledge **enhancement** model called Reference Knowledgeable Network (RekNet), which simulates human reading strategies to refine critical information from the passage and quote explicit knowledge in necessity. In detail, RekNet refines finegrained critical information and defines it as Reference Span, then quotes explicit knowledge quadruples by the co-occurrence information of Reference Span and candidates. The proposed RekNet is evaluated on three multi-choice MRC benchmarks: RACE, DREAM and Cosmos QA, obtaining consistent and remarkable performance improvement with observable statistical significance level over strong baselines. Our code is available at https://github.com/Yilin1111/RekNet.  
### Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v3 [cs.CV] UPDATED)
- Authors : Alessandro Sebastianelli, Artur Nowakowski, Erika Puglisi, Maria Pia, Del Rosso, Jamila Mifdal, Fiora Pirri, Pierre Philippe, Silvia Liberata
- Link : [http://arxiv.org/abs/2106.12226](http://arxiv.org/abs/2106.12226)
> ABSTRACT  :  Cloud removal is a relevant topic in Remote Sensing as it fosters the usability of high-resolution optical images for Earth monitoring and study. Related techniques have been analyzed for years with a progressively clearer view of the appropriate methods to adopt, from multi-spectral to inpainting methods. Recent applications of deep generative models and sequence-to-sequence-based models have proved their capability to advance the field significantly. Nevertheless, there are still some gaps, mostly related to the amount of cloud coverage, the density and thickness of clouds, and the occurred temporal landscape changes. In this work, we fill some of these gaps by introducing a novel multi-modal method that uses different sources of information, both spatial and temporal, to restore the whole optical scene of interest. The proposed method introduces an innovative deep model, using the outcomes of both temporal-sequence blending and direct translation from Synthetic Aperture Radar (SAR) to optical images to obtain a pixel-wise **restoration** of the whole scene. The advantage of our approach is demonstrated across a variety of atmospheric conditions tested on a dataset we have generated and made available. Quantitative and qualitative results prove that the proposed method obtains cloud-free images, preserving scene details without resorting to a huge portion of a clean image and coping with landscape changes.  
### Self-Supervised Pre-Training of **Swin** Transformers for 3D Medical Image Analysis. (arXiv:2111.14791v2 [cs.CV] UPDATED)
- Authors : Yucheng Tang, Dong Yang, Wenqi Li, Holger Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, Ali Hatamizadeh
- Link : [http://arxiv.org/abs/2111.14791](http://arxiv.org/abs/2111.14791)
> ABSTRACT  :  Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed **Swin** UNEt TRansformers (**Swin** UNETR), with a hierarchical encoder for self-supervised pre-training; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art (i.e. ranked 1st) on the public test leaderboards of both MSD and BTCV datasets. Code: https://monai.io/research/swin-unetr  
### Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v3 [cs.CV] UPDATED)
- Authors : Xiuwei Xu, Yifan Wang, Yu Zheng, Yongming Rao, Jie Zhou, Jiwen Lu
- Link : [http://arxiv.org/abs/2203.05238](http://arxiv.org/abs/2203.05238)
> ABSTRACT  :  In this paper, we propose a weakly-supervised approach for 3D object detection, which makes it possible to train a strong 3D detector with position-level annotations (i.e. annotations of object centers). In order to remedy the information loss from box annotations to centers, our method, namely Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak labels into fully-annotated virtual scenes as stronger supervision, and in turn utilizes the perfect virtual labels to complement and refine the real labels. Specifically, we first assemble 3D shapes into physically reasonable virtual scenes according to the coarse scene layout extracted from position-level annotations. Then we go back to reality by applying a virtual-to-real domain adaptation method, which refine the weak labels and additionally supervise the training of detector with the virtual scenes. Furthermore, we propose a more challenging benckmark for indoor 3D object detection with more diversity in object sizes to better show the potential of BR. With less than 5% of the labeling labor, we achieve comparable detection performance with some popular fully-supervised approaches on the widely used ScanNet dataset. Code is available at: https://github.com/wyf-ACCEPT/BackToReality  
### FullSubNet+: Channel Attention FullSubNet with Complex Spectrograms for Speech **Enhancement**. (arXiv:2203.12188v2 [cs.SD] UPDATED)
- Authors : Jun Chen, Zilin Wang, Deyi Tuo, Zhiyong Wu, Shiyin Kang, Helen Meng
- Link : [http://arxiv.org/abs/2203.12188](http://arxiv.org/abs/2203.12188)
> ABSTRACT  :  Previously proposed FullSubNet has achieved outstanding performance in Deep Noise Suppression (DNS) Challenge and attracted much attention. However, it still encounters issues such as input-output mismatch and coarse processing for frequency bands. In this paper, we propose an extended single-channel real-time speech **enhancement** framework called FullSubNet+ with following significant improvements. First, we design a lightweight multi-scale time sensitive channel attention (MulCA) module which adopts multi-scale convolution and channel attention mechanism to help the network focus on more discriminative frequency bands for noise reduction. Then, to make full use of the phase information in noisy speech, our model takes all the magnitude, real and imaginary spectrograms as inputs. Moreover, by replacing the long short-term memory (LSTM) layers in original full-band model with stacked temporal convolutional network (TCN) blocks, we design a more efficient full-band module called full-band extractor. The experimental results in DNS Challenge dataset show the superior performance of our FullSubNet+, which reaches the state-of-the-art (SOTA) performance and outperforms other existing speech **enhancement** approaches.  
# Paper List
---
## cs.CV
---
**202** new papers in cs.CV:-) 
1. A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration. (arXiv:2203.13834v1 [cs.CV])
2. Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas. (arXiv:2203.13838v1 [cs.CV])
3. Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets. (arXiv:2203.13856v1 [eess.IV])
4. TimeReplayer: Unlocking the Potential of Event Cameras for Video Interpolation. (arXiv:2203.13859v1 [cs.CV])
5. FD-SLAM: 3-D Reconstruction Using Features and Dense Matching. (arXiv:2203.13861v1 [cs.CV])
6. Intelligent Masking: Deep Q-Learning for Context Encoding in Medical Image Analysis. (arXiv:2203.13865v1 [cs.CV])
7. Self-supervised Semantic Segmentation Grounded in Visual Concepts. (arXiv:2203.13868v1 [cs.CV])
8. Self-supervised machine learning model for analysis of nanowire morphologies from transmission electron microscopy images. (arXiv:2203.13875v1 [cond-mat.mtrl-sci])
9. Reinforcement Learning with Action-Free Pre-Training from Videos. (arXiv:2203.13880v1 [cs.CV])
10. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v1 [cs.LG])
11. Sylph: A Hypernetwork Framework for Incremental Few-shot Object Detection. (arXiv:2203.13903v1 [cs.CV])
12. Concept Embedding Analysis: A Review. (arXiv:2203.13909v1 [cs.LG])
13. A Cross-Domain Approach for Continuous Impression Recognition from Dyadic Audio-Visual-Physio Signals. (arXiv:2203.13932v1 [cs.MM])
14. SolidGen: An Autoregressive Model for Direct B-rep Synthesis. (arXiv:2203.13944v1 [cs.LG])
15. AI-augmented histopathologic review using image analysis to optimize DNA yield and tumor purity from FFPE slides. (arXiv:2203.13948v1 [cs.CV])
16. GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection. (arXiv:2203.13954v1 [cs.CV])
17. Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution. (arXiv:2203.13963v1 [eess.IV])
18. Fusing Global and Local Features for Generalized AI-Synthesized Image Detection. (arXiv:2203.13964v1 [cs.CV])
19. Exploring Self-Attention for Visual Intersection Classification. (arXiv:2203.13977v1 [cs.CV])
20. Current Source Localization Using Deep Prior with Depth Weighting. (arXiv:2203.13981v1 [eess.SP])
21. Does Monocular Depth Estimation Provide Better Pre-training than Classification for Semantic Segmentation?. (arXiv:2203.13987v1 [cs.CV])
22. RSCFed: Random Sampling Consensus Federated Semi-supervised Learning. (arXiv:2203.13993v1 [cs.CV])
23. Learning to Predict RNA Sequence Expressions from Whole Slide Images with Applications for Search and Classification. (arXiv:2203.13997v1 [eess.IV])
24. Knowledge Distillation with the Reused Teacher Classifier. (arXiv:2203.14001v1 [cs.CV])
25. Learn to Adapt for Monocular Depth Estimation. (arXiv:2203.14005v1 [cs.CV])
26. EYNet: Extended YOLO for Airport Detection in Remote Sensing Images. (arXiv:2203.14007v1 [cs.CV])
27. SGDR: Semantic-guided Disentangled Representation for Unsupervised Cross-modality Medical Image Segmentation. (arXiv:2203.14025v1 [cs.CV])
28. Medicinal Boxes Recognition on a Deep Transfer Learning Augmented Reality Mobile Application. (arXiv:2203.14031v1 [cs.CV])
29. Visual Abductive Reasoning. (arXiv:2203.14040v1 [cs.CV])
30. Semantic Segmentation by Early Region Proxy. (arXiv:2203.14043v1 [cs.CV])
31. Adaptively Lighting up Facial Expression Crucial Regions via Local Non-Local Joint Network. (arXiv:2203.14045v1 [cs.CV])
32. A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies. (arXiv:2203.14046v1 [cs.CV])
33. Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds. (arXiv:2203.14048v1 [cs.CV])
34. FaceVerse: a Fine-grained and Detail-changeable 3D Neural Face Model from a Hybrid Dataset. (arXiv:2203.14057v1 [cs.CV])
35. Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture. (arXiv:2203.14065v1 [cs.CV])
36. Learning to Answer Questions in Dynamic Audio-Visual Scenarios. (arXiv:2203.14072v1 [cs.CV])
37. V3GAN: Decomposing Background, Foreground and Motion for Video Generation. (arXiv:2203.14074v1 [cs.CV])
38. Self-Supervised Point Cloud Representation Learning with Occlusion Auto-Encoder. (arXiv:2203.14084v1 [cs.CV])
39. Near-Infrared Depth-Independent Image Dehazing using Haar Wavelets. (arXiv:2203.14085v1 [cs.CV])
40. Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition. (arXiv:2203.14092v1 [cs.CV])
41. Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation. (arXiv:2203.14098v1 [cs.CV])
42. Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos. (arXiv:2203.14104v1 [cs.CV])
43. Probabilistic Registration for Gaussian Process 3D shape modelling in the presence of extensive missing data. (arXiv:2203.14113v1 [cs.CV])
44. Feature Selective Transformer for Semantic Image Segmentation. (arXiv:2203.14124v1 [cs.CV])
45. Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])
46. RGBD Object Tracking: An In-depth Review. (arXiv:2203.14134v1 [cs.CV])
47. Reverse Engineering of Imperceptible Adversarial Image Perturbations. (arXiv:2203.14145v1 [cs.CV])
48. Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image Matching. (arXiv:2203.14148v1 [cs.CV])
49. A Robust Optimization Method for Label Noisy Datasets Based on Adaptive Threshold: Adaptive-k. (arXiv:2203.14165v1 [cs.LG])
50. ThunderNet: Towards **Real-time** Generic Object Detection. (arXiv:1903.11752v3 [cs.CV] UPDATED)
51. Unsupervised **Low-light** Image **Enhancement** with Decoupled Networks. (arXiv:2005.02818v2 [eess.IV] UPDATED)
52. ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection. (arXiv:2008.06254v4 [cs.CV] UPDATED)
53. Learning to Compress Videos without Computing Motion. (arXiv:2009.14110v3 [eess.IV] UPDATED)
54. Sparse Representations of Positive Functions via First and Second-Order Pseudo-Mirror Descent. (arXiv:2011.07142v3 [stat.ML] UPDATED)
55. Unsupervised Segmentation for Terracotta Warrior Point Cloud (SRG-Net). (arXiv:2012.00433v2 [cs.CV] UPDATED)
56. Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v3 [cs.CV] UPDATED)
57. Are DNNs fooled by extremely unrecognizable images?. (arXiv:2012.03843v2 [cs.CV] UPDATED)
58. Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v3 [cs.CL] UPDATED)
59. From Handheld to Unconstrained Object Detection: a Weakly-supervised On-line Learning Approach. (arXiv:2012.14345v2 [cs.CV] UPDATED)
60. Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v8 [cs.CV] UPDATED)
61. Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery. (arXiv:2103.01623v4 [cs.CV] UPDATED)
62. Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. (arXiv:2103.04922v4 [cs.LG] UPDATED)
63. Stepwise Goal-Driven Networks for Trajectory Prediction. (arXiv:2103.14107v3 [cs.CV] UPDATED)
64. AlignMixup: Improving Representations By Interpolating Aligned Features. (arXiv:2103.15375v2 [cs.CV] UPDATED)
65. FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation. (arXiv:2103.17235v3 [cs.CV] UPDATED)
66. Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes. (arXiv:2104.07300v2 [cs.CV] UPDATED)
67. E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion. (arXiv:2104.07661v2 [cs.CV] UPDATED)
68. A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v5 [cs.CV] UPDATED)
69. Smaller Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning. (arXiv:2105.14106v3 [cs.CV] UPDATED)
70. Simultaneous Multi-View Object Recognition and Grasping in Open-Ended Domains. (arXiv:2106.01866v3 [cs.RO] UPDATED)
71. DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering. (arXiv:2106.03798v4 [cs.CV] UPDATED)
72. Structured Sparse R-CNN for Direct Scene Graph Generation. (arXiv:2106.10815v2 [cs.CV] UPDATED)
73. Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v3 [cs.CV] UPDATED)
74. Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation. (arXiv:2107.02718v4 [cs.CV] UPDATED)
75. Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers. (arXiv:2107.11472v3 [cs.LG] UPDATED)
76. Neural Rays for Occlusion-aware Image-based Rendering. (arXiv:2107.13421v2 [cs.CV] UPDATED)
77. **Dark**Lighter: Light Up the **Dark**ness for UAV Tracking. (arXiv:2107.14389v2 [cs.CV] UPDATED)
78. Statistical Dependency Guided Contrastive Learning for Multiple Labeling in Prenatal Ultrasound. (arXiv:2108.05055v2 [cs.CV] UPDATED)
79. YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v7 [cs.CV] UPDATED)
80. Iterative Filter Adaptive Network for Single Image Defocus Deblurring. (arXiv:2108.13610v2 [cs.CV] UPDATED)
81. WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v4 [cs.CL] UPDATED)
82. Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v2 [cs.CV] UPDATED)
83. Adaptive Attribute and Structure Subspace Clustering Network. (arXiv:2109.13742v2 [cs.CV] UPDATED)
84. DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v3 [cs.CV] UPDATED)
85. Learning Higher-Order Dynamics in Video-Based Cardiac Measurement. (arXiv:2110.03690v2 [eess.IV] UPDATED)
86. Salient ImageNet: How to discover spurious features in Deep Learning?. (arXiv:2110.04301v4 [cs.LG] UPDATED)
87. Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v4 [cs.CV] UPDATED)
88. TDACNN: Target-domain-free Domain Adaptation Convolutional Neural Network for Drift Compensation in Gas Sensors. (arXiv:2110.07509v3 [q-bio.QM] UPDATED)
89. A unifying framework for $n$-dimensional quasi-conformal mappings. (arXiv:2110.10437v2 [cs.CG] UPDATED)
90. Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v3 [cs.CV] UPDATED)
91. Learning Co-segmentation by Segment Swapping for Retrieval and Discovery. (arXiv:2110.15904v2 [cs.CV] UPDATED)
92. Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images. (arXiv:2111.11057v2 [cs.CV] UPDATED)
93. DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion. (arXiv:2111.11326v2 [cs.CV] UPDATED)
94. Mip-**NeRF** 360: Unbounded Anti-Aliased Neural Radiance Fields. (arXiv:2111.12077v3 [cs.CV] UPDATED)
95. Scaling Up Vision-Language Pre-training for Image Captioning. (arXiv:2111.12233v2 [cs.CV] UPDATED)
96. MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. (arXiv:2111.12707v2 [cs.CV] UPDATED)
97. Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation. (arXiv:2111.12903v3 [cs.CV] UPDATED)
98. Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation. (arXiv:2111.12940v2 [cs.CV] UPDATED)
99. **Swin**BERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v2 [cs.CV] UPDATED)
100. Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers. (arXiv:2111.13587v2 [cs.CV] UPDATED)
101. GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v2 [cs.CV] UPDATED)
102. Video Frame Interpolation Transformer. (arXiv:2111.13817v3 [cs.CV] UPDATED)
103. Deblur-**NeRF**: Neural Radiance Fields from Blurry Images. (arXiv:2111.14292v2 [cs.CV] UPDATED)
104. Multi-instance Point Cloud Registration by Efficient Correspondence Clustering. (arXiv:2111.14582v2 [cs.CV] UPDATED)
105. Self-Supervised Pre-Training of **Swin** Transformers for 3D Medical Image Analysis. (arXiv:2111.14791v2 [cs.CV] UPDATED)
106. Blended Diffusion for Text-driven Editing of Natural Images. (arXiv:2111.14818v2 [cs.CV] UPDATED)
107. Multi-domain Integrative **Swin** Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v6 [eess.IV] UPDATED)
108. ZZ-Net: A Universal Rotation Equivariant Architecture for 2D Point Clouds. (arXiv:2111.15341v2 [cs.CV] UPDATED)
109. 360MonoDepth: High-Resolution 360{\deg} Monocular Depth Estimation. (arXiv:2111.15669v2 [cs.CV] UPDATED)
110. The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification. (arXiv:2112.00412v3 [cs.CV] UPDATED)
111. MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions. (arXiv:2112.00431v2 [cs.CV] UPDATED)
112. Revisiting the Transferability of Supervised Pretraining: an MLP Perspective. (arXiv:2112.00496v3 [cs.CV] UPDATED)
113. N-ImageNet: Towards Robust, Fine-Grained Object Recognition with Event Cameras. (arXiv:2112.01041v2 [cs.CV] UPDATED)
114. Neural Head Avatars from Monocular RGB Videos. (arXiv:2112.01554v2 [cs.CV] UPDATED)
115. Efficient Two-Stage Detection of Human-Object Interactions with a Novel Unary-Pairwise Transformer. (arXiv:2112.01838v2 [cs.CV] UPDATED)
116. Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training. (arXiv:2112.03552v4 [cs.CV] UPDATED)
117. BACON: Band-limited Coordinate Networks for Multiscale Scene Representation. (arXiv:2112.04645v2 [cs.CV] UPDATED)
118. Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic Prior. (arXiv:2112.05077v2 [cs.CV] UPDATED)
119. Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v4 [cs.CV] UPDATED)
120. Active learning with MaskAL reduces annotation effort for training Mask R-CNN. (arXiv:2112.06586v3 [cs.CV] UPDATED)
121. Logically at Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v2 [cs.CV] UPDATED)
122. Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v2 [eess.IV] UPDATED)
123. Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization. (arXiv:2112.11177v3 [cs.CV] UPDATED)
124. Recur, Attend or Convolve? Frame Dependency Modeling Matters for Cross-Domain Robustness in Action Recognition. (arXiv:2112.12175v2 [cs.CV] UPDATED)
125. Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v2 [cs.SD] UPDATED)
126. Decoupling Makes Weakly Supervised Local Feature Better. (arXiv:2201.02861v2 [cs.CV] UPDATED)
127. A statistical shape model for radiation-free assessment and classification of craniosynostosis. (arXiv:2201.03288v2 [eess.IV] UPDATED)
128. Saliency Constrained Arbitrary Image Style Transfer using SIFT and DCNN. (arXiv:2201.05346v2 [cs.CV] UPDATED)
129. STURE: Spatial-Temporal Mutual Representation Learning for Robust Data Association in Online Multi-Object Tracking. (arXiv:2201.06824v3 [cs.CV] UPDATED)
130. CP-Net: Contour-Perturbed Reconstruction Network for Self-Supervised Point Cloud Learning. (arXiv:2201.08215v2 [cs.CV] UPDATED)
131. Exploring Global Diversity and Local Context for Video Summarization. (arXiv:2201.11345v2 [cs.CV] UPDATED)
132. RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures. (arXiv:2201.12763v2 [cs.CV] UPDATED)
133. Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion. (arXiv:2202.02000v3 [eess.IV] UPDATED)
134. Message Passing Neural PDE Solvers. (arXiv:2202.03376v2 [cs.LG] UPDATED)
135. One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v2 [cs.AI] UPDATED)
136. (2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering. (arXiv:2202.09277v2 [cs.CV] UPDATED)
137. MSSNet: Multi-Scale-Stage Network for Single Image Deblurring. (arXiv:2202.09652v2 [cs.CV] UPDATED)
138. A Self-Supervised Descriptor for Image Copy Detection. (arXiv:2202.10261v2 [cs.CV] UPDATED)
139. Self-Supervised Bulk Motion Artifact Removal in Optical Coherence Tomography Angiography. (arXiv:2202.10360v2 [cs.CV] UPDATED)
140. Vision-Language Pre-Training with Triple Contrastive Learning. (arXiv:2202.10401v4 [cs.CV] UPDATED)
141. M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction. (arXiv:2202.11884v2 [cs.RO] UPDATED)
142. NeuralHOFusion: Neural Volumetric Rendering under Human-object Interactions. (arXiv:2202.12825v3 [cs.CV] UPDATED)
143. Uncertainty-Aware Deep Multi-View Photometric Stereo. (arXiv:2202.13071v2 [cs.CV] UPDATED)
144. Multi-modal Alignment using Representation Codebook. (arXiv:2203.00048v3 [cs.CV] UPDATED)
145. Generalizable Person Re-Identification via Self-Supervised Batch Norm Test-Time Adaption. (arXiv:2203.00672v2 [cs.CV] UPDATED)
146. There is a Time and Place for Reasoning Beyond the Image. (arXiv:2203.00758v2 [cs.CV] UPDATED)
147. MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v3 [cs.CV] UPDATED)
148. CAFE: Learning to Condense Dataset by Aligning Features. (arXiv:2203.01531v2 [cs.CV] UPDATED)
149. Syntax-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2203.01601v3 [cs.CV] UPDATED)
150. TCTrack: Temporal Contexts for Aerial Tracking. (arXiv:2203.01885v3 [cs.CV] UPDATED)
151. Interactive Image Synthesis with Panoptic Layout Generation. (arXiv:2203.02104v3 [cs.CV] UPDATED)
152. HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v3 [cs.CV] UPDATED)
153. Machine Learning Applications in Lung Cancer Diagnosis, Treatment and Prognosis. (arXiv:2203.02794v3 [cs.LG] UPDATED)
154. Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos. (arXiv:2203.03014v2 [cs.CV] UPDATED)
155. CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild. (arXiv:2203.03089v2 [cs.CV] UPDATED)
156. Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer. (arXiv:2203.03121v2 [cs.CV] UPDATED)
157. End-to-End Semi-Supervised Learning for Video Action Detection. (arXiv:2203.04251v2 [cs.CV] UPDATED)
158. Adaptive Trajectory Prediction via Transferable GNN. (arXiv:2203.05046v2 [cs.CV] UPDATED)
159. Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v3 [cs.CV] UPDATED)
160. Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability. (arXiv:2203.05180v2 [cs.CV] UPDATED)
161. Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v3 [cs.CV] UPDATED)
162. Font Shape-to-Impression Translation. (arXiv:2203.05808v2 [cs.CV] UPDATED)
163. SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection. (arXiv:2203.06398v2 [cs.CV] UPDATED)
164. Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning. (arXiv:2203.06541v2 [cs.CV] UPDATED)
165. Masked Autoencoders for Point Cloud Self-supervised Learning. (arXiv:2203.06604v2 [cs.CV] UPDATED)
166. Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v5 [cs.CV] UPDATED)
167. Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v2 [cs.SD] UPDATED)
168. UnseenNet: Fast Training Detector for Any Unseen Concept. (arXiv:2203.08759v2 [cs.CV] UPDATED)
169. Visualizing Global Explanations of Point Cloud DNNs. (arXiv:2203.09505v2 [cs.CV] UPDATED)
170. CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance. (arXiv:2203.09887v2 [cs.CV] UPDATED)
171. Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking. (arXiv:2203.09948v2 [cs.CV] UPDATED)
172. Diffusion and Volume Maximization-Based Clustering of Highly Mixed Hyperspectral Images. (arXiv:2203.09992v2 [cs.CV] UPDATED)
173. Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation. (arXiv:2203.10026v2 [cs.CV] UPDATED)
174. TO-FLOW: Efficient Continuous Normalizing Flows with Temporal Optimization adjoint with Moving Speed. (arXiv:2203.10335v2 [cs.CV] UPDATED)
175. Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v2 [eess.IV] UPDATED)
176. Breast Cancer Induced Bone Osteolysis Prediction Using Temporal Variational Auto-Encoders. (arXiv:2203.10645v2 [eess.IV] UPDATED)
177. Boost Test-Time Performance with Closed-Loop Inference. (arXiv:2203.10853v2 [cs.CV] UPDATED)
178. MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer. (arXiv:2203.10981v2 [cs.CV] UPDATED)
179. Stereo Neural Vernier Caliper. (arXiv:2203.11018v2 [cs.CV] UPDATED)
180. Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v3 [cs.CV] UPDATED)
181. A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v2 [cs.CV] UPDATED)
182. GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v3 [cs.CV] UPDATED)
183. Scale-Equivalent Distillation for Semi-Supervised Object Detection. (arXiv:2203.12244v2 [cs.CV] UPDATED)
184. Ev-TTA: Test-Time Adaptation for Event-Based Object Recognition. (arXiv:2203.12247v2 [cs.CV] UPDATED)
185. DR.VIC: Decomposition and Reasoning for Video Individual Counting. (arXiv:2203.12335v2 [cs.CV] UPDATED)
186. Multi-label Transformer for Action Unit Detection. (arXiv:2203.12531v2 [cs.CV] UPDATED)
187. StructToken : Rethinking Semantic Segmentation with Structural Prior. (arXiv:2203.12612v2 [cs.CV] UPDATED)
188. Learning to generate line drawings that convey geometry and semantics. (arXiv:2203.12691v2 [cs.CV] UPDATED)
189. The Challenges of Continuous Self-Supervised Learning. (arXiv:2203.12710v2 [cs.CV] UPDATED)
190. UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection. (arXiv:2203.12745v2 [cs.CV] UPDATED)
191. Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v2 [cs.CV] UPDATED)
192. NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks. (arXiv:2203.12915v2 [cs.LG] UPDATED)
193. CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. (arXiv:2203.13009v2 [cs.CV] UPDATED)
194. Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v3 [cs.CV] UPDATED)
195. Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning. (arXiv:2203.13049v2 [cs.CV] UPDATED)
196. Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v3 [cs.CV] UPDATED)
197. Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v2 [cs.CV] UPDATED)
198. Multi-modal Multi-label Facial Action Unit Detection with Transformer. (arXiv:2203.13301v2 [cs.CV] UPDATED)
199. MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v2 [cs.CV] UPDATED)
200. A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration Framework. (arXiv:2003.06513v2 [cs.LG] CROSS LISTED)
201. Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV] CROSS LISTED)
202. Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration. (arXiv:2111.11581v1 [cs.LG] CROSS LISTED)
## eess.IV
---
**44** new papers in eess.IV:-) 
1. Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets. (arXiv:2203.13856v1 [eess.IV])
2. Self-supervised machine learning model for analysis of nanowire morphologies from transmission electron microscopy images. (arXiv:2203.13875v1 [cond-mat.mtrl-sci])
3. Ultrafast Ultrasound Imaging for 3D Shear Wave Absolute Vibro-Elastography. (arXiv:2203.13949v1 [eess.IV])
4. Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution. (arXiv:2203.13963v1 [eess.IV])
5. Learning to Predict RNA Sequence Expressions from Whole Slide Images with Applications for Search and Classification. (arXiv:2203.13997v1 [eess.IV])
6. Contrastive Graph Learning for Population-based fMRI Classification. (arXiv:2203.14044v1 [cs.LG])
7. Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])
8. Deep Polarimetric **HDR** Reconstruction. (arXiv:2203.14190v1 [eess.IV])
9. A Survey of Super-Resolution in Iris Biometrics with Evaluation of Dictionary-Learning. (arXiv:2203.14203v1 [cs.CV])
10. Enhancing Speckle Statistics for Imaging inside Scattering Media. (arXiv:2203.14214v1 [physics.optics])
11. Efficient and Degradation-Adaptive Network for Real-World Image Super-Resolution. (arXiv:2203.14216v1 [cs.CV])
12. Image quality assessment for machine learning tasks using meta-reinforcement learning. (arXiv:2203.14258v1 [eess.IV])
13. Diagnosis of COVID-19 Cases from Chest X-ray Images Using Deep Neural Network and LightGBM. (arXiv:2203.14275v1 [eess.IV])
14. Video Polyp Segmentation: A Deep Learning Perspective. (arXiv:2203.14291v1 [eess.IV])
15. MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation. (arXiv:2203.14341v1 [eess.IV])
16. Velocity continuation with Fourier neural operators for accelerated uncertainty quantification. (arXiv:2203.14386v1 [physics.geo-ph])
17. A Novel Remote Sensing Approach to Recognize and Monitor Red Palm Weevil in Date Palm Trees. (arXiv:2203.14476v1 [cs.CV])
18. Leveraging Clinically Relevant Biometric Constraints To Supervise A Deep Learning Model For The Accurate Caliper Placement To Obtain Sonographic Measurements Of The Fetal Brain. (arXiv:2203.14482v1 [eess.IV])
19. Adaptation to CT Reconstruction Kernels by Enforcing Cross-domain Feature Maps Consistency. (arXiv:2203.14616v1 [eess.IV])
20. Limited Parameter Denoising for Low-dose X-ray Computed Tomography Using Deep Reinforcement Learning. (arXiv:2203.14794v1 [eess.IV])
21. An attention mechanism based convolutional network for satellite precipitation downscaling over China. (arXiv:2203.14812v1 [cs.CV])
22. HUNIS: High-Performance Unsupervised Nuclei Instance Segmentation. (arXiv:2203.14887v1 [eess.IV])
23. RAVIR: A Dataset and Methodology for the Semantic Segmentation and Quantitative Analysis of Retinal Arteries and Veins in Infrared Reflectance Imaging. (arXiv:2203.14928v1 [eess.IV])
24. Differentiable Microscopy for Content and Task Aware Compressive Fluorescence Imaging. (arXiv:2203.14945v1 [eess.IV])
25. Unsupervised **Low-light** Image **Enhancement** with Decoupled Networks. (arXiv:2005.02818v2 [eess.IV] UPDATED)
26. Learning to Compress Videos without Computing Motion. (arXiv:2009.14110v3 [eess.IV] UPDATED)
27. FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation. (arXiv:2103.17235v3 [cs.CV] UPDATED)
28. Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v3 [cs.CV] UPDATED)
29. Iterative Filter Adaptive Network for Single Image Defocus Deblurring. (arXiv:2108.13610v2 [cs.CV] UPDATED)
30. Learning Higher-Order Dynamics in Video-Based Cardiac Measurement. (arXiv:2110.03690v2 [eess.IV] UPDATED)
31. Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v4 [cs.CV] UPDATED)
32. Multi-domain Integrative **Swin** Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v6 [eess.IV] UPDATED)
33. Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v2 [eess.IV] UPDATED)
34. A statistical shape model for radiation-free assessment and classification of craniosynostosis. (arXiv:2201.03288v2 [eess.IV] UPDATED)
35. Validation and Generalizability of Self-Supervised Image Reconstruction Methods for Undersampled MRI. (arXiv:2201.12535v2 [eess.IV] UPDATED)
36. Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion. (arXiv:2202.02000v3 [eess.IV] UPDATED)
37. Thermographic detection of internal defects using 2D photothermal super resolution reconstruction with sequential laser heating. (arXiv:2203.02060v2 [eess.IV] UPDATED)
38. HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v3 [cs.CV] UPDATED)
39. Machine Learning Applications in Lung Cancer Diagnosis, Treatment and Prognosis. (arXiv:2203.02794v3 [cs.LG] UPDATED)
40. Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v2 [eess.IV] UPDATED)
41. Breast Cancer Induced Bone Osteolysis Prediction Using Temporal Variational Auto-Encoders. (arXiv:2203.10645v2 [eess.IV] UPDATED)
42. Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v2 [cs.CV] UPDATED)
43. Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v3 [cs.CV] UPDATED)
44. MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v2 [cs.CV] UPDATED)
## cs.LG
---
**180** new papers in cs.LG:-) 
1. A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration. (arXiv:2203.13834v1 [cs.CV])
2. Optimal quantum kernels for small data classification. (arXiv:2203.13848v1 [quant-ph])
3. Quasi-Newton Iteration in Deterministic Policy Gradient. (arXiv:2203.13854v1 [cs.LG])
4. Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets. (arXiv:2203.13856v1 [eess.IV])
5. Intelligent Masking: Deep Q-Learning for Context Encoding in Medical Image Analysis. (arXiv:2203.13865v1 [cs.CV])
6. Data Selection Curriculum for Neural Machine Translation. (arXiv:2203.13867v1 [cs.CL])
7. Self-supervised machine learning model for analysis of nanowire morphologies from transmission electron microscopy images. (arXiv:2203.13875v1 [cond-mat.mtrl-sci])
8. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v1 [cs.LG])
9. A Conservative Q-Learning approach for handling distribution shift in sepsis treatment strategies. (arXiv:2203.13884v1 [cs.LG])
10. Predicting Peak Day and Peak Hour of Electricity Demand with Ensemble Machine Learning. (arXiv:2203.13886v1 [cs.LG])
11. Automatic Debiased Machine Learning for Dynamic Treatment Effects. (arXiv:2203.13887v1 [econ.EM])
12. Improving robustness of jet tagging algorithms with adversarial training. (arXiv:2203.13890v1 [physics.data-an])
13. Using Multiple Instance Learning for Explainable Solar Flare Prediction. (arXiv:2203.13896v1 [astro-ph.SR])
14. On efficient algorithms for computing near-best polynomial approximations to high-dimensional, Hilbert-valued functions from limited samples. (arXiv:2203.13908v1 [math.NA])
15. Concept Embedding Analysis: A Review. (arXiv:2203.13909v1 [cs.LG])
16. Theoretical Connection between Locally Linear Embedding, Factor Analysis, and Probabilistic PCA. (arXiv:2203.13911v1 [stat.ML])
17. SpeqNets: Sparsity-aware Permutation-equivariant Graph Networks. (arXiv:2203.13913v1 [cs.LG])
18. Canary Extraction in Natural Language Understanding Models. (arXiv:2203.13920v1 [cs.CL])
19. Offline Reinforcement Learning Under Value and Density-Ratio Realizability: the Power of Gaps. (arXiv:2203.13935v1 [cs.LG])
20. Neural Network Layers for Prediction of Positive Definite Elastic Stiffness Tensors. (arXiv:2203.13938v1 [cs.LG])
21. SolidGen: An Autoregressive Model for Direct B-rep Synthesis. (arXiv:2203.13944v1 [cs.LG])
22. Multi-Edge Server-Assisted Dynamic Federated Learning with an Optimized Floating Aggregation Point. (arXiv:2203.13950v1 [cs.LG])
23. Mode decomposition-based time-varying phase synchronization for fMRI Data. (arXiv:2203.13955v1 [eess.SP])
24. Tuning Particle Accelerators with Safety Constraints using Bayesian Optimization. (arXiv:2203.13968v1 [physics.acc-ph])
25. Current Source Localization Using Deep Prior with Depth Weighting. (arXiv:2203.13981v1 [eess.SP])
26. Transfer of codebook latent factors for cross-domain recommendation with non-overlapping data. (arXiv:2203.13995v1 [cs.IR])
27. EYNet: Extended YOLO for Airport Detection in Remote Sensing Images. (arXiv:2203.14007v1 [cs.CV])
28. Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey. (arXiv:2203.14009v1 [cs.LG])
29. Continual learning of quantum state classification with gradient episodic memory. (arXiv:2203.14032v1 [quant-ph])
30. Data Augmentation Strategies for Improving Sequential Recommender Systems. (arXiv:2203.14037v1 [cs.IR])
31. Contrastive Graph Learning for Population-based fMRI Classification. (arXiv:2203.14044v1 [cs.LG])
32. A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies. (arXiv:2203.14046v1 [cs.CV])
33. Joint Transformer/RNN Architecture for Gesture Typing in Indic Languages. (arXiv:2203.14049v1 [cs.LG])
34. Computationally efficient joint coordination of multiple electric vehicle charging points using reinforcement learning. (arXiv:2203.14078v1 [cs.AI])
35. Metropolis-Hastings Data Augmentation for Graph Neural Networks. (arXiv:2203.14082v1 [cs.LG])
36. Distributed data analytics. (arXiv:2203.14088v1 [cs.DC])
37. MQDD -- Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain. (arXiv:2203.14093v1 [cs.CL])
38. SlimFL: Federated Learning with Superposition Coding over Slimmable Neural Networks. (arXiv:2203.14094v1 [cs.LG])
39. A Roadmap for Big Model. (arXiv:2203.14101v1 [cs.LG])
40. Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos. (arXiv:2203.14104v1 [cs.CV])
41. Robust No-Regret Learning in Min-Max Stackelberg Games. (arXiv:2203.14126v1 [cs.GT])
42. Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])
43. Nash, Conley, and Computation: Impossibility and Incompleteness in Game Dynamics. (arXiv:2203.14129v1 [cs.GT])
44. A comparative analysis of Graph Neural Networks and commonly used machine learning algorithms on fake news detection. (arXiv:2203.14132v1 [cs.LG])
45. Discovering dynamical features of Hodgkin-Huxley-type model of physiological neuron using artificial neural network. (arXiv:2203.14138v1 [nlin.PS])
46. Efficient Global Robustness Certification of Neural Networks via Interleaving Twin-Network Encoding. (arXiv:2203.14141v1 [cs.LG])
47. Reverse Engineering of Imperceptible Adversarial Image Perturbations. (arXiv:2203.14145v1 [cs.CV])
48. NUNet: Deep Learning for Non-Uniform Super-Resolution of Turbulent Flows. (arXiv:2203.14154v1 [physics.flu-dyn])
49. How Do We Fail? Stress Testing Perception in Autonomous Vehicles. (arXiv:2203.14155v1 [cs.RO])
50. A Novel Neuromorphic Processors Realization of Spiking Deep Reinforcement Learning for Portfolio Management. (arXiv:2203.14159v1 [cs.LG])
51. A Robust Optimization Method for Label Noisy Datasets Based on Adaptive Threshold: Adaptive-k. (arXiv:2203.14165v1 [cs.LG])
52. Causal Modeling of Dynamical Systems. (arXiv:1803.08784v4 [cs.AI] UPDATED)
53. Greedy Algorithms for Sparse Sensor Placement via Deep Learning. (arXiv:1809.06025v6 [cs.LG] UPDATED)
54. FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability and Transparency. (arXiv:1909.05167v2 [cs.LG] UPDATED)
55. TriMap: Large-scale Dimensionality Reduction Using Triplets. (arXiv:1910.00204v2 [cs.LG] UPDATED)
56. Boosting Simple Learners. (arXiv:2001.11704v4 [cs.LG] UPDATED)
57. Distributional robustness of K-class estimators and the PULSE. (arXiv:2005.03353v3 [econ.EM] UPDATED)
58. Escaping Saddle Points Efficiently with Occupation-Time-Adapted Perturbations. (arXiv:2005.04507v3 [math.OC] UPDATED)
59. P-ADMMiRNN: Training RNN with Stable Convergence via An Efficient and Paralleled ADMM Approach. (arXiv:2006.05622v3 [cs.LG] UPDATED)
60. Uncovering the Folding Landscape of RNA Secondary Structure with Deep Graph Embeddings. (arXiv:2006.06885v3 [cs.LG] UPDATED)
61. Stochastic Low-rank Tensor Bandits for Multi-dimensional Online Decision Making. (arXiv:2007.15788v2 [stat.ML] UPDATED)
62. Community recovery in non-binary and temporal stochastic block models. (arXiv:2008.04790v4 [math.ST] UPDATED)
63. A Dynamical Central Limit Theorem for Shallow Neural Networks. (arXiv:2008.09623v3 [math.PR] UPDATED)
64. Statistically Robust, Risk-Averse Best Arm Identification in Multi-Armed Bandits. (arXiv:2008.13629v2 [cs.LG] UPDATED)
65. Lifelong Graph Learning. (arXiv:2009.00647v4 [cs.LG] UPDATED)
66. Data-Driven Learning of Geometric Scattering Networks. (arXiv:2010.02415v3 [cs.LG] UPDATED)
67. On Using Hamiltonian Monte Carlo Sampling for Reinforcement Learning Problems in High-dimension. (arXiv:2011.05927v3 [cs.LG] UPDATED)
68. Sparse Representations of Positive Functions via First and Second-Order Pseudo-Mirror Descent. (arXiv:2011.07142v3 [stat.ML] UPDATED)
69. Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v3 [cs.CV] UPDATED)
70. Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v3 [cs.CL] UPDATED)
71. Tighter expected generalization error bounds via Wasserstein distance. (arXiv:2101.09315v2 [stat.ML] UPDATED)
72. Meta-Reinforcement Learning for Reliable Communication in THz/VLC Wireless VR Networks. (arXiv:2102.12277v2 [cs.NI] UPDATED)
73. Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. (arXiv:2103.04922v4 [cs.LG] UPDATED)
74. DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation. (arXiv:2103.11109v6 [cs.LG] UPDATED)
75. NuPS: A Parameter Server for Machine Learning with Non-Uniform Parameter Access. (arXiv:2104.00501v3 [cs.DB] UPDATED)
76. State and Topology Estimation for Unobservable Distribution Systems using Deep Neural Networks. (arXiv:2104.07208v2 [cs.LG] UPDATED)
77. Neural Mean Discrepancy for Efficient Out-of-Distribution Detection. (arXiv:2104.11408v4 [cs.LG] UPDATED)
78. Hyperspherically Regularized Networks for Self-Supervision. (arXiv:2105.00925v4 [cs.LG] UPDATED)
79. Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads. (arXiv:2105.05720v5 [cs.DC] UPDATED)
80. Meta-Inductive Node Classification across Graphs. (arXiv:2105.06725v2 [cs.LG] UPDATED)
81. Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data. (arXiv:2105.07536v3 [stat.ML] UPDATED)
82. PDE-constrained Models with Neural Network Terms: Optimization and Global Convergence. (arXiv:2105.08633v3 [cs.LG] UPDATED)
83. Marginalising over Stationary Kernels with Bayesian Quadrature. (arXiv:2106.07452v2 [stat.ML] UPDATED)
84. Poisoning and Backdooring Contrastive Learning. (arXiv:2106.09667v2 [cs.LG] UPDATED)
85. Coarse to Fine Two-Stage Approach to Robust Tensor Completion of Visual Data. (arXiv:2106.10422v3 [cs.LG] UPDATED)
86. DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?. (arXiv:2106.12576v2 [cs.LG] UPDATED)
87. You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks. (arXiv:2106.13264v4 [cs.LG] UPDATED)
88. Combining Feature and Instance Attribution to Detect Artifacts. (arXiv:2107.00323v2 [cs.CL] UPDATED)
89. Prescient teleoperation of humanoid robots. (arXiv:2107.01281v3 [cs.RO] UPDATED)
90. Weighted Gaussian Process Bandits for Non-stationary Environments. (arXiv:2107.02371v4 [cs.LG] UPDATED)
91. Bootstrapping Generalization of Process Models Discovered From Event Data. (arXiv:2107.03876v2 [cs.AI] UPDATED)
92. Inferring Probabilistic Reward Machines from Non-Markovian Reward Processes for Reinforcement Learning. (arXiv:2107.04633v2 [cs.LG] UPDATED)
93. WeightScale: Interpreting Weight Change in Neural Networks. (arXiv:2107.07005v2 [cs.LG] UPDATED)
94. USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization Problems. (arXiv:2107.07508v3 [cs.LG] UPDATED)
95. Parametric Scattering Networks. (arXiv:2107.09539v2 [cs.LG] UPDATED)
96. Federated Learning using Smart Contracts on Blockchains, based on Reward Driven Approach. (arXiv:2107.10243v3 [cs.CR] UPDATED)
97. Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers. (arXiv:2107.11472v3 [cs.LG] UPDATED)
98. Embedding Signals on Knowledge Graphs with Unbalanced Diffusion Earth Mover's Distance. (arXiv:2107.12334v2 [cs.LG] UPDATED)
99. PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series. (arXiv:2108.00981v3 [cs.LG] UPDATED)
100. Deep Reinforcement Learning Based Networked Control with Network Delays for Signal Temporal Logic Specifications. (arXiv:2108.01317v3 [eess.SY] UPDATED)
101. SMOTified-GAN for class imbalanced pattern classification problems. (arXiv:2108.03235v2 [cs.LG] UPDATED)
102. Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100. (arXiv:2108.04062v2 [cs.LG] UPDATED)
103. WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v4 [cs.CL] UPDATED)
104. On the regularized risk of distributionally robust learning over deep neural networks. (arXiv:2109.06294v2 [math.OC] UPDATED)
105. FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining. (arXiv:2109.07323v2 [cs.IR] UPDATED)
106. Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v2 [cs.CV] UPDATED)
107. The Role of Tactile Sensing in Learning and Deploying Grasp Refinement Algorithms. (arXiv:2109.11234v2 [cs.RO] UPDATED)
108. Adaptive Attribute and Structure Subspace Clustering Network. (arXiv:2109.13742v2 [cs.CV] UPDATED)
109. DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v3 [cs.CV] UPDATED)
110. Geometric and Physical Quantities Improve E(3) Equivariant Message Passing. (arXiv:2110.02905v3 [cs.LG] UPDATED)
111. Learning Higher-Order Dynamics in Video-Based Cardiac Measurement. (arXiv:2110.03690v2 [eess.IV] UPDATED)
112. Salient ImageNet: How to discover spurious features in Deep Learning?. (arXiv:2110.04301v4 [cs.LG] UPDATED)
113. A neural simulation-based inference approach for characterizing the Galactic Center $\gamma$-ray excess. (arXiv:2110.06931v2 [astro-ph.HE] UPDATED)
114. Joint Gaussian Graphical Model Estimation: A Survey. (arXiv:2110.10281v2 [stat.ME] UPDATED)
115. Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs. (arXiv:2110.10545v2 [cs.LG] UPDATED)
116. Variational Predictive Routing with Nested Subjective Timescales. (arXiv:2110.11236v2 [cs.LG] UPDATED)
117. Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v3 [cs.CV] UPDATED)
118. Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations. (arXiv:2110.12088v2 [cs.LG] UPDATED)
119. Towards the D-Optimal Online Experiment Design for Recommender Selection. (arXiv:2110.12132v2 [cs.IR] UPDATED)
120. CAN-PINN: A Fast Physics-Informed Neural Network Based on Coupled-Automatic-Numerical Differentiation Method. (arXiv:2110.15832v2 [cs.LG] UPDATED)
121. InQSS: a speech intelligibility and quality assessment model using a multi-task learning network. (arXiv:2111.02585v2 [cs.SD] UPDATED)
122. Perturbational Complexity by Distribution Mismatch: A Systematic Analysis of Reinforcement Learning in Reproducing Kernel Hilbert Space. (arXiv:2111.03469v2 [cs.LG] UPDATED)
123. Beyond NDCG: behavioral testing of recommender systems with RecList. (arXiv:2111.09963v2 [cs.IR] UPDATED)
124. DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion. (arXiv:2111.11326v2 [cs.CV] UPDATED)
125. MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. (arXiv:2111.12707v2 [cs.CV] UPDATED)
126. Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation. (arXiv:2111.12940v2 [cs.CV] UPDATED)
127. Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers. (arXiv:2111.13587v2 [cs.CV] UPDATED)
128. Self-Supervised Pre-Training of **Swin** Transformers for 3D Medical Image Analysis. (arXiv:2111.14791v2 [cs.CV] UPDATED)
129. Blended Diffusion for Text-driven Editing of Natural Images. (arXiv:2111.14818v2 [cs.CV] UPDATED)
130. Multi-domain Integrative **Swin** Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v6 [eess.IV] UPDATED)
131. CO-SNE: Dimensionality Reduction and Visualization for Hyperbolic Data. (arXiv:2111.15037v2 [cs.LG] UPDATED)
132. ZZ-Net: A Universal Rotation Equivariant Architecture for 2D Point Clouds. (arXiv:2111.15341v2 [cs.CV] UPDATED)
133. Public Data-Assisted Mirror Descent for Private Model Training. (arXiv:2112.00193v2 [cs.LG] UPDATED)
134. Efficient Two-Stage Detection of Human-Object Interactions with a Novel Unary-Pairwise Transformer. (arXiv:2112.01838v2 [cs.CV] UPDATED)
135. BACON: Band-limited Coordinate Networks for Multiscale Scene Representation. (arXiv:2112.04645v2 [cs.CV] UPDATED)
136. Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic Prior. (arXiv:2112.05077v2 [cs.CV] UPDATED)
137. Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v4 [cs.CV] UPDATED)
138. Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v2 [cs.SD] UPDATED)
139. PowerGear: Early-Stage Power Estimation in FPGA HLS via Heterogeneous Edge-Centric GNNs. (arXiv:2201.10114v2 [cs.LG] UPDATED)
140. Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion. (arXiv:2202.02000v3 [eess.IV] UPDATED)
141. Message Passing Neural PDE Solvers. (arXiv:2202.03376v2 [cs.LG] UPDATED)
142. One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v2 [cs.AI] UPDATED)
143. (2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering. (arXiv:2202.09277v2 [cs.CV] UPDATED)
144. Cross-Task Knowledge Distillation in Multi-Task Recommendation. (arXiv:2202.09852v2 [cs.IR] UPDATED)
145. A Self-Supervised Descriptor for Image Copy Detection. (arXiv:2202.10261v2 [cs.CV] UPDATED)
146. COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v2 [cs.CL] UPDATED)
147. Debugging Differential Privacy: A Case Study for Privacy Auditing. (arXiv:2202.12219v2 [cs.LG] UPDATED)
148. Machine Learning Applications in Lung Cancer Diagnosis, Treatment and Prognosis. (arXiv:2203.02794v3 [cs.LG] UPDATED)
149. HEAR 2021: Holistic Evaluation of Audio Representations. (arXiv:2203.03022v2 [cs.SD] UPDATED)
150. Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer. (arXiv:2203.03466v2 [cs.LG] UPDATED)
151. Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v3 [cs.CV] UPDATED)
152. Generalized Bandit Regret Minimizer Framework in Imperfect Information Extensive-Form Game. (arXiv:2203.05920v2 [cs.LG] UPDATED)
153. The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning. (arXiv:2203.06498v3 [cs.LG] UPDATED)
154. The Role of Local Steps in Local SGD. (arXiv:2203.06798v2 [cs.LG] UPDATED)
155. Simplicial Attention Neural Networks. (arXiv:2203.07485v2 [cs.LG] UPDATED)
156. Improving Word Translation via Two-Stage Contrastive Learning. (arXiv:2203.08307v3 [cs.CL] UPDATED)
157. PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks. (arXiv:2203.09289v2 [cs.LG] UPDATED)
158. Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking. (arXiv:2203.09948v2 [cs.CV] UPDATED)
159. Diffusion and Volume Maximization-Based Clustering of Highly Mixed Hyperspectral Images. (arXiv:2203.09992v2 [cs.CV] UPDATED)
160. Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v2 [eess.IV] UPDATED)
161. ECAPA-TDNN for Multi-speaker Text-to-speech Synthesis. (arXiv:2203.10473v2 [cs.SD] UPDATED)
162. Encoder-Decoder Architecture for Supervised Dynamic Graph Learning: A Survey. (arXiv:2203.10480v2 [cs.LG] UPDATED)
163. TinyMLOps: Operational Challenges for Widespread Edge AI Adoption. (arXiv:2203.10923v2 [cs.LG] UPDATED)
164. BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Partition-Parallelism and Random Boundary Node Sampling. (arXiv:2203.10983v2 [cs.LG] UPDATED)
165. BEFANA: A Tool for Biodiversity-Ecosystem Functioning Assessment by Network Analysis. (arXiv:2203.11687v2 [q-bio.QM] UPDATED)
166. GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v3 [cs.CV] UPDATED)
167. Linearizing Transformer with Key-Value Memory Bank. (arXiv:2203.12644v2 [cs.CL] UPDATED)
168. A Supervised Machine Learning Approach for Sequence Based Protein-protein Interaction (PPI) Prediction. (arXiv:2203.12659v2 [cs.LG] UPDATED)
169. The Challenges of Continuous Self-Supervised Learning. (arXiv:2203.12710v2 [cs.CV] UPDATED)
170. Graph Neural Networks in Particle Physics: Implementations, Innovations, and Challenges. (arXiv:2203.12852v2 [hep-ex] UPDATED)
171. Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v2 [cs.CV] UPDATED)
172. NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks. (arXiv:2203.12915v2 [cs.LG] UPDATED)
173. Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v2 [cs.CV] UPDATED)
174. Statistic Selection and MCMC for Differentially Private Bayesian Estimation. (arXiv:2203.13377v2 [stat.ME] UPDATED)
175. A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v2 [cs.LG] UPDATED)
176. Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v2 [cs.SD] UPDATED)
177. A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration Framework. (arXiv:2003.06513v2 [cs.LG] CROSS LISTED)
178. Crop and weed classification based on AutoML. (arXiv:2010.14708v2 [cs.AI] CROSS LISTED)
179. Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV] CROSS LISTED)
180. Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration. (arXiv:2111.11581v1 [cs.LG] CROSS LISTED)
## cs.AI
---
**116** new papers in cs.AI:-) 
1. Reactive Whole-Body Obstacle Avoidance for Collision-Free Human-Robot Interaction with Topological Manifold Learning. (arXiv:2203.13821v1 [cs.RO])
2. A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration. (arXiv:2203.13834v1 [cs.CV])
3. Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas. (arXiv:2203.13838v1 [cs.CV])
4. TimeReplayer: Unlocking the Potential of Event Cameras for Video Interpolation. (arXiv:2203.13859v1 [cs.CV])
5. Self-supervised Semantic Segmentation Grounded in Visual Concepts. (arXiv:2203.13868v1 [cs.CV])
6. Reinforcement Learning with Action-Free Pre-Training from Videos. (arXiv:2203.13880v1 [cs.CV])
7. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v1 [cs.LG])
8. Concept Embedding Analysis: A Review. (arXiv:2203.13909v1 [cs.LG])
9. SpeqNets: Sparsity-aware Permutation-equivariant Graph Networks. (arXiv:2203.13913v1 [cs.LG])
10. Spatial Processing Front-End For Distant ASR Exploiting Self-Attention Channel Combinator. (arXiv:2203.13919v1 [eess.AS])
11. Canary Extraction in Natural Language Understanding Models. (arXiv:2203.13920v1 [cs.CL])
12. A Semi-Decoupled Approach to Fast and Optimal Hardware-Software Co-Design of Neural Accelerators. (arXiv:2203.13921v1 [cs.AR])
13. CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues. (arXiv:2203.13926v1 [cs.CL])
14. A Meta Survey of Quality Evaluation Criteria in Explanation Methods. (arXiv:2203.13929v1 [cs.AI])
15. GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection. (arXiv:2203.13954v1 [cs.CV])
16. Augmenting Knowledge Graphs for Better Link Prediction. (arXiv:2203.13965v1 [cs.AI])
17. Learn to Adapt for Monocular Depth Estimation. (arXiv:2203.14005v1 [cs.CV])
18. EYNet: Extended YOLO for Airport Detection in Remote Sensing Images. (arXiv:2203.14007v1 [cs.CV])
19. Model Transformations for Ranking Functions and Total Preorders. (arXiv:2203.14018v1 [cs.AI])
20. Computationally efficient joint coordination of multiple electric vehicle charging points using reinforcement learning. (arXiv:2203.14078v1 [cs.AI])
21. Generalization in Automated Process Discovery: A Framework based on Event Log Patterns. (arXiv:2203.14079v1 [cs.AI])
22. Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition. (arXiv:2203.14092v1 [cs.CV])
23. SlimFL: Federated Learning with Superposition Coding over Slimmable Neural Networks. (arXiv:2203.14094v1 [cs.LG])
24. A Roadmap for Big Model. (arXiv:2203.14101v1 [cs.LG])
25. Lite Unified Modeling for Discriminative Reading Comprehension. (arXiv:2203.14103v1 [cs.CL])
26. Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos. (arXiv:2203.14104v1 [cs.CV])
27. Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v1 [cs.CV])
28. Reverse Engineering of Imperceptible Adversarial Image Perturbations. (arXiv:2203.14145v1 [cs.CV])
29. Collaborative Intelligent Reflecting Surface Networks with Multi-Agent Reinforcement Learning. (arXiv:2203.14152v1 [cs.MA])
30. How Do We Fail? Stress Testing Perception in Autonomous Vehicles. (arXiv:2203.14155v1 [cs.RO])
31. SpeechSplit 2.0: Unsupervised speech disentanglement for voice conversion Without tuning autoencoder Bottlenecks. (arXiv:2203.14156v1 [eess.AS])
32. A Robust Optimization Method for Label Noisy Datasets Based on Adaptive Threshold: Adaptive-k. (arXiv:2203.14165v1 [cs.LG])
33. Mysteries of Visual Experience. (arXiv:1604.08612v6 [q-bio.NC] UPDATED)
34. Causal Modeling of Dynamical Systems. (arXiv:1803.08784v4 [cs.AI] UPDATED)
35. Listening for Sirens: Locating and Classifying Acoustic Alarms in City Scenes. (arXiv:1810.04989v2 [cs.SD] UPDATED)
36. FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability and Transparency. (arXiv:1909.05167v2 [cs.LG] UPDATED)
37. Efficient Conformance Checking using Approximate Alignment Computation with Tandem Repeats. (arXiv:2004.01781v2 [cs.SE] UPDATED)
38. Achilles Heels for AGI/ASI via Decision Theoretic Adversaries. (arXiv:2010.05418v4 [cs.AI] UPDATED)
39. Visibility Optimization for Surveillance-Evasion Games. (arXiv:2010.09001v2 [cs.AI] UPDATED)
40. Unsupervised Segmentation for Terracotta Warrior Point Cloud (SRG-Net). (arXiv:2012.00433v2 [cs.CV] UPDATED)
41. Reference Knowledgeable Network for Machine Reading Comprehension. (arXiv:2012.03709v3 [cs.CL] UPDATED)
42. Predicting Events in MOBA Games: Prediction, Attribution, and Evaluation. (arXiv:2012.09424v5 [cs.AI] UPDATED)
43. A Benchmark and Comprehensive Survey on Knowledge Graph Entity Alignment via Representation Learnin. (arXiv:2103.15059v3 [cs.AI] UPDATED)
44. NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v5 [cs.CL] UPDATED)
45. A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v5 [cs.CV] UPDATED)
46. Trajectory Optimization of Chance-Constrained Nonlinear Stochastic Systems for Motion Planning Under Uncertainty. (arXiv:2106.02801v2 [cs.RO] UPDATED)
47. Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v3 [cs.CV] UPDATED)
48. DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?. (arXiv:2106.12576v2 [cs.LG] UPDATED)
49. You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks. (arXiv:2106.13264v4 [cs.LG] UPDATED)
50. Weighted Gaussian Process Bandits for Non-stationary Environments. (arXiv:2107.02371v4 [cs.LG] UPDATED)
51. Bootstrapping Generalization of Process Models Discovered From Event Data. (arXiv:2107.03876v2 [cs.AI] UPDATED)
52. WeightScale: Interpreting Weight Change in Neural Networks. (arXiv:2107.07005v2 [cs.LG] UPDATED)
53. Human Perception of Audio Deepfakes. (arXiv:2107.09667v3 [cs.HC] UPDATED)
54. A Theory of Consciousness from a Theoretical Computer Science Perspective: Insights from the Conscious Turing Machine. (arXiv:2107.13704v8 [cs.AI] UPDATED)
55. SMOTified-GAN for class imbalanced pattern classification problems. (arXiv:2108.03235v2 [cs.LG] UPDATED)
56. Completion and Augmentation based Spatiotemporal Deep Learning Approach for Short-Term Metro Origin-Destination Matrix Prediction under Limited Observable Data. (arXiv:2108.03900v7 [cs.AI] UPDATED)
57. Multilingual Multi-Aspect Explainability Analyses on Machine Reading Comprehension Models. (arXiv:2108.11574v2 [cs.CL] UPDATED)
58. WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v4 [cs.CL] UPDATED)
59. It is AI's Turn to Ask Humans a Question: Question-Answer Pair Generation for Children's Story Books. (arXiv:2109.03423v4 [cs.CL] UPDATED)
60. Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideation. (arXiv:2109.10149v4 [cs.HC] UPDATED)
61. Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v2 [cs.CV] UPDATED)
62. The Role of Tactile Sensing in Learning and Deploying Grasp Refinement Algorithms. (arXiv:2109.11234v2 [cs.RO] UPDATED)
63. DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v3 [cs.CV] UPDATED)
64. Geometric and Physical Quantities Improve E(3) Equivariant Message Passing. (arXiv:2110.02905v3 [cs.LG] UPDATED)
65. Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction. (arXiv:2110.08345v2 [cs.CL] UPDATED)
66. Variational Predictive Routing with Nested Subjective Timescales. (arXiv:2110.11236v2 [cs.LG] UPDATED)
67. Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v3 [cs.CV] UPDATED)
68. Bootstrapping Concept Formation in Small Neural Networks. (arXiv:2110.13665v2 [cs.AI] UPDATED)
69. Multi-Agent Advisor Q-Learning. (arXiv:2111.00345v3 [cs.AI] UPDATED)
70. Beyond NDCG: behavioral testing of recommender systems with RecList. (arXiv:2111.09963v2 [cs.IR] UPDATED)
71. MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. (arXiv:2111.12707v2 [cs.CV] UPDATED)
72. Self-Supervised Pre-Training of **Swin** Transformers for 3D Medical Image Analysis. (arXiv:2111.14791v2 [cs.CV] UPDATED)
73. The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification. (arXiv:2112.00412v3 [cs.CV] UPDATED)
74. MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions. (arXiv:2112.00431v2 [cs.CV] UPDATED)
75. Efficient Two-Stage Detection of Human-Object Interactions with a Novel Unary-Pairwise Transformer. (arXiv:2112.01838v2 [cs.CV] UPDATED)
76. Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training. (arXiv:2112.03552v4 [cs.CV] UPDATED)
77. Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v4 [cs.CV] UPDATED)
78. Safe Equilibrium. (arXiv:2201.04266v5 [cs.GT] UPDATED)
79. Neural Network Compression of ACAS Xu Early Prototype is Unsafe: Closed-Loop Verification through Quantized State Backreachability. (arXiv:2201.06626v3 [math.NA] UPDATED)
80. Learning to Act with Affordance-Aware Multimodal Neural SLAM. (arXiv:2201.09862v3 [cs.RO] UPDATED)
81. Exploring Global Diversity and Local Context for Video Summarization. (arXiv:2201.11345v2 [cs.CV] UPDATED)
82. Validation and Generalizability of Self-Supervised Image Reconstruction Methods for Undersampled MRI. (arXiv:2201.12535v2 [eess.IV] UPDATED)
83. Auto-ABSA: Automatic Detection of Aspects in Aspect-Based Sentiment Analysis. (arXiv:2202.00484v2 [cs.CL] UPDATED)
84. One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v2 [cs.AI] UPDATED)
85. (2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering. (arXiv:2202.09277v2 [cs.CV] UPDATED)
86. Cross-Task Knowledge Distillation in Multi-Task Recommendation. (arXiv:2202.09852v2 [cs.IR] UPDATED)
87. COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v2 [cs.CL] UPDATED)
88. M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction. (arXiv:2202.11884v2 [cs.RO] UPDATED)
89. Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction. (arXiv:2202.12109v2 [cs.CL] UPDATED)
90. Multi-modal Alignment using Representation Codebook. (arXiv:2203.00048v3 [cs.CV] UPDATED)
91. There is a Time and Place for Reasoning Beyond the Image. (arXiv:2203.00758v2 [cs.CV] UPDATED)
92. Leveraging Pre-trained BERT for Audio Captioning. (arXiv:2203.02838v2 [eess.AS] UPDATED)
93. HEAR 2021: Holistic Evaluation of Audio Representations. (arXiv:2203.03022v2 [cs.SD] UPDATED)
94. Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v3 [cs.CV] UPDATED)
95. Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v5 [cs.CV] UPDATED)
96. Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v2 [cs.SD] UPDATED)
97. Improving Word Translation via Two-Stage Contrastive Learning. (arXiv:2203.08307v3 [cs.CL] UPDATED)
98. M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization. (arXiv:2203.09707v2 [cs.SE] UPDATED)
99. Data Smells: Categories, Causes and Consequences, and Detection of Suspicious Data in AI-based Systems. (arXiv:2203.10384v2 [cs.SE] UPDATED)
100. Fictitious Play with Maximin Initialization. (arXiv:2203.10774v3 [cs.GT] UPDATED)
101. Unsupervised Heterophilous Network Embedding via r-Ego Network Discrimination. (arXiv:2203.10866v2 [cs.SI] UPDATED)
102. BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Partition-Parallelism and Random Boundary Node Sampling. (arXiv:2203.10983v2 [cs.LG] UPDATED)
103. GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v3 [cs.CV] UPDATED)
104. FullSubNet+: Channel Attention FullSubNet with Complex Spectrograms for Speech **Enhancement**. (arXiv:2203.12188v2 [cs.SD] UPDATED)
105. Scale-Equivalent Distillation for Semi-Supervised Object Detection. (arXiv:2203.12244v2 [cs.CV] UPDATED)
106. Certified Symmetry and Dominance Breaking for Combinatorial Optimisation. (arXiv:2203.12275v2 [cs.AI] UPDATED)
107. Asynchronous Reinforcement Learning for Real-Time Control of Physical Robots. (arXiv:2203.12759v2 [cs.RO] UPDATED)
108. A platform for causal knowledge representation and inference in industrial fault diagnosis based on cubic DUCG. (arXiv:2203.12802v2 [cs.AI] UPDATED)
109. MERLIN -- Malware Evasion with Reinforcement LearnINg. (arXiv:2203.12980v2 [cs.CR] UPDATED)
110. MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v2 [cs.CV] UPDATED)
111. Learning Relational Rules from Rewards. (arXiv:2203.13599v2 [cs.AI] UPDATED)
112. Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v2 [cs.SD] UPDATED)
113. A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration Framework. (arXiv:2003.06513v2 [cs.LG] CROSS LISTED)
114. Crop and weed classification based on AutoML. (arXiv:2010.14708v2 [cs.AI] CROSS LISTED)
115. Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV] CROSS LISTED)
116. Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration. (arXiv:2111.11581v1 [cs.LG] CROSS LISTED)

