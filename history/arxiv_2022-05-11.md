# Your interest papers
---
## cs.CV
---
### Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network. (arXiv:2205.04721v1 [eess.IV])
- Authors : Dasong Li, Yi Zhang, Ka Lung, Xiaogang Wang, Hongwei Qin, Hongsheng Li
- Link : [http://arxiv.org/abs/2205.04721](http://arxiv.org/abs/2205.04721)
> ABSTRACT  :  With the growing popularity of smartphones, capturing high-quality images is of vital importance to smartphones. The cameras of smartphones have small apertures and small sensor cells, which lead to the noisy images in **low light** environment. Denoising based on a burst of multiple frames generally outperforms single frame denoising but with the larger compututional cost. In this paper, we propose an efficient yet effective burst denoising system. We adopt a three-stage design: noise prior integration, multi-frame alignment and multi-frame denoising. First, we integrate noise prior by pre-processing raw signals into a variance-stabilization space, which allows using a small-scale network to achieve competitive performance. Second, we observe that it is essential to adopt an explicit alignment for burst denoising, but it is not necessary to integrate a learning-based method to perform multi-frame alignment. Instead, we resort to a conventional and efficient alignment method and combine it with our multi-frame denoising network. At last, we propose a denoising strategy that processes multiple frames sequentially. Sequential denoising avoids filtering a large number of frames by decomposing multiple frames denoising into several efficient sub-network denoising. As for each sub-network, we propose an efficient multi-frequency denoising network to remove noise of different frequencies. Our three-stage design is efficient and shows strong performance on burst denoising. Experiments on synthetic and real raw datasets demonstrate that our method outperforms state-of-the-art methods, with less computational cost. Furthermore, the low complexity and high-quality performance make deployment on smartphones possible.  
### **NeRF**-Editing: Geometry Editing of Neural Radiance Fields. (arXiv:2205.04978v1 [cs.GR])
- Authors : Jie Yuan, Tian Sun, Kun Lai, Yuewen Ma, Rongfei Jia, Lin Gao
- Link : [http://arxiv.org/abs/2205.04978](http://arxiv.org/abs/2205.04978)
> ABSTRACT  :  Implicit neural rendering, especially Neural Radiance Field (**NeRF**), has shown great potential in novel view synthesis of a scene. However, current **NeRF**-based methods cannot enable users to perform user-controlled shape deformation in the scene. While existing works have proposed some approaches to modify the radiance field according to the user's constraints, the modification is limited to color editing or object translation and rotation. In this paper, we propose a method that allows users to perform controllable shape deformation on the implicit representation of the scene, and synthesizes the novel view images of the edited scene without re-training the network. Specifically, we establish a correspondence between the extracted explicit mesh representation and the **implicit neural representation** of the target scene. Users can first utilize well-developed mesh-based deformation methods to deform the mesh representation of the scene. Our method then utilizes user edits from the mesh representation to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining the rendering results of the edited scene. Extensive experiments demonstrate that our framework can achieve ideal editing results not only on synthetic data, but also on real scenes captured by users.  
### Keypoint**NeRF**: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints. (arXiv:2205.04992v1 [cs.CV])
- Authors : Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, Shunsuke Saito
- Link : [http://arxiv.org/abs/2205.04992](http://arxiv.org/abs/2205.04992)
> ABSTRACT  :  Image-based volumetric avatars using pixel-aligned features promise generalization to unseen poses and identities. Prior work leverages global spatial encodings and multi-view geometric consistency to reduce spatial ambiguity. However, global encodings often suffer from overfitting to the distribution of the training data, and it is difficult to learn multi-view consistent reconstruction from sparse views. In this work, we investigate common issues with existing spatial encodings and propose a simple yet highly effective approach to modeling high-fidelity volumetric avatars from sparse views. One of the key ideas is to encode relative spatial 3D information via sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and cross-dataset domain gap. Our approach outperforms state-of-the-art methods for head reconstruction. On human body reconstruction for unseen subjects, we also achieve performance comparable to prior work that uses a parametric human body model and temporal feature aggregation. Our experiments show that a majority of errors in prior work stem from an inappropriate choice of spatial encoding and thus we suggest a new direction for high-fidelity image-based avatar modeling. https://markomih.github.io/Keypoint**NeRF**  
### Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v1 [cs.CV])
- Authors : Chong Mou, Yanze Wu, Xintao Wang, Chao Dong, Jian Zhang, Ying Shan
- Link : [http://arxiv.org/abs/2205.05065](http://arxiv.org/abs/2205.05065)
> ABSTRACT  :  Interactive image **restoration** aims to restore images by adjusting several controlling coefficients, which determine the **restoration** strength. Existing methods are restricted in learning the controllable functions under the supervision of known degradation types and levels. They usually suffer from a severe performance drop when the real degradation is different from their assumptions. Such a limitation is due to the complexity of real-world degradations, which can not provide explicit supervision to the interactive modulation during training. However, how to realize the interactive modulation in real-world super-resolution has not yet been studied. In this work, we present a Metric Learning based Interactive Modulation for Real-World Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised degradation estimation strategy to estimate the degradation level in real-world scenarios. Instead of using known degradation levels as explicit supervision to the interactive mechanism, we propose a metric learning strategy to map the unquantifiable degradation levels in real-world scenarios to a metric space, which is trained in an unsupervised manner. Moreover, we introduce an anchor point strategy in the metric learning process to normalize the distribution of metric space. Extensive experiments demonstrate that the proposed MM-RealSR achieves excellent modulation and **restoration** performance in real-world super-resolution. Codes are available at https://github.com/TencentARC/MM-RealSR.  
### Learning Neural Light Fields with Ray-Space Embedding Networks. (arXiv:2112.01523v3 [cs.CV] UPDATED)
- Authors : Benjamin Attal, Bin Huang, Michael Zollhoefer, Johannes Kopf, Changil Kim
- Link : [http://arxiv.org/abs/2112.01523](http://arxiv.org/abs/2112.01523)
> ABSTRACT  :  Neural radiance fields (**NeRF**s) produce state-of-the-art view synthesis results. However, they are slow to render, requiring hundreds of network evaluations per pixel to approximate a volume rendering integral. Baking **NeRF**s into explicit data structures enables efficient rendering, but results in a large increase in memory footprint and, in many cases, a quality reduction. In this paper, we propose a novel neural light field representation that, in contrast, is compact and directly predicts integrated radiance along rays. Our method supports rendering with a single network evaluation per pixel for small baseline light field datasets and can also be applied to larger baselines with only a few evaluations per pixel. At the core of our approach is a ray-space embedding network that maps the 4D ray-space manifold into an intermediate, interpolable latent space. Our method achieves state-of-the-art quality on dense forward-facing datasets such as the Stanford Light Field dataset. In addition, for forward-facing scenes with sparser inputs we achieve results that are competitive with **NeRF**-based approaches in terms of quality while providing a better speed/quality/memory trade-off with far fewer network evaluations.  
### Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion **Enhancement**. (arXiv:2205.03569v2 [cs.CV] UPDATED)
- Authors : Bing Li, Jiaxin Chen, Dongming Zhang, Xiuguo Bao, Di Huang
- Link : [http://arxiv.org/abs/2205.03569](http://arxiv.org/abs/2205.03569)
> ABSTRACT  :  Compressed video action recognition has recently drawn growing attention, since it remarkably reduces the storage and computational cost via replacing raw videos by sparsely sampled RGB frames and compressed motion cues (e.g., motion vectors and residuals). However, this task severely suffers from the coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB and motion modalities. To address the two issues above, this paper proposes a novel framework, namely Attentive Cross-modal Interaction Network with Motion **Enhancement** (MEACI-Net). It follows the two-stream architecture, i.e. one for the RGB modality and the other for the motion modality. Particularly, the motion stream employs a multi-scale block embedded with a denoising module to enhance representation learning. The interaction between the two streams is then strengthened by introducing the Selective Motion Complement (SMC) and Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality with spatio-temporally attentive local motion features and CMA further combines the two modalities with selective feature augmentation. Extensive experiments on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the effectiveness and efficiency of MEACI-Net.  
## eess.IV
---
### Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network. (arXiv:2205.04721v1 [eess.IV])
- Authors : Dasong Li, Yi Zhang, Ka Lung, Xiaogang Wang, Hongwei Qin, Hongsheng Li
- Link : [http://arxiv.org/abs/2205.04721](http://arxiv.org/abs/2205.04721)
> ABSTRACT  :  With the growing popularity of smartphones, capturing high-quality images is of vital importance to smartphones. The cameras of smartphones have small apertures and small sensor cells, which lead to the noisy images in **low light** environment. Denoising based on a burst of multiple frames generally outperforms single frame denoising but with the larger compututional cost. In this paper, we propose an efficient yet effective burst denoising system. We adopt a three-stage design: noise prior integration, multi-frame alignment and multi-frame denoising. First, we integrate noise prior by pre-processing raw signals into a variance-stabilization space, which allows using a small-scale network to achieve competitive performance. Second, we observe that it is essential to adopt an explicit alignment for burst denoising, but it is not necessary to integrate a learning-based method to perform multi-frame alignment. Instead, we resort to a conventional and efficient alignment method and combine it with our multi-frame denoising network. At last, we propose a denoising strategy that processes multiple frames sequentially. Sequential denoising avoids filtering a large number of frames by decomposing multiple frames denoising into several efficient sub-network denoising. As for each sub-network, we propose an efficient multi-frequency denoising network to remove noise of different frequencies. Our three-stage design is efficient and shows strong performance on burst denoising. Experiments on synthetic and real raw datasets demonstrate that our method outperforms state-of-the-art methods, with less computational cost. Furthermore, the low complexity and high-quality performance make deployment on smartphones possible.  
### Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v1 [cs.CV])
- Authors : Chong Mou, Yanze Wu, Xintao Wang, Chao Dong, Jian Zhang, Ying Shan
- Link : [http://arxiv.org/abs/2205.05065](http://arxiv.org/abs/2205.05065)
> ABSTRACT  :  Interactive image **restoration** aims to restore images by adjusting several controlling coefficients, which determine the **restoration** strength. Existing methods are restricted in learning the controllable functions under the supervision of known degradation types and levels. They usually suffer from a severe performance drop when the real degradation is different from their assumptions. Such a limitation is due to the complexity of real-world degradations, which can not provide explicit supervision to the interactive modulation during training. However, how to realize the interactive modulation in real-world super-resolution has not yet been studied. In this work, we present a Metric Learning based Interactive Modulation for Real-World Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised degradation estimation strategy to estimate the degradation level in real-world scenarios. Instead of using known degradation levels as explicit supervision to the interactive mechanism, we propose a metric learning strategy to map the unquantifiable degradation levels in real-world scenarios to a metric space, which is trained in an unsupervised manner. Moreover, we introduce an anchor point strategy in the metric learning process to normalize the distribution of metric space. Extensive experiments demonstrate that the proposed MM-RealSR achieves excellent modulation and **restoration** performance in real-world super-resolution. Codes are available at https://github.com/TencentARC/MM-RealSR.  
## cs.LG
---
### Insights into the origin of halo mass profiles from machine learning. (arXiv:2205.04474v1 [astro-ph.CO])
- Authors : Luisa Lucie, Susmita Adhikari
- Link : [http://arxiv.org/abs/2205.04474](http://arxiv.org/abs/2205.04474)
> ABSTRACT  :  The mass distribution of **dark** matter haloes is the result of the hierarchical growth of initial density perturbations through mass accretion and mergers. We use an interpretable machine-learning framework to provide physical insights into the origin of the spherically-averaged mass profile of **dark** matter haloes. We train a gradient-boosted-trees algorithm to predict the final mass profiles of cluster-sized haloes, and measure the importance of the different inputs provided to the algorithm. We find two primary scales in the initial conditions (ICs) that impact the final mass profile: the density at approximately the scale of the haloes' Lagrangian patch $R_L$ ($R\sim 0.7\, R_L$) and that in the large-scale environment ($R\sim 1.7~R_L$). The model also identifies three primary time-scales in the halo assembly history that affect the final profile: (i) the formation time of the virialized, collapsed material inside the halo, (ii) the dynamical time, which captures the dynamically unrelaxed, infalling component of the halo over its first orbit, (iii) a third, most recent time-scale, which captures the impact on the outer profile of recent massive merger events. While the inner profile retains memory of the ICs, this information alone is insufficient to yield accurate predictions for the outer profile. As we add information about the haloes' mass accretion history, we find a significant improvement in the predicted profiles at all radii. Our machine-learning framework provides novel insights into the role of the ICs and the mass assembly history in determining the final mass profile of cluster-sized haloes.  
### **Night**ly Automobile Claims Prediction from Telematics-Derived Features: A Multilevel Approach. (arXiv:2205.04616v1 [cs.LG])
- Authors : Yoolim Jin, Anthony Duer, Tuka Alhanai, Mohammad Ghassemi
- Link : [http://arxiv.org/abs/2205.04616](http://arxiv.org/abs/2205.04616)
> ABSTRACT  :  In recent years it has become possible to collect GPS data from drivers and to incorporate this data into automobile insurance pricing for the driver. This data is continuously collected and processed **night**ly into metadata consisting of mileage and time summaries of each discrete trip taken, and a set of behavioral scores describing attributes of the trip (e.g, driver fatigue or driver distraction) so we examine whether it can be used to identify periods of increased risk by successfully classifying trips that occur immediately before a trip in which there was an incident leading to a claim for that driver. Identification of periods of increased risk for a driver is valuable because it creates an opportunity for intervention and, potentially, avoidance of a claim. We examine metadata for each trip a driver takes and train a classifier to predict whether \textit{the following trip} is one in which a claim occurs for that driver. By achieving a area under the receiver-operator characteristic above 0.6, we show that it is possible to predict claims in advance. Additionally, we compare the predictive power, as measured by the area under the receiver-operator characteristic of XGBoost classifiers trained to predict whether a driver will have a claim using **exposure** features such as driven miles, and those trained using behavioral features such as a computed speed score.  
### Real-Time Wearable Gait Phase Segmentation For Running And Walking. (arXiv:2205.04668v1 [cs.LG])
- Authors : De Sui, Han Chen, Yuang Shiang, Sheuan Chang
- Link : [http://arxiv.org/abs/2205.04668](http://arxiv.org/abs/2205.04668)
> ABSTRACT  :  Previous gait phase detection as convolutional neural network (CNN) based classification task requires cumbersome manual setting of time delay or heavy overlapped sliding windows to accurately classify each phase under different test cases, which is not suitable for streaming Inertial-Measurement-Unit (IMU) sensor data and fails to adapt to different scenarios. This paper presents a segmentation based gait phase detection with only a single six-axis IMU sensor, which can easily adapt to both walking and running at various speeds. The proposed segmentation uses CNN with gait phase aware receptive field setting and IMU oriented processing order, which can fit to high sampling rate of IMU up to 1000Hz for high accuracy and low sampling rate down to 20Hz for **real time** calculation. The proposed model on the 20Hz sampling rate data can achieve average error of 8.86 ms in swing time, 9.12 ms in stance time and 96.44\% accuracy of gait phase detection and 99.97\% accuracy of stride detection. Its real-time implementation on mobile phone only takes 36 ms for 1 second length of sensor data.  
### **Real-time** Forecasting of Time Series in Financial Markets Using Sequentially Trained Many-to-one LSTMs. (arXiv:2205.04678v1 [cs.LG])
- Authors : Kelum Gajamannage, Yonggi Park
- Link : [http://arxiv.org/abs/2205.04678](http://arxiv.org/abs/2205.04678)
> ABSTRACT  :  Financial markets are highly complex and volatile; thus, learning about such markets for the sake of making predictions is vital to make early alerts about crashes and subsequent recoveries. People have been using learning tools from diverse fields such as financial mathematics and machine learning in the attempt of making trustworthy predictions on such markets. However, the accuracy of such techniques had not been adequate until artificial neural network (ANN) frameworks were developed. Moreover, making accurate real-time predictions of financial time series is highly subjective to the ANN architecture in use and the procedure of training it. Long short-term memory (LSTM) is a member of the recurrent neural network family which has been widely utilized for time series predictions. Especially, we train two LSTMs with a known length, say $T$ time steps, of previous data and predict only one time step ahead. At each iteration, while one LSTM is employed to find the best number of epochs, the second LSTM is trained only for the best number of epochs to make predictions. We treat the current prediction as in the training set for the next prediction and train the same LSTM. While classic ways of training result in more error when the predictions are made further away in the test period, our approach is capable of maintaining a superior accuracy as training increases when it proceeds through the testing period. The forecasting accuracy of our approach is validated using three time series from each of the three diverse financial markets: stock, cryptocurrency, and commodity. The results are compared with those of an extended Kalman filter, an autoregressive model, and an autoregressive integrated moving average model.  
### Pediatric Automatic Sleep Staging: A comparative study of state-of-the-art deep learning methods. (arXiv:2108.10211v3 [eess.SP] UPDATED)
- Authors : Huy Phan, Alfred Mertins, Mathias Baumert
- Link : [http://arxiv.org/abs/2108.10211](http://arxiv.org/abs/2108.10211)
> ABSTRACT  :  Background: Despite the tremendous progress recently made towards automatic sleep staging in adults, it is currently unknown if the most advanced algorithms generalize to the pediatric population, which displays distinctive characteristics in over**night** polysomnography (PSG). Methods: To answer the question, in this work, we conduct a large-scale comparative study on the state-of-the-art deep learning methods for pediatric automatic sleep staging. Six different deep neural networks with diverging features are adopted to evaluate a sample of more than 1,200 children across a wide spectrum of obstructive sleep apnea (OSA) severity. Results: Our experimental results show that the individual performance of automated pediatric sleep stagers when evaluated on new subjects is equivalent to the expert-level one reported on adults. Combining the six stagers into ensemble models further boosts the staging accuracy, reaching an overall accuracy of 88.8%, a Cohen's kappa of 0.852, and a macro F1-score of 85.8%. At the same time, the ensemble models lead to reduced predictive uncertainty. The results also show that the studied algorithms and their ensembles are robust to concept drift when the training and test data were recorded seven months apart and after clinical intervention. Conclusion: However, we show that the improvements in the staging performance are not necessarily clinically significant although the ensemble models lead to more favorable clinical measures than the six standalone models. Significance: Detailed analyses further demonstrate "almost perfect" agreement between the automatic stagers to one another and their similar patterns on the staging errors, suggesting little room for improvement.  
### Why **Exposure** Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v2 [cs.CL] UPDATED)
- Authors : Kushal Arora, Layla El, Hareesh Bahuleyan, Jackie Chi, Kit Cheung
- Link : [http://arxiv.org/abs/2204.01171](http://arxiv.org/abs/2204.01171)
> ABSTRACT  :  Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis is that this brittleness of generation models is caused by the training and the generation procedure mismatch, also referred to as **exposure** bias. In this paper, we verify this hypothesis by analyzing **exposure** bias from an imitation learning perspective. We show that **exposure** bias leads to an accumulation of errors, analyze why perplexity fails to capture this accumulation, and empirically show that this accumulation results in poor generation quality. Source code to reproduce these experiments is available at https://github.com/kushalarora/quantifying_**exposure**_bias  
## cs.AI
---
### Insights into the origin of halo mass profiles from machine learning. (arXiv:2205.04474v1 [astro-ph.CO])
- Authors : Luisa Lucie, Susmita Adhikari
- Link : [http://arxiv.org/abs/2205.04474](http://arxiv.org/abs/2205.04474)
> ABSTRACT  :  The mass distribution of **dark** matter haloes is the result of the hierarchical growth of initial density perturbations through mass accretion and mergers. We use an interpretable machine-learning framework to provide physical insights into the origin of the spherically-averaged mass profile of **dark** matter haloes. We train a gradient-boosted-trees algorithm to predict the final mass profiles of cluster-sized haloes, and measure the importance of the different inputs provided to the algorithm. We find two primary scales in the initial conditions (ICs) that impact the final mass profile: the density at approximately the scale of the haloes' Lagrangian patch $R_L$ ($R\sim 0.7\, R_L$) and that in the large-scale environment ($R\sim 1.7~R_L$). The model also identifies three primary time-scales in the halo assembly history that affect the final profile: (i) the formation time of the virialized, collapsed material inside the halo, (ii) the dynamical time, which captures the dynamically unrelaxed, infalling component of the halo over its first orbit, (iii) a third, most recent time-scale, which captures the impact on the outer profile of recent massive merger events. While the inner profile retains memory of the ICs, this information alone is insufficient to yield accurate predictions for the outer profile. As we add information about the haloes' mass accretion history, we find a significant improvement in the predicted profiles at all radii. Our machine-learning framework provides novel insights into the role of the ICs and the mass assembly history in determining the final mass profile of cluster-sized haloes.  
### Why **Exposure** Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v2 [cs.CL] UPDATED)
- Authors : Kushal Arora, Layla El, Hareesh Bahuleyan, Jackie Chi, Kit Cheung
- Link : [http://arxiv.org/abs/2204.01171](http://arxiv.org/abs/2204.01171)
> ABSTRACT  :  Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis is that this brittleness of generation models is caused by the training and the generation procedure mismatch, also referred to as **exposure** bias. In this paper, we verify this hypothesis by analyzing **exposure** bias from an imitation learning perspective. We show that **exposure** bias leads to an accumulation of errors, analyze why perplexity fails to capture this accumulation, and empirically show that this accumulation results in poor generation quality. Source code to reproduce these experiments is available at https://github.com/kushalarora/quantifying_**exposure**_bias  
# Paper List
---
## cs.CV
---
**78** new papers in cs.CV:-) 
1. Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v1 [q-bio.QM])
2. Skin disease diagnosis using image analysis and natural language processing. (arXiv:2205.04468v1 [eess.IV])
3. Multiview Stereo with Cascaded Epipolar RAFT. (arXiv:2205.04502v1 [cs.CV])
4. Image2Gif: Generating Continuous Realistic Animations with Warping NODEs. (arXiv:2205.04519v1 [cs.CV])
5. Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns. (arXiv:2205.04523v1 [cs.LG])
6. How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations?. (arXiv:2205.04533v1 [cs.LG])
7. Is my Depth Ground-Truth Good Enough? HAMMER -- Highly Accurate Multi-Modal Dataset for DEnse 3D Scene Regression. (arXiv:2205.04565v1 [cs.CV])
8. When does dough become a bagel? Analyzing the remaining mistakes on ImageNet. (arXiv:2205.04596v1 [cs.CV])
9. CoDo: Contrastive Learning with Downstream Background Invariance for Detection. (arXiv:2205.04617v1 [cs.CV])
10. KEMP: Keyframe-Based Hierarchical End-to-End Deep Model for Long-Term Trajectory Prediction. (arXiv:2205.04624v1 [cs.CV])
11. Using frequency attention to make adversarial patch powerful against person detector. (arXiv:2205.04638v1 [cs.CV])
12. STDC-MA Network for Semantic Segmentation. (arXiv:2205.04639v1 [cs.CV])
13. Spatial Monitoring and Insect Behavioural Analysis Using Computer Vision for Precision Pollination. (arXiv:2205.04675v1 [cs.CV])
14. UNITS: Unsupervised Intermediate Training Stage for Scene Text Detection. (arXiv:2205.04683v1 [cs.CV])
15. OTFPF: Optimal Transport-Based Feature Pyramid Fusion Network for Brain Age Estimation with 3D Overlapped ConvNeXt. (arXiv:2205.04684v1 [cs.CV])
16. An asynchronous event-based algorithm for periodic signals. (arXiv:2205.04691v1 [cs.CV])
17. Automatic Detection of Microaneurysms in OCT Images Using Bag of Features. (arXiv:2205.04695v1 [eess.IV])
18. Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network. (arXiv:2205.04721v1 [eess.IV])
19. Robust Medical Image Classification from Noisy Labeled Data with Global and Local Representation Guided Co-training. (arXiv:2205.04723v1 [eess.IV])
20. Weakly-supervised segmentation of referring expressions. (arXiv:2205.04725v1 [cs.CV])
21. Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild. (arXiv:2205.04749v1 [cs.CV])
22. WG-VITON: Wearing-Guide Virtual Try-On for Top and Bottom Clothes. (arXiv:2205.04759v1 [cs.CV])
23. Explainable Deep Learning Methods in Medical Diagnosis: A Survey. (arXiv:2205.04766v1 [eess.IV])
24. Domain Invariant Masked Autoencoders for Self-supervised Learning from Multi-domains. (arXiv:2205.04771v1 [cs.CV])
25. Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases. (arXiv:2205.04800v1 [cs.CV])
26. The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v1 [cs.CV])
27. Self-supervised regression learning using domain knowledge: Applications to improving self-supervised denoising in imaging. (arXiv:2205.04821v1 [eess.IV])
28. Object Detection in Indian Food Platters using Transfer Learning with YOLOv4. (arXiv:2205.04841v1 [cs.CV])
29. Assessing Streamline Plausibility Through Randomized Iterative Spherical-Deconvolution Informed Tractogram Filtering. (arXiv:2205.04843v1 [cs.CV])
30. MNet: Rethinking 2D/3D Networks for Anisotropic Medical Image Segmentation. (arXiv:2205.04846v1 [eess.IV])
31. Hyperparameter optimization of hybrid quantum neural networks for car classification. (arXiv:2205.04878v1 [quant-ph])
32. Identical Image Retrieval using Deep Learning. (arXiv:2205.04883v1 [cs.CV])
33. Learning Non-target Knowledge for Few-shot Semantic Segmentation. (arXiv:2205.04903v1 [cs.CV])
34. Shadow-Aware Dynamic Convolution for Shadow Removal. (arXiv:2205.04908v1 [cs.CV])
35. A Closer Look at Blind Super-Resolution: Degradation Models, Baselines, and Performance Upper Bounds. (arXiv:2205.04910v1 [eess.IV])
36. Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training. (arXiv:2205.04948v1 [cs.CV])
37. **NeRF**-Editing: Geometry Editing of Neural Radiance Fields. (arXiv:2205.04978v1 [cs.GR])
38. Disentangling A Single MR Modality. (arXiv:2205.04982v1 [eess.IV])
39. Keypoint**NeRF**: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints. (arXiv:2205.04992v1 [cs.CV])
40. Using Deep Learning-based Features Extracted from CT scans to Predict Outcomes in COVID-19 Patients. (arXiv:2205.05009v1 [eess.IV])
41. Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v1 [cs.CV])
42. Classification and mapping of low-statured 'shrubland' cover types in post-agricultural landscapes of the US Northeast. (arXiv:2205.05047v1 [cs.CV])
43. Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v1 [cs.CV])
44. Accelerating the Training of Video Super-Resolution. (arXiv:2205.05069v1 [cs.CV])
45. Learning Visual Styles from Audio-Visual Associations. (arXiv:2205.05072v1 [cs.CV])
46. Reduce Information Loss in Transformers for Pluralistic Image Inpainting. (arXiv:2205.05076v1 [cs.CV])
47. EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Dynamic Vision Sensors. (arXiv:2006.00422v4 [cs.CV] UPDATED)
48. Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting. (arXiv:2010.04456v6 [stat.ML] UPDATED)
49. ResNet-LDDMM: Advancing the LDDMM Framework using Deep Residual Networks. (arXiv:2102.07951v3 [cs.AI] UPDATED)
50. Domain Generalization: A Survey. (arXiv:2103.02503v5 [cs.LG] UPDATED)
51. TubeR: Tubelet Transformer for Video Action Detection. (arXiv:2104.00969v5 [cs.CV] UPDATED)
52. DeepTag: A General Framework for Fiducial Marker Design and Detection. (arXiv:2105.13731v2 [cs.CV] UPDATED)
53. Towards Streaming Egocentric Action Anticipation. (arXiv:2110.05386v2 [cs.CV] UPDATED)
54. Skeleton-Based Mutually Assisted Interacted Object Localization and Human Action Recognition. (arXiv:2110.14994v2 [cs.CV] UPDATED)
55. Authentication Attacks on Projection-based Cancelable Biometric Schemes (long version). (arXiv:2110.15163v4 [cs.CR] UPDATED)
56. Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization. (arXiv:2111.07898v2 [cs.CV] UPDATED)
57. Learning Neural Light Fields with Ray-Space Embedding Networks. (arXiv:2112.01523v3 [cs.CV] UPDATED)
58. GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation. (arXiv:2112.02841v2 [cs.CV] UPDATED)
59. SVIP: Sequence VerIfication for Procedures in Videos. (arXiv:2112.06447v4 [cs.CV] UPDATED)
60. Self-Paced Deep Regression Forests with Consideration on Ranking Fairness. (arXiv:2112.06455v6 [cs.CV] UPDATED)
61. Autoencoder-based background reconstruction and foreground segmentation with background noise estimation. (arXiv:2112.08001v2 [cs.CV] UPDATED)
62. Deep Domain Adversarial Adaptation for Photon-efficient Imaging. (arXiv:2201.02475v2 [eess.IV] UPDATED)
63. End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v2 [cs.CV] UPDATED)
64. Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition. (arXiv:2201.10394v2 [cs.CV] UPDATED)
65. Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. (arXiv:2201.12604v2 [cs.LG] UPDATED)
66. VOS: Learning What You Don't Know by Virtual Outlier Synthesis. (arXiv:2202.01197v4 [cs.LG] UPDATED)
67. Learning Smooth Neural Functions via Lipschitz Regularization. (arXiv:2202.08345v2 [cs.CV] UPDATED)
68. Speaker Extraction with Co-Speech Gestures Cue. (arXiv:2203.16840v2 [eess.AS] UPDATED)
69. A novel stereo matching pipeline with robustness and unfixed disparity search range. (arXiv:2204.04865v2 [cs.CV] UPDATED)
70. Cross-Image Relational Knowledge Distillation for Semantic Segmentation. (arXiv:2204.06986v2 [cs.CV] UPDATED)
71. ResT V2: Simpler, Faster and Stronger. (arXiv:2204.07366v2 [cs.CV] UPDATED)
72. Application of Transfer Learning and Ensemble Learning in Image-level Classification for Breast Histopathology. (arXiv:2204.08311v2 [cs.CV] UPDATED)
73. Deep Model-Based Super-Resolution with Non-uniform Blur. (arXiv:2204.10109v2 [cs.CV] UPDATED)
74. BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix. (arXiv:2204.11425v2 [eess.IV] UPDATED)
75. AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching. (arXiv:2205.02849v2 [eess.IV] UPDATED)
76. Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing. (arXiv:2205.02953v2 [cs.RO] UPDATED)
77. Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion **Enhancement**. (arXiv:2205.03569v2 [cs.CV] UPDATED)
78. HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v2 [cs.CV] UPDATED)
## eess.IV
---
**20** new papers in eess.IV:-) 
1. Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v1 [q-bio.QM])
2. Skin disease diagnosis using image analysis and natural language processing. (arXiv:2205.04468v1 [eess.IV])
3. Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns. (arXiv:2205.04523v1 [cs.LG])
4. Automatic Detection of Microaneurysms in OCT Images Using Bag of Features. (arXiv:2205.04695v1 [eess.IV])
5. Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network. (arXiv:2205.04721v1 [eess.IV])
6. Robust Medical Image Classification from Noisy Labeled Data with Global and Local Representation Guided Co-training. (arXiv:2205.04723v1 [eess.IV])
7. Explainable Deep Learning Methods in Medical Diagnosis: A Survey. (arXiv:2205.04766v1 [eess.IV])
8. The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v1 [cs.CV])
9. Self-supervised regression learning using domain knowledge: Applications to improving self-supervised denoising in imaging. (arXiv:2205.04821v1 [eess.IV])
10. MNet: Rethinking 2D/3D Networks for Anisotropic Medical Image Segmentation. (arXiv:2205.04846v1 [eess.IV])
11. A Closer Look at Blind Super-Resolution: Degradation Models, Baselines, and Performance Upper Bounds. (arXiv:2205.04910v1 [eess.IV])
12. Disentangling A Single MR Modality. (arXiv:2205.04982v1 [eess.IV])
13. Using Deep Learning-based Features Extracted from CT scans to Predict Outcomes in COVID-19 Patients. (arXiv:2205.05009v1 [eess.IV])
14. Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v1 [cs.CV])
15. EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Dynamic Vision Sensors. (arXiv:2006.00422v4 [cs.CV] UPDATED)
16. An overview of artificial intelligence techniques for diagnosis of Schizophrenia based on magnetic resonance imaging modalities: Methods, challenges, and future works. (arXiv:2103.03081v3 [cs.LG] UPDATED)
17. Deep Domain Adversarial Adaptation for Photon-efficient Imaging. (arXiv:2201.02475v2 [eess.IV] UPDATED)
18. Deep Model-Based Super-Resolution with Non-uniform Blur. (arXiv:2204.10109v2 [cs.CV] UPDATED)
19. BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix. (arXiv:2204.11425v2 [eess.IV] UPDATED)
20. AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching. (arXiv:2205.02849v2 [eess.IV] UPDATED)
## cs.LG
---
**141** new papers in cs.LG:-) 
1. Rethinking Fairness: An Interdisciplinary Survey of Critiques of Hegemonic ML Fairness Approaches. (arXiv:2205.04460v1 [cs.LG])
2. Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v1 [q-bio.QM])
3. Insights into the origin of halo mass profiles from machine learning. (arXiv:2205.04474v1 [astro-ph.CO])
4. Are Quantum Computers Practical Yet? A Case for Feature Selection in Recommender Systems using Tensor Networks. (arXiv:2205.04490v1 [cs.IR])
5. Statistical Guarantees for Approximate Stationary Points of Simple Neural Networks. (arXiv:2205.04491v1 [cs.LG])
6. PinnerFormer: Sequence Modeling for User Representation at Pinterest. (arXiv:2205.04507v1 [cs.LG])
7. Image2Gif: Generating Continuous Realistic Animations with Warping NODEs. (arXiv:2205.04519v1 [cs.CV])
8. Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns. (arXiv:2205.04523v1 [cs.LG])
9. Towards a multi-stakeholder value-based assessment framework for algorithmic systems. (arXiv:2205.04525v1 [cs.LG])
10. Selectively Contextual Bandits. (arXiv:2205.04528v1 [cs.LG])
11. How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations?. (arXiv:2205.04533v1 [cs.LG])
12. A Probabilistic Generative Model of Free Categories. (arXiv:2205.04545v1 [cs.AI])
13. A Song of (Dis)agreement: Evaluating the Evaluation of Explainable Artificial Intelligence in Natural Language Processing. (arXiv:2205.04559v1 [cs.CL])
14. Towards Optimal VPU Compiler Cost Modeling by using Neural Networks to Infer Hardware Performances. (arXiv:2205.04586v1 [cs.LG])
15. A Verification Framework for Certifying Learning-Based Safety-Critical Aviation Systems. (arXiv:2205.04590v1 [eess.SY])
16. Affective Medical Estimation and Decision Making via Visualized Learning and Deep Learning. (arXiv:2205.04599v1 [cs.LG])
17. Long-term stability and generalization of observationally-constrained stochastic data-driven models for geophysical turbulence. (arXiv:2205.04601v1 [cs.LG])
18. Sentence-level Privacy for Document Embeddings. (arXiv:2205.04605v1 [cs.LG])
19. Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresentation, and Performing Evaluation. (arXiv:2205.04610v1 [cs.LG])
20. Calibrating for Class Weights by Modeling Machine Learning. (arXiv:2205.04613v1 [cs.LG])
21. **Night**ly Automobile Claims Prediction from Telematics-Derived Features: A Multilevel Approach. (arXiv:2205.04616v1 [cs.LG])
22. Risk Aversion In Learning Algorithms and an Application To Recommendation Systems. (arXiv:2205.04619v1 [cs.LG])
23. An Edge-Cloud Integrated Framework for Flexible and Dynamic Stream Analytics. (arXiv:2205.04622v1 [cs.DC])
24. KEMP: Keyframe-Based Hierarchical End-to-End Deep Model for Long-Term Trajectory Prediction. (arXiv:2205.04624v1 [cs.CV])
25. On some studies of Fraud Detection Pipeline and related issues from the scope of Ensemble Learning and Graph-based Learning. (arXiv:2205.04626v1 [cs.LG])
26. On Causality in Domain Adaptation and Semi-Supervised Learning: an Information-Theoretic Analysis. (arXiv:2205.04641v1 [cs.LG])
27. Crypto Pump and Dump via Deep Learning Techniques. (arXiv:2205.04646v1 [cs.LG])
28. Robust Learning of Parsimonious Deep Neural Networks. (arXiv:2205.04650v1 [cs.LG])
29. SuMe: A Dataset Towards Summarizing Biomedical Mechanisms. (arXiv:2205.04652v1 [cs.CL])
30. A 14uJ/Decision Keyword Spotting Accelerator with In-SRAM-Computing and On Chip Learning for Customization. (arXiv:2205.04665v1 [cs.AR])
31. Deep Gait Tracking With Inertial Measurement Unit. (arXiv:2205.04666v1 [cs.LG])
32. Variational Inference MPC using Normalizing Flows and Out-of-Distribution Projection. (arXiv:2205.04667v1 [cs.RO])
33. Real-Time Wearable Gait Phase Segmentation For Running And Walking. (arXiv:2205.04668v1 [cs.LG])
34. Improving genetic risk prediction across diverse population by disentangling ancestry representations. (arXiv:2205.04673v1 [cs.LG])
35. **Real-time** Forecasting of Time Series in Financial Markets Using Sequentially Trained Many-to-one LSTMs. (arXiv:2205.04678v1 [cs.LG])
36. OTFPF: Optimal Transport-Based Feature Pyramid Fusion Network for Brain Age Estimation with 3D Overlapped ConvNeXt. (arXiv:2205.04684v1 [cs.CV])
37. DNS based In-Browser Cryptojacking Detection. (arXiv:2205.04685v1 [cs.CR])
38. Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random. (arXiv:2205.04701v1 [cs.LG])
39. Training Personalized Recommendation Systems from (GPU) Scratch: Look Forward not Backwards. (arXiv:2205.04702v1 [cs.AR])
40. SmartSAGE: Training Large-scale Graph Neural Networks using In-Storage Processing Architectures. (arXiv:2205.04711v1 [cs.AR])
41. Knowledge Augmented Machine Learning with Applications in Autonomous Driving: A Survey. (arXiv:2205.04712v1 [cs.LG])
42. Serving and Optimizing Machine Learning Workflows on Heterogeneous Infrastructures. (arXiv:2205.04713v1 [cs.LG])
43. Weakly-supervised segmentation of referring expressions. (arXiv:2205.04725v1 [cs.CV])
44. Theory of Quantum Generative Learning Models with Maximum Mean Discrepancy. (arXiv:2205.04730v1 [quant-ph])
45. Explainable Data Imputation using Constraints. (arXiv:2205.04731v1 [cs.AI])
46. AI training resources for GLAM: a snapshot. (arXiv:2205.04738v1 [cs.LG])
47. Flow Completion Network: Inferring the Fluid Dynamics from Incomplete Flow Information using Graph Neural Networks. (arXiv:2205.04739v1 [physics.flu-dyn])
48. Deep learning based Chinese text sentiment mining and stock market correlation research. (arXiv:2205.04743v1 [q-fin.CP])
49. A spatial-temporal short-term traffic flow prediction model based on dynamical-learning graph convolution mechanism. (arXiv:2205.04762v1 [cs.LG])
50. Explainable Deep Learning Methods in Medical Diagnosis: A Survey. (arXiv:2205.04766v1 [eess.IV])
51. Spike-based computational models of bio-inspired memories in the hippocampal CA3 region on SpiNNaker. (arXiv:2205.04782v1 [cs.NE])
52. Matrix and graph representations of vine copula structures. (arXiv:2205.04783v1 [stat.ML])
53. Don't Throw it Away! The Utility of Unlabeled Data in Fair Decision Making. (arXiv:2205.04790v1 [stat.ML])
54. Designing a Recurrent Neural Network to Learn a Motion Planner for High-Dimensional Inputs. (arXiv:2205.04799v1 [cs.RO])
55. Reconstruction Enhanced Multi-View Contrastive Learning for Anomaly Detection on Attributed Networks. (arXiv:2205.04816v1 [cs.LG])
56. Cognitive Visual-learning Environment for PostgreSQL. (arXiv:2205.04834v1 [cs.LG])
57. Secure Distributed/Federated Learning: Prediction-Privacy Trade-Off for Multi-Agent System. (arXiv:2205.04855v1 [cs.MA])
58. Universal Caching. (arXiv:2205.04860v1 [cs.IT])
59. THOR: Threshold-Based Ranking Loss for Ordinal Regression. (arXiv:2205.04864v1 [cs.LG])
60. Accelerated functional brain aging in major depressive disorder: evidence from a large scale fMRI analysis of Chinese participants. (arXiv:2205.04871v1 [q-bio.NC])
61. Turtle Score -- Similarity Based Developer Analyzer. (arXiv:2205.04876v1 [stat.ML])
62. Hyperparameter optimization of hybrid quantum neural networks for car classification. (arXiv:2205.04878v1 [quant-ph])
63. Adaptive Graph Convolutional Network Framework for Multidimensional Time Series Prediction. (arXiv:2205.04885v1 [cs.LG])
64. Impact of L1 Batch Normalization on Analog Noise Resistant Property of Deep Learning Models. (arXiv:2205.04886v1 [cs.LG])
65. Search-Based Testing of Reinforcement Learning. (arXiv:2205.04887v1 [cs.LG])
66. GRU-TV: Time- and velocity-aware GRU for patient representation on multivariate clinical time-series data. (arXiv:2205.04892v1 [cs.LG])
67. Adjusted Expected Improvement for Cumulative Regret Minimization in Noisy Bayesian Optimization. (arXiv:2205.04901v1 [cs.LG])
68. Hybrid Far- and Near-Field Channel Estimation for THz Ultra-Massive MIMO via Fixed Point Networks. (arXiv:2205.04944v1 [eess.SP])
69. ALLSH: Active Learning Guided by Local Sensitivity and Hardness. (arXiv:2205.04980v1 [cs.CL])
70. Disentangling A Single MR Modality. (arXiv:2205.04982v1 [eess.IV])
71. Exploring Viable Algorithmic Options for Learning from Demonstration (LfD): A Parameterized Complexity Approach. (arXiv:2205.04989v1 [cs.LG])
72. Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v1 [cs.CV])
73. A Communication-Efficient Distributed Gradient Clipping Algorithm for Training Deep Neural Networks. (arXiv:2205.05040v1 [cs.LG])
74. Classification and mapping of low-statured 'shrubland' cover types in post-agricultural landscapes of the US Northeast. (arXiv:2205.05047v1 [cs.CV])
75. White-box Testing of NLP models with Mask Neuron Coverage. (arXiv:2205.05050v1 [cs.CL])
76. On learning agent-based models from data. (arXiv:2205.05052v1 [physics.soc-ph])
77. Fundamental limitations on optimization in variational quantum algorithms. (arXiv:2205.05056v1 [quant-ph])
78. Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory. (arXiv:2205.05057v1 [cs.HC])
79. On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer. (arXiv:2205.05061v1 [cs.LG])
80. Secure and Private Source Coding with Private Key and Decoder Side Information. (arXiv:2205.05068v1 [cs.IT])
81. Tensor-based Collaborative Filtering With Smooth Ratings Scale. (arXiv:2205.05070v1 [cs.IR])
82. Adaptive Ranking Based Constraint Handling for Explicitly Constrained Black-Box Optimization. (arXiv:1811.00764v3 [cs.NE] UPDATED)
83. A Wasserstein distance approach for concentration of empirical risk estimates. (arXiv:1902.10709v4 [math.ST] UPDATED)
84. Differentially Private Learning with Adaptive Clipping. (arXiv:1905.03871v5 [cs.LG] UPDATED)
85. Adaptation Strategies for Automated Machine Learning on Evolving Data. (arXiv:2006.06480v3 [cs.LG] UPDATED)
86. Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting. (arXiv:2010.04456v6 [stat.ML] UPDATED)
87. Nearly Minimax Optimal Regret for Learning Infinite-horizon Average-reward MDPs with Linear Function Approximation. (arXiv:2102.07301v2 [cs.LG] UPDATED)
88. Representing Hierarchical Structure by Using Cone Embedding. (arXiv:2102.08014v2 [cs.AI] UPDATED)
89. Domain Generalization: A Survey. (arXiv:2103.02503v5 [cs.LG] UPDATED)
90. An overview of artificial intelligence techniques for diagnosis of Schizophrenia based on magnetic resonance imaging modalities: Methods, challenges, and future works. (arXiv:2103.03081v3 [cs.LG] UPDATED)
91. PyRCN: A Toolbox for Exploration and Application of Reservoir Computing Networks. (arXiv:2103.04807v3 [cs.LG] UPDATED)
92. Text-Aware Predictive Monitoring of Business Processes. (arXiv:2104.09962v3 [cs.AI] UPDATED)
93. On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning. (arXiv:2105.01648v4 [cs.LG] UPDATED)
94. DeepTag: A General Framework for Fiducial Marker Design and Detection. (arXiv:2105.13731v2 [cs.CV] UPDATED)
95. Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path. (arXiv:2106.02073v4 [cs.LG] UPDATED)
96. Learning Combinatorial Node Labeling Algorithms. (arXiv:2106.03594v3 [cs.LG] UPDATED)
97. DeepAuditor: Distributed Online Intrusion Detection System for IoT devices via Power Side-channel Auditing. (arXiv:2106.12753v3 [cs.CR] UPDATED)
98. Pediatric Automatic Sleep Staging: A comparative study of state-of-the-art deep learning methods. (arXiv:2108.10211v3 [eess.SP] UPDATED)
99. Entity Linking and Discovery via Arborescence-based Supervised Clustering. (arXiv:2109.01242v2 [cs.CL] UPDATED)
100. Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v2 [cs.CL] UPDATED)
101. Concave Utility Reinforcement Learning with Zero-Constraint Violations. (arXiv:2109.05439v2 [cs.LG] UPDATED)
102. Modeling Regime Shifts in Multiple Time Series. (arXiv:2109.09692v3 [cs.LG] UPDATED)
103. Memory-Efficient Convex Optimization for Self-Dictionary Separable Nonnegative Matrix Factorization: A Frank-Wolfe Approach. (arXiv:2109.11135v2 [eess.SP] UPDATED)
104. Unsupervised Belief Representation Learning with Information-Theoretic Variational Graph Auto-Encoders. (arXiv:2110.00210v5 [cs.SI] UPDATED)
105. Human-robot collaboration and machine learning: a systematic review of recent research. (arXiv:2110.07448v3 [cs.RO] UPDATED)
106. Control Prefixes for Parameter-Efficient Text Generation. (arXiv:2110.08329v2 [cs.CL] UPDATED)
107. Labeled sample compression schemes for complexes of oriented matroids. (arXiv:2110.15168v2 [math.CO] UPDATED)
108. Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization. (arXiv:2111.07898v2 [cs.CV] UPDATED)
109. Automatic Sleep Staging of EEG Signals: Recent Development, Challenges, and Future Directions. (arXiv:2111.08446v3 [eess.SP] UPDATED)
110. Gradient flows on graphons: existence, convergence, continuity equations. (arXiv:2111.09459v2 [math.PR] UPDATED)
111. On Recurrent Neural Networks for learning-based control: recent results and ideas for future developments. (arXiv:2111.13557v2 [eess.SY] UPDATED)
112. Optimizing over an ensemble of neural networks. (arXiv:2112.07007v2 [cs.LG] UPDATED)
113. Measure and Improve Robustness in NLP Models: A Survey. (arXiv:2112.08313v2 [cs.CL] UPDATED)
114. Deep Learning Based Cloud Cover Parameterization for ICON. (arXiv:2112.11317v2 [physics.ao-ph] UPDATED)
115. Online AutoML: An adaptive AutoML framework for online learning. (arXiv:2201.09750v2 [cs.LG] UPDATED)
116. An Algorithmic Framework for Bias Bounties. (arXiv:2201.10408v4 [cs.LG] UPDATED)
117. A Robust and Flexible EM Algorithm for Mixtures of Elliptical Distributions with Missing Data. (arXiv:2201.12020v2 [stat.ML] UPDATED)
118. Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. (arXiv:2201.12604v2 [cs.LG] UPDATED)
119. VOS: Learning What You Don't Know by Virtual Outlier Synthesis. (arXiv:2202.01197v4 [cs.LG] UPDATED)
120. Semantic features of object concepts generated with GPT-3. (arXiv:2202.03753v2 [cs.CL] UPDATED)
121. Learning Relative Return Policies With Upside-Down Reinforcement Learning. (arXiv:2202.12742v2 [cs.LG] UPDATED)
122. Semi-Targeted Model Poisoning Attack on Federated Learning via Backward Error Analysis. (arXiv:2203.11633v2 [cs.LG] UPDATED)
123. Why **Exposure** Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v2 [cs.CL] UPDATED)
124. Application of Transfer Learning and Ensemble Learning in Image-level Classification for Breast Histopathology. (arXiv:2204.08311v2 [cs.CV] UPDATED)
125. Tight Last-Iterate Convergence of the Extragradient and the Optimistic Gradient Descent-Ascent Algorithm for Constrained Monotone Variational Inequalities. (arXiv:2204.09228v2 [math.OC] UPDATED)
126. Analysis of Temporal Difference Learning: Linear System Approach. (arXiv:2204.10479v4 [cs.LG] UPDATED)
127. Forecasting foreign exchange rates with regression networks tuned by Bayesian optimization. (arXiv:2204.12914v2 [q-fin.ST] UPDATED)
128. Engineering flexible machine learning systems by traversing functionally invariant paths in weight space. (arXiv:2205.00334v2 [cs.LG] UPDATED)
129. VICE: Variational Interpretable Concept Embeddings. (arXiv:2205.00756v5 [cs.LG] UPDATED)
130. Large Neighborhood Search based on Neural Construction Heuristics. (arXiv:2205.00772v2 [cs.LG] UPDATED)
131. RLFlow: Optimising Neural Network Subgraph Transformation with World Models. (arXiv:2205.01435v2 [cs.LG] UPDATED)
132. Modelling calibration uncertainty in networks of environmental sensors. (arXiv:2205.01988v2 [cs.LG] UPDATED)
133. FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization. (arXiv:2205.02215v2 [cs.LG] UPDATED)
134. AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching. (arXiv:2205.02849v2 [eess.IV] UPDATED)
135. Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing. (arXiv:2205.02953v2 [cs.RO] UPDATED)
136. Semi-Supervised Imitation Learning of Team Policies from Suboptimal Demonstrations. (arXiv:2205.02959v3 [cs.AI] UPDATED)
137. Federated Random Reshuffling with Compression and Variance Reduction. (arXiv:2205.03914v2 [cs.LG] UPDATED)
138. Verifying Integrity of Deep Ensemble Models by Lossless Black-box Watermarking with Sensitive Samples. (arXiv:2205.04145v2 [cs.CR] UPDATED)
139. NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality. (arXiv:2205.04421v2 [eess.AS] UPDATED)
140. Cluster-based Input Weight Initialization for Echo State Networks. (arXiv:2103.04710v3 [cs.LG] CROSS LISTED)
141. Text Simplification by Tagging. (arXiv:2103.05070v1 [cs.CL] CROSS LISTED)
## cs.AI
---
**78** new papers in cs.AI:-) 
1. Rethinking Fairness: An Interdisciplinary Survey of Critiques of Hegemonic ML Fairness Approaches. (arXiv:2205.04460v1 [cs.LG])
2. Concepts and Algorithms for Agent-based Decentralized and Integrated Scheduling of Production and Auxiliary Processes. (arXiv:2205.04461v1 [cs.MA])
3. Skin disease diagnosis using image analysis and natural language processing. (arXiv:2205.04468v1 [eess.IV])
4. Insights into the origin of halo mass profiles from machine learning. (arXiv:2205.04474v1 [astro-ph.CO])
5. Image2Gif: Generating Continuous Realistic Animations with Warping NODEs. (arXiv:2205.04519v1 [cs.CV])
6. Assessing Confidence with Assurance 2.0. (arXiv:2205.04522v1 [cs.AI])
7. On Nested Justification Systems (full version). (arXiv:2205.04541v1 [cs.AI])
8. A Probabilistic Generative Model of Free Categories. (arXiv:2205.04545v1 [cs.AI])
9. Machine Learning Diffusion Monte Carlo Energy Densities. (arXiv:2205.04547v1 [cond-mat.mes-hall])
10. Informed Steiner Trees: Sampling and Pruning for Multi-Goal Path Finding in High Dimensions. (arXiv:2205.04548v1 [cs.MA])
11. Towards Optimal VPU Compiler Cost Modeling by using Neural Networks to Infer Hardware Performances. (arXiv:2205.04586v1 [cs.LG])
12. A Verification Framework for Certifying Learning-Based Safety-Critical Aviation Systems. (arXiv:2205.04590v1 [eess.SY])
13. Galois theory for analogical classifiers. (arXiv:2205.04593v1 [cs.AI])
14. Affective Medical Estimation and Decision Making via Visualized Learning and Deep Learning. (arXiv:2205.04599v1 [cs.LG])
15. CoDo: Contrastive Learning with Downstream Background Invariance for Detection. (arXiv:2205.04617v1 [cs.CV])
16. Risk Aversion In Learning Algorithms and an Application To Recommendation Systems. (arXiv:2205.04619v1 [cs.LG])
17. KEMP: Keyframe-Based Hierarchical End-to-End Deep Model for Long-Term Trajectory Prediction. (arXiv:2205.04624v1 [cs.CV])
18. SuMe: A Dataset Towards Summarizing Biomedical Mechanisms. (arXiv:2205.04652v1 [cs.CL])
19. Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting. (arXiv:2205.04692v1 [cs.CL])
20. Training Personalized Recommendation Systems from (GPU) Scratch: Look Forward not Backwards. (arXiv:2205.04702v1 [cs.AR])
21. SmartSAGE: Training Large-scale Graph Neural Networks using In-Storage Processing Architectures. (arXiv:2205.04711v1 [cs.AR])
22. Weakly-supervised segmentation of referring expressions. (arXiv:2205.04725v1 [cs.CV])
23. Explainable Data Imputation using Constraints. (arXiv:2205.04731v1 [cs.AI])
24. AI training resources for GLAM: a snapshot. (arXiv:2205.04738v1 [cs.LG])
25. Controlling Extra-Textual Attributes about Dialogue Participants: A Case Study of English-to-Polish Neural Machine Translation. (arXiv:2205.04747v1 [cs.CL])
26. Explainable Deep Learning Methods in Medical Diagnosis: A Survey. (arXiv:2205.04766v1 [eess.IV])
27. Don't Throw it Away! The Utility of Unlabeled Data in Fair Decision Making. (arXiv:2205.04790v1 [stat.ML])
28. Probabilistic and Non-deterministic Event Data in Process Mining: Embedding Uncertainty in Process Analysis Techniques. (arXiv:2205.04827v1 [cs.AI])
29. Assessing Streamline Plausibility Through Randomized Iterative Spherical-Deconvolution Informed Tractogram Filtering. (arXiv:2205.04843v1 [cs.CV])
30. Scaling-up Generalized Planning as Heuristic Search with Landmarks. (arXiv:2205.04850v1 [cs.AI])
31. Joint Study of Above Ground Biomass and Soil Organic Carbon for Total Carbon Estimation using Satellite Imagery in Scotland. (arXiv:2205.04870v1 [stat.AP])
32. Strong Equivalence of Logic Programs with Ordered Disjunction: a Logical Perspective. (arXiv:2205.04882v1 [cs.LO])
33. Adaptive Graph Convolutional Network Framework for Multidimensional Time Series Prediction. (arXiv:2205.04885v1 [cs.LG])
34. Search-Based Testing of Reinforcement Learning. (arXiv:2205.04887v1 [cs.LG])
35. GRU-TV: Time- and velocity-aware GRU for patient representation on multivariate clinical time-series data. (arXiv:2205.04892v1 [cs.LG])
36. Reasoning in the Description Logic ALC under Category Semantics. (arXiv:2205.04911v1 [cs.LO])
37. Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts. (arXiv:2205.04952v1 [cs.RO])
38. ALLSH: Active Learning Guided by Local Sensitivity and Hardness. (arXiv:2205.04980v1 [cs.CL])
39. A Quantitative Symbolic Approach to Individual Human Reasoning. (arXiv:2205.05030v1 [cs.AI])
40. Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers. (arXiv:2205.05055v1 [cs.CL])
41. Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory. (arXiv:2205.05057v1 [cs.HC])
42. Accelerating the Training of Video Super-Resolution. (arXiv:2205.05069v1 [cs.CV])
43. Nested conformal prediction and quantile out-of-bag ensemble methods. (arXiv:1910.10562v4 [stat.ME] UPDATED)
44. Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting. (arXiv:2010.04456v6 [stat.ML] UPDATED)
45. MirrorAlign: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning. (arXiv:2102.04009v3 [cs.CL] UPDATED)
46. ResNet-LDDMM: Advancing the LDDMM Framework using Deep Residual Networks. (arXiv:2102.07951v3 [cs.AI] UPDATED)
47. Representing Hierarchical Structure by Using Cone Embedding. (arXiv:2102.08014v2 [cs.AI] UPDATED)
48. A Discrete-Time Switching System Analysis of Q-learning. (arXiv:2102.08583v8 [math.OC] UPDATED)
49. Domain Generalization: A Survey. (arXiv:2103.02503v5 [cs.LG] UPDATED)
50. Text-Aware Predictive Monitoring of Business Processes. (arXiv:2104.09962v3 [cs.AI] UPDATED)
51. On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning. (arXiv:2105.01648v4 [cs.LG] UPDATED)
52. Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path. (arXiv:2106.02073v4 [cs.LG] UPDATED)
53. Concave Utility Reinforcement Learning with Zero-Constraint Violations. (arXiv:2109.05439v2 [cs.LG] UPDATED)
54. Control Prefixes for Parameter-Efficient Text Generation. (arXiv:2110.08329v2 [cs.CL] UPDATED)
55. A Framework for Deprecating Datasets: Standardizing Documentation, Identification, and Communication. (arXiv:2111.04424v2 [cs.CY] UPDATED)
56. Understanding Jargon: Combining Extraction and Generation for Definition Modeling. (arXiv:2111.07267v2 [cs.CL] UPDATED)
57. Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization. (arXiv:2111.07898v2 [cs.CV] UPDATED)
58. Automatic Sleep Staging of EEG Signals: Recent Development, Challenges, and Future Directions. (arXiv:2111.08446v3 [eess.SP] UPDATED)
59. MultiVerS: Improving scientific claim verification with weak supervision and full-document context. (arXiv:2112.01640v2 [cs.CL] UPDATED)
60. Learning to Repair: Repairing model output errors after deployment using a dynamic memory of feedback. (arXiv:2112.09737v2 [cs.CL] UPDATED)
61. End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v2 [cs.CV] UPDATED)
62. Online AutoML: An adaptive AutoML framework for online learning. (arXiv:2201.09750v2 [cs.LG] UPDATED)
63. Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. (arXiv:2201.12604v2 [cs.LG] UPDATED)
64. Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting. (arXiv:2202.11946v2 [cs.NE] UPDATED)
65. Learning Relative Return Policies With Upside-Down Reinforcement Learning. (arXiv:2202.12742v2 [cs.LG] UPDATED)
66. Fingerprinting of DNN with Black-box Design and Verification. (arXiv:2203.10902v3 [cs.CR] UPDATED)
67. Converse: A Tree-Based Modular Task-Oriented Dialogue System. (arXiv:2203.12187v3 [cs.CL] UPDATED)
68. Why **Exposure** Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v2 [cs.CL] UPDATED)
69. Learning Generalized Policy Classes for Stochastic Shortest Path Problems. (arXiv:2204.04301v2 [cs.AI] UPDATED)
70. Engineering flexible machine learning systems by traversing functionally invariant paths in weight space. (arXiv:2205.00334v2 [cs.LG] UPDATED)
71. Large Neighborhood Search based on Neural Construction Heuristics. (arXiv:2205.00772v2 [cs.LG] UPDATED)
72. Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing. (arXiv:2205.02953v2 [cs.RO] UPDATED)
73. Semi-Supervised Imitation Learning of Team Policies from Suboptimal Demonstrations. (arXiv:2205.02959v3 [cs.AI] UPDATED)
74. Learning from Drivers to Tackle the Amazon Last Mile Routing Research Challenge. (arXiv:2205.04001v2 [cs.AI] UPDATED)
75. HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v2 [cs.CV] UPDATED)
76. NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality. (arXiv:2205.04421v2 [eess.AS] UPDATED)
77. Cluster-based Input Weight Initialization for Echo State Networks. (arXiv:2103.04710v3 [cs.LG] CROSS LISTED)
78. Text Simplification by Tagging. (arXiv:2103.05070v1 [cs.CL] CROSS LISTED)

