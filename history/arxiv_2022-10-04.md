# Your interest papers
---
## cs.CV
---
### Basic Binary Convolution Unit for Binarized Image **Restoration** Network. (arXiv:2210.00405v1 [cs.CV])
- Authors : Bin Xia, Yulun Zhang, Yitong Wang, Yapeng Tian, Wenming Yang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2210.00405](http://arxiv.org/abs/2210.00405)
> ABSTRACT  :  Lighter and faster image **restoration** (IR) models are crucial for the deployment on resource-limited devices. Binary neural network (BNN), one of the most promising model compression methods, can dramatically reduce the computations and parameters of full-precision convolutional neural networks (CNN). However, there are different properties between BNN and full-precision CNN, and we can hardly use the experience of designing CNN to develop BNN. In this study, we reconsider components in binary convolution, such as residual connection, BatchNorm, activation function, and structure, for IR tasks. We conduct systematic analyses to explain each component's role in binary convolution and discuss the pitfalls. Specifically, we find that residual connection can reduce the information loss caused by binarization; BatchNorm can solve the value range gap between residual connection and binary convolution; The position of the activation function dramatically affects the performance of BNN. Based on our findings and analyses, we design a simple yet efficient basic binary convolution unit (BBCU). Furthermore, we divide IR networks into four parts and specially design variants of BBCU for each part to explore the benefit of binarizing these parts. We conduct experiments on different IR tasks, and our BBCU significantly outperforms other BNNs and lightweight models, which shows that BBCU can serve as a basic unit for binarized IR networks. All codes and models will be released.  
### Seeing Through The Noisy **Dark**: Toward Real-world **Low-Light** Image **Enhancement** and Denoising. (arXiv:2210.00545v1 [cs.CV])
- Authors : Jiahuan Ren, Zhao Zhang, Richang Hong, Mingliang Xu, Yi Yang, Shuicheng Yan
- Link : [http://arxiv.org/abs/2210.00545](http://arxiv.org/abs/2210.00545)
> ABSTRACT  :  Images collected in real-world **low-light** environment usually suffer from lower visibility and heavier noise, due to the insufficient light or hardware limitation. While existing **low-light** image **enhancement** (LLIE) methods basically ignored the noise interference and mainly focus on refining the illumination of the **low-light** images based on benchmarked noise-negligible datasets. Such operations will make them inept for the real-world LLIE (RLLIE) with heavy noise, and result in speckle noise and blur in the enhanced images. Although several LLIE methods considered the noise in **low-light** image, they are trained on the raw data and hence cannot be used for sRGB images, since the domains of data are different and lack of expertise or unknown protocols. In this paper, we clearly consider the task of seeing through the noisy **dark** in sRGB color space, and propose a novel end-to-end method termed Real-world **Low-light** **Enhancement** &amp; Denoising Network (RLED-Net). Since natural images can usually be characterized by low-rank subspaces in which the redundant information and noise can be removed, we design a Latent Subspace Reconstruction Block (LSRB) for feature extraction and denoising. To reduce the loss of global feature (e.g., color/shape information) and extract more accurate local features (e.g., edge/texture information), we also present a basic layer with two branches, called Cross-channel &amp; Shift-window Transformer (CST). Based on the CST, we further present a new backbone to design a U-structure Network (CSTNet) for deep feature recovery, and also design a Feature Refine Block (FRB) to refine the final features. Extensive experiments on real noisy images and public databases verified the effectiveness of our RLED-Net for both RLLIE and denoising.  
### A Review on Deep Learning in Medical Image Reconstruction. (arXiv:1906.10643v3 [eess.IV] UPDATED)
- Authors : Haimiao Zhang, Bin Dong
- Link : [http://arxiv.org/abs/1906.10643](http://arxiv.org/abs/1906.10643)
> ABSTRACT  :  Medical imaging is crucial in modern clinics to guide the diagnosis and treatment of diseases. Medical image reconstruction is one of the most fundamental and important components of medical imaging, whose major objective is to acquire high-quality medical images for clinical usage at minimal cost and risk to the patients. Mathematical models in medical image reconstruction or, more generally, image **restoration** in computer vision, have been playing a prominent role. Earlier mathematical models are mostly designed by human knowledge or hypothesis on the image to be reconstructed, and we shall call these models handcrafted models. Later, handcrafted plus data-driven modeling started to emerge which still mostly relies on human designs, while part of the model is learned from the observed data. More recently, as more data and computation resources are made available, deep learning based models (or deep models) pushed data-driven modeling to the extreme where the models are mostly based on learning with minimal human designs. Both handcrafted and data-driven modeling have their own advantages and disadvantages. One of the major research trends in medical imaging is to combine handcrafted modeling with deep modeling so that we can enjoy benefits from both approaches. The major part of this article is to provide a conceptual review of some recent works on deep modeling from the unrolling dynamics viewpoint. This viewpoint stimulates new designs of neural network architectures with inspiration from optimization algorithms and numerical differential equations. Given the popularity of deep modeling, there are still vast remaining challenges in the field, as well as opportunities which we shall discuss at the end of this article.  
### Learning Spatially-Adaptive Squeeze-Excitation Networks for Image Synthesis and Image Recognition. (arXiv:2112.14804v3 [cs.CV] UPDATED)
- Authors : Jianghao Shen, Tianfu Wu
- Link : [http://arxiv.org/abs/2112.14804](http://arxiv.org/abs/2112.14804)
> ABSTRACT  :  Learning light-weight yet expressive deep networks in both image synthesis and image recognition remains a challenging problem. Inspired by a more recent observation that it is the data-specificity that makes the multi-head self-attention (MHSA) in the Transformer model so powerful, this paper proposes to extend the widely adopted light-weight Squeeze-Excitation (SE) module to be spatially-adaptive to reinforce its data specificity, as a convolutional alternative of the MHSA, while retaining the efficiency of SE and the inductive basis of convolution. It presents two designs of spatially-adaptive squeeze-excitation (SASE) modules for image synthesis and image recognition respectively. For image synthesis tasks, the proposed SASE is tested in both low-shot and one-shot learning tasks. It shows better performance than prior arts. For image recognition tasks, the proposed SASE is used as a drop-in replacement for convolution layers in ResNets and achieves much better accuracy than the vanilla ResNets, and slightly better than the MHSA counterparts such as the **Swin**-Transformer and Pyramid-Transformer in the ImageNet-1000 dataset, with significantly smaller models.  
### Colour alignment for relative colour constancy via non-standard references. (arXiv:2112.15106v2 [eess.IV] UPDATED)
- Authors : Yunfeng Zhao, Stuart Ferguson, Huiyu Zhou, Chris Elliott, Karen Rafferty
- Link : [http://arxiv.org/abs/2112.15106](http://arxiv.org/abs/2112.15106)
> ABSTRACT  :  Relative colour constancy is an essential requirement for many scientific imaging applications. However, most digital cameras differ in their image formations and native sensor output is usually inaccessible, e.g., in smartphone camera applications. This makes it hard to achieve consistent colour assessment across a range of devices, and that undermines the performance of computer vision algorithms. To resolve this issue, we propose a colour alignment model that considers the camera image formation as a black-box and formulates colour alignment as a three-step process: camera response calibration, response linearisation, and colour matching. The proposed model works with non-standard colour references, i.e., colour patches without knowing the true colour values, by utilising a novel balance-of-linear-distances feature. It is equivalent to determining the camera parameters through an unsupervised process. It also works with a minimum number of corresponding colour patches across the images to be colour aligned to deliver the applicable processing. Two challenging image datasets collected by multiple cameras under various illumination and **exposure** conditions were used to evaluate the model. Performance benchmarks demonstrated that our model achieved superior performance compared to other popular and state-of-the-art methods.  
### Global Context Vision Transformers. (arXiv:2206.09959v3 [cs.CV] UPDATED)
- Authors : Ali Hatamizadeh, Hongxu Yin, Jan Kautz, Pavlo Molchanov
- Link : [http://arxiv.org/abs/2206.09959](http://arxiv.org/abs/2206.09959)
> ABSTRACT  :  We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision tasks. The core of the novel model are global context self-attention modules, joint with standard local self-attention, to effectively yet efficiently model both long and short-range spatial interactions, as an alternative to complex operations such as an attention masks or local windows shifting. While the local self-attention modules are responsible for modeling short-range information, the global query tokens are shared across all global self-attention modules to interact with local key and values. In addition, we address the lack of inductive bias in ViTs and improve the modeling of inter-channel dependencies by proposing a novel downsampler which leverages a parameter-efficient fused inverted residual block. The proposed GC ViT achieves new state-of-the-art performance across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the tiny, small and base variants of GC ViT with 28M, 51M and 90M parameters achieve 83.4%, 83.9% and 84.4% Top-1 accuracy, respectively, surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based **Swin** Transformer. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets outperform prior work consistently, sometimes by large margins. Code and pre-trained models are available at https://github.com/NVlabs/GCViT.  
### Domain Gap Estimation for Source Free Unsupervised Domain Adaptation with Many Classifiers. (arXiv:2207.05785v2 [cs.CV] UPDATED)
- Authors : Ziyang Zong, Jun He, **Lei Zhang**, Hai Huan
- Link : [http://arxiv.org/abs/2207.05785](http://arxiv.org/abs/2207.05785)
> ABSTRACT  :  In theory, the success of unsupervised domain adaptation (UDA) largely relies on domain gap estimation. However, for source free UDA, the source domain data can not be accessed during adaptation, which poses great challenge of measuring the domain gap. In this paper, we propose to use many classifiers to learn the source domain decision boundaries, which provides a tighter upper bound of the domain gap, even if both of the domain data can not be simultaneously accessed. The source model is trained to push away each pair of classifiers whilst ensuring the correctness of the decision boundaries. In this sense, our many classifiers model separates the source different categories as far as possible which induces the maximum disagreement of many classifiers in the target domain, thus the transferable source domain knowledge is maximized. For adaptation, the source model is adapted to maximize the agreement among pairs of the classifiers. Thus the target features are pushed away from the decision boundaries. Experiments on several datasets of UDA show that our approach achieves state of the art performance among source free UDA approaches and can even compete to source available UDA methods.  
### Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v7 [cs.LG] UPDATED)
- Authors : Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui, Hsuan Yang
- Link : [http://arxiv.org/abs/2209.00796](http://arxiv.org/abs/2209.00796)
> ABSTRACT  :  Diffusion models are a class of deep generative models that have shown impressive results on various tasks with a solid theoretical foundation. Despite demonstrated success than state-of-the-art approaches, diffusion models often entail costly sampling procedures and sub-optimal likelihood estimation. Significant efforts have been made to improve the performance of diffusion models in various aspects. In this article, we present a comprehensive review of existing variants of diffusion models. Specifically, we provide the taxonomy of research in diffusion models and categorize them into three types: sampling-efficiency **enhancement**, likelihood-maximization **enhancement**, and data-generalization **enhancement**. We also introduce the other generative models (i.e., variational autoencoders, generative adversarial networks, normalizing flow, autoregressive models, and energy-based models) and discuss the connections between diffusion models and these generative models. Then we review the applications of diffusion models, including computer vision, natural language processing, temporal data modeling, multi-modal learning, robust learning, molecular graph modeling, material design, and inverse problem solving. Furthermore, we propose new perspectives pertaining to the development of generative models. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.  
### Text2Light: Zero-Shot Text-Driven **HDR** Panorama Generation. (arXiv:2209.09898v2 [cs.CV] UPDATED)
- Authors : Zhaoxi Chen, Guangcong Wang, Ziwei Liu
- Link : [http://arxiv.org/abs/2209.09898](http://arxiv.org/abs/2209.09898)
> ABSTRACT  :  High-quality **HDR**Is(**High Dynamic Range** Images), typically **HDR** panoramas, are one of the most popular ways to create photorealistic lighting and 360-degree reflections of 3D scenes in graphics. Given the difficulty of capturing **HDR**Is, a versatile and controllable generative model is highly desired, where layman users can intuitively control the generation process. However, existing state-of-the-art methods still struggle to synthesize high-quality panoramas for complex scenes. In this work, we propose a zero-shot text-driven framework, Text2Light, to generate 4K+ resolution **HDR**Is without paired training data. Given a free-form text as the description of the scene, we synthesize the corresponding **HDR**I with two dedicated steps: 1) text-driven panorama generation in low dynamic range(LDR) and low resolution, and 2) super-resolution inverse tone mapping to scale up the LDR panorama both in resolution and dynamic range. Specifically, to achieve zero-shot text-driven panorama generation, we first build dual codebooks as the discrete representation for diverse environmental textures. Then, driven by the pre-trained CLIP model, a text-conditioned global sampler learns to sample holistic semantics from the global codebook according to the input text. Furthermore, a structure-aware local sampler learns to synthesize LDR panoramas patch-by-patch, guided by holistic semantics. To achieve super-resolution inverse tone mapping, we derive a continuous representation of 360-degree imaging from the LDR panorama as a set of structured latent codes anchored to the sphere. This continuous representation enables a versatile module to upscale the resolution and dynamic range simultaneously. Extensive experiments demonstrate the superior capability of Text2Light in generating high-quality **HDR** panoramas. In addition, we show the feasibility of our work in realistic rendering and immersive VR.  
### Multi-stage image denoising with the wavelet transform. (arXiv:2209.12394v3 [eess.IV] UPDATED)
- Authors : Chunwei Tian, Menghua Zheng, Wangmeng Zuo, Bob Zhang, Yanning Zhang, David Zhang
- Link : [http://arxiv.org/abs/2209.12394](http://arxiv.org/abs/2209.12394)
> ABSTRACT  :  Deep convolutional neural networks (CNNs) are used for image denoising via automatically mining accurate structure information. However, most of existing CNNs depend on enlarging depth of designed networks to obtain better denoising performance, which may cause training difficulty. In this paper, we propose a multi-stage image denoising CNN with the wavelet transform (MWDCNN) via three stages, i.e., a dynamic convolutional block (DCB), two cascaded wavelet transform and **enhancement** blocks (WEBs) and a residual block (RB). DCB uses a dynamic convolution to dynamically adjust parameters of several convolutions for making a tradeoff between denoising performance and computational costs. WEB uses a combination of signal processing technique (i.e., wavelet transformation) and discriminative learning to suppress noise for recovering more detailed information in image denoising. To further remove redundant features, RB is used to refine obtained features for improving denoising effects and reconstruct clean images via improved residual dense architectures. Experimental results show that the proposed MWDCNN outperforms some popular denoising methods in terms of quantitative and qualitative analysis. Codes are available at https://github.com/hellloxiaotian/MWDCNN.  
### 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v2 [cs.CV] UPDATED)
- Authors : Ho Hin, Shunxing Bao, Yuankai Huo
- Link : [http://arxiv.org/abs/2209.15076](http://arxiv.org/abs/2209.15076)
> ABSTRACT  :  Vision transformers (ViTs) have quickly superseded convolutional networks (ConvNets) as the current state-of-the-art (SOTA) models for medical image segmentation. Hierarchical transformers (e.g., **Swin** Transformers) reintroduced several ConvNet priors and further enhanced the practical viability of adapting volumetric segmentation in 3D medical datasets. The effectiveness of hybrid approaches is largely credited to the large receptive field for non-local self-attention and the large number of model parameters. In this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the hierarchical transformer using ConvNet modules for robust volumetric segmentation. Specifically, we revisit volumetric depth-wise convolutions with large kernel size (e.g. starting from $7\times7\times7$) to enable the larger global receptive fields, inspired by **Swin** Transformer. We further substitute the multi-layer perceptron (MLP) in **Swin** Transformer blocks with pointwise depth convolutions and enhance model performances with fewer normalization and activation layers, thus reducing the number of model parameters. 3D UX-Net competes favorably with current SOTA transformers (e.g. **Swin**UNETR) using three challenging public datasets on volumetric brain and abdominal imaging: 1) MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI Challenge 2022 AMOS. 3D UX-Net consistently outperforms **Swin**UNETR with improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice (Feta2021). We further evaluate the transfer learning capability of 3D UX-Net with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880 to 0.900). The source code with our proposed model are available at https://github.com/MASILab/3DUX-Net.  
## eess.IV
---
### Basic Binary Convolution Unit for Binarized Image **Restoration** Network. (arXiv:2210.00405v1 [cs.CV])
- Authors : Bin Xia, Yulun Zhang, Yitong Wang, Yapeng Tian, Wenming Yang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2210.00405](http://arxiv.org/abs/2210.00405)
> ABSTRACT  :  Lighter and faster image **restoration** (IR) models are crucial for the deployment on resource-limited devices. Binary neural network (BNN), one of the most promising model compression methods, can dramatically reduce the computations and parameters of full-precision convolutional neural networks (CNN). However, there are different properties between BNN and full-precision CNN, and we can hardly use the experience of designing CNN to develop BNN. In this study, we reconsider components in binary convolution, such as residual connection, BatchNorm, activation function, and structure, for IR tasks. We conduct systematic analyses to explain each component's role in binary convolution and discuss the pitfalls. Specifically, we find that residual connection can reduce the information loss caused by binarization; BatchNorm can solve the value range gap between residual connection and binary convolution; The position of the activation function dramatically affects the performance of BNN. Based on our findings and analyses, we design a simple yet efficient basic binary convolution unit (BBCU). Furthermore, we divide IR networks into four parts and specially design variants of BBCU for each part to explore the benefit of binarizing these parts. We conduct experiments on different IR tasks, and our BBCU significantly outperforms other BNNs and lightweight models, which shows that BBCU can serve as a basic unit for binarized IR networks. All codes and models will be released.  
### Random Data Augmentation based **Enhancement**: A Generalized **Enhancement** Approach for Medical Datasets. (arXiv:2210.00824v1 [eess.IV])
- Authors : Sidra Aleem, Teerath Kumar, Suzanne Little, Malika Bendechache, Rob Brennan, Kevin McGuinness
- Link : [http://arxiv.org/abs/2210.00824](http://arxiv.org/abs/2210.00824)
> ABSTRACT  :  Over the years, the paradigm of medical image analysis has shifted from manual expertise to automated systems, often using deep learning (DL) systems. The performance of deep learning algorithms is highly dependent on data quality. Particularly for the medical domain, it is an important aspect as medical data is very sensitive to quality and poor quality can lead to misdiagnosis. To improve the diagnostic performance, research has been done both in complex DL architectures and in improving data quality using dataset dependent static hyperparameters. However, the performance is still constrained due to data quality and overfitting of hyperparameters to a specific dataset. To overcome these issues, this paper proposes random data augmentation based **enhancement**. The main objective is to develop a generalized, data-independent and computationally efficient **enhancement** approach to improve medical data quality for DL. The quality is enhanced by improving the brightness and contrast of images. In contrast to the existing methods, our method generates **enhancement** hyperparameters randomly within a defined range, which makes it robust and prevents overfitting to a specific dataset. To evaluate the generalization of the proposed method, we use four medical datasets and compare its performance with state-of-the-art methods for both classification and segmentation tasks. For grayscale imagery, experiments have been performed with: COVID-19 chest X-ray, KiTS19, and for RGB imagery with: LC25000 datasets. Experimental results demonstrate that with the proposed **enhancement** methodology, DL architectures outperform other existing methods. Our code is publicly available at: https://github.com/aleemsidra/Augmentation-Based-Generalized-**Enhancement**  
### Perceptual Attacks of No-Reference Image Quality Models with Human-in-the-Loop. (arXiv:2210.00933v1 [cs.CV])
- Authors : Weixia Zhang, Dingquan Li, Xiongkuo Min, Guangtao Zhai, Guodong Guo, Xiaokang Yang, **Kede Ma**
- Link : [http://arxiv.org/abs/2210.00933](http://arxiv.org/abs/2210.00933)
> ABSTRACT  :  No-reference image quality assessment (NR-IQA) aims to quantify how humans perceive visual distortions of digital images without access to their undistorted references. NR-IQA models are extensively studied in computational vision, and are widely used for performance evaluation and perceptual optimization of man-made vision systems. Here we make one of the first attempts to examine the perceptual robustness of NR-IQA models. Under a Lagrangian formulation, we identify insightful connections of the proposed perceptual attack to previous beautiful ideas in computer vision and machine learning. We test one knowledge-driven and three data-driven NR-IQA methods under four full-reference IQA models (as approximations to human perception of just-noticeable differences). Through carefully designed psychophysical experiments, we find that all four NR-IQA models are vulnerable to the proposed perceptual attack. More interestingly, we observe that the generated counterexamples are not transferable, manifesting themselves as distinct design flows of respective NR-IQA methods.  
### A Review on Deep Learning in Medical Image Reconstruction. (arXiv:1906.10643v3 [eess.IV] UPDATED)
- Authors : Haimiao Zhang, Bin Dong
- Link : [http://arxiv.org/abs/1906.10643](http://arxiv.org/abs/1906.10643)
> ABSTRACT  :  Medical imaging is crucial in modern clinics to guide the diagnosis and treatment of diseases. Medical image reconstruction is one of the most fundamental and important components of medical imaging, whose major objective is to acquire high-quality medical images for clinical usage at minimal cost and risk to the patients. Mathematical models in medical image reconstruction or, more generally, image **restoration** in computer vision, have been playing a prominent role. Earlier mathematical models are mostly designed by human knowledge or hypothesis on the image to be reconstructed, and we shall call these models handcrafted models. Later, handcrafted plus data-driven modeling started to emerge which still mostly relies on human designs, while part of the model is learned from the observed data. More recently, as more data and computation resources are made available, deep learning based models (or deep models) pushed data-driven modeling to the extreme where the models are mostly based on learning with minimal human designs. Both handcrafted and data-driven modeling have their own advantages and disadvantages. One of the major research trends in medical imaging is to combine handcrafted modeling with deep modeling so that we can enjoy benefits from both approaches. The major part of this article is to provide a conceptual review of some recent works on deep modeling from the unrolling dynamics viewpoint. This viewpoint stimulates new designs of neural network architectures with inspiration from optimization algorithms and numerical differential equations. Given the popularity of deep modeling, there are still vast remaining challenges in the field, as well as opportunities which we shall discuss at the end of this article.  
### Colour alignment for relative colour constancy via non-standard references. (arXiv:2112.15106v2 [eess.IV] UPDATED)
- Authors : Yunfeng Zhao, Stuart Ferguson, Huiyu Zhou, Chris Elliott, Karen Rafferty
- Link : [http://arxiv.org/abs/2112.15106](http://arxiv.org/abs/2112.15106)
> ABSTRACT  :  Relative colour constancy is an essential requirement for many scientific imaging applications. However, most digital cameras differ in their image formations and native sensor output is usually inaccessible, e.g., in smartphone camera applications. This makes it hard to achieve consistent colour assessment across a range of devices, and that undermines the performance of computer vision algorithms. To resolve this issue, we propose a colour alignment model that considers the camera image formation as a black-box and formulates colour alignment as a three-step process: camera response calibration, response linearisation, and colour matching. The proposed model works with non-standard colour references, i.e., colour patches without knowing the true colour values, by utilising a novel balance-of-linear-distances feature. It is equivalent to determining the camera parameters through an unsupervised process. It also works with a minimum number of corresponding colour patches across the images to be colour aligned to deliver the applicable processing. Two challenging image datasets collected by multiple cameras under various illumination and **exposure** conditions were used to evaluate the model. Performance benchmarks demonstrated that our model achieved superior performance compared to other popular and state-of-the-art methods.  
### Multi-stage image denoising with the wavelet transform. (arXiv:2209.12394v3 [eess.IV] UPDATED)
- Authors : Chunwei Tian, Menghua Zheng, Wangmeng Zuo, Bob Zhang, Yanning Zhang, David Zhang
- Link : [http://arxiv.org/abs/2209.12394](http://arxiv.org/abs/2209.12394)
> ABSTRACT  :  Deep convolutional neural networks (CNNs) are used for image denoising via automatically mining accurate structure information. However, most of existing CNNs depend on enlarging depth of designed networks to obtain better denoising performance, which may cause training difficulty. In this paper, we propose a multi-stage image denoising CNN with the wavelet transform (MWDCNN) via three stages, i.e., a dynamic convolutional block (DCB), two cascaded wavelet transform and **enhancement** blocks (WEBs) and a residual block (RB). DCB uses a dynamic convolution to dynamically adjust parameters of several convolutions for making a tradeoff between denoising performance and computational costs. WEB uses a combination of signal processing technique (i.e., wavelet transformation) and discriminative learning to suppress noise for recovering more detailed information in image denoising. To further remove redundant features, RB is used to refine obtained features for improving denoising effects and reconstruct clean images via improved residual dense architectures. Experimental results show that the proposed MWDCNN outperforms some popular denoising methods in terms of quantitative and qualitative analysis. Codes are available at https://github.com/hellloxiaotian/MWDCNN.  
## cs.LG
---
### Leveraging Industry 4.0 -- Deep Learning, Surrogate Model and Transfer Learning with Uncertainty Quantification Incorporated into Digital Twin for Nuclear System. (arXiv:2210.00074v1 [cs.LG])
- Authors : Abid Khan, Sayeed Anowar, Md Al, Richa Verma, Dinesh Kumar, Kazuma Kobayashi, Syed Alam
- Link : [http://arxiv.org/abs/2210.00074](http://arxiv.org/abs/2210.00074)
> ABSTRACT  :  Industry 4.0 targets the conversion of the traditional industries into intelligent ones through technological revolution. This revolution is only possible through innovation, optimization, interconnection, and rapid decision-making capability. Numerical models are believed to be the key components of Industry 4.0, facilitating quick decision-making through simulations instead of costly experiments. However, numerical investigation of precise, high-fidelity models for optimization or decision-making is usually time-consuming and computationally expensive. In such instances, data-driven surrogate models are excellent substitutes for fast computational analysis and the probabilistic prediction of the output parameter for new input parameters. The emergence of Internet of Things (IoT) and Machine Learning (ML) has made the concept of surrogate modeling even more viable. However, these surrogate models contain intrinsic uncertainties, originate from modeling defects, or both. These uncertainties, if not quantified and minimized, can produce a skewed result. Therefore, proper implementation of uncertainty quantification techniques is crucial during optimization, cost reduction, or safety **enhancement** processes analysis. This chapter begins with a brief overview of the concept of surrogate modeling, transfer learning, IoT and digital twins. After that, a detailed overview of uncertainties, uncertainty quantification frameworks, and specifics of uncertainty quantification methodologies for a surrogate model linked to a digital twin is presented. Finally, the use of uncertainty quantification approaches in the nuclear industry has been addressed.  
### CR**ISP**: Curriculum based Sequential Neural Decoders for Polar Code Family. (arXiv:2210.00313v1 [cs.IT])
- Authors : Ashwin Hebbar, Viraj Nadkarni, Ashok Vardhan, Suma Bhat, Sewoong Oh, Pramod Viswanath
- Link : [http://arxiv.org/abs/2210.00313](http://arxiv.org/abs/2210.00313)
> ABSTRACT  :  Polar codes are widely used state-of-the-art codes for reliable communication that have recently been included in the 5th generation wireless standards (5G). However, there remains room for the design of polar decoders that are both efficient and reliable in the short blocklength regime. Motivated by recent successes of data-driven channel decoders, we introduce a novel $\textbf{C}$ur$\textbf{RI}$culum based $\textbf{S}$equential neural decoder for $\textbf{P}$olar codes (CR**ISP**). We design a principled curriculum, guided by information-theoretic insights, to train CR**ISP** and show that it outperforms the successive-cancellation (SC) decoder and attains near-optimal reliability performance on the Polar(16,32) and Polar(22, 64) codes. The choice of the proposed curriculum is critical in achieving the accuracy gains of CR**ISP**, as we show by comparing against other curricula. More notably, CR**ISP** can be readily extended to Polarization-Adjusted-Convolutional (PAC) codes, where existing SC decoders are significantly less reliable. To the best of our knowledge, CR**ISP** constructs the first data-driven decoder for PAC codes and attains near-optimal performance on the PAC(16, 32) code.  
### Implementation of a Three-class Classification LS-SVM Model for the Hybrid Antenna Array with Bowtie Elements in the Adaptive Beamforming Application. (arXiv:2210.00317v1 [cs.LG])
- Authors : Somayeh Komeylian, Christopher Paolini
- Link : [http://arxiv.org/abs/2210.00317](http://arxiv.org/abs/2210.00317)
> ABSTRACT  :  To address three significant challenges of massive wireless communications including propagation loss, long-distance transmission, and channel fading, we aim at establishing the hybrid antenna array with bowtie elements in a compact size for beamforming applications. In this work we rigorously demonstrate that bowtie elements allow for a significant improvement in the beamforming performance of the hybrid antenna array compared to not only other available antenna arrays, but also its geometrical counterpart with dipole elements. We have achieved a greater than 15 dB increase in SINR values, a greater than 20% improvement in the antenna efficiency, a significant **enhancement** in the DoA estimation, and 20 increments in the directivity for the hybrid antenna array with bowtie elements, compared to its geometrical counterpart, by performing a three-class classification LS-SVM (LeastSquares Support Vector Machine) optimization method. The proposed hybrid antenna array has shown a 3D uniform directivity, which is accompanied by its superior performance in the 3D uniform beam-scanning capability. The directivities remain almost constant at 40.83 dBi with the variation of angle {\theta}, and 41.21 dBi with the variation of angle {\phi}. The unrivaled functionality and performance of the hybrid antenna array with bowtie elements makes it a potential candidate for beamforming applications in massive wireless communications.  
### Robust Bayesian optimization with reinforcement learned acquisition functions. (arXiv:2210.00476v1 [cs.LG])
- Authors : Zijing Liu, Xiyao Qu, Xuejun Liu, Hongqiang Lyu
- Link : [http://arxiv.org/abs/2210.00476](http://arxiv.org/abs/2210.00476)
> ABSTRACT  :  In Bayesian optimization (BO) for expensive black-box optimization tasks, acquisition function (AF) guides sequential sampling and plays a pivotal role for efficient convergence to better optima. Prevailing AFs usually rely on artificial experiences in terms of preferences for exploration or exploitation, which runs a risk of a computational waste or traps in local optima and resultant re-optimization. To address the crux, the idea of data-driven AF selection is proposed, and the sequential AF selection task is further formalized as a Markov decision process (MDP) and resort to powerful reinforcement learning (RL) technologies. Appropriate selection policy for AFs is learned from superior BO trajectories to balance between exploration and exploitation in **real time**, which is called reinforcement-learning-assisted Bayesian optimization (RLABO). Competitive and robust BO evaluations on five benchmark problems demonstrate RL's recognition of the implicit AF selection pattern and imply the proposal's potential practicality for intelligent AF selection as well as efficient optimization in expensive black-box problems.  
### A Review on Deep Learning in Medical Image Reconstruction. (arXiv:1906.10643v3 [eess.IV] UPDATED)
- Authors : Haimiao Zhang, Bin Dong
- Link : [http://arxiv.org/abs/1906.10643](http://arxiv.org/abs/1906.10643)
> ABSTRACT  :  Medical imaging is crucial in modern clinics to guide the diagnosis and treatment of diseases. Medical image reconstruction is one of the most fundamental and important components of medical imaging, whose major objective is to acquire high-quality medical images for clinical usage at minimal cost and risk to the patients. Mathematical models in medical image reconstruction or, more generally, image **restoration** in computer vision, have been playing a prominent role. Earlier mathematical models are mostly designed by human knowledge or hypothesis on the image to be reconstructed, and we shall call these models handcrafted models. Later, handcrafted plus data-driven modeling started to emerge which still mostly relies on human designs, while part of the model is learned from the observed data. More recently, as more data and computation resources are made available, deep learning based models (or deep models) pushed data-driven modeling to the extreme where the models are mostly based on learning with minimal human designs. Both handcrafted and data-driven modeling have their own advantages and disadvantages. One of the major research trends in medical imaging is to combine handcrafted modeling with deep modeling so that we can enjoy benefits from both approaches. The major part of this article is to provide a conceptual review of some recent works on deep modeling from the unrolling dynamics viewpoint. This viewpoint stimulates new designs of neural network architectures with inspiration from optimization algorithms and numerical differential equations. Given the popularity of deep modeling, there are still vast remaining challenges in the field, as well as opportunities which we shall discuss at the end of this article.  
### Unsupervised Speech **Enhancement** using Dynamical Variational Auto-Encoders. (arXiv:2106.12271v3 [cs.SD] UPDATED)
- Authors : Xiaoyu Bie, Simon Leglaive, Xavier Alameda, Laurent Girin
- Link : [http://arxiv.org/abs/2106.12271](http://arxiv.org/abs/2106.12271)
> ABSTRACT  :  Dynamical variational autoencoders (DVAEs) are a class of deep generative models with latent variables, dedicated to model time series of high-dimensional data. DVAEs can be considered as extensions of the variational autoencoder (VAE) that include temporal dependencies between successive observed and/or latent vectors. Previous work has shown the interest of using DVAEs over the VAE for speech spectrograms modeling. Independently, the VAE has been successfully applied to speech **enhancement** in noise, in an unsupervised noise-agnostic set-up that requires neither noise samples nor noisy speech samples at training time, but only requires clean speech signals. In this paper, we extend these works to DVAE-based single-channel unsupervised speech **enhancement**, hence exploiting both speech signals unsupervised representation learning and dynamics modeling. We propose an unsupervised speech **enhancement** algorithm that combines a DVAE speech prior pre-trained on clean speech signals with a noise model based on nonnegative matrix factorization, and we derive a variational expectation-maximization (VEM) algorithm to perform speech **enhancement**. The algorithm is presented with the most general DVAE formulation and is then applied with three specific DVAE models to illustrate the versatility of the framework. Experimental results show that the proposed DVAE-based approach outperforms its VAE-based counterpart, as well as several supervised and unsupervised noise-dependent baselines, especially when the noise type is unseen during training.  
### Discovery of New Multi-Level Features for Domain Generalization via Knowledge Corruption. (arXiv:2109.04320v3 [cs.LG] UPDATED)
- Authors : Ahmed Frikha, Denis Krompa, Volker Tresp
- Link : [http://arxiv.org/abs/2109.04320](http://arxiv.org/abs/2109.04320)
> ABSTRACT  :  Machine learning models that can generalize to unseen domains are essential when applied in real-world scenarios involving strong domain shifts. We address the challenging domain generalization (DG) problem, where a model trained on a set of source domains is expected to generalize well in unseen domains without any **exposure** to their data. The main challenge of DG is that the features learned from the source domains are not necessarily present in the unseen target domains, leading to performance deterioration. We assume that learning a richer set of features is crucial to improve the transfer to a wider set of unknown domains. For this reason, we propose COLUMBUS, a method that enforces new feature discovery via a targeted corruption of the most relevant input and multi-level representations of the data. We conduct an extensive empirical evaluation to demonstrate the effectiveness of the proposed approach which achieves new state-of-the-art results by outperforming 18 DG algorithms on multiple DG benchmark datasets in the DomainBed framework.  
### Learning Spatially-Adaptive Squeeze-Excitation Networks for Image Synthesis and Image Recognition. (arXiv:2112.14804v3 [cs.CV] UPDATED)
- Authors : Jianghao Shen, Tianfu Wu
- Link : [http://arxiv.org/abs/2112.14804](http://arxiv.org/abs/2112.14804)
> ABSTRACT  :  Learning light-weight yet expressive deep networks in both image synthesis and image recognition remains a challenging problem. Inspired by a more recent observation that it is the data-specificity that makes the multi-head self-attention (MHSA) in the Transformer model so powerful, this paper proposes to extend the widely adopted light-weight Squeeze-Excitation (SE) module to be spatially-adaptive to reinforce its data specificity, as a convolutional alternative of the MHSA, while retaining the efficiency of SE and the inductive basis of convolution. It presents two designs of spatially-adaptive squeeze-excitation (SASE) modules for image synthesis and image recognition respectively. For image synthesis tasks, the proposed SASE is tested in both low-shot and one-shot learning tasks. It shows better performance than prior arts. For image recognition tasks, the proposed SASE is used as a drop-in replacement for convolution layers in ResNets and achieves much better accuracy than the vanilla ResNets, and slightly better than the MHSA counterparts such as the **Swin**-Transformer and Pyramid-Transformer in the ImageNet-1000 dataset, with significantly smaller models.  
### **Bilateral** Deep Reinforcement Learning Approach for Better-than-human Car Following Model. (arXiv:2203.04749v3 [cs.RO] UPDATED)
- Authors : Tianyu Shi, Yifei Ai, Omar ElSamadisy, Baher Abdulhai
- Link : [http://arxiv.org/abs/2203.04749](http://arxiv.org/abs/2203.04749)
> ABSTRACT  :  In the coming years and decades, autonomous vehicles (AVs) will become increasingly prevalent, offering new opportunities for safer and more convenient travel and potentially smarter traffic control methods exploiting automation and connectivity. Car following is a prime function in autonomous driving. Car following based on reinforcement learning has received attention in recent years with the goal of learning and achieving performance levels comparable to humans. However, most existing RL methods model car following as a unilateral problem, sensing only the vehicle ahead. Recent literature, however, Wang and Horn [16] has shown that **bilateral** car following that considers the vehicle ahead and the vehicle behind exhibits better system stability. In this paper we hypothesize that this **bilateral** car following can be learned using RL, while learning other goals such as efficiency maximisation, jerk minimization, and safety rewards leading to a learned model that outperforms human driving.    We propose and introduce a Deep Reinforcement Learning (DRL) framework for car following control by integrating **bilateral** information into both state and reward function based on the **bilateral** control model (BCM) for car following control. Furthermore, we use a decentralized multi-agent reinforcement learning framework to generate the corresponding control action for each agent. Our simulation results demonstrate that our learned policy is better than the human driving policy in terms of (a) inter-vehicle headways, (b) average speed, (c) jerk, (d) Time to Collision (TTC) and (e) string stability.  
### Global Context Vision Transformers. (arXiv:2206.09959v3 [cs.CV] UPDATED)
- Authors : Ali Hatamizadeh, Hongxu Yin, Jan Kautz, Pavlo Molchanov
- Link : [http://arxiv.org/abs/2206.09959](http://arxiv.org/abs/2206.09959)
> ABSTRACT  :  We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision tasks. The core of the novel model are global context self-attention modules, joint with standard local self-attention, to effectively yet efficiently model both long and short-range spatial interactions, as an alternative to complex operations such as an attention masks or local windows shifting. While the local self-attention modules are responsible for modeling short-range information, the global query tokens are shared across all global self-attention modules to interact with local key and values. In addition, we address the lack of inductive bias in ViTs and improve the modeling of inter-channel dependencies by proposing a novel downsampler which leverages a parameter-efficient fused inverted residual block. The proposed GC ViT achieves new state-of-the-art performance across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the tiny, small and base variants of GC ViT with 28M, 51M and 90M parameters achieve 83.4%, 83.9% and 84.4% Top-1 accuracy, respectively, surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based **Swin** Transformer. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets outperform prior work consistently, sometimes by large margins. Code and pre-trained models are available at https://github.com/NVlabs/GCViT.  
### Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v7 [cs.LG] UPDATED)
- Authors : Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui, Hsuan Yang
- Link : [http://arxiv.org/abs/2209.00796](http://arxiv.org/abs/2209.00796)
> ABSTRACT  :  Diffusion models are a class of deep generative models that have shown impressive results on various tasks with a solid theoretical foundation. Despite demonstrated success than state-of-the-art approaches, diffusion models often entail costly sampling procedures and sub-optimal likelihood estimation. Significant efforts have been made to improve the performance of diffusion models in various aspects. In this article, we present a comprehensive review of existing variants of diffusion models. Specifically, we provide the taxonomy of research in diffusion models and categorize them into three types: sampling-efficiency **enhancement**, likelihood-maximization **enhancement**, and data-generalization **enhancement**. We also introduce the other generative models (i.e., variational autoencoders, generative adversarial networks, normalizing flow, autoregressive models, and energy-based models) and discuss the connections between diffusion models and these generative models. Then we review the applications of diffusion models, including computer vision, natural language processing, temporal data modeling, multi-modal learning, robust learning, molecular graph modeling, material design, and inverse problem solving. Furthermore, we propose new perspectives pertaining to the development of generative models. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.  
### 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v2 [cs.CV] UPDATED)
- Authors : Ho Hin, Shunxing Bao, Yuankai Huo
- Link : [http://arxiv.org/abs/2209.15076](http://arxiv.org/abs/2209.15076)
> ABSTRACT  :  Vision transformers (ViTs) have quickly superseded convolutional networks (ConvNets) as the current state-of-the-art (SOTA) models for medical image segmentation. Hierarchical transformers (e.g., **Swin** Transformers) reintroduced several ConvNet priors and further enhanced the practical viability of adapting volumetric segmentation in 3D medical datasets. The effectiveness of hybrid approaches is largely credited to the large receptive field for non-local self-attention and the large number of model parameters. In this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the hierarchical transformer using ConvNet modules for robust volumetric segmentation. Specifically, we revisit volumetric depth-wise convolutions with large kernel size (e.g. starting from $7\times7\times7$) to enable the larger global receptive fields, inspired by **Swin** Transformer. We further substitute the multi-layer perceptron (MLP) in **Swin** Transformer blocks with pointwise depth convolutions and enhance model performances with fewer normalization and activation layers, thus reducing the number of model parameters. 3D UX-Net competes favorably with current SOTA transformers (e.g. **Swin**UNETR) using three challenging public datasets on volumetric brain and abdominal imaging: 1) MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI Challenge 2022 AMOS. 3D UX-Net consistently outperforms **Swin**UNETR with improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice (Feta2021). We further evaluate the transfer learning capability of 3D UX-Net with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880 to 0.900). The source code with our proposed model are available at https://github.com/MASILab/3DUX-Net.  
## cs.AI
---
### CR**ISP**: Curriculum based Sequential Neural Decoders for Polar Code Family. (arXiv:2210.00313v1 [cs.IT])
- Authors : Ashwin Hebbar, Viraj Nadkarni, Ashok Vardhan, Suma Bhat, Sewoong Oh, Pramod Viswanath
- Link : [http://arxiv.org/abs/2210.00313](http://arxiv.org/abs/2210.00313)
> ABSTRACT  :  Polar codes are widely used state-of-the-art codes for reliable communication that have recently been included in the 5th generation wireless standards (5G). However, there remains room for the design of polar decoders that are both efficient and reliable in the short blocklength regime. Motivated by recent successes of data-driven channel decoders, we introduce a novel $\textbf{C}$ur$\textbf{RI}$culum based $\textbf{S}$equential neural decoder for $\textbf{P}$olar codes (CR**ISP**). We design a principled curriculum, guided by information-theoretic insights, to train CR**ISP** and show that it outperforms the successive-cancellation (SC) decoder and attains near-optimal reliability performance on the Polar(16,32) and Polar(22, 64) codes. The choice of the proposed curriculum is critical in achieving the accuracy gains of CR**ISP**, as we show by comparing against other curricula. More notably, CR**ISP** can be readily extended to Polarization-Adjusted-Convolutional (PAC) codes, where existing SC decoders are significantly less reliable. To the best of our knowledge, CR**ISP** constructs the first data-driven decoder for PAC codes and attains near-optimal performance on the PAC(16, 32) code.  
### Robust Bayesian optimization with reinforcement learned acquisition functions. (arXiv:2210.00476v1 [cs.LG])
- Authors : Zijing Liu, Xiyao Qu, Xuejun Liu, Hongqiang Lyu
- Link : [http://arxiv.org/abs/2210.00476](http://arxiv.org/abs/2210.00476)
> ABSTRACT  :  In Bayesian optimization (BO) for expensive black-box optimization tasks, acquisition function (AF) guides sequential sampling and plays a pivotal role for efficient convergence to better optima. Prevailing AFs usually rely on artificial experiences in terms of preferences for exploration or exploitation, which runs a risk of a computational waste or traps in local optima and resultant re-optimization. To address the crux, the idea of data-driven AF selection is proposed, and the sequential AF selection task is further formalized as a Markov decision process (MDP) and resort to powerful reinforcement learning (RL) technologies. Appropriate selection policy for AFs is learned from superior BO trajectories to balance between exploration and exploitation in **real time**, which is called reinforcement-learning-assisted Bayesian optimization (RLABO). Competitive and robust BO evaluations on five benchmark problems demonstrate RL's recognition of the implicit AF selection pattern and imply the proposal's potential practicality for intelligent AF selection as well as efficient optimization in expensive black-box problems.  
### Unsupervised Speech **Enhancement** using Dynamical Variational Auto-Encoders. (arXiv:2106.12271v3 [cs.SD] UPDATED)
- Authors : Xiaoyu Bie, Simon Leglaive, Xavier Alameda, Laurent Girin
- Link : [http://arxiv.org/abs/2106.12271](http://arxiv.org/abs/2106.12271)
> ABSTRACT  :  Dynamical variational autoencoders (DVAEs) are a class of deep generative models with latent variables, dedicated to model time series of high-dimensional data. DVAEs can be considered as extensions of the variational autoencoder (VAE) that include temporal dependencies between successive observed and/or latent vectors. Previous work has shown the interest of using DVAEs over the VAE for speech spectrograms modeling. Independently, the VAE has been successfully applied to speech **enhancement** in noise, in an unsupervised noise-agnostic set-up that requires neither noise samples nor noisy speech samples at training time, but only requires clean speech signals. In this paper, we extend these works to DVAE-based single-channel unsupervised speech **enhancement**, hence exploiting both speech signals unsupervised representation learning and dynamics modeling. We propose an unsupervised speech **enhancement** algorithm that combines a DVAE speech prior pre-trained on clean speech signals with a noise model based on nonnegative matrix factorization, and we derive a variational expectation-maximization (VEM) algorithm to perform speech **enhancement**. The algorithm is presented with the most general DVAE formulation and is then applied with three specific DVAE models to illustrate the versatility of the framework. Experimental results show that the proposed DVAE-based approach outperforms its VAE-based counterpart, as well as several supervised and unsupervised noise-dependent baselines, especially when the noise type is unseen during training.  
### Global Context Vision Transformers. (arXiv:2206.09959v3 [cs.CV] UPDATED)
- Authors : Ali Hatamizadeh, Hongxu Yin, Jan Kautz, Pavlo Molchanov
- Link : [http://arxiv.org/abs/2206.09959](http://arxiv.org/abs/2206.09959)
> ABSTRACT  :  We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision tasks. The core of the novel model are global context self-attention modules, joint with standard local self-attention, to effectively yet efficiently model both long and short-range spatial interactions, as an alternative to complex operations such as an attention masks or local windows shifting. While the local self-attention modules are responsible for modeling short-range information, the global query tokens are shared across all global self-attention modules to interact with local key and values. In addition, we address the lack of inductive bias in ViTs and improve the modeling of inter-channel dependencies by proposing a novel downsampler which leverages a parameter-efficient fused inverted residual block. The proposed GC ViT achieves new state-of-the-art performance across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the tiny, small and base variants of GC ViT with 28M, 51M and 90M parameters achieve 83.4%, 83.9% and 84.4% Top-1 accuracy, respectively, surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based **Swin** Transformer. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets outperform prior work consistently, sometimes by large margins. Code and pre-trained models are available at https://github.com/NVlabs/GCViT.  
### Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v7 [cs.LG] UPDATED)
- Authors : Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui, Hsuan Yang
- Link : [http://arxiv.org/abs/2209.00796](http://arxiv.org/abs/2209.00796)
> ABSTRACT  :  Diffusion models are a class of deep generative models that have shown impressive results on various tasks with a solid theoretical foundation. Despite demonstrated success than state-of-the-art approaches, diffusion models often entail costly sampling procedures and sub-optimal likelihood estimation. Significant efforts have been made to improve the performance of diffusion models in various aspects. In this article, we present a comprehensive review of existing variants of diffusion models. Specifically, we provide the taxonomy of research in diffusion models and categorize them into three types: sampling-efficiency **enhancement**, likelihood-maximization **enhancement**, and data-generalization **enhancement**. We also introduce the other generative models (i.e., variational autoencoders, generative adversarial networks, normalizing flow, autoregressive models, and energy-based models) and discuss the connections between diffusion models and these generative models. Then we review the applications of diffusion models, including computer vision, natural language processing, temporal data modeling, multi-modal learning, robust learning, molecular graph modeling, material design, and inverse problem solving. Furthermore, we propose new perspectives pertaining to the development of generative models. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.  
# Paper List
---
## cs.CV
---
**150** new papers in cs.CV:-) 
1. VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. (arXiv:2210.00030v1 [cs.RO])
2. Differentially Private Bias-Term only Fine-tuning of Foundation Models. (arXiv:2210.00036v1 [cs.LG])
3. Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v1 [cs.LG])
4. $MaskTune$: Mitigating Spurious Correlations by Forcing to Explore. (arXiv:2210.00055v1 [cs.LG])
5. D-Align: Dual Query Co-attention Network for 3D Object Detection Based on Multi-frame Point Cloud Sequence. (arXiv:2210.00087v1 [cs.CV])
6. Federated Training of Dual Encoding Models on Small Non-IID Client Datasets. (arXiv:2210.00092v1 [cs.LG])
7. Adaptive Weight Decay: On The Fly Weight Decay Tuning for Improving Robustness. (arXiv:2210.00094v1 [cs.LG])
8. Image-Based Detection of Modifications in Gas Pump PCBs with Deep Convolutional Autoencoders. (arXiv:2210.00100v1 [cs.CV])
9. Contrastive Corpus Attribution for Explaining Representations. (arXiv:2210.00107v1 [cs.LG])
10. Robust Person Identification: A WiFi Vision-based Approach. (arXiv:2210.00127v1 [cs.CV])
11. An In-depth Study of Stochastic Backpropagation. (arXiv:2210.00129v1 [cs.CV])
12. Alignment-guided Temporal Attention for Video Action Recognition. (arXiv:2210.00132v1 [cs.CV])
13. IMB-NAS: Neural Architecture Search for Imbalanced Datasets. (arXiv:2210.00136v1 [cs.LG])
14. Automated segmentation of microvessels in intravascular OCT images using deep learning. (arXiv:2210.00166v1 [eess.IV])
15. Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022. (arXiv:2210.00174v1 [cs.CV])
16. EAPruning: Evolutionary Pruning for Vision Transformers and CNNs. (arXiv:2210.00181v1 [cs.CV])
17. Structure-Aware NeRF without Posed Camera via Epipolar Constraint. (arXiv:2210.00183v1 [cs.CV])
18. Cut-Paste Consistency Learning for Semi-Supervised Lesion Segmentation. (arXiv:2210.00191v1 [cs.CV])
19. Differentiable Parsing and Visual Grounding of Verbal Instructions for Object Placement. (arXiv:2210.00215v1 [cs.RO])
20. A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering. (arXiv:2210.00220v1 [cs.CV])
21. Motion-inductive Self-supervised Object Discovery in Videos. (arXiv:2210.00221v1 [cs.CV])
22. Contour-Aware Equipotential Learning for Semantic Segmentation. (arXiv:2210.00223v1 [cs.CV])
23. Attention Augmented ConvNeXt UNet For Rectal Tumour Segmentation. (arXiv:2210.00227v1 [eess.IV])
24. Learnable Distribution Calibration for Few-Shot Class-Incremental Learning. (arXiv:2210.00232v1 [cs.CV])
25. Blindly Deconvolving Super-noisy Blurry Image Sequences. (arXiv:2210.00252v1 [cs.CV])
26. Cascaded Multi-Modal Mixing Transformers for Alzheimer's Disease Classification with Incomplete Data. (arXiv:2210.00255v1 [eess.IV])
27. Long-Tailed Class Incremental Learning. (arXiv:2210.00266v1 [cs.CV])
28. Offline Handwritten Amharic Character Recognition Using Few-shot Learning. (arXiv:2210.00275v1 [cs.CV])
29. Det-SLAM: A semantic visual SLAM for highly dynamic scenes using Detectron2. (arXiv:2210.00278v1 [cs.RO])
30. Gait-based Age Group Classification with Adaptive Graph Neural Network. (arXiv:2210.00294v1 [cs.LG])
31. An Ensemble of Convolutional Neural Networks to Detect Foliar Diseases in Apple Plants. (arXiv:2210.00298v1 [cs.CV])
32. Concurrent Recognition and Segmentation with Adaptive Segment Tokens. (arXiv:2210.00314v1 [cs.CV])
33. Longitudinal Sentiment Analyses for Radicalization Research: Intertemporal Dynamics on Social Media Platforms and their Implications. (arXiv:2210.00339v1 [cs.CL])
34. Evaluation of Pre-Trained CNN Models for Geographic Fake Image Detection. (arXiv:2210.00361v1 [cs.CV])
35. DCI-ES: An Extended Disentanglement Framework with Connections to Identifiability. (arXiv:2210.00364v1 [cs.LG])
36. NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review. (arXiv:2210.00379v1 [cs.CV])
37. Basic Binary Convolution Unit for Binarized Image **Restoration** Network. (arXiv:2210.00405v1 [cs.CV])
38. PCONet: A Convolutional Neural Network Architecture to Detect Polycystic Ovary Syndrome (PCOS) from Ovarian Ultrasound Images. (arXiv:2210.00407v1 [eess.IV])
39. Self-Supervised Monocular Depth Estimation: Solving the Edge-Fattening Problem. (arXiv:2210.00411v1 [cs.CV])
40. Unsupervised Vision and Vision-motion Calibration Strategies for PointGoal Navigation in Indoor Environment. (arXiv:2210.00413v1 [cs.CV])
41. ROSIA: Rotation-Search-Based Star Identification Algorithm. (arXiv:2210.00429v1 [cs.CV])
42. ManiCLIP: Multi-Attribute Face Manipulation from Text. (arXiv:2210.00445v1 [cs.CV])
43. A Smart Recycling Bin Using Waste Image Classification At The Edge. (arXiv:2210.00448v1 [cs.CV])
44. Fast OT for Latent Domain Adaptation. (arXiv:2210.00479v1 [cs.LG])
45. Compositional Generalization in Unsupervised Compositional Representation Learning: A Study on Disentanglement and Emergent Language. (arXiv:2210.00482v1 [cs.LG])
46. Unsupervised Multi-View Object Segmentation Using Radiance Field Propagation. (arXiv:2210.00489v1 [cs.CV])
47. DARE: A large-scale handwritten date recognition system. (arXiv:2210.00503v1 [cs.CV])
48. Loc-VAE: Learning Structurally Localized Representation from 3D Brain MR Images for Content-Based Image Retrieval. (arXiv:2210.00506v1 [eess.IV])
49. Fast and Robust Video-Based Exercise Classification via Body Pose Tracking and Scalable Multivariate Time Series Classifiers. (arXiv:2210.00507v1 [cs.CV])
50. Deep-OCTA: Ensemble Deep Learning Approaches for Diabetic Retinopathy Analysis on OCTA Images. (arXiv:2210.00515v1 [eess.IV])
51. Exploiting More Information in Sparse Point Cloud for 3D Single Object Tracking. (arXiv:2210.00519v1 [cs.CV])
52. Semi-autonomous Prosthesis Control Using Minimal Depth Information and Vibrotactile Feedback. (arXiv:2210.00541v1 [cs.CV])
53. Seeing Through The Noisy **Dark**: Toward Real-world **Low-Light** Image **Enhancement** and Denoising. (arXiv:2210.00545v1 [cs.CV])
54. Siamese-NAS: Using Trained Samples Efficiently to Find Lightweight Neural Architecture by Prior Knowledge. (arXiv:2210.00546v1 [cs.CV])
55. Scene Text Synthesis for Efficient and Effective Deep Network Training. (arXiv:1901.09193v2 [cs.CV] UPDATED)
56. A Review on Deep Learning in Medical Image Reconstruction. (arXiv:1906.10643v3 [eess.IV] UPDATED)
57. On the Risk of Cancelable Biometrics. (arXiv:1910.07770v4 [cs.CV] UPDATED)
58. Spatial-Aware GAN for Unsupervised Person Re-identification. (arXiv:1911.11312v3 [cs.CV] UPDATED)
59. Towards Realistic 3D Embedding via View Alignment. (arXiv:2007.07066v2 [cs.CV] UPDATED)
60. Bridging the Performance Gap between FGSM and PGD Adversarial Training. (arXiv:2011.05157v2 [cs.CR] UPDATED)
61. Achieving Domain Generalization in Underwater Object Detection by Domain Mixup and Contrastive Learning. (arXiv:2104.02230v5 [cs.CV] UPDATED)
62. Geometric Model Checking of Continuous Space. (arXiv:2105.06194v3 [cs.LO] UPDATED)
63. FoveaTer: Foveated Transformer for Image Classification. (arXiv:2105.14173v3 [cs.CV] UPDATED)
64. Neural Mixture Models with Expectation-Maximization for End-to-end Deep Clustering. (arXiv:2107.02453v2 [cs.LG] UPDATED)
65. Self-Supervised Aggregation of Diverse Experts for Test-Agnostic Long-Tailed Recognition. (arXiv:2107.09249v3 [cs.CV] UPDATED)
66. Automatic cerebral hemisphere segmentation in rat MRI with lesions via attention-based convolutional neural networks. (arXiv:2108.01941v3 [eess.IV] UPDATED)
67. Availability Attacks Against Neural Network Certifiers Based on Backdoors. (arXiv:2108.11299v4 [cs.LG] UPDATED)
68. ReLaX: Retinal Layer Attribution for Guided Explanations of Automated Optical Coherence Tomography Classification. (arXiv:2109.02436v3 [eess.IV] UPDATED)
69. Weak Novel Categories without Tears: A Survey on Weak-Shot Learning. (arXiv:2110.02651v3 [cs.CV] UPDATED)
70. Towards Data-Free Domain Generalization. (arXiv:2110.04545v3 [cs.LG] UPDATED)
71. Simultaneous Perturbation Method for Multi-Task Weight Optimization in One-Shot Meta-Learning. (arXiv:2110.13188v3 [cs.LG] UPDATED)
72. A General Divergence Modeling Strategy for Salient Object Detection. (arXiv:2111.11827v2 [cs.CV] UPDATED)
73. PU-Transformer: Point Cloud Upsampling Transformer. (arXiv:2111.12242v2 [cs.CV] UPDATED)
74. Sharpness-aware Quantization for Deep Neural Networks. (arXiv:2111.12273v3 [cs.CV] UPDATED)
75. Implicit Neural Deformation for Sparse-View Face Reconstruction. (arXiv:2112.02494v2 [cs.CV] UPDATED)
76. From Coarse to Fine-grained Concept based Discrimination for Phrase Detection. (arXiv:2112.03237v4 [cs.CV] UPDATED)
77. Superpixel-Based Building Damage Detection from Post-earthquake Very High Resolution Imagery Using Deep Neural Networks. (arXiv:2112.04744v4 [cs.CV] UPDATED)
78. Change Detection Meets Visual Question Answering. (arXiv:2112.06343v2 [cs.CV] UPDATED)
79. Hallucinating Pose-Compatible Scenes. (arXiv:2112.06909v2 [cs.CV] UPDATED)
80. A Simple Single-Scale Vision Transformer for Object Localization and Instance Segmentation. (arXiv:2112.09747v3 [cs.CV] UPDATED)
81. Learning Spatially-Adaptive Squeeze-Excitation Networks for Image Synthesis and Image Recognition. (arXiv:2112.14804v3 [cs.CV] UPDATED)
82. Colour alignment for relative colour constancy via non-standard references. (arXiv:2112.15106v2 [eess.IV] UPDATED)
83. A Novel Skeleton-Based Human Activity Discovery Using Particle Swarm Optimization with Gaussian Mutation. (arXiv:2201.05314v3 [cs.CV] UPDATED)
84. Offline-Online Associated Camera-Aware Proxies for Unsupervised Person Re-identification. (arXiv:2201.05820v2 [cs.CV] UPDATED)
85. Domain Adaptation via Bidirectional Cross-Attention Transformer. (arXiv:2201.05887v2 [cs.CV] UPDATED)
86. DermX: an end-to-end framework for explainable automated dermatological diagnosis. (arXiv:2202.06956v2 [eess.IV] UPDATED)
87. DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training. (arXiv:2202.13808v2 [cs.CV] UPDATED)
88. Adversarial Counterfactual Augmentation: Application in Alzheimer's Disease Classification. (arXiv:2203.07815v2 [eess.IV] UPDATED)
89. PressureVision: Estimating Hand Pressure from a Single RGB Image. (arXiv:2203.10385v3 [cs.CV] UPDATED)
90. Simplicial Embeddings in Self-Supervised Learning and Downstream Classification. (arXiv:2204.00616v2 [cs.LG] UPDATED)
91. KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v2 [cs.CV] UPDATED)
92. ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v4 [cs.CV] UPDATED)
93. Zero-Shot Category-Level Object Pose Estimation. (arXiv:2204.03635v2 [cs.CV] UPDATED)
94. Interpretable Vertebral Fracture Quantification via Anchor-Free Landmarks Localization. (arXiv:2204.06818v2 [eess.IV] UPDATED)
95. Translating Clinical Delineation of Diabetic Foot Ulcers into Machine Interpretable Segmentation. (arXiv:2204.11618v2 [eess.IV] UPDATED)
96. Sound Localization by Self-Supervised Time Delay Estimation. (arXiv:2204.12489v2 [cs.CV] UPDATED)
97. Generative Adversarial Networks for Image Super-Resolution: A Survey. (arXiv:2204.13620v2 [eess.IV] UPDATED)
98. Kernel Normalized Convolutional Networks. (arXiv:2205.10089v2 [cs.LG] UPDATED)
99. Structure Aware and Class Balanced 3D Object Detection on nuScenes Dataset. (arXiv:2205.12519v2 [cs.CV] UPDATED)
100. Matryoshka Representation Learning. (arXiv:2205.13147v3 [cs.LG] UPDATED)
101. PREF: Phasorial Embedding Fields for Compact Neural Representations. (arXiv:2205.13524v3 [cs.CV] UPDATED)
102. Harnessing spectral representations for subgraph alignment. (arXiv:2205.14938v3 [cs.LG] UPDATED)
103. What Knowledge Gets Distilled in Knowledge Distillation?. (arXiv:2205.16004v2 [cs.CV] UPDATED)
104. Improving Diffusion Models for Inverse Problems using Manifold Constraints. (arXiv:2206.00941v2 [cs.LG] UPDATED)
105. Learning Probabilistic Topological Representations Using Discrete Morse Theory. (arXiv:2206.01742v2 [eess.IV] UPDATED)
106. OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression. (arXiv:2206.02338v2 [cs.CV] UPDATED)
107. SHRED: 3D Shape Region Decomposition with Learned Local Operations. (arXiv:2206.03480v2 [cs.CV] UPDATED)
108. Global Context Vision Transformers. (arXiv:2206.09959v3 [cs.CV] UPDATED)
109. Generative Modelling With Inverse Heat Dissipation. (arXiv:2206.13397v4 [cs.CV] UPDATED)
110. Network Pruning via Feature Shift Minimization. (arXiv:2207.02632v2 [cs.CV] UPDATED)
111. SphereVLAD++: Attention-based and Signal-enhanced Viewpoint Invariant Descriptor. (arXiv:2207.02958v2 [cs.CV] UPDATED)
112. Enhancing Fairness of Visual Attribute Predictors. (arXiv:2207.05727v3 [cs.CV] UPDATED)
113. Domain Gap Estimation for Source Free Unsupervised Domain Adaptation with Many Classifiers. (arXiv:2207.05785v2 [cs.CV] UPDATED)
114. PointNorm: Dual Normalization is All You Need for Point Cloud Analysis. (arXiv:2207.06324v3 [cs.CV] UPDATED)
115. EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations. (arXiv:2207.06635v2 [cs.CV] UPDATED)
116. Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v2 [cs.LG] UPDATED)
117. Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain Adaptation. (arXiv:2207.09280v2 [cs.CV] UPDATED)
118. Fully Sparse 3D Object Detection. (arXiv:2207.10035v2 [cs.CV] UPDATED)
119. Affective Behavior Analysis using Action Unit Relation Graph and Multi-task Cross Attention. (arXiv:2207.10293v2 [cs.CV] UPDATED)
120. TVCalib: Camera Calibration for Sports Field Registration in Soccer. (arXiv:2207.11709v2 [cs.CV] UPDATED)
121. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v3 [cs.LG] UPDATED)
122. Robust Quantitative Susceptibility Mapping via Approximate Message Passing. (arXiv:2207.14709v2 [eess.IV] UPDATED)
123. DALLE-URBAN: Capturing the urban design expertise of large text to image transformers. (arXiv:2208.04139v2 [cs.CV] UPDATED)
124. RelPose: Predicting Probabilistic Relative Rotation for Single Objects in the Wild. (arXiv:2208.05963v2 [cs.CV] UPDATED)
125. Anomaly segmentation model for defects detection in electroluminescence images of heterojunction solar cells. (arXiv:2208.05994v3 [cs.CV] UPDATED)
126. BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers. (arXiv:2208.06366v2 [cs.CV] UPDATED)
127. AdaBin: Improving Binary Neural Networks with Adaptive Binary Sets. (arXiv:2208.08084v2 [cs.CV] UPDATED)
128. Contrastive Domain Adaptation for Early Misinformation Detection: A Case Study on COVID-19. (arXiv:2208.09578v4 [cs.CV] UPDATED)
129. DeepInteraction: 3D Object Detection via Modality Interaction. (arXiv:2208.11112v3 [cs.CV] UPDATED)
130. Light-YOLOv5: A Lightweight Algorithm for Improved YOLOv5 in Complex Fire Scenarios. (arXiv:2208.13422v2 [cs.CV] UPDATED)
131. Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v7 [cs.LG] UPDATED)
132. TrackletMapper: Ground Surface Segmentation and Mapping from Traffic Participant Trajectories. (arXiv:2209.05247v3 [cs.RO] UPDATED)
133. DevNet: Self-supervised Monocular Depth Learning via Density Volume Construction. (arXiv:2209.06351v4 [cs.CV] UPDATED)
134. SCULPTOR: Skeleton-Consistent Face Creation Using a Learned Parametric Generator. (arXiv:2209.06423v2 [cs.CV] UPDATED)
135. PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6 DoF Tracking. (arXiv:2209.07589v2 [cs.CV] UPDATED)
136. DMMGAN: Diverse Multi Motion Prediction of 3D Human Joints using Attention-Based Generative Adverserial Network. (arXiv:2209.09124v2 [cs.CV] UPDATED)
137. Text2Light: Zero-Shot Text-Driven **HDR** Panorama Generation. (arXiv:2209.09898v2 [cs.CV] UPDATED)
138. An Image Processing approach to identify solar plages observed at 393.37 nm by Kodaikanal Solar Observatory. (arXiv:2209.10631v2 [astro-ph.SR] UPDATED)
139. LGDN: Language-Guided Denoising Network for Video-Language Modeling. (arXiv:2209.11388v2 [cs.CV] UPDATED)
140. Towards Stable Co-saliency Detection and Object Co-segmentation. (arXiv:2209.12138v2 [cs.CV] UPDATED)
141. InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction. (arXiv:2209.12354v2 [cs.CV] UPDATED)
142. Multi-stage image denoising with the wavelet transform. (arXiv:2209.12394v3 [eess.IV] UPDATED)
143. TokenFlow: Rethinking Fine-grained Cross-modal Alignment in Vision-Language Retrieval. (arXiv:2209.13822v2 [cs.CV] UPDATED)
144. 360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance. (arXiv:2209.14265v2 [cs.CV] UPDATED)
145. Re-Imagen: Retrieval-Augmented Text-to-Image Generator. (arXiv:2209.14491v2 [cs.CV] UPDATED)
146. Regularizing Neural Network Training via Identity-wise Discriminative Feature Suppression. (arXiv:2209.14553v2 [cs.CV] UPDATED)
147. Human Motion Diffusion Model. (arXiv:2209.14916v2 [cs.CV] UPDATED)
148. 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v2 [cs.CV] UPDATED)
149. Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods. (arXiv:2209.15589v2 [cs.CV] UPDATED)
150. Bias Mimicking: A Simple Sampling Approach for Bias Mitigation. (arXiv:2209.15605v2 [cs.CV] UPDATED)
## eess.IV
---
**31** new papers in eess.IV:-) 
1. Automated segmentation of microvessels in intravascular OCT images using deep learning. (arXiv:2210.00166v1 [eess.IV])
2. Attention Augmented ConvNeXt UNet For Rectal Tumour Segmentation. (arXiv:2210.00227v1 [eess.IV])
3. Cascaded Multi-Modal Mixing Transformers for Alzheimer's Disease Classification with Incomplete Data. (arXiv:2210.00255v1 [eess.IV])
4. Federated Representation Learning via Maximal Coding Rate Reduction. (arXiv:2210.00299v1 [cs.LG])
5. Basic Binary Convolution Unit for Binarized Image **Restoration** Network. (arXiv:2210.00405v1 [cs.CV])
6. PCONet: A Convolutional Neural Network Architecture to Detect Polycystic Ovary Syndrome (PCOS) from Ovarian Ultrasound Images. (arXiv:2210.00407v1 [eess.IV])
7. Accelerated partial separable model using dimension-reduced optimization technique for ultra-fast cardiac MRI. (arXiv:2210.00493v1 [eess.IV])
8. Loc-VAE: Learning Structurally Localized Representation from 3D Brain MR Images for Content-Based Image Retrieval. (arXiv:2210.00506v1 [eess.IV])
9. Deep-OCTA: Ensemble Deep Learning Approaches for Diabetic Retinopathy Analysis on OCTA Images. (arXiv:2210.00515v1 [eess.IV])
10. Energy-Rate-Quality Tradeoffs of State-of-the-Art Video Codecs. (arXiv:2210.00618v1 [eess.IV])
11. EraseNet: A Recurrent Residual Network for Supervised Document Cleaning. (arXiv:2210.00708v1 [cs.CV])
12. Privacy-Preserving Feature Coding for Machines. (arXiv:2210.00727v1 [eess.IV])
13. BVI-VFI: A Video Quality Database for Video Frame Interpolation. (arXiv:2210.00823v1 [eess.IV])
14. Random Data Augmentation based **Enhancement**: A Generalized **Enhancement** Approach for Medical Datasets. (arXiv:2210.00824v1 [eess.IV])
15. Perceptual Attacks of No-Reference Image Quality Models with Human-in-the-Loop. (arXiv:2210.00933v1 [cs.CV])
16. Unsupervised Multimodal Change Detection Based on Structural Relationship Graph Representation Learning. (arXiv:2210.00941v1 [cs.CV])
17. Hip Fracture Prediction using the First Principal Component Derived from FEA-Computed Fracture Loads. (arXiv:2210.01032v1 [cs.LG])
18. A Review on Deep Learning in Medical Image Reconstruction. (arXiv:1906.10643v3 [eess.IV] UPDATED)
19. Automatic cerebral hemisphere segmentation in rat MRI with lesions via attention-based convolutional neural networks. (arXiv:2108.01941v3 [eess.IV] UPDATED)
20. ReLaX: Retinal Layer Attribution for Guided Explanations of Automated Optical Coherence Tomography Classification. (arXiv:2109.02436v3 [eess.IV] UPDATED)
21. Superpixel-Based Building Damage Detection from Post-earthquake Very High Resolution Imagery Using Deep Neural Networks. (arXiv:2112.04744v4 [cs.CV] UPDATED)
22. Colour alignment for relative colour constancy via non-standard references. (arXiv:2112.15106v2 [eess.IV] UPDATED)
23. DermX: an end-to-end framework for explainable automated dermatological diagnosis. (arXiv:2202.06956v2 [eess.IV] UPDATED)
24. Training Generative Adversarial Networks for Optical Property Mapping using Synthetic Image Data. (arXiv:2203.07793v2 [eess.IV] UPDATED)
25. Adversarial Counterfactual Augmentation: Application in Alzheimer's Disease Classification. (arXiv:2203.07815v2 [eess.IV] UPDATED)
26. Interpretable Vertebral Fracture Quantification via Anchor-Free Landmarks Localization. (arXiv:2204.06818v2 [eess.IV] UPDATED)
27. Translating Clinical Delineation of Diabetic Foot Ulcers into Machine Interpretable Segmentation. (arXiv:2204.11618v2 [eess.IV] UPDATED)
28. Generative Adversarial Networks for Image Super-Resolution: A Survey. (arXiv:2204.13620v2 [eess.IV] UPDATED)
29. Learning Probabilistic Topological Representations Using Discrete Morse Theory. (arXiv:2206.01742v2 [eess.IV] UPDATED)
30. Robust Quantitative Susceptibility Mapping via Approximate Message Passing. (arXiv:2207.14709v2 [eess.IV] UPDATED)
31. Multi-stage image denoising with the wavelet transform. (arXiv:2209.12394v3 [eess.IV] UPDATED)
## cs.LG
---
**261** new papers in cs.LG:-) 
1. ModelAngelo: Automated Model Building in Cryo-EM Maps. (arXiv:2210.00006v1 [q-bio.QM])
2. Adversarial Attacks on Transformers-Based Malware Detectors. (arXiv:2210.00008v1 [cs.CR])
3. Artificial Replay: A Meta-Algorithm for Harnessing Historical Data in Bandits. (arXiv:2210.00025v1 [cs.LG])
4. VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. (arXiv:2210.00030v1 [cs.RO])
5. Direct Embedding of Temporal Network Edges via Time-Decayed Line Graphs. (arXiv:2210.00032v1 [cs.LG])
6. Neural Causal Models for Counterfactual Identification and Estimation. (arXiv:2210.00035v1 [cs.LG])
7. Differentially Private Bias-Term only Fine-tuning of Foundation Models. (arXiv:2210.00036v1 [cs.LG])
8. Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v1 [cs.LG])
9. Direct Estimation of Porosity from Seismic Data using Rock and Wave Physics Informed Neural Networks (RW-PINN). (arXiv:2210.00042v1 [physics.geo-ph])
10. Task Formulation Matters When Learning Continually: A Case Study in Visual Question Answering. (arXiv:2210.00044v1 [cs.LG])
11. Kernel Normalized Convolutional Networks for Privacy-Preserving Machine Learning. (arXiv:2210.00053v1 [cs.LG])
12. $MaskTune$: Mitigating Spurious Correlations by Forcing to Explore. (arXiv:2210.00055v1 [cs.LG])
13. FedTrees: A Novel Computation-Communication Efficient Federated Learning Framework Investigated in Smart Grids. (arXiv:2210.00060v1 [cs.LG])
14. Learning Robust Kernel Ensembles with Kernel Average Pooling. (arXiv:2210.00062v1 [cs.LG])
15. DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. (arXiv:2210.00063v1 [cs.CL])
16. CEREAL: Few-Sample Clustering Evaluation. (arXiv:2210.00064v1 [cs.LG])
17. Application of Deep Q Learning with Stimulation Results for Elevator Optimization. (arXiv:2210.00065v1 [cs.LG])
18. Improving Policy Learning via Language Dynamics Distillation. (arXiv:2210.00066v1 [cs.LG])
19. Multi-Task Option Learning and Discovery for Stochastic Path Planning. (arXiv:2210.00068v1 [cs.LG])
20. TOAST: Topological Algorithm for Singularity Tracking. (arXiv:2210.00069v1 [cs.LG])
21. Digital Twin and Artificial Intelligence Incorporated With Surrogate Modeling for Hybrid and Sustainable Energy Systems. (arXiv:2210.00073v1 [cs.AI])
22. Leveraging Industry 4.0 -- Deep Learning, Surrogate Model and Transfer Learning with Uncertainty Quantification Incorporated into Digital Twin for Nuclear System. (arXiv:2210.00074v1 [cs.LG])
23. E-Branchformer: Branchformer with Enhanced merging for speech recognition. (arXiv:2210.00077v1 [eess.AS])
24. Causal Estimation for Text Data with (Apparent) Overlap Violations. (arXiv:2210.00079v1 [stat.ML])
25. Contrastive Graph Few-Shot Learning. (arXiv:2210.00084v1 [cs.LG])
26. A Multi-label Time Series Classification Approach for Non-intrusive Water End-Use Monitoring. (arXiv:2210.00089v1 [cs.LG])
27. Data-driven discovery of non-Newtonian astronomy via learning non-Euclidean Hamiltonian. (arXiv:2210.00090v1 [cs.LG])
28. Federated Training of Dual Encoding Models on Small Non-IID Client Datasets. (arXiv:2210.00092v1 [cs.LG])
29. Adaptive Weight Decay: On The Fly Weight Decay Tuning for Improving Robustness. (arXiv:2210.00094v1 [cs.LG])
30. Image-Based Detection of Modifications in Gas Pump PCBs with Deep Convolutional Autoencoders. (arXiv:2210.00100v1 [cs.CV])
31. MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization. (arXiv:2210.00102v1 [cs.LG])
32. Contrastive Corpus Attribution for Explaining Representations. (arXiv:2210.00107v1 [cs.LG])
33. ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks. (arXiv:2210.00108v1 [cs.LG])
34. Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information. (arXiv:2210.00116v1 [cs.LG])
35. Visuo-Tactile Transformers for Manipulation. (arXiv:2210.00121v1 [cs.RO])
36. Adversarial Robustness of Representation Learning for Knowledge Graphs. (arXiv:2210.00122v1 [cs.LG])
37. Implicit Neural Spatial Representations for Time-dependent PDEs. (arXiv:2210.00124v1 [cs.LG])
38. IMB-NAS: Neural Architecture Search for Imbalanced Datasets. (arXiv:2210.00136v1 [cs.LG])
39. Efficiently Learning Small Policies for Locomotion and Manipulation. (arXiv:2210.00140v1 [cs.RO])
40. Diving into Unified Data-Model Sparsity for Class-Imbalanced Graph Representation Learning. (arXiv:2210.00162v1 [cs.LG])
41. Automated segmentation of microvessels in intravascular OCT images using deep learning. (arXiv:2210.00166v1 [eess.IV])
42. Multi-stage Progressive Compression of Conformer Transducer for On-device Speech Recognition. (arXiv:2210.00169v1 [cs.SD])
43. Predictive Inference with Feature Conformal Prediction. (arXiv:2210.00173v1 [cs.LG])
44. Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022. (arXiv:2210.00174v1 [cs.CV])
45. Technical Report-IoT Devices Proximity Authentication In Ad Hoc Network Environment. (arXiv:2210.00175v1 [cs.CR])
46. A Combinatorial Perspective on the Optimization of Shallow ReLU Networks. (arXiv:2210.00176v1 [cs.LG])
47. On the tightness of linear relaxation based robustness certification methods. (arXiv:2210.00178v1 [cs.LG])
48. Pitfalls of Gaussians as a noise distribution in NCE. (arXiv:2210.00189v1 [cs.LG])
49. Cut-Paste Consistency Learning for Semi-Supervised Lesion Segmentation. (arXiv:2210.00191v1 [cs.CV])
50. Boosting Exploration in Actor-Critic Algorithms by Incentivizing Plausible Novel States. (arXiv:2210.00211v1 [cs.LG])
51. Efficient Quantum Agnostic Improper Learning of Decision Trees. (arXiv:2210.00212v1 [quant-ph])
52. HyperHawkes: Hypernetwork based Neural Temporal Point Process. (arXiv:2210.00213v1 [cs.LG])
53. Differentiable Parsing and Visual Grounding of Verbal Instructions for Object Placement. (arXiv:2210.00215v1 [cs.RO])
54. Solving practical multi-body dynamics problems using a single neural operator. (arXiv:2210.00222v1 [cs.LG])
55. Towards Understanding and Mitigating Dimensional Collapse in Heterogeneous Federated Learning. (arXiv:2210.00226v1 [cs.LG])
56. Attention Augmented ConvNeXt UNet For Rectal Tumour Segmentation. (arXiv:2210.00227v1 [eess.IV])
57. On The Relative Error of Random Fourier Features for Preserving Kernel Distance. (arXiv:2210.00244v1 [cs.LG])
58. Heterogeneous Graph Contrastive Multi-view Learning. (arXiv:2210.00248v1 [cs.LG])
59. Fine-tuning Wav2vec for Vocal-burst Emotion Recognition. (arXiv:2210.00263v1 [eess.AS])
60. Solar Power Time Series Forecasting Utilising Wavelet Coefficients. (arXiv:2210.00269v1 [cs.LG])
61. FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities. (arXiv:2210.00272v1 [cs.LG])
62. NeuroEvo: A Cloud-based Platform for Automated Design and Training of Neural Networks using Evolutionary and Particle Swarm Algorithms. (arXiv:2210.00286v1 [cs.NE])
63. DeltaBound Attack: Efficient decision-based attack in low queries regime. (arXiv:2210.00292v1 [cs.LG])
64. Deep Intrinsically Motivated Exploration in Continuous Control. (arXiv:2210.00293v1 [cs.LG])
65. Gait-based Age Group Classification with Adaptive Graph Neural Network. (arXiv:2210.00294v1 [cs.LG])
66. An Ensemble of Convolutional Neural Networks to Detect Foliar Diseases in Apple Plants. (arXiv:2210.00298v1 [cs.CV])
67. Federated Representation Learning via Maximal Coding Rate Reduction. (arXiv:2210.00299v1 [cs.LG])
68. Learning Globally Smooth Functions on Manifolds. (arXiv:2210.00301v1 [cs.LG])
69. Clustering for directed graphs using parametrized random walk diffusion kernels. (arXiv:2210.00310v1 [cs.LG])
70. CR**ISP**: Curriculum based Sequential Neural Decoders for Polar Code Family. (arXiv:2210.00313v1 [cs.IT])
71. Concurrent Recognition and Segmentation with Adaptive Segment Tokens. (arXiv:2210.00314v1 [cs.CV])
72. Implementation of a Three-class Classification LS-SVM Model for the Hybrid Antenna Array with Bowtie Elements in the Adaptive Beamforming Application. (arXiv:2210.00317v1 [cs.LG])
73. PathFinder: Discovering Decision Pathways in Deep Neural Networks. (arXiv:2210.00319v1 [cs.LG])
74. MALM: Mixing Augmented Language Modeling for Zero-Shot Machine Translation. (arXiv:2210.00320v1 [cs.CL])
75. Privacy-preserving Decentralized Federated Learning over Time-varying Communication Graph. (arXiv:2210.00325v1 [cs.CR])
76. Deep Recurrent Q-learning for Energy-constrained Coverage with a Mobile Robot. (arXiv:2210.00327v1 [cs.RO])
77. Speed Up the Cold-Start Learning in Two-Sided Bandits with Many Arms. (arXiv:2210.00340v1 [cs.LG])
78. Identifying Selections Operating on HIV-1 Reverse Transcriptase via Uniform Manifold Approximation and Projection. (arXiv:2210.00345v1 [q-bio.GN])
79. Behind the Scenes of Gradient Descent: A Trajectory Analysis via Basis Function Decomposition. (arXiv:2210.00346v1 [cs.LG])
80. Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning. (arXiv:2210.00350v1 [cs.RO])
81. Social and environmental impact of recent developments in machine learning on biology and chemistry research. (arXiv:2210.00356v1 [cs.CY])
82. DCI-ES: An Extended Disentanglement Framework with Connections to Identifiability. (arXiv:2210.00364v1 [cs.LG])
83. Parameter-varying neural ordinary differential equations with partition-of-unity networks. (arXiv:2210.00368v1 [cs.LG])
84. Convolutional Neural Networks on Manifolds: From Graphs and Back. (arXiv:2210.00376v1 [eess.SP])
85. Causal Knowledge Transfer from Task Affinity. (arXiv:2210.00380v1 [cs.LG])
86. Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks. (arXiv:2210.00400v1 [cs.LG])
87. PCONet: A Convolutional Neural Network Architecture to Detect Polycystic Ovary Syndrome (PCOS) from Ovarian Ultrasound Images. (arXiv:2210.00407v1 [eess.IV])
88. Self-Supervised Monocular Depth Estimation: Solving the Edge-Fattening Problem. (arXiv:2210.00411v1 [cs.CV])
89. Metric Distribution to Vector: Constructing Data Representation via Broad-Scale Discrepancies. (arXiv:2210.00415v1 [cs.LG])
90. Subspace Learning for Feature Selection via Rank Revealing QR Factorization: Unsupervised and Hybrid Approaches with Non-negative Matrix Factorization and Evolutionary Algorithm. (arXiv:2210.00418v1 [cs.LG])
91. Improved Algorithms for Neural Active Learning. (arXiv:2210.00423v1 [cs.LG])
92. Understanding Adversarial Robustness Against On-manifold Adversarial Examples. (arXiv:2210.00430v1 [cs.LG])
93. A Unified Framework for Optimization-Based Graph Coarsening. (arXiv:2210.00437v1 [stat.ML])
94. Grouped self-attention mechanism for a memory-efficient Transformer. (arXiv:2210.00440v1 [cs.LG])
95. ReAct: A Review Comment Dataset for Actionability (and more). (arXiv:2210.00443v1 [cs.CL])
96. Neural Graphical Models. (arXiv:2210.00453v1 [cs.LG])
97. Improved Stein Variational Gradient Descent with Importance Weights. (arXiv:2210.00462v1 [cs.LG])
98. OCD: Learning to Overfit with Conditional Diffusion Models. (arXiv:2210.00471v1 [cs.LG])
99. Robust Bayesian optimization with reinforcement learned acquisition functions. (arXiv:2210.00476v1 [cs.LG])
100. Fast OT for Latent Domain Adaptation. (arXiv:2210.00479v1 [cs.LG])
101. Compositional Generalization in Unsupervised Compositional Representation Learning: A Study on Disentanglement and Emergent Language. (arXiv:2210.00482v1 [cs.LG])
102. Learning Algorithm Generalization Error Bounds via Auxiliary Distributions. (arXiv:2210.00483v1 [cs.LG])
103. Approximate Computing and the Efficient Machine Learning Expedition. (arXiv:2210.00497v1 [cs.AR])
104. EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model. (arXiv:2210.00498v1 [cs.LG])
105. Loc-VAE: Learning Structurally Localized Representation from 3D Brain MR Images for Content-Based Image Retrieval. (arXiv:2210.00506v1 [eess.IV])
106. Fast and Robust Video-Based Exercise Classification via Body Pose Tracking and Scalable Multivariate Time Series Classifiers. (arXiv:2210.00507v1 [cs.CV])
107. Gradient Gating for Deep Multi-Rate Learning on Graphs. (arXiv:2210.00513v1 [cs.LG])
108. High Precision Differentiation Techniques for Data-Driven Solution of Nonlinear PDEs by Physics-Informed Neural Networks. (arXiv:2210.00518v1 [math.NA])
109. Leveraging unsupervised data and domain adaptation for deep regression in low-cost sensor calibration. (arXiv:2210.00521v1 [cs.LG])
110. Comparison of Data Representations and Machine Learning Architectures for User Identification on Arbitrary Motion Sequences. (arXiv:2210.00527v1 [cs.LG])
111. Heterogeneous Graph Neural Network for Privacy-Preserving Recommendation. (arXiv:2210.00538v1 [cs.LG])
112. Siamese-NAS: Using Trained Samples Efficiently to Find Lightweight Neural Architecture by Prior Knowledge. (arXiv:2210.00546v1 [cs.CV])
113. Occlusion-Aware Crowd Navigation Using People as Sensors. (arXiv:2210.00552v1 [cs.RO])
114. Adaptive Smoothness-weighted Adversarial Training for Multiple Perturbations with Its Stability Analysis. (arXiv:2210.00557v1 [cs.LG])
115. Deep CNN Framework for Audio Event Recognition using Weakly Labeled Web Data. (arXiv:1707.02530v3 [cs.SD] UPDATED)
116. GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification. (arXiv:1905.11475v4 [cs.LG] UPDATED)
117. A Review on Deep Learning in Medical Image Reconstruction. (arXiv:1906.10643v3 [eess.IV] UPDATED)
118. Prior Specification for Bayesian Matrix Factorization via Prior Predictive Matching. (arXiv:1910.12263v2 [stat.ML] UPDATED)
119. Regularized and Smooth Double Core Tensor Factorization for Heterogeneous Data. (arXiv:1911.10454v3 [stat.ML] UPDATED)
120. Analysis of Trade-offs in Fair Principal Component Analysis Based on Multi-objective Optimization. (arXiv:2006.06137v3 [cs.LG] UPDATED)
121. Multi-Partition Embedding Interaction with Block Term Format for Knowledge Graph Completion. (arXiv:2006.16365v2 [cs.LG] UPDATED)
122. Interpretable Representations in Explainable AI: From Theory to Practice. (arXiv:2008.07007v2 [cs.LG] UPDATED)
123. Dimension Reduction in Contextual Online Learning via Nonparametric Variable Selection. (arXiv:2009.08265v2 [cs.LG] UPDATED)
124. Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking. (arXiv:2010.00577v3 [cs.CL] UPDATED)
125. Bridging the Performance Gap between FGSM and PGD Adversarial Training. (arXiv:2011.05157v2 [cs.CR] UPDATED)
126. Probabilistic Federated Learning of Neural Networks Incorporated with Global Posterior Information. (arXiv:2012.03178v2 [cs.LG] UPDATED)
127. Lower Complexity Bounds of Finite-Sum Optimization Problems: The Results and Construction. (arXiv:2103.08280v4 [math.OC] UPDATED)
128. A contrastive rule for meta-learning. (arXiv:2104.01677v3 [cs.LG] UPDATED)
129. Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack based on the Critic Network. (arXiv:2104.03154v3 [cs.LG] UPDATED)
130. SMLSOM: The shrinking maximum likelihood self-organizing map. (arXiv:2104.13971v3 [cs.LG] UPDATED)
131. Adversarial Inverse Reinforcement Learning for Mean Field Games. (arXiv:2104.14654v4 [cs.LG] UPDATED)
132. RAIDER: Reinforcement-aided Spear Phishing Detector. (arXiv:2105.07582v2 [cs.CR] UPDATED)
133. Scalable Safety-Critical Policy Evaluation with Accelerated Rare Event Sampling. (arXiv:2106.10566v2 [cs.LG] UPDATED)
134. Bayesian Joint Chance Constrained Optimization: Approximations and Statistical Consistency. (arXiv:2106.12199v3 [math.ST] UPDATED)
135. Unsupervised Speech **Enhancement** using Dynamical Variational Auto-Encoders. (arXiv:2106.12271v3 [cs.SD] UPDATED)
136. Molecule Generation by Principal Subgraph Mining and Assembling. (arXiv:2106.15098v3 [cs.LG] UPDATED)
137. Neural Mixture Models with Expectation-Maximization for End-to-end Deep Clustering. (arXiv:2107.02453v2 [cs.LG] UPDATED)
138. Improving Model Robustness with Latent Distribution Locally and Globally. (arXiv:2107.04401v2 [cs.LG] UPDATED)
139. Universal Multilayer Network Exploration by Random Walk with Restart. (arXiv:2107.04565v2 [cs.LG] UPDATED)
140. Predicting Influential Higher-Order Patterns in Temporal Network Data. (arXiv:2107.12100v2 [cs.SI] UPDATED)
141. A Simple Approach to Automated Spectral Clustering. (arXiv:2107.12183v4 [cs.LG] UPDATED)
142. Pixyz: a library for developing deep generative models. (arXiv:2107.13109v2 [cs.LG] UPDATED)
143. Availability Attacks Against Neural Network Certifiers Based on Backdoors. (arXiv:2108.11299v4 [cs.LG] UPDATED)
144. ReLaX: Retinal Layer Attribution for Guided Explanations of Automated Optical Coherence Tomography Classification. (arXiv:2109.02436v3 [eess.IV] UPDATED)
145. Discovery of New Multi-Level Features for Domain Generalization via Knowledge Corruption. (arXiv:2109.04320v3 [cs.LG] UPDATED)
146. Minimax Mixing Time of the Metropolis-Adjusted Langevin Algorithm for Log-Concave Sampling. (arXiv:2109.13055v2 [stat.ML] UPDATED)
147. On Margin Maximization in Linear and ReLU Networks. (arXiv:2110.02732v4 [cs.LG] UPDATED)
148. Hierarchical Potential-based Reward Shaping from Task Specifications. (arXiv:2110.02792v3 [cs.LG] UPDATED)
149. Towards Data-Free Domain Generalization. (arXiv:2110.04545v3 [cs.LG] UPDATED)
150. Convergence of Random Reshuffling Under The Kurdyka-{\L}ojasiewicz Inequality. (arXiv:2110.04926v3 [math.OC] UPDATED)
151. Inferring Manifolds From Noisy Data Using Gaussian Processes. (arXiv:2110.07478v2 [stat.ML] UPDATED)
152. Optimistic Policy Optimization is Provably Efficient in Non-stationary MDPs. (arXiv:2110.08984v2 [cs.LG] UPDATED)
153. RoMA: a Method for Neural Network Robustness Measurement and Assessment. (arXiv:2110.11088v5 [cs.LG] UPDATED)
154. Learning Stable Vector Fields on Lie Groups. (arXiv:2110.11774v2 [cs.RO] UPDATED)
155. Simultaneous Perturbation Method for Multi-Task Weight Optimization in One-Shot Meta-Learning. (arXiv:2110.13188v3 [cs.LG] UPDATED)
156. Graph Structural Attack by Perturbing Spectral Distance. (arXiv:2111.00684v3 [cs.LG] UPDATED)
157. Exact Representation of Sparse Networks with Symmetric Nonnegative Embeddings. (arXiv:2111.03030v2 [cs.LG] UPDATED)
158. An Instance-Dependent Analysis for the Cooperative Multi-Player Multi-Armed Bandit. (arXiv:2111.04873v2 [cs.LG] UPDATED)
159. How to See Hidden Patterns in Metamaterials with Interpretable Machine Learning. (arXiv:2111.05949v4 [cs.LG] UPDATED)
160. Deep Safe Multi-Task Learning. (arXiv:2111.10601v2 [cs.LG] UPDATED)
161. Sharpness-aware Quantization for Deep Neural Networks. (arXiv:2111.12273v3 [cs.CV] UPDATED)
162. Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning. (arXiv:2112.11534v2 [physics.app-ph] UPDATED)
163. Learning Spatially-Adaptive Squeeze-Excitation Networks for Image Synthesis and Image Recognition. (arXiv:2112.14804v3 [cs.CV] UPDATED)
164. Uncertainty Detection and Reduction in Neural Decoding of EEG Signals. (arXiv:2201.00627v2 [eess.SP] UPDATED)
165. Domain Adaptation via Bidirectional Cross-Attention Transformer. (arXiv:2201.05887v2 [cs.CV] UPDATED)
166. Active Learning Polynomial Threshold Functions. (arXiv:2201.09433v2 [cs.LG] UPDATED)
167. Deep Generative Model for Periodic Graphs. (arXiv:2201.11932v2 [cs.LG] UPDATED)
168. Towards Robust Deep Active Learning for Scientific Computing. (arXiv:2201.12632v2 [cs.LG] UPDATED)
169. Rewiring with Positional Encodings for Graph Neural Networks. (arXiv:2201.12674v3 [cs.LG] UPDATED)
170. Graph Representation Learning Through Recoverability. (arXiv:2201.12843v3 [cs.LG] UPDATED)
171. When do Models Generalize? A Perspective from Data-Algorithm Compatibility. (arXiv:2202.06054v3 [cs.LG] UPDATED)
172. DermX: an end-to-end framework for explainable automated dermatological diagnosis. (arXiv:2202.06956v2 [eess.IV] UPDATED)
173. Path of Destruction: Learning an Iterative Level Generator Using a Small Dataset. (arXiv:2202.10184v2 [cs.LG] UPDATED)
174. Sign and Basis Invariant Networks for Spectral Graph Representation Learning. (arXiv:2202.13013v4 [cs.LG] UPDATED)
175. Graph Attention Retrospective. (arXiv:2202.13060v3 [cs.LG] UPDATED)
176. Naturally-meaningful and efficient descriptors: machine learning of material properties based on robust one-shot ab initio descriptors. (arXiv:2203.03392v2 [cond-mat.mtrl-sci] UPDATED)
177. **Bilateral** Deep Reinforcement Learning Approach for Better-than-human Car Following Model. (arXiv:2203.04749v3 [cs.RO] UPDATED)
178. Adversarial Counterfactual Augmentation: Application in Alzheimer's Disease Classification. (arXiv:2203.07815v2 [eess.IV] UPDATED)
179. Landscape Analysis for Surrogate Models in the Evolutionary Black-Box Context. (arXiv:2203.11315v2 [cs.NE] UPDATED)
180. SlimFL: Federated Learning with Superposition Coding over Slimmable Neural Networks. (arXiv:2203.14094v2 [cs.LG] UPDATED)
181. Towards Collaborative Intelligence: Routability Estimation based on Decentralized Private Data. (arXiv:2203.16009v2 [cs.LG] UPDATED)
182. Adversarial Speaker Distillation for Countermeasure Model on Automatic Speaker Verification. (arXiv:2203.17031v6 [cs.SD] UPDATED)
183. Simplicial Embeddings in Self-Supervised Learning and Downstream Classification. (arXiv:2204.00616v2 [cs.LG] UPDATED)
184. KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v2 [cs.CV] UPDATED)
185. Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v3 [cs.LG] UPDATED)
186. One-Way Matching of Datasets with Low Rank Signals. (arXiv:2204.13858v2 [math.ST] UPDATED)
187. A Simple and General Duality Proof for Wasserstein Distributionally Robust Optimization. (arXiv:2205.00362v2 [math.OC] UPDATED)
188. Convergence of Deep Neural Networks with General Activation Functions and Pooling. (arXiv:2205.06570v2 [cs.LG] UPDATED)
189. Graph Neural Networks Are More Powerful Than we Think. (arXiv:2205.09801v2 [cs.LG] UPDATED)
190. Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems. (arXiv:2205.09809v3 [cs.LG] UPDATED)
191. On Tackling Explanation Redundancy in Decision Trees. (arXiv:2205.09971v2 [cs.AI] UPDATED)
192. Kernel Normalized Convolutional Networks. (arXiv:2205.10089v2 [cs.LG] UPDATED)
193. Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks. (arXiv:2205.10664v2 [cs.LG] UPDATED)
194. Covariance Matrix Adaptation MAP-Annealing. (arXiv:2205.10752v2 [cs.LG] UPDATED)
195. A Deep Conjugate Direction Method for Iteratively Solving Linear Systems. (arXiv:2205.10763v2 [cs.LG] UPDATED)
196. Large Language Models are Zero-Shot Reasoners. (arXiv:2205.11916v3 [cs.CL] UPDATED)
197. Conformal Prediction Intervals with Temporal Dependence. (arXiv:2205.12940v3 [stat.ML] UPDATED)
198. BiT: Robustly Binarized Multi-distilled Transformer. (arXiv:2205.13016v2 [cs.LG] UPDATED)
199. Matryoshka Representation Learning. (arXiv:2205.13147v3 [cs.LG] UPDATED)
200. Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v2 [cs.LG] UPDATED)
201. On the Robustness of Safe Reinforcement Learning under Observational Perturbations. (arXiv:2205.14691v2 [cs.LG] UPDATED)
202. Harnessing spectral representations for subgraph alignment. (arXiv:2205.14938v3 [cs.LG] UPDATED)
203. Backpropagation through Combinatorial Algorithms: Identity with Projection Works. (arXiv:2205.15213v2 [cs.LG] UPDATED)
204. Achieving Lightweight Federated Advertising with Self-Supervised Split Distillation. (arXiv:2205.15987v3 [cs.LG] UPDATED)
205. What Knowledge Gets Distilled in Knowledge Distillation?. (arXiv:2205.16004v2 [cs.CV] UPDATED)
206. Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization. (arXiv:2206.00057v2 [cs.LG] UPDATED)
207. Continuous Prediction with Experts' Advice. (arXiv:2206.00236v2 [cs.LG] UPDATED)
208. Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search. (arXiv:2206.00702v3 [cs.AI] UPDATED)
209. On the Generalization of Neural Combinatorial Optimization Heuristics. (arXiv:2206.00787v2 [cs.LG] UPDATED)
210. Improving Diffusion Models for Inverse Problems using Manifold Constraints. (arXiv:2206.00941v2 [cs.LG] UPDATED)
211. Mesh-free Eulerian Physics-Informed Neural Networks. (arXiv:2206.01545v2 [cs.LG] UPDATED)
212. CROM: Continuous Reduced-Order Modeling of PDEs Using Implicit Neural Representations. (arXiv:2206.02607v2 [cs.LG] UPDATED)
213. Neuro-Symbolic Procedural Planning with Commonsense Prompting. (arXiv:2206.02928v4 [cs.CL] UPDATED)
214. SHRED: 3D Shape Region Decomposition with Learned Local Operations. (arXiv:2206.03480v2 [cs.CV] UPDATED)
215. Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting. (arXiv:2206.04038v3 [cs.LG] UPDATED)
216. Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel Recombination. (arXiv:2206.04734v2 [cs.LG] UPDATED)
217. Confident Sinkhorn Allocation for Pseudo-Labeling. (arXiv:2206.05880v2 [cs.LG] UPDATED)
218. Taxonomy of Benchmarks in Graph Representation Learning. (arXiv:2206.07729v2 [cs.LG] UPDATED)
219. Adversarial Scrutiny of Evidentiary Statistical Software. (arXiv:2206.09305v2 [cs.CY] UPDATED)
220. Global Context Vision Transformers. (arXiv:2206.09959v3 [cs.CV] UPDATED)
221. Learning Deep Input-Output Stable Dynamics. (arXiv:2206.13093v2 [math.DS] UPDATED)
222. Generative Modelling With Inverse Heat Dissipation. (arXiv:2206.13397v4 [cs.CV] UPDATED)
223. TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. (arXiv:2207.01848v2 [cs.LG] UPDATED)
224. Improved conformalized quantile regression. (arXiv:2207.02808v4 [stat.ML] UPDATED)
225. Boosting Heterogeneous Catalyst Discovery by Structurally Constrained Deep Learning Models. (arXiv:2207.05013v3 [cond-mat.mtrl-sci] UPDATED)
226. Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v2 [cs.LG] UPDATED)
227. Fast Composite Optimization and Statistical Recovery in Federated Learning. (arXiv:2207.08204v2 [cs.LG] UPDATED)
228. Beyond Transmitting Bits: Context, Semantics, and Task-Oriented Communications. (arXiv:2207.09353v2 [cs.IT] UPDATED)
229. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v3 [cs.LG] UPDATED)
230. Decentralized Machine Learning for Intelligent Health Care Systems on the Computing Continuum. (arXiv:2207.14584v2 [cs.DC] UPDATED)
231. Unsupervised machine learning framework for discriminating major variants of concern during COVID-19. (arXiv:2208.01439v2 [q-bio.OT] UPDATED)
232. Learning programs with magic values. (arXiv:2208.03238v2 [cs.LG] UPDATED)
233. Statistical Properties of the log-cosh Loss Function Used in Machine Learning. (arXiv:2208.04564v3 [stat.ML] UPDATED)
234. Near-Optimal Algorithms for Making the Gradient Small in Stochastic Minimax Optimization. (arXiv:2208.05925v2 [cs.LG] UPDATED)
235. RelPose: Predicting Probabilistic Relative Rotation for Single Objects in the Wild. (arXiv:2208.05963v2 [cs.CV] UPDATED)
236. Syntax-driven Data Augmentation for Named Entity Recognition. (arXiv:2208.06957v2 [cs.CL] UPDATED)
237. Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition. (arXiv:2208.07657v2 [eess.AS] UPDATED)
238. The Final Ascent: When Bigger Models Generalize Worse on Noisy-Labeled Data. (arXiv:2208.08003v3 [cs.LG] UPDATED)
239. LEAPER: Fast and Accurate FPGA-based System Performance Prediction via Transfer Learning. (arXiv:2208.10606v2 [cs.AR] UPDATED)
240. Listen2YourHeart: A Self-Supervised Approach for Detecting Murmur in Heart-Beat Sounds. (arXiv:2208.14845v4 [cs.LG] UPDATED)
241. Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v7 [cs.LG] UPDATED)
242. A Method for Discovering Novel Classes in Tabular Data. (arXiv:2209.01217v2 [cs.LG] UPDATED)
243. Bispectral Neural Networks. (arXiv:2209.03416v3 [cs.LG] UPDATED)
244. Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL. (arXiv:2209.03993v2 [cs.LG] UPDATED)
245. Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v2 [cs.LG] UPDATED)
246. Online Regenerative Learning. (arXiv:2209.08657v2 [math.OC] UPDATED)
247. Quantifying How Hateful Communities Radicalize Online Users. (arXiv:2209.08697v2 [cs.SI] UPDATED)
248. DMMGAN: Diverse Multi Motion Prediction of 3D Human Joints using Attention-Based Generative Adverserial Network. (arXiv:2209.09124v2 [cs.CV] UPDATED)
249. Predicting Drug-Drug Interactions using Deep Generative Models on Graphs. (arXiv:2209.09941v2 [q-bio.BM] UPDATED)
250. Explaining Anomalies using Denoising Autoencoders for Financial Tabular Data. (arXiv:2209.10658v2 [cs.LG] UPDATED)
251. A Neural Model for Regular Grammar Induction. (arXiv:2209.11628v2 [cs.LG] UPDATED)
252. Tiered Pruning for Efficient Differentialble Inference-Aware Neural Architecture Search. (arXiv:2209.11785v2 [cs.LG] UPDATED)
253. Convergence of score-based generative modeling for general data distributions. (arXiv:2209.12381v2 [cs.LG] UPDATED)
254. Power System Anomaly Detection and Classification Utilizing WLS-EKF State Estimation and Machine Learning. (arXiv:2209.12629v2 [eess.SY] UPDATED)
255. Learning GFlowNets from partial episodes for improved convergence and stability. (arXiv:2209.12782v2 [cs.LG] UPDATED)
256. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v3 [cs.CY] UPDATED)
257. Re-Imagen: Retrieval-Augmented Text-to-Image Generator. (arXiv:2209.14491v2 [cs.CV] UPDATED)
258. Regularizing Neural Network Training via Identity-wise Discriminative Feature Suppression. (arXiv:2209.14553v2 [cs.CV] UPDATED)
259. 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v2 [cs.CV] UPDATED)
260. Graph Neural Networks for Link Prediction with Subgraph Sketching. (arXiv:2209.15486v2 [cs.LG] UPDATED)
261. Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods. (arXiv:2209.15589v2 [cs.CV] UPDATED)
## cs.AI
---
**97** new papers in cs.AI:-) 
1. ModelAngelo: Automated Model Building in Cryo-EM Maps. (arXiv:2210.00006v1 [q-bio.QM])
2. Adversarial Attacks on Transformers-Based Malware Detectors. (arXiv:2210.00008v1 [cs.CR])
3. VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. (arXiv:2210.00030v1 [cs.RO])
4. Learning Robust Kernel Ensembles with Kernel Average Pooling. (arXiv:2210.00062v1 [cs.LG])
5. DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. (arXiv:2210.00063v1 [cs.CL])
6. Application of Deep Q Learning with Stimulation Results for Elevator Optimization. (arXiv:2210.00065v1 [cs.LG])
7. Improving Policy Learning via Language Dynamics Distillation. (arXiv:2210.00066v1 [cs.LG])
8. Multi-Task Option Learning and Discovery for Stochastic Path Planning. (arXiv:2210.00068v1 [cs.LG])
9. TOAST: Topological Algorithm for Singularity Tracking. (arXiv:2210.00069v1 [cs.LG])
10. Digital Twin and Artificial Intelligence Incorporated With Surrogate Modeling for Hybrid and Sustainable Energy Systems. (arXiv:2210.00073v1 [cs.AI])
11. Safety-Critical Adaptation in Self-Adaptive Systems. (arXiv:2210.00095v1 [cs.SE])
12. Image-Based Detection of Modifications in Gas Pump PCBs with Deep Convolutional Autoencoders. (arXiv:2210.00100v1 [cs.CV])
13. A Decade of Knowledge Graphs in Natural Language Processing: A Survey. (arXiv:2210.00105v1 [cs.CL])
14. Contrastive Corpus Attribution for Explaining Representations. (arXiv:2210.00107v1 [cs.LG])
15. Institutional Foundations of Adaptive Planning: Exploration of Flood Planning in the Lower Rio Grande Valley, Texas, USA. (arXiv:2210.00113v1 [cs.CL])
16. Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information. (arXiv:2210.00116v1 [cs.LG])
17. Adversarial Robustness of Representation Learning for Knowledge Graphs. (arXiv:2210.00122v1 [cs.LG])
18. Exploiting Selection Bias on Underspecified Tasks in Large Language Models. (arXiv:2210.00131v1 [cs.CL])
19. Predictive Inference with Feature Conformal Prediction. (arXiv:2210.00173v1 [cs.LG])
20. Design of Economical Fuzzy Logic Controller for Washing Machine. (arXiv:2210.00187v1 [eess.SY])
21. Nested Search versus Limited Discrepancy Search. (arXiv:2210.00216v1 [cs.AI])
22. Swift Markov Logic for Probabilistic Reasoning on Knowledge Graphs. (arXiv:2210.00283v1 [cs.AI])
23. DeltaBound Attack: Efficient decision-based attack in low queries regime. (arXiv:2210.00292v1 [cs.LG])
24. Deep Intrinsically Motivated Exploration in Continuous Control. (arXiv:2210.00293v1 [cs.LG])
25. An Ensemble of Convolutional Neural Networks to Detect Foliar Diseases in Apple Plants. (arXiv:2210.00298v1 [cs.CV])
26. CR**ISP**: Curriculum based Sequential Neural Decoders for Polar Code Family. (arXiv:2210.00313v1 [cs.IT])
27. Concurrent Recognition and Segmentation with Adaptive Segment Tokens. (arXiv:2210.00314v1 [cs.CV])
28. Using Argumentation Schemes to Model Legal Reasoning. (arXiv:2210.00315v1 [cs.AI])
29. DCI-ES: An Extended Disentanglement Framework with Connections to Identifiability. (arXiv:2210.00364v1 [cs.LG])
30. Physical computation and compositionality. (arXiv:2210.00392v1 [quant-ph])
31. Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks. (arXiv:2210.00400v1 [cs.LG])
32. Self-Supervised Monocular Depth Estimation: Solving the Edge-Fattening Problem. (arXiv:2210.00411v1 [cs.CV])
33. Metric Distribution to Vector: Constructing Data Representation via Broad-Scale Discrepancies. (arXiv:2210.00415v1 [cs.LG])
34. Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. (arXiv:2210.00434v1 [eess.AS])
35. Grouped self-attention mechanism for a memory-efficient Transformer. (arXiv:2210.00440v1 [cs.LG])
36. Citation Trajectory Prediction via Publication Influence Representation Using Temporal Knowledge Graph. (arXiv:2210.00450v1 [cs.AI])
37. Neural Graphical Models. (arXiv:2210.00453v1 [cs.LG])
38. Robust Bayesian optimization with reinforcement learned acquisition functions. (arXiv:2210.00476v1 [cs.LG])
39. Compositional Generalization in Unsupervised Compositional Representation Learning: A Study on Disentanglement and Emergent Language. (arXiv:2210.00482v1 [cs.LG])
40. EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model. (arXiv:2210.00498v1 [cs.LG])
41. Cognitive modelling with multilayer networks: Insights, advancements and future challenges. (arXiv:2210.00500v1 [cs.CL])
42. RISC-V Toolchain and Agile Development based Open-source Neuromorphic Processor. (arXiv:2210.00562v1 [cs.AR])
43. Multi-Partition Embedding Interaction with Block Term Format for Knowledge Graph Completion. (arXiv:2006.16365v2 [cs.LG] UPDATED)
44. Interpretable Representations in Explainable AI: From Theory to Practice. (arXiv:2008.07007v2 [cs.LG] UPDATED)
45. Probabilistic Federated Learning of Neural Networks Incorporated with Global Posterior Information. (arXiv:2012.03178v2 [cs.LG] UPDATED)
46. Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack based on the Critic Network. (arXiv:2104.03154v3 [cs.LG] UPDATED)
47. Geometric Model Checking of Continuous Space. (arXiv:2105.06194v3 [cs.LO] UPDATED)
48. Scalable Safety-Critical Policy Evaluation with Accelerated Rare Event Sampling. (arXiv:2106.10566v2 [cs.LG] UPDATED)
49. Unsupervised Speech **Enhancement** using Dynamical Variational Auto-Encoders. (arXiv:2106.12271v3 [cs.SD] UPDATED)
50. Neural Mixture Models with Expectation-Maximization for End-to-end Deep Clustering. (arXiv:2107.02453v2 [cs.LG] UPDATED)
51. A Simple Approach to Automated Spectral Clustering. (arXiv:2107.12183v4 [cs.LG] UPDATED)
52. Hierarchical Potential-based Reward Shaping from Task Specifications. (arXiv:2110.02792v3 [cs.LG] UPDATED)
53. RoMA: a Method for Neural Network Robustness Measurement and Assessment. (arXiv:2110.11088v5 [cs.LG] UPDATED)
54. Simultaneous Perturbation Method for Multi-Task Weight Optimization in One-Shot Meta-Learning. (arXiv:2110.13188v3 [cs.LG] UPDATED)
55. Graph Structural Attack by Perturbing Spectral Distance. (arXiv:2111.00684v3 [cs.LG] UPDATED)
56. Deep Learning Transformer Architecture for Named Entity Recognition on Low Resourced Languages: State of the art results. (arXiv:2111.00830v2 [cs.CL] UPDATED)
57. An Instance-Dependent Analysis for the Cooperative Multi-Player Multi-Armed Bandit. (arXiv:2111.04873v2 [cs.LG] UPDATED)
58. Uncertainty Detection and Reduction in Neural Decoding of EEG Signals. (arXiv:2201.00627v2 [eess.SP] UPDATED)
59. Towards Robust Deep Active Learning for Scientific Computing. (arXiv:2201.12632v2 [cs.LG] UPDATED)
60. Path of Destruction: Learning an Iterative Level Generator Using a Small Dataset. (arXiv:2202.10184v2 [cs.LG] UPDATED)
61. Automated Extraction of Energy Systems Information from Remotely Sensed Data: A Review and Analysis. (arXiv:2202.12939v2 [eess.SP] UPDATED)
62. SlimFL: Federated Learning with Superposition Coding over Slimmable Neural Networks. (arXiv:2203.14094v2 [cs.LG] UPDATED)
63. KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v2 [cs.CV] UPDATED)
64. Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v3 [cs.LG] UPDATED)
65. Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v4 [cs.CL] UPDATED)
66. Graph Neural Networks Are More Powerful Than we Think. (arXiv:2205.09801v2 [cs.LG] UPDATED)
67. On Tackling Explanation Redundancy in Decision Trees. (arXiv:2205.09971v2 [cs.AI] UPDATED)
68. Covariance Matrix Adaptation MAP-Annealing. (arXiv:2205.10752v2 [cs.LG] UPDATED)
69. Large Language Models are Zero-Shot Reasoners. (arXiv:2205.11916v3 [cs.CL] UPDATED)
70. Towards Using Data-Influence Methods to Detect Noisy Samples in Source Code Corpora. (arXiv:2205.13022v2 [cs.SE] UPDATED)
71. Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v2 [cs.LG] UPDATED)
72. On the Robustness of Safe Reinforcement Learning under Observational Perturbations. (arXiv:2205.14691v2 [cs.LG] UPDATED)
73. Refining Low-Resource Unsupervised Translation by Language Disentanglement of Multilingual Model. (arXiv:2205.15544v3 [cs.CL] UPDATED)
74. Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search. (arXiv:2206.00702v3 [cs.AI] UPDATED)
75. On the Generalization of Neural Combinatorial Optimization Heuristics. (arXiv:2206.00787v2 [cs.LG] UPDATED)
76. Improving Diffusion Models for Inverse Problems using Manifold Constraints. (arXiv:2206.00941v2 [cs.LG] UPDATED)
77. Neuro-Symbolic Procedural Planning with Commonsense Prompting. (arXiv:2206.02928v4 [cs.CL] UPDATED)
78. Global Context Vision Transformers. (arXiv:2206.09959v3 [cs.CV] UPDATED)
79. Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v2 [cs.LG] UPDATED)
80. Beyond Transmitting Bits: Context, Semantics, and Task-Oriented Communications. (arXiv:2207.09353v2 [cs.IT] UPDATED)
81. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v3 [cs.LG] UPDATED)
82. Knowledge-Driven Mechanistic Enrichment of the Preeclampsia Ignorome. (arXiv:2207.14294v2 [q-bio.GN] UPDATED)
83. Decentralized Machine Learning for Intelligent Health Care Systems on the Computing Continuum. (arXiv:2207.14584v2 [cs.DC] UPDATED)
84. DALLE-URBAN: Capturing the urban design expertise of large text to image transformers. (arXiv:2208.04139v2 [cs.CV] UPDATED)
85. Syntax-driven Data Augmentation for Named Entity Recognition. (arXiv:2208.06957v2 [cs.CL] UPDATED)
86. Contrastive Domain Adaptation for Early Misinformation Detection: A Case Study on COVID-19. (arXiv:2208.09578v4 [cs.CV] UPDATED)
87. LEAPER: Fast and Accurate FPGA-based System Performance Prediction via Transfer Learning. (arXiv:2208.10606v2 [cs.AR] UPDATED)
88. Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v7 [cs.LG] UPDATED)
89. Domain Adaptation for Question Answering via Question Classification. (arXiv:2209.04998v2 [cs.CL] UPDATED)
90. Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v2 [cs.LG] UPDATED)
91. Examining Large Pre-Trained Language Models for Machine Translation: What You Don't Know About It. (arXiv:2209.07417v3 [cs.CL] UPDATED)
92. LGDN: Language-Guided Denoising Network for Video-Language Modeling. (arXiv:2209.11388v2 [cs.CV] UPDATED)
93. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v3 [cs.CY] UPDATED)
94. Re-Imagen: Retrieval-Augmented Text-to-Image Generator. (arXiv:2209.14491v2 [cs.CV] UPDATED)
95. Regularizing Neural Network Training via Identity-wise Discriminative Feature Suppression. (arXiv:2209.14553v2 [cs.CV] UPDATED)
96. Stepwise Acquisition of Dialogue Act Through Human-Robot Interaction. (arXiv:1810.09949v2 [cs.AI] CROSS LISTED)
97. Particle swarm optimization in constrained maximum likelihood estimation a case study. (arXiv:2104.10041v1 [cs.NE] CROSS LISTED)

