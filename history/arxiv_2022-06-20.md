# Your interest papers
---
## cs.CV
---
### **Real-time** motion amplification on mobile devices. (arXiv:2206.08422v1 [cs.GR])
- Authors : 
- Link : [http://arxiv.org/abs/2206.08422](http://arxiv.org/abs/2206.08422)
> ABSTRACT  :  A simple motion amplification algorithm suitable for real-time applications on mobile devices is presented. It is based on motion **enhancement** by moving average differencing (MEMAD), a temporal high-pass filter for video streams. MEMAD can amplify small moving objects or subtle motion in larger objects. It is computationally sufficiently simple to be implemented in **real time** on smartphones. In the specific implementation as an Android phone app, MEMAD is demonstrated on examples chosen such as to motivate applications in the engineering, biological, and medical sciences.  
### Eye**NeRF**: A Hybrid Representation for Photorealistic Synthesis, Animation and Relighting of Human Eyes. (arXiv:2206.08428v1 [cs.CV])
- Authors : Gengyan Li, Abhimitra Meka, Otmar Hilliges, Google Inc
- Link : [http://arxiv.org/abs/2206.08428](http://arxiv.org/abs/2206.08428)
> ABSTRACT  :  A unique challenge in creating high-quality animatable and relightable 3D avatars of people is modeling human eyes. The challenge of synthesizing eyes is multifold as it requires 1) appropriate representations for the various components of the eye and the periocular region for coherent viewpoint synthesis, capable of representing diffuse, refractive and highly reflective surfaces, 2) disentangling skin and eye appearance from environmental illumination such that it may be rendered under novel lighting conditions, and 3) capturing eyeball motion and the deformation of the surrounding skin to enable re-gazing. These challenges have traditionally necessitated the use of expensive and cumbersome capture setups to obtain high-quality results, and even then, modeling of the eye region holistically has remained elusive. We present a novel geometry and appearance representation that enables high-fidelity capture and photorealistic animation, view synthesis and relighting of the eye region using only a sparse set of lights and cameras. Our hybrid representation combines an explicit parametric surface model for the eyeball with implicit deformable volumetric representations for the periocular region and the interior of the eye. This novel hybrid model has been designed to address the various parts of that challenging facial area - the explicit eyeball surface allows modeling refraction and high-frequency specular reflection at the cornea, whereas the implicit representation is well suited to model lower-frequency skin reflection via spherical harmonics and can represent non-surface structures such as hair or diffuse volumetric bodies, both of which are a challenge for explicit surface models. We show that for high-resolution close-ups of the eye, our model can synthesize high-fidelity animated gaze from novel views under unseen illumination conditions.  
### Controllable Image **Enhancement**. (arXiv:2206.08488v1 [cs.CV])
- Authors : Heewon Kim, Kyoung Mu
- Link : [http://arxiv.org/abs/2206.08488](http://arxiv.org/abs/2206.08488)
> ABSTRACT  :  Editing flat-looking images into stunning photographs requires skill and time. Automated image **enhancement** algorithms have attracted increased interest by generating high-quality images without user interaction. However, the quality assessment of a photograph is subjective. Even in tone and color adjustments, a single photograph of auto-**enhancement** is challenging to fit user preferences which are subtle and even changeable. To address this problem, we present a semiautomatic image **enhancement** algorithm that can generate high-quality images with multiple styles by controlling a few parameters. We first disentangle photo retouching skills from high-quality images and build an efficient **enhancement** system for each skill. Specifically, an encoder-decoder framework encodes the retouching skills into latent codes and decodes them into the parameters of image signal processing (ISP) functions. The ISP functions are computationally efficient and consist of only 19 parameters. Despite our approach requiring multiple inferences to obtain the desired result, experimental results present that the proposed method achieves state-of-the-art performances on the benchmark dataset for image quality and model efficiency.  
### Learning Implicit Feature Alignment Function for Semantic Segmentation. (arXiv:2206.08655v1 [cs.CV])
- Authors : Hanzhe Hu, Yinbo Chen, Jiarui Xu, Shubhankar Borse, Hong Cai, Fatih Porikli, Xiaolong Wang
- Link : [http://arxiv.org/abs/2206.08655](http://arxiv.org/abs/2206.08655)
> ABSTRACT  :  Integrating high-level context information with low-level details is of central importance in semantic segmentation. Towards this end, most existing segmentation models apply bilinear up-sampling and convolutions to feature maps of different scales, and then align them at the same resolution. However, bilinear up-sampling blurs the precise information learned in these feature maps and convolutions incur extra computation costs. To address these issues, we propose the Implicit Feature Alignment function (IFA). Our method is inspired by the rapidly expanding topic of **implicit neural representation**s, where coordinate-based neural networks are used to designate fields of signals. In IFA, feature vectors are viewed as representing a 2D field of information. Given a query coordinate, nearby feature vectors with their relative coordinates are taken from the multi-level feature maps and then fed into an MLP to generate the corresponding output. As such, IFA implicitly aligns the feature maps at different levels and is capable of producing segmentation maps in arbitrary resolutions. We demonstrate the efficacy of IFA on multiple datasets, including Cityscapes, PASCAL Context, and ADE20K. Our method can be combined with improvement on various architectures, and it achieves state-of-the-art computation-accuracy trade-off on common benchmarks. Code will be made available at https://github.com/hzhupku/IFA.  
### A Database for Perceived Quality Assessment of User-Generated VR Videos. (arXiv:2206.08751v1 [cs.CV])
- Authors : Yuming Fang, Yiru Yao, Xiangjie Sui, **Kede Ma**
- Link : [http://arxiv.org/abs/2206.08751](http://arxiv.org/abs/2206.08751)
> ABSTRACT  :  Virtual reality (VR) videos (typically in the form of 360$^\circ$ videos) have gained increasing attention due to the fast development of VR technologies and the remarkable popularization of consumer-grade 360$^\circ$ cameras and displays. Thus it is pivotal to understand how people perceive user-generated VR videos, which may suffer from commingled authentic distortions, often localized in space and time. In this paper, we establish one of the largest 360$^\circ$ video databases, containing 502 user-generated videos with rich content and distortion diversities. We capture viewing behaviors (i.e., scanpaths) of 139 users, and collect their opinion scores of perceived quality under four different viewing conditions (two starting points $\times$ two exploration times). We provide a thorough statistical analysis of recorded data, resulting in several interesting observations, such as the significant impact of viewing conditions on viewing behaviors and perceived quality. Besides, we explore other usage of our data and analysis, including evaluation of computational models for quality assessment and saliency detection of 360$^\circ$ videos. We have made the dataset and code available at https://github.com/Yao-Yiru/VR-Video-Database.  
### CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume Segmentation on Cone Beam Computed Tomography Images. (arXiv:2206.08778v1 [cs.CV])
- Authors : Weiwei Cui, Yaqi Wang, Qianni Zhang, Huiyu Zhou, Dan Song, Xingyong Zuo, Gangyong Jia, Liaoyuan Zeng
- Link : [http://arxiv.org/abs/2206.08778](http://arxiv.org/abs/2206.08778)
> ABSTRACT  :  3D tooth segmentation is a prerequisite for computer-aided dental diagnosis and treatment. However, segmenting all tooth regions manually is subjective and time-consuming. Recently, deep learning-based segmentation methods produce convincing results and reduce manual annotation efforts, but it requires a large quantity of ground truth for training. To our knowledge, there are few tooth data available for the 3D segmentation study. In this paper, we establish a fully annotated cone beam computed tomography dataset CTooth with tooth gold standard. This dataset contains 22 volumes (7363 slices) with fine tooth labels annotated by experienced radiographic interpreters. To ensure a relative even data sampling distribution, data variance is included in the CTooth including missing teeth and dental **restoration**. Several state-of-the-art segmentation methods are evaluated on this dataset. Afterwards, we further summarise and apply a series of 3D attention-based Unet variants for segmenting tooth volumes. This work provides a new benchmark for the tooth volume segmentation task. Experimental evidence proves that attention modules of the 3D UNet structure boost responses in tooth areas and inhibit the influence of background and noise. The best performance is achieved by 3D Unet with SKNet attention module, of 88.04 \% Dice and 78.71 \% IOU, respectively. The attention-based Unet framework outperforms other state-of-the-art methods on the CTooth dataset. The codebase and dataset are released.  
### Representational Multiplicity Should Be Exposed, Not Eliminated. (arXiv:2206.08890v1 [cs.LG])
- Authors : Ari Heljakka, Martin Trapp, Juho Kannala, Arno Solin
- Link : [http://arxiv.org/abs/2206.08890](http://arxiv.org/abs/2206.08890)
> ABSTRACT  :  It is prevalent and well-observed, but poorly understood, that two machine learning models with similar performance during training can have very different real-world performance characteristics. This implies elusive differences in the internals of the models, manifesting as representational multiplicity (RM). We introduce a conceptual and experimental setup for analyzing RM and show that certain training methods systematically result in greater RM than others, measured by activation similarity via singular vector canonical correlation analysis (SVCCA). We further correlate it with predictive multiplicity measured by the variance in i.i.d. and out-of-distribution test set predictions, in four common image data sets. We call for systematic measurement and maximal **exposure**, not elimination, of RM in models. Qualitative tools such as our confabulator analysis can facilitate understanding and communication of RM effects to stakeholders.  
### TransWeather: Transformer-based **Restoration** of Images Degraded by Adverse Weather Conditions. (arXiv:2111.14813v2 [cs.CV] UPDATED)
- Authors : Jeya Maria, Jose Valanarasu, Rajeev Yasarla
- Link : [http://arxiv.org/abs/2111.14813](http://arxiv.org/abs/2111.14813)
> ABSTRACT  :  Removing adverse weather conditions like rain, fog, and snow from images is an important problem in many applications. Most methods proposed in the literature have been designed to deal with just removing one type of degradation. Recently, a CNN-based method using neural architecture search (All-in-One) was proposed to remove all the weather conditions at once. However, it has a large number of parameters as it uses multiple encoders to cater to each weather removal task and still has scope for improvement in its performance. In this work, we focus on developing an efficient solution for the all adverse weather removal problem. To this end, we propose TransWeather, a transformer-based end-to-end model with just a single encoder and a decoder that can restore an image degraded by any weather condition. Specifically, we utilize a novel transformer encoder using intra-patch transformer blocks to enhance attention inside the patches to effectively remove smaller weather degradations. We also introduce a transformer decoder with learnable weather type embeddings to adjust to the weather degradation at hand. TransWeather achieves improvements across multiple test datasets over both All-in-One network as well as methods fine-tuned for specific tasks. TransWeather is also validated on real world test images and found to be more effective than previous methods. Implementation code can be accessed at https://github.com/jeya-maria-jose/TransWeather .  
### Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)
- Authors : Liunian Harold, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, **Lei Zhang**, Neng Hwang, Wei Chang, Jianfeng Gao
- Link : [http://arxiv.org/abs/2112.03857](http://arxiv.org/abs/2112.03857)
> ABSTRACT  :  This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.  
### Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v3 [cs.CV] UPDATED)
- Authors : Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, Hang Xu
- Link : [http://arxiv.org/abs/2202.06767](http://arxiv.org/abs/2202.06767)
> ABSTRACT  :  Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/**Swin**T) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, $Wukong_{ViT-L}$ achieves an average accuracy of 73.03%. For the image-text retrieval task, it achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et~al. More information can be referred to: https://wukong-dataset.github.io/wukong-dataset/.  
### UV Volumes for **Real-time** Rendering of Editable Free-view Human Performance. (arXiv:2203.14402v3 [cs.CV] UPDATED)
- Authors : Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue Wang, Fei Wang
- Link : [http://arxiv.org/abs/2203.14402](http://arxiv.org/abs/2203.14402)
> ABSTRACT  :  Neural volume rendering enables photo-realistic renderings of a human performer in free-view, a critical task in immersive VR/AR applications. But the practice is severely limited by high computational costs in the rendering process. To solve this problem, we propose the UV Volumes, a new approach that can render an editable free-view video of a human performer in realtime. It separates the high-frequency (i.e., non-smooth) human appearance from the 3D volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV volumes allow much smaller and shallower neural networks to obtain densities and texture coordinates in 3D while capturing detailed appearance in 2D NTS. For editability, the mapping between the parameterized human model and the smooth texture coordinates allows us a better generalization on novel poses and shapes. Furthermore, the use of NTS enables interesting applications, e.g., retexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M datasets show that our model can render 960 * 540 images in 30FPS on average with comparable photo-realism to state-of-the-art methods. The project and supplementary materials are available at https://github.com/fanegg/UV-Volumes.  
### Recurrent Video **Restoration** Transformer with Guided Deformable Attention. (arXiv:2206.02146v2 [cs.CV] UPDATED)
- Authors : Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2206.02146](http://arxiv.org/abs/2206.02146)
> ABSTRACT  :  Video **restoration** aims at restoring multiple high-quality frames from multiple low-quality frames. Existing video **restoration** methods generally fall into two extreme cases, i.e., they either restore all frames in parallel or restore the video frame by frame in a recurrent way, which would result in different merits and drawbacks. Typically, the former has the advantage of temporal information fusion. However, it suffers from large model size and intensive memory consumption; the latter has a relatively small model size as it shares parameters across frames; however, it lacks long-range dependency modeling ability and parallelizability. In this paper, we attempt to integrate the advantages of the two cases by proposing a recurrent video **restoration** transformer, namely RVRT. RVRT processes local neighboring frames in parallel within a globally recurrent framework which can achieve a good trade-off between model size, effectiveness, and efficiency. Specifically, RVRT divides the video into multiple clips and uses the previously inferred clip feature to estimate the subsequent clip feature. Within each clip, different frame features are jointly updated with implicit feature aggregation. Across different clips, the guided deformable attention is designed for clip-to-clip alignment, which predicts multiple relevant locations from the whole inferred clip and aggregates their features by the attention mechanism. Extensive experiments on video super-resolution, deblurring, and denoising show that the proposed RVRT achieves state-of-the-art performance on benchmark datasets with balanced model size, testing memory and runtime.  
## eess.IV
---
### A Database for Perceived Quality Assessment of User-Generated VR Videos. (arXiv:2206.08751v1 [cs.CV])
- Authors : Yuming Fang, Yiru Yao, Xiangjie Sui, **Kede Ma**
- Link : [http://arxiv.org/abs/2206.08751](http://arxiv.org/abs/2206.08751)
> ABSTRACT  :  Virtual reality (VR) videos (typically in the form of 360$^\circ$ videos) have gained increasing attention due to the fast development of VR technologies and the remarkable popularization of consumer-grade 360$^\circ$ cameras and displays. Thus it is pivotal to understand how people perceive user-generated VR videos, which may suffer from commingled authentic distortions, often localized in space and time. In this paper, we establish one of the largest 360$^\circ$ video databases, containing 502 user-generated videos with rich content and distortion diversities. We capture viewing behaviors (i.e., scanpaths) of 139 users, and collect their opinion scores of perceived quality under four different viewing conditions (two starting points $\times$ two exploration times). We provide a thorough statistical analysis of recorded data, resulting in several interesting observations, such as the significant impact of viewing conditions on viewing behaviors and perceived quality. Besides, we explore other usage of our data and analysis, including evaluation of computational models for quality assessment and saliency detection of 360$^\circ$ videos. We have made the dataset and code available at https://github.com/Yao-Yiru/VR-Video-Database.  
### Recurrent Video **Restoration** Transformer with Guided Deformable Attention. (arXiv:2206.02146v2 [cs.CV] UPDATED)
- Authors : Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2206.02146](http://arxiv.org/abs/2206.02146)
> ABSTRACT  :  Video **restoration** aims at restoring multiple high-quality frames from multiple low-quality frames. Existing video **restoration** methods generally fall into two extreme cases, i.e., they either restore all frames in parallel or restore the video frame by frame in a recurrent way, which would result in different merits and drawbacks. Typically, the former has the advantage of temporal information fusion. However, it suffers from large model size and intensive memory consumption; the latter has a relatively small model size as it shares parameters across frames; however, it lacks long-range dependency modeling ability and parallelizability. In this paper, we attempt to integrate the advantages of the two cases by proposing a recurrent video **restoration** transformer, namely RVRT. RVRT processes local neighboring frames in parallel within a globally recurrent framework which can achieve a good trade-off between model size, effectiveness, and efficiency. Specifically, RVRT divides the video into multiple clips and uses the previously inferred clip feature to estimate the subsequent clip feature. Within each clip, different frame features are jointly updated with implicit feature aggregation. Across different clips, the guided deformable attention is designed for clip-to-clip alignment, which predicts multiple relevant locations from the whole inferred clip and aggregates their features by the attention mechanism. Extensive experiments on video super-resolution, deblurring, and denoising show that the proposed RVRT achieves state-of-the-art performance on benchmark datasets with balanced model size, testing memory and runtime.  
## cs.LG
---
### Representational Multiplicity Should Be Exposed, Not Eliminated. (arXiv:2206.08890v1 [cs.LG])
- Authors : Ari Heljakka, Martin Trapp, Juho Kannala, Arno Solin
- Link : [http://arxiv.org/abs/2206.08890](http://arxiv.org/abs/2206.08890)
> ABSTRACT  :  It is prevalent and well-observed, but poorly understood, that two machine learning models with similar performance during training can have very different real-world performance characteristics. This implies elusive differences in the internals of the models, manifesting as representational multiplicity (RM). We introduce a conceptual and experimental setup for analyzing RM and show that certain training methods systematically result in greater RM than others, measured by activation similarity via singular vector canonical correlation analysis (SVCCA). We further correlate it with predictive multiplicity measured by the variance in i.i.d. and out-of-distribution test set predictions, in four common image data sets. We call for systematic measurement and maximal **exposure**, not elimination, of RM in models. Qualitative tools such as our confabulator analysis can facilitate understanding and communication of RM effects to stakeholders.  
### Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)
- Authors : Liunian Harold, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, **Lei Zhang**, Neng Hwang, Wei Chang, Jianfeng Gao
- Link : [http://arxiv.org/abs/2112.03857](http://arxiv.org/abs/2112.03857)
> ABSTRACT  :  This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.  
### Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v3 [cs.CV] UPDATED)
- Authors : Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, Hang Xu
- Link : [http://arxiv.org/abs/2202.06767](http://arxiv.org/abs/2202.06767)
> ABSTRACT  :  Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/**Swin**T) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, $Wukong_{ViT-L}$ achieves an average accuracy of 73.03%. For the image-text retrieval task, it achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et~al. More information can be referred to: https://wukong-dataset.github.io/wukong-dataset/.  
## cs.AI
---
### CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume Segmentation on Cone Beam Computed Tomography Images. (arXiv:2206.08778v1 [cs.CV])
- Authors : Weiwei Cui, Yaqi Wang, Qianni Zhang, Huiyu Zhou, Dan Song, Xingyong Zuo, Gangyong Jia, Liaoyuan Zeng
- Link : [http://arxiv.org/abs/2206.08778](http://arxiv.org/abs/2206.08778)
> ABSTRACT  :  3D tooth segmentation is a prerequisite for computer-aided dental diagnosis and treatment. However, segmenting all tooth regions manually is subjective and time-consuming. Recently, deep learning-based segmentation methods produce convincing results and reduce manual annotation efforts, but it requires a large quantity of ground truth for training. To our knowledge, there are few tooth data available for the 3D segmentation study. In this paper, we establish a fully annotated cone beam computed tomography dataset CTooth with tooth gold standard. This dataset contains 22 volumes (7363 slices) with fine tooth labels annotated by experienced radiographic interpreters. To ensure a relative even data sampling distribution, data variance is included in the CTooth including missing teeth and dental **restoration**. Several state-of-the-art segmentation methods are evaluated on this dataset. Afterwards, we further summarise and apply a series of 3D attention-based Unet variants for segmenting tooth volumes. This work provides a new benchmark for the tooth volume segmentation task. Experimental evidence proves that attention modules of the 3D UNet structure boost responses in tooth areas and inhibit the influence of background and noise. The best performance is achieved by 3D Unet with SKNet attention module, of 88.04 \% Dice and 78.71 \% IOU, respectively. The attention-based Unet framework outperforms other state-of-the-art methods on the CTooth dataset. The codebase and dataset are released.  
### Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)
- Authors : Liunian Harold, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, **Lei Zhang**, Neng Hwang, Wei Chang, Jianfeng Gao
- Link : [http://arxiv.org/abs/2112.03857](http://arxiv.org/abs/2112.03857)
> ABSTRACT  :  This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.  
# Paper List
---
## cs.CV
---
**118** new papers in cs.CV:-) 
1. Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature Extraction from Downstream Tasks. (arXiv:2206.08398v1 [eess.IV])
2. Going Deeper than Tracking: a Survey of Computer-Vision Based Recognition of Animal Pain and Affective States. (arXiv:2206.08405v1 [cs.CV])
3. **Real-time** motion amplification on mobile devices. (arXiv:2206.08422v1 [cs.GR])
4. IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes. (arXiv:2206.08423v1 [cs.CV])
5. SATBench: Benchmarking the speed-accuracy tradeoff in object recognition by humans and dynamic neural networks. (arXiv:2206.08427v1 [cs.CV])
6. Eye**NeRF**: A Hybrid Representation for Photorealistic Synthesis, Animation and Relighting of Human Eyes. (arXiv:2206.08428v1 [cs.CV])
7. Scalable Temporal Localization of Sensitive Activities in Movies and TV Episodes. (arXiv:2206.08429v1 [cs.CV])
8. OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology. (arXiv:2206.08439v1 [eess.IV])
9. TUSK: Task-Agnostic Unsupervised Keypoints. (arXiv:2206.08460v1 [cs.CV])
10. Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v1 [cs.CV])
11. Zero-Shot AutoML with Pretrained Models. (arXiv:2206.08476v1 [cs.LG])
12. Backdoor Attacks on Vision Transformers. (arXiv:2206.08477v1 [cs.CV])
13. Orientation-guided Graph Convolutional Network for Bone Surface Segmentation. (arXiv:2206.08481v1 [eess.IV])
14. Controllable Image **Enhancement**. (arXiv:2206.08488v1 [cs.CV])
15. Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape Collections. (arXiv:2206.08497v1 [cs.GR])
16. What do navigation agents learn about their environment?. (arXiv:2206.08500v1 [cs.CV])
17. Neural Architecture Adaptation for Object Detection by Searching Channel Dimensions and Mapping Pre-trained Parameters. (arXiv:2206.08509v1 [cs.CV])
18. Effective Solid State LiDAR Odometry Using Continuous-time Filter Registration. (arXiv:2206.08517v1 [cs.RO])
19. VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation. (arXiv:2206.08522v1 [cs.RO])
20. CDNet: Contrastive Disentangled Network for Fine-Grained Image Categorization of Ocular B-Scan Ultrasound. (arXiv:2206.08524v1 [cs.CV])
21. Large-Margin Representation Learning for Texture Classification. (arXiv:2206.08537v1 [cs.CV])
22. Multi-Classification of Brain Tumor Images Using Transfer Learning Based Deep Neural Network. (arXiv:2206.08543v1 [eess.IV])
23. Texture Generation Using Graph Generative Adversarial Network And Differentiable Rendering. (arXiv:2206.08547v1 [cs.CV])
24. Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images. (arXiv:2206.08549v1 [cs.CV])
25. COVID-19 Detection using Transfer Learning with Convolutional Neural Network. (arXiv:2206.08557v1 [eess.IV])
26. Active Data Discovery: Mining Unknown Data using Submodular Information Measures. (arXiv:2206.08566v1 [cs.CV])
27. Rectify ViT Shortcut Learning by Visual Saliency. (arXiv:2206.08567v1 [cs.CV])
28. Multi-Contextual Predictions with Vision Transformer for Video Anomaly Detection. (arXiv:2206.08568v1 [cs.CV])
29. Enhanced Bi-directional Motion Estimation for Video Frame Interpolation. (arXiv:2206.08572v1 [cs.CV])
30. HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignment and Semantic-Region-Aware Inpainting. (arXiv:2206.08585v1 [cs.CV])
31. On Efficient Real-Time Semantic Segmentation: A Survey. (arXiv:2206.08605v1 [cs.CV])
32. Masked Autoencoders for Generic Event Boundary Detection CVPR'2022 Kinetics-GEBD Challenge. (arXiv:2206.08610v1 [cs.CV])
33. OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing. (arXiv:2206.08612v1 [eess.IV])
34. Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment. (arXiv:2206.08614v1 [cs.CV])
35. Learning Using Privileged Information for Zero-Shot Action Recognition. (arXiv:2206.08632v1 [cs.CV])
36. Minimum Noticeable Difference based Adversarial Privacy Preserving Image Generation. (arXiv:2206.08638v1 [cs.CV])
37. Uncertainty-aware Evaluation of Time-Series Classification for Online Handwriting Recognition with Domain Shift. (arXiv:2206.08640v1 [cs.CV])
38. Improving Diversity of Multiple Trajectory Prediction based on Map-adaptive Lane Loss. (arXiv:2206.08641v1 [cs.CV])
39. Local Slot Attention for Vision-and-Language Navigation. (arXiv:2206.08645v1 [cs.CV])
40. All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP). (arXiv:2206.08653v1 [cs.LG])
41. Learning Implicit Feature Alignment Function for Semantic Segmentation. (arXiv:2206.08655v1 [cs.CV])
42. Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v1 [cs.CV])
43. FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification. (arXiv:2206.08671v1 [stat.ML])
44. AggNet: Learning to Aggregate Faces for Group Membership Verification. (arXiv:2206.08683v1 [cs.CV])
45. Sparse Double Descent: Where Network Pruning Aggravates Overfitting. (arXiv:2206.08684v1 [cs.LG])
46. Towards Real-Time Visual Tracking with Graded Color-names Features. (arXiv:2206.08701v1 [cs.CV])
47. Maximum Class Separation as Inductive Bias in One Matrix. (arXiv:2206.08704v1 [cs.LG])
48. An Algorithm for the SE(3)-Transformation on Neural Implicit Maps for Remapping Functions. (arXiv:2206.08712v1 [cs.CV])
49. ReViSe: Remote Vital Signs Measurement Using Smartphone Camera. (arXiv:2206.08748v1 [cs.CV])
50. From a few Accurate 2D Correspondences to 3D Point Clouds. (arXiv:2206.08749v1 [cs.CV])
51. A Database for Perceived Quality Assessment of User-Generated VR Videos. (arXiv:2206.08751v1 [cs.CV])
52. CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume Segmentation on Cone Beam Computed Tomography Images. (arXiv:2206.08778v1 [cs.CV])
53. Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma Grading. (arXiv:2206.08787v1 [eess.IV])
54. Reconstructing vehicles from orthographic drawings using deep neural networks. (arXiv:2206.08789v1 [cs.CV])
55. DU-Net based Unsupervised Contrastive Learning for Cancer Segmentation in Histology Images. (arXiv:2206.08791v1 [cs.CV])
56. FD-CAM: Improving Faithfulness and Discriminability of Visual Explanation for CNNs. (arXiv:2206.08792v1 [cs.CV])
57. The Importance of Background Information for Out of Distribution Generalization. (arXiv:2206.08794v1 [cs.CV])
58. Video Shadow Detection via Spatio-Temporal Interpolation Consistency Training. (arXiv:2206.08801v1 [cs.CV])
59. Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets. (arXiv:2206.08802v1 [cs.LG])
60. Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis. (arXiv:2206.08826v1 [cs.LG])
61. A Comparative Study of Confidence Calibration in Deep Learning: From Computer Vision to Medical Imaging. (arXiv:2206.08833v1 [cs.CV])
62. Entity-Graph Enhanced Cross-Modal Pretraining for Instance-level Product Retrieval. (arXiv:2206.08842v1 [cs.MM])
63. MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v1 [cs.LG])
64. DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2206.08861v1 [cs.CV])
65. Fast Lossless Neural Compression with Integer-Only Discrete Flows. (arXiv:2206.08869v1 [cs.LG])
66. Improving Generalization of Metric Learning via Listwise Self-distillation. (arXiv:2206.08880v1 [cs.CV])
67. Edge-Aided Sensor Data Sharing in Vehicular Communication Networks. (arXiv:2206.08882v1 [cs.MA])
68. CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer. (arXiv:2206.08883v1 [cs.CV])
69. Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling. (arXiv:2206.08885v1 [eess.IV])
70. Representational Multiplicity Should Be Exposed, Not Eliminated. (arXiv:2206.08890v1 [cs.LG])
71. SimA: Simple Softmax-free Attention for Vision Transformers. (arXiv:2206.08898v1 [cs.CV])
72. Colonoscopy 3D Video Dataset with Paired Depth from 2D-3D Registration. (arXiv:2206.08903v1 [cs.CV])
73. Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks. (arXiv:2206.08916v1 [cs.CV])
74. VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix. (arXiv:2206.08919v1 [cs.CV])
75. VectorMapNet: End-to-end Vectorized HD Map Learning. (arXiv:2206.08920v1 [cs.CV])
76. Cross-task Attention Mechanism for Dense Multi-task Learning. (arXiv:2206.08927v1 [cs.CV])
77. TAVA: Template-free Animatable Volumetric Actors. (arXiv:2206.08929v1 [cs.CV])
78. Variational Nested Dropout. (arXiv:2101.11353v2 [cs.LG] UPDATED)
79. NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification. (arXiv:2103.04053v5 [cs.CV] UPDATED)
80. Conditional GANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v5 [cs.LG] UPDATED)
81. Automatic Foot Ulcer Segmentation Using an Ensemble of Convolutional Neural Networks. (arXiv:2109.01408v2 [eess.IV] UPDATED)
82. Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v6 [cs.CV] UPDATED)
83. Unsolved Problems in ML Safety. (arXiv:2109.13916v5 [cs.LG] UPDATED)
84. Boosting Fast Adversarial Training with Learnable Adversarial Initialization. (arXiv:2110.05007v3 [cs.CV] UPDATED)
85. whu-nercms at trecvid2021:instance search task. (arXiv:2111.00228v2 [cs.CV] UPDATED)
86. Extracting Triangular 3D Models, Materials, and Lighting From Images. (arXiv:2111.12503v4 [cs.CV] UPDATED)
87. TransWeather: Transformer-based **Restoration** of Images Degraded by Adverse Weather Conditions. (arXiv:2111.14813v2 [cs.CV] UPDATED)
88. Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)
89. PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning. (arXiv:2201.10029v2 [cs.CV] UPDATED)
90. Learning To Recognize Procedural Activities with Distant Supervision. (arXiv:2201.10990v3 [cs.CV] UPDATED)
91. PINs: Progressive Implicit Networks for Multi-Scale Neural Representations. (arXiv:2202.04713v2 [cs.CV] UPDATED)
92. Artemis: Articulated Neural Pets with Appearance and Motion synthesis. (arXiv:2202.05628v3 [cs.GR] UPDATED)
93. Borrowing from yourself: Faster future video segmentation with partial channel update. (arXiv:2202.05748v2 [cs.CV] UPDATED)
94. Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v3 [cs.CV] UPDATED)
95. BED: A Real-Time Object Detection System for Edge Devices. (arXiv:2202.07503v2 [cs.CV] UPDATED)
96. Coarse-to-Fine Vision Transformer. (arXiv:2203.03821v2 [cs.CV] UPDATED)
97. MetAug: Contrastive Learning via Meta Feature Augmentation. (arXiv:2203.05119v2 [cs.CV] UPDATED)
98. TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers. (arXiv:2203.10726v3 [eess.IV] UPDATED)
99. Reinforcement Learning with Action-Free Pre-Training from Videos. (arXiv:2203.13880v2 [cs.CV] UPDATED)
100. UV Volumes for **Real-time** Rendering of Editable Free-view Human Performance. (arXiv:2203.14402v3 [cs.CV] UPDATED)
101. CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI. (arXiv:2203.15163v2 [eess.IV] UPDATED)
102. Semi-Weakly Supervised Object Detection by Sampling Pseudo Ground-Truth Boxes. (arXiv:2204.00147v2 [cs.CV] UPDATED)
103. Out-of-Distribution Detection with Deep Nearest Neighbors. (arXiv:2204.06507v2 [cs.LG] UPDATED)
104. Recognising Known Configurations of Garments For Dual-Arm Robotic Flattening. (arXiv:2205.00225v2 [cs.RO] UPDATED)
105. Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction. (arXiv:2205.06672v2 [cs.CV] UPDATED)
106. Dynamic Split Computing for Efficient Deep Edge Intelligence. (arXiv:2205.11269v2 [cs.CV] UPDATED)
107. Scalable Interpretability via Polynomials. (arXiv:2205.14108v3 [cs.LG] UPDATED)
108. Neural Basis Models for Interpretability. (arXiv:2205.14120v3 [cs.LG] UPDATED)
109. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. (arXiv:2206.00843v2 [cs.LG] UPDATED)
110. Supernet Training for Federated Image Classification under System Heterogeneity. (arXiv:2206.01366v2 [cs.LG] UPDATED)
111. Recurrent Video **Restoration** Transformer with Guided Deformable Attention. (arXiv:2206.02146v2 [cs.CV] UPDATED)
112. SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v2 [cs.LG] UPDATED)
113. Geometrically Guided Integrated Gradients. (arXiv:2206.05903v2 [cs.CV] UPDATED)
114. Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v4 [cs.CV] UPDATED)
115. ELUDE: Generating interpretable explanations via a decomposition into labelled and unlabelled features. (arXiv:2206.07690v2 [cs.CV] UPDATED)
116. VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids. (arXiv:2206.07695v2 [cs.CV] UPDATED)
117. Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos. (arXiv:2206.07981v2 [cs.CV] UPDATED)
118. Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v2 [cs.CV] UPDATED)
## eess.IV
---
**19** new papers in eess.IV:-) 
1. Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature Extraction from Downstream Tasks. (arXiv:2206.08398v1 [eess.IV])
2. OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology. (arXiv:2206.08439v1 [eess.IV])
3. Orientation-guided Graph Convolutional Network for Bone Surface Segmentation. (arXiv:2206.08481v1 [eess.IV])
4. Multi-Classification of Brain Tumor Images Using Transfer Learning Based Deep Neural Network. (arXiv:2206.08543v1 [eess.IV])
5. COVID-19 Detection using Transfer Learning with Convolutional Neural Network. (arXiv:2206.08557v1 [eess.IV])
6. OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing. (arXiv:2206.08612v1 [eess.IV])
7. A common lines approach for ab-initio modeling of molecules with tetrahedral and octahedral symmetry. (arXiv:2206.08623v1 [physics.comp-ph])
8. ReViSe: Remote Vital Signs Measurement Using Smartphone Camera. (arXiv:2206.08748v1 [cs.CV])
9. A Database for Perceived Quality Assessment of User-Generated VR Videos. (arXiv:2206.08751v1 [cs.CV])
10. An incomplete taxonomy of self-assigned color specialties. (arXiv:2206.08779v1 [eess.IV])
11. Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma Grading. (arXiv:2206.08787v1 [eess.IV])
12. Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis. (arXiv:2206.08826v1 [cs.LG])
13. Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling. (arXiv:2206.08885v1 [eess.IV])
14. Linear Revolution-Invariance: Modeling and Deblurring Spatially-Varying Imaging Systems. (arXiv:2206.08928v1 [eess.IV])
15. Automatic Foot Ulcer Segmentation Using an Ensemble of Convolutional Neural Networks. (arXiv:2109.01408v2 [eess.IV] UPDATED)
16. VORTEX: Physics-Driven Data Augmentations Using Consistency Training for Robust Accelerated MRI Reconstruction. (arXiv:2111.02549v2 [eess.IV] UPDATED)
17. TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers. (arXiv:2203.10726v3 [eess.IV] UPDATED)
18. CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI. (arXiv:2203.15163v2 [eess.IV] UPDATED)
19. Recurrent Video **Restoration** Transformer with Guided Deformable Attention. (arXiv:2206.02146v2 [cs.CV] UPDATED)
## cs.LG
---
**242** new papers in cs.LG:-) 
1. Embarrassingly Parallel Independent Training of Multi-Layer Perceptrons with Heterogeneous Architectures. (arXiv:2206.08369v1 [cs.LG])
2. Powershap: A Power-full Shapley Feature Selection Method. (arXiv:2206.08394v1 [cs.LG])
3. Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature Extraction from Downstream Tasks. (arXiv:2206.08398v1 [eess.IV])
4. Learning to Teach Fairness-aware Deep Multi-task Learning. (arXiv:2206.08403v1 [cs.LG])
5. SATBench: Benchmarking the speed-accuracy tradeoff in object recognition by humans and dynamic neural networks. (arXiv:2206.08427v1 [cs.CV])
6. OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology. (arXiv:2206.08439v1 [eess.IV])
7. Understanding Decision-Time vs. Background Planning in Model-Based Reinforcement Learning. (arXiv:2206.08442v1 [cs.LG])
8. Empirical Bayesian Approaches for Robust Constraint-based Causal Discovery under Insufficient Data. (arXiv:2206.08448v1 [cs.LG])
9. Active Fairness Auditing. (arXiv:2206.08450v1 [cs.LG])
10. I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. (arXiv:2206.08451v1 [cs.LG])
11. GOOD: A Graph Out-of-Distribution Benchmark. (arXiv:2206.08452v1 [cs.LG])
12. Quantifying Feature Contributions to Overall Disparity Using Information Theory. (arXiv:2206.08454v1 [cs.LG])
13. Local overlap reduction procedure for dynamic ensemble selection. (arXiv:2206.08455v1 [cs.LG])
14. TUSK: Task-Agnostic Unsupervised Keypoints. (arXiv:2206.08460v1 [cs.CV])
15. Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v1 [cs.CV])
16. PRANC: Pseudo RAndom Networks for Compacting deep models. (arXiv:2206.08464v1 [cs.LG])
17. Variational Estimators of the Degree-corrected Latent Block Model for Bipartite Networks. (arXiv:2206.08465v1 [stat.ML])
18. A Robust Stacking Framework for Training Deep Graph Models with Multifaceted Node Features. (arXiv:2206.08473v1 [cs.LG])
19. XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence. (arXiv:2206.08474v1 [cs.SE])
20. Zero-Shot AutoML with Pretrained Models. (arXiv:2206.08476v1 [cs.LG])
21. Backdoor Attacks on Vision Transformers. (arXiv:2206.08477v1 [cs.CV])
22. Classification of datasets with imputed missing values: does imputation quality matter?. (arXiv:2206.08478v1 [cs.LG])
23. High-Speed Accurate Robot Control using Learned Forward Kinodynamics and Non-linear Least Squares Optimization. (arXiv:2206.08487v1 [cs.RO])
24. Debugging using Orthogonal Gradient Descent. (arXiv:2206.08489v1 [cs.LG])
25. Revisiting Self-Distillation. (arXiv:2206.08491v1 [cs.LG])
26. TKIL: Tangent Kernel Approach for Class Balanced Incremental Learning. (arXiv:2206.08492v1 [cs.LG])
27. Self-Supervised Contrastive Pre-Training For Time Series via Time-Frequency Consistency. (arXiv:2206.08496v1 [cs.LG])
28. A Parametric Class of Approximate Gradient Updates for Policy Optimization. (arXiv:2206.08499v1 [cs.LG])
29. What do navigation agents learn about their environment?. (arXiv:2206.08500v1 [cs.CV])
30. TLETA: Deep Transfer Learning and Integrated Cellular Knowledge for Estimated Time of Arrival Prediction. (arXiv:2206.08513v1 [cs.LG])
31. A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. (arXiv:2206.08514v1 [cs.LG])
32. ComENet: Towards Complete and Efficient Message Passing for 3D Molecular Graphs. (arXiv:2206.08515v1 [cs.LG])
33. MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare. (arXiv:2206.08516v1 [cs.LG])
34. Thompson Sampling Achieves $\tilde O(\sqrt{T})$ Regret in Linear Quadratic Control. (arXiv:2206.08520v1 [cs.LG])
35. A Spatio-Temporal Neural Network Forecasting Approach for Emulation of Firefront Models. (arXiv:2206.08523v1 [cs.LG])
36. SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe Autonomous Driving. (arXiv:2206.08528v1 [cs.LG])
37. Accelerating Shapley Explanation via Contributive Cooperator Selection. (arXiv:2206.08529v1 [cs.LG])
38. Reframed GES with a Neural Conditional Dependence Measure. (arXiv:2206.08531v1 [stat.ML])
39. Large-Margin Representation Learning for Texture Classification. (arXiv:2206.08537v1 [cs.CV])
40. Strategic Representation. (arXiv:2206.08542v1 [cs.LG])
41. NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates. (arXiv:2206.08545v1 [eess.AS])
42. SOS: Score-based Oversampling for Tabular Data. (arXiv:2206.08555v1 [cs.LG])
43. Thompson Sampling for Robust Transfer in Multi-Task Bandits. (arXiv:2206.08556v1 [cs.LG])
44. How You Start Matters for Generalization. (arXiv:2206.08558v1 [cs.LG])
45. Boosting Graph Structure Learning with Dummy Nodes. (arXiv:2206.08561v1 [cs.LG])
46. MET: Masked Encoding for Tabular Data. (arXiv:2206.08564v1 [cs.LG])
47. Bootstrapped Transformer for Offline Reinforcement Learning. (arXiv:2206.08569v1 [cs.LG])
48. Optimal Extragradient-Based Bilinearly-Coupled Saddle-Point Optimization. (arXiv:2206.08573v1 [math.OC])
49. Query-Efficient and Scalable Black-Box Adversarial Attacks on Discrete Sequential Data via Bayesian Optimization. (arXiv:2206.08575v1 [cs.LG])
50. DFG-NAS: Deep and Flexible Graph Neural Architecture Search. (arXiv:2206.08582v1 [cs.LG])
51. NAFS: A Simple yet Tough-to-beat Baseline for Graph Representation Learning. (arXiv:2206.08583v1 [cs.LG])
52. Automatic Correction of Human Translations. (arXiv:2206.08593v1 [cs.CL])
53. Accelerating numerical methods by gradient-based meta-solving. (arXiv:2206.08594v1 [math.NA])
54. On the Influence of Enforcing Model Identifiability on Learning dynamics of Gaussian Mixture Models. (arXiv:2206.08598v1 [cs.LG])
55. On Integrating Prior Knowledge into Gaussian Processes for Prognostic Health Monitoring. (arXiv:2206.08600v1 [stat.ML])
56. On Efficient Real-Time Semantic Segmentation: A Survey. (arXiv:2206.08605v1 [cs.CV])
57. The Role of Depth, Width, and Activation Complexity in the Number of Linear Regions of Neural Networks. (arXiv:2206.08615v1 [cs.LG])
58. RECAPP: Crafting a More Efficient Catalyst for Convex Optimization. (arXiv:2206.08627v1 [math.OC])
59. Minimum Noticeable Difference based Adversarial Privacy Preserving Image Generation. (arXiv:2206.08638v1 [cs.CV])
60. Scalable Differentially Private Clustering via Hierarchically Separated Trees. (arXiv:2206.08646v1 [cs.DS])
61. Orthonormal Expansions for Translation-Invariant Kernels. (arXiv:2206.08648v1 [math.CA])
62. All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP). (arXiv:2206.08653v1 [cs.LG])
63. tinySNN: Towards Memory- and Energy-Efficient Spiking Neural Networks. (arXiv:2206.08656v1 [cs.NE])
64. Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v1 [cs.CV])
65. Digital Twin Data Modelling by Randomized Orthogonal Decomposition and Deep Learning. (arXiv:2206.08659v1 [math.NA])
66. Boosting Factorization Machines via Saliency-Guided Mixup. (arXiv:2206.08661v1 [cs.IR])
67. The Sensorium competition on predicting large-scale mouse primary visual cortex activity. (arXiv:2206.08666v1 [q-bio.NC])
68. FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification. (arXiv:2206.08671v1 [stat.ML])
69. A Deep Learning Approach for the Segmentation of Electroencephalography Data in Eye Tracking Applications. (arXiv:2206.08672v1 [cs.LG])
70. Understanding Robust Overfitting of Adversarial Training and Beyond. (arXiv:2206.08675v1 [cs.LG])
71. BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish Text Using Transformers. (arXiv:2206.08680v1 [cs.CL])
72. Sparse Double Descent: Where Network Pruning Aggravates Overfitting. (arXiv:2206.08684v1 [cs.LG])
73. Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning. (arXiv:2206.08686v1 [cs.RO])
74. Sheaf Neural Networks with Connection Laplacians. (arXiv:2206.08702v1 [cs.LG])
75. Plotly-Resampler: Effective Visual Analytics for Large Time Series. (arXiv:2206.08703v1 [cs.HC])
76. Maximum Class Separation as Inductive Bias in One Matrix. (arXiv:2206.08704v1 [cs.LG])
77. Explainability's Gain is Optimality's Loss? -- How Explanations Bias Decision-making. (arXiv:2206.08705v1 [cs.HC])
78. Statistical and Neural Methods for Cross-lingual Entity Label Mapping in Knowledge Graphs. (arXiv:2206.08709v1 [cs.CL])
79. Evaluating the Impact of Source Code Parsers on ML4SE Models. (arXiv:2206.08713v1 [cs.SE])
80. Fast Finite Width Neural Tangent Kernel. (arXiv:2206.08720v1 [cs.LG])
81. Evaluation of Contrastive Learning with Various Code Representations for Code Clone Detection. (arXiv:2206.08726v1 [cs.SE])
82. Generalised Policy Improvement with Geometric Policy Composition. (arXiv:2206.08736v1 [stat.ML])
83. Detecting Adversarial Examples in Batches -- a geometrical approach. (arXiv:2206.08738v1 [cs.LG])
84. Near-Optimal No-Regret Learning for General Convex Games. (arXiv:2206.08742v1 [cs.GT])
85. Learning Fair Representation via Distributional Contrastive Disentanglement. (arXiv:2206.08743v1 [cs.LG])
86. Machine Learning-Driven Process of Alumina Ceramics Laser Machining. (arXiv:2206.08747v1 [cs.CE])
87. ReViSe: Remote Vital Signs Measurement Using Smartphone Camera. (arXiv:2206.08748v1 [cs.CV])
88. Federated learning with incremental clustering for heterogeneous data. (arXiv:2206.08752v1 [cs.LG])
89. Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay. (arXiv:2206.08756v1 [math.ST])
90. Beyond Ridge Regression for Distribution-Free Data. (arXiv:2206.08757v1 [cs.LG])
91. Multiple-Play Stochastic Bandits with Shareable Finite-Capacity Arms. (arXiv:2206.08776v1 [cs.LG])
92. Spherical Sliced-Wasserstein. (arXiv:2206.08780v1 [stat.ML])
93. Reinforcement Learning in Macroeconomic Policy Design: A New Frontier?. (arXiv:2206.08781v1 [cs.LG])
94. Discovery of the Content and Engagement with the Content. (arXiv:2206.08786v1 [cs.IR])
95. Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma Grading. (arXiv:2206.08787v1 [eess.IV])
96. Reconstructing vehicles from orthographic drawings using deep neural networks. (arXiv:2206.08789v1 [cs.CV])
97. Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets. (arXiv:2206.08802v1 [cs.LG])
98. Truly Unordered Probabilistic Rule Sets for Multi-class Classification. (arXiv:2206.08804v1 [cs.LG])
99. Holistic Transformer: A Joint Neural Network for Trajectory Prediction and Decision-Making of Autonomous Vehicles. (arXiv:2206.08809v1 [cs.LG])
100. Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis. (arXiv:2206.08826v1 [cs.LG])
101. FedNew: A Communication-Efficient and Privacy-Preserving Newton-Type Method for Federated Learning. (arXiv:2206.08829v1 [cs.LG])
102. Prediction of Solar Radiation Based on Spatial and Temporal Embeddings for Solar Generation Forecast. (arXiv:2206.08832v1 [cs.LG])
103. Decentralized adaptive clustering of deep nets is beneficial for client collaboration. (arXiv:2206.08839v1 [cs.LG])
104. Random projections and Kernelised Leave One Cluster Out Cross-Validation: Universal baselines and evaluation tools for supervised machine learning for materials properties. (arXiv:2206.08841v1 [cs.LG])
105. AutoML Two-Sample Test. (arXiv:2206.08843v1 [cs.LG])
106. SMPL: Simulated Industrial Manufacturing and Process Control Learning Environments. (arXiv:2206.08851v1 [cs.LG])
107. Channel-wise Mixed-precision Assignment for DNN Inference on Constrained Edge Nodes. (arXiv:2206.08852v1 [cs.LG])
108. MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v1 [cs.LG])
109. Avoid Overfitting User Specific Information in Federated Keyword Spotting. (arXiv:2206.08864v1 [cs.LG])
110. Generalized Frank-Wolfe Algorithm for Bilevel Optimization. (arXiv:2206.08868v1 [math.OC])
111. Fast Lossless Neural Compression with Integer-Only Discrete Flows. (arXiv:2206.08869v1 [cs.LG])
112. How robust are pre-trained models to distribution shift?. (arXiv:2206.08871v1 [cs.LG])
113. Mirror Descent with Relative Smoothness in Measure Spaces, with application to Sinkhorn and EM. (arXiv:2206.08873v1 [math.OC])
114. Improving Generalization of Metric Learning via Listwise Self-distillation. (arXiv:2206.08880v1 [cs.CV])
115. CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer. (arXiv:2206.08883v1 [cs.CV])
116. Resolution Limits of Non-Adaptive 20 Questions Search for a Moving Target. (arXiv:2206.08884v1 [cs.IT])
117. Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling. (arXiv:2206.08885v1 [eess.IV])
118. Fast Population-Based Reinforcement Learning on a Single Machine. (arXiv:2206.08888v1 [cs.LG])
119. Lossy Compression with Gaussian Diffusion. (arXiv:2206.08889v1 [stat.ML])
120. Representational Multiplicity Should Be Exposed, Not Eliminated. (arXiv:2206.08890v1 [cs.LG])
121. Scaling multi-species occupancy models to large citizen science datasets. (arXiv:2206.08894v1 [stat.AP])
122. Popular decision tree algorithms are provably noise tolerant. (arXiv:2206.08899v1 [cs.LG])
123. Adapting the Linearised Laplace Model Evidence for Modern Deep Learning. (arXiv:2206.08900v1 [stat.ML])
124. SYMBA: Symbolic Computation of Squared Amplitudes in High Energy Physics with Machine ALearning. (arXiv:2206.08901v1 [hep-ph])
125. The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysis. (arXiv:2206.08917v1 [cond-mat.mtrl-sci])
126. Learning a Single Neuron with Adversarial Label Noise via Gradient Descent. (arXiv:2206.08918v1 [cs.LG])
127. Smoothing Policies and Safe Policy Gradients. (arXiv:1905.03231v2 [cs.LG] UPDATED)
128. Spectral CUSUM for Online Network Structure Change Detection. (arXiv:1910.09083v3 [math.ST] UPDATED)
129. Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification. (arXiv:2002.10061v3 [cs.LG] UPDATED)
130. CausalVAE: Structured Causal Disentanglement in Variational Autoencoder. (arXiv:2004.08697v6 [cs.LG] UPDATED)
131. Beyond Worst-Case Analysis in Stochastic Approximation: Moment Estimation Improves Instance Complexity. (arXiv:2006.04429v3 [math.OC] UPDATED)
132. Active Sampling for Min-Max Fairness. (arXiv:2006.06879v3 [stat.ML] UPDATED)
133. Nudge: Accelerating Overdue Pull Requests Towards Completion. (arXiv:2011.12468v5 [cs.SE] UPDATED)
134. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. (arXiv:2101.03961v3 [cs.LG] UPDATED)
135. Yet Another Representation of Binary Decision Trees: A Mathematical Demonstration. (arXiv:2101.07077v6 [cs.LG] UPDATED)
136. Variational Nested Dropout. (arXiv:2101.11353v2 [cs.LG] UPDATED)
137. Domain Adaptation for Time Series Forecasting via Attention Sharing. (arXiv:2102.06828v7 [cs.LG] UPDATED)
138. Fairness in Credit Scoring: Assessment, Implementation and Profit Implications. (arXiv:2103.01907v4 [stat.ML] UPDATED)
139. GrASP: A Library for Extracting and Exploring Human-Interpretable Textual Patterns. (arXiv:2104.03958v2 [cs.CL] UPDATED)
140. Sanity Simulations for Saliency Methods. (arXiv:2105.06506v3 [cs.LG] UPDATED)
141. A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1. (arXiv:2106.03076v2 [cs.LG] UPDATED)
142. Feature and Parameter Selection in Stochastic Linear Bandits. (arXiv:2106.05378v3 [cs.LG] UPDATED)
143. Learngene: From Open-World to Your Learning Task. (arXiv:2106.06788v3 [cs.LG] UPDATED)
144. Author Clustering and Topic Estimation for Short Texts. (arXiv:2106.09533v2 [cs.IR] UPDATED)
145. Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets. (arXiv:2107.00472v3 [math.OC] UPDATED)
146. Conditional GANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v5 [cs.LG] UPDATED)
147. Learning to Hash Robustly, Guaranteed. (arXiv:2108.05433v4 [cs.DS] UPDATED)
148. Neural Ensemble Search via Bayesian Sampling. (arXiv:2109.02533v2 [cs.LG] UPDATED)
149. A Survey of Sound Source Localization with Deep Learning Methods. (arXiv:2109.03465v3 [cs.SD] UPDATED)
150. Local Augmentation for Graph Neural Networks. (arXiv:2109.03856v3 [cs.LG] UPDATED)
151. New Versions of Gradient Temporal Difference Learning. (arXiv:2109.04033v3 [cs.LG] UPDATED)
152. On the Compression of Neural Networks Using $\ell_0$-Norm Regularization and Weight Pruning. (arXiv:2109.05075v2 [cs.LG] UPDATED)
153. Unsolved Problems in ML Safety. (arXiv:2109.13916v5 [cs.LG] UPDATED)
154. Distinguishing rule- and exemplar-based generalization in learning systems. (arXiv:2110.04328v2 [cs.LG] UPDATED)
155. Boosting Fast Adversarial Training with Learnable Adversarial Initialization. (arXiv:2110.05007v3 [cs.CV] UPDATED)
156. Neural Network Weights Do Not Converge to Stationary Points: An Invariant Measure Perspective. (arXiv:2110.06256v2 [cs.LG] UPDATED)
157. Dropout Prediction Uncertainty Estimation Using Neuron Activation Strength. (arXiv:2110.06435v3 [cs.LG] UPDATED)
158. Label-Descriptive Patterns and Their Application to Characterizing Classification Errors. (arXiv:2110.09599v3 [cs.LG] UPDATED)
159. abess: A Fast Best Subset Selection Library in Python and R. (arXiv:2110.09697v2 [stat.ML] UPDATED)
160. You Are the Best Reviewer of Your Own Papers: An Owner-Assisted Scoring Mechanism. (arXiv:2110.14802v2 [cs.LG] UPDATED)
161. PDE-READ: Human-readable Partial Differential Equation Discovery using Deep Learning. (arXiv:2111.00998v5 [cs.LG] UPDATED)
162. A Theoretical Analysis on Independence-driven Importance Weighting for Covariate-shift Generalization. (arXiv:2111.02355v2 [cs.LG] UPDATED)
163. Toward Learning Human-aligned Cross-domain Robust Models by Countering Misaligned Features. (arXiv:2111.03740v2 [cs.LG] UPDATED)
164. Personalized Federated Learning through Local Memorization. (arXiv:2111.09360v3 [cs.LG] UPDATED)
165. SaDe: Learning Models that Provably Satisfy Domain Constraints. (arXiv:2112.00552v3 [cs.LG] UPDATED)
166. Decision-Focused Learning: Through the Lens of Learning to Rank. (arXiv:2112.03609v4 [cs.LG] UPDATED)
167. Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)
168. Learning over All Stabilizing Nonlinear Controllers for a Partially-Observed Linear System. (arXiv:2112.04219v3 [eess.SY] UPDATED)
169. Anti-Money Laundering Alert Optimization Using Machine Learning with Graphs. (arXiv:2112.07508v3 [cs.LG] UPDATED)
170. Tight query complexity bounds for learning graph partitions. (arXiv:2112.07897v2 [cs.LG] UPDATED)
171. Bayesian Calibration of imperfect computer models using Physics-informed priors. (arXiv:2201.06463v3 [stat.ML] UPDATED)
172. Human Interpretation of Saliency-based Explanation Over Text. (arXiv:2201.11569v2 [cs.CL] UPDATED)
173. Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks. (arXiv:2201.11729v2 [cs.LG] UPDATED)
174. Constrained Variational Policy Optimization for Safe Reinforcement Learning. (arXiv:2201.11927v3 [cs.LG] UPDATED)
175. Generative Coarse-Graining of Molecular Conformations. (arXiv:2201.12176v2 [cs.LG] UPDATED)
176. Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism. (arXiv:2201.12987v3 [cs.LG] UPDATED)
177. ROCK: Causal Inference Principles for Reasoning about Commonsense Causality. (arXiv:2202.00436v2 [cs.CL] UPDATED)
178. Meta-Learning Hypothesis Spaces for Sequential Decision-making. (arXiv:2202.00602v3 [stat.ML] UPDATED)
179. Achieving Fairness at No Utility Cost via Data Reweighing with Influence. (arXiv:2202.00787v2 [cs.LG] UPDATED)
180. Optimizing Sequential Experimental Design with Deep Reinforcement Learning. (arXiv:2202.00821v3 [cs.LG] UPDATED)
181. Structure-preserving GANs. (arXiv:2202.01129v2 [cs.LG] UPDATED)
182. Deep Networks on Toroids: Removing Symmetries Reveals the Structure of Flat Regions in the Landscape Geometry. (arXiv:2202.03038v2 [cs.LG] UPDATED)
183. Adversarial Attack and Defense for Non-Parametric Two-Sample Tests. (arXiv:2202.03077v2 [cs.LG] UPDATED)
184. Modeling Structure with Undirected Neural Networks. (arXiv:2202.03760v2 [cs.LG] UPDATED)
185. Distribution Regression with Sliced Wasserstein Kernels. (arXiv:2202.03926v2 [stat.ML] UPDATED)
186. Universal Hopfield Networks: A General Framework for Single-Shot Associative Memory Models. (arXiv:2202.04557v2 [cs.NE] UPDATED)
187. Low-Rank Approximation with $1/\epsilon^{1/3}$ Matrix-Vector Products. (arXiv:2202.05120v4 [cs.DS] UPDATED)
188. A Modern Self-Referential Weight Matrix That Learns to Modify Itself. (arXiv:2202.05780v2 [cs.LG] UPDATED)
189. The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention. (arXiv:2202.05798v2 [cs.LG] UPDATED)
190. Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. (arXiv:2202.06767v3 [cs.CV] UPDATED)
191. BED: A Real-Time Object Detection System for Edge Devices. (arXiv:2202.07503v2 [cs.CV] UPDATED)
192. Stochastic Perturbations of Tabular Features for Non-Deterministic Inference with Automunge. (arXiv:2202.09248v2 [cs.LG] UPDATED)
193. Residual Bootstrap Exploration for Stochastic Linear Bandit. (arXiv:2202.11474v2 [stat.ML] UPDATED)
194. Capturing Actionable Dynamics with Structured Latent Ordinary Differential Equations. (arXiv:2202.12932v2 [stat.ML] UPDATED)
195. On Testability of the Front-Door Model via Verma Constraints. (arXiv:2203.00161v2 [stat.ME] UPDATED)
196. Graph Neural Networks for Multimodal Single-Cell Data Integration. (arXiv:2203.01884v2 [cs.LG] UPDATED)
197. Bayesian Spillover Graphs for Dynamic Networks. (arXiv:2203.01912v2 [stat.ME] UPDATED)
198. DISCO: Comprehensive and Explainable Disinformation Detection. (arXiv:2203.04928v2 [cs.LG] UPDATED)
199. Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs. (arXiv:2203.09251v2 [cs.LG] UPDATED)
200. Importance Sampling Placement in Off-Policy Temporal-Difference Methods. (arXiv:2203.10172v2 [cs.LG] UPDATED)
201. Scalable Deep Reinforcement Learning Algorithms for Mean Field Games. (arXiv:2203.11973v2 [cs.LG] UPDATED)
202. A Sparsity-promoting Dictionary Model for Variational Autoencoders. (arXiv:2203.15758v2 [cs.LG] UPDATED)
203. Deep learning, stochastic gradient descent and diffusion maps. (arXiv:2204.01365v3 [stat.ML] UPDATED)
204. Modelling Evolutionary and Stationary User Preferences for Temporal Sets Prediction. (arXiv:2204.05490v4 [cs.LG] UPDATED)
205. Out-of-Distribution Detection with Deep Nearest Neighbors. (arXiv:2204.06507v2 [cs.LG] UPDATED)
206. Sketching Algorithms and Lower Bounds for Ridge Regression. (arXiv:2204.06653v2 [cs.DS] UPDATED)
207. MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. (arXiv:2204.08582v2 [cs.CL] UPDATED)
208. Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering. (arXiv:2204.09634v2 [cs.SD] UPDATED)
209. Adversarial Estimators. (arXiv:2204.10495v3 [econ.EM] UPDATED)
210. Toward Compositional Generalization in Object-Oriented World Modeling. (arXiv:2204.13661v2 [cs.LG] UPDATED)
211. NeuralEF: Deconstructing Kernels by Deep Neural Networks. (arXiv:2205.00165v3 [cs.LG] UPDATED)
212. Communication-Efficient Adaptive Federated Learning. (arXiv:2205.02719v2 [cs.LG] UPDATED)
213. Online Algorithms with Multiple Predictions. (arXiv:2205.03921v2 [cs.LG] UPDATED)
214. Towards a multi-stakeholder value-based assessment framework for algorithmic systems. (arXiv:2205.04525v2 [cs.LG] UPDATED)
215. Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction. (arXiv:2205.06672v2 [cs.CV] UPDATED)
216. BayesPCN: A Continually Learnable Predictive Coding Associative Memory. (arXiv:2205.09930v2 [cs.LG] UPDATED)
217. How Powerful are Spectral Graph Neural Networks. (arXiv:2205.11172v2 [cs.LG] UPDATED)
218. Penalized Proximal Policy Optimization for Safe Reinforcement Learning. (arXiv:2205.11814v2 [cs.LG] UPDATED)
219. Distributional Hamilton-Jacobi-Bellman Equations for Continuous-Time Reinforcement Learning. (arXiv:2205.12184v2 [cs.LG] UPDATED)
220. Scalable Interpretability via Polynomials. (arXiv:2205.14108v3 [cs.LG] UPDATED)
221. Neural Basis Models for Interpretability. (arXiv:2205.14120v3 [cs.LG] UPDATED)
222. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. (arXiv:2206.00843v2 [cs.LG] UPDATED)
223. Exponential Separations in Symmetric Neural Networks. (arXiv:2206.01266v2 [cs.LG] UPDATED)
224. Supernet Training for Federated Image Classification under System Heterogeneity. (arXiv:2206.01366v2 [cs.LG] UPDATED)
225. Diffusion-GAN: Training GANs with Diffusion. (arXiv:2206.02262v2 [cs.LG] UPDATED)
226. Generalized Data Distribution Iteration. (arXiv:2206.03192v3 [cs.LG] UPDATED)
227. pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning. (arXiv:2206.03655v3 [cs.LG] UPDATED)
228. FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization. (arXiv:2206.03966v3 [cs.LG] UPDATED)
229. SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v2 [cs.LG] UPDATED)
230. Geometrically Guided Integrated Gradients. (arXiv:2206.05903v2 [cs.CV] UPDATED)
231. A Stochastic Proximal Method for Nonsmooth Regularized Finite Sum Optimization. (arXiv:2206.06531v2 [stat.ML] UPDATED)
232. Zeroth-Order Topological Insights into Iterative Magnitude Pruning. (arXiv:2206.06563v2 [cs.LG] UPDATED)
233. On the Finite-Time Performance of the Knowledge Gradient Algorithm. (arXiv:2206.06847v2 [stat.ML] UPDATED)
234. Can pruning improve certified robustness of neural networks?. (arXiv:2206.07311v2 [cs.LG] UPDATED)
235. QONNX: Representing Arbitrary-Precision Quantized Neural Networks. (arXiv:2206.07527v2 [cs.LG] UPDATED)
236. ELUDE: Generating interpretable explanations via a decomposition into labelled and unlabelled features. (arXiv:2206.07690v2 [cs.CV] UPDATED)
237. Participation and Data Valuation in IoT Data Markets through Distributed Coalitions. (arXiv:2206.07785v2 [cs.NI] UPDATED)
238. Metric-Fair Classifier Derandomization. (arXiv:2206.07826v2 [cs.LG] UPDATED)
239. Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning. (arXiv:2206.07842v2 [cs.LG] UPDATED)
240. Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v2 [cs.CV] UPDATED)
241. All the World's a (Hyper)Graph: A Data Drama. (arXiv:2206.08225v2 [cs.LG] UPDATED)
242. EGRU: Event-based GRU for activity-sparse inference and learning. (arXiv:2206.06178v1 [cs.LG] CROSS LISTED)
## cs.AI
---
**91** new papers in cs.AI:-) 
1. Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature Extraction from Downstream Tasks. (arXiv:2206.08398v1 [eess.IV])
2. Learning to Teach Fairness-aware Deep Multi-task Learning. (arXiv:2206.08403v1 [cs.LG])
3. Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v1 [cs.SI])
4. The Case for a Wholistic Serverless Programming Paradigm and Full Stack Automation for AI and Beyond -- The Philosophy of Jaseci and Jac. (arXiv:2206.08434v1 [cs.DC])
5. Understanding Decision-Time vs. Background Planning in Model-Based Reinforcement Learning. (arXiv:2206.08442v1 [cs.LG])
6. Methods for Estimating and Improving Robustness of Language Models. (arXiv:2206.08446v1 [cs.CL])
7. I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. (arXiv:2206.08451v1 [cs.LG])
8. GOOD: A Graph Out-of-Distribution Benchmark. (arXiv:2206.08452v1 [cs.LG])
9. Quantifying Feature Contributions to Overall Disparity Using Information Theory. (arXiv:2206.08454v1 [cs.LG])
10. Belief-Desire-Intention (BDI) Multi-agent System for Cloud Marketplace Negotiation. (arXiv:2206.08468v1 [cs.MA])
11. XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence. (arXiv:2206.08474v1 [cs.SE])
12. Zero-Shot AutoML with Pretrained Models. (arXiv:2206.08476v1 [cs.LG])
13. High-Speed Accurate Robot Control using Learned Forward Kinodynamics and Non-linear Least Squares Optimization. (arXiv:2206.08487v1 [cs.RO])
14. Debugging using Orthogonal Gradient Descent. (arXiv:2206.08489v1 [cs.LG])
15. TKIL: Tangent Kernel Approach for Class Balanced Incremental Learning. (arXiv:2206.08492v1 [cs.LG])
16. Factorization Approach for Sparse Spatio-Temporal Brain-Computer Interface. (arXiv:2206.08494v1 [cs.AI])
17. Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid Rank Valuations. (arXiv:2206.08495v1 [cs.DS])
18. Self-Supervised Contrastive Pre-Training For Time Series via Time-Frequency Consistency. (arXiv:2206.08496v1 [cs.LG])
19. A Parametric Class of Approximate Gradient Updates for Policy Optimization. (arXiv:2206.08499v1 [cs.LG])
20. Neural Architecture Adaptation for Object Detection by Searching Channel Dimensions and Mapping Pre-trained Parameters. (arXiv:2206.08509v1 [cs.CV])
21. SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe Autonomous Driving. (arXiv:2206.08528v1 [cs.LG])
22. Accelerating Shapley Explanation via Contributive Cooperator Selection. (arXiv:2206.08529v1 [cs.LG])
23. SOS: Score-based Oversampling for Tabular Data. (arXiv:2206.08555v1 [cs.LG])
24. Bootstrapped Transformer for Offline Reinforcement Learning. (arXiv:2206.08569v1 [cs.LG])
25. An F-shape Click Model for Information Retrieval on Multi-block Mobile Pages. (arXiv:2206.08604v1 [cs.IR])
26. Medical Dialogue Response Generation with Pivotal Information Recalling. (arXiv:2206.08611v1 [cs.AI])
27. A Graph-Enhanced Click Model for Web Search. (arXiv:2206.08621v1 [cs.IR])
28. MSDF: A General Open-Domain Multi-Skill Dialog Framework. (arXiv:2206.08626v1 [cs.AI])
29. Minimum Noticeable Difference based Adversarial Privacy Preserving Image Generation. (arXiv:2206.08638v1 [cs.CV])
30. Uncertainty-aware Evaluation of Time-Series Classification for Online Handwriting Recognition with Domain Shift. (arXiv:2206.08640v1 [cs.CV])
31. All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP). (arXiv:2206.08653v1 [cs.LG])
32. tinySNN: Towards Memory- and Energy-Efficient Spiking Neural Networks. (arXiv:2206.08656v1 [cs.NE])
33. The Sensorium competition on predicting large-scale mouse primary visual cortex activity. (arXiv:2206.08666v1 [q-bio.NC])
34. A Deep Learning Approach for the Segmentation of Electroencephalography Data in Eye Tracking Applications. (arXiv:2206.08672v1 [cs.LG])
35. Understanding Robust Overfitting of Adversarial Training and Beyond. (arXiv:2206.08675v1 [cs.LG])
36. Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning. (arXiv:2206.08686v1 [cs.RO])
37. You Only Derive Once (YODO): Automatic Differentiation for Efficient Sensitivity Analysis in Bayesian Networks. (arXiv:2206.08687v1 [cs.AI])
38. Fast Finite Width Neural Tangent Kernel. (arXiv:2206.08720v1 [cs.LG])
39. N$^2$M$^2$: Learning Navigation for Arbitrary Mobile Manipulation Motions in Unseen and Dynamic Environments. (arXiv:2206.08737v1 [cs.RO])
40. Learning Fair Representation via Distributional Contrastive Disentanglement. (arXiv:2206.08743v1 [cs.LG])
41. Rectifying Mono-Label Boolean Classifiers. (arXiv:2206.08758v1 [cs.AI])
42. C-Pack of IPAs: A C90 Program Benchmark of Introductory Programming Assignments. (arXiv:2206.08768v1 [cs.SE])
43. CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume Segmentation on Cone Beam Computed Tomography Images. (arXiv:2206.08778v1 [cs.CV])
44. Reinforcement Learning in Macroeconomic Policy Design: A New Frontier?. (arXiv:2206.08781v1 [cs.LG])
45. Is Multi-Modal Necessarily Better? Robustness Evaluation of Multi-modal Fake News Detection. (arXiv:2206.08788v1 [cs.AI])
46. Reconstructing vehicles from orthographic drawings using deep neural networks. (arXiv:2206.08789v1 [cs.CV])
47. DU-Net based Unsupervised Contrastive Learning for Cancer Segmentation in Histology Images. (arXiv:2206.08791v1 [cs.CV])
48. Holistic Transformer: A Joint Neural Network for Trajectory Prediction and Decision-Making of Autonomous Vehicles. (arXiv:2206.08809v1 [cs.LG])
49. MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. (arXiv:2206.08853v1 [cs.LG])
50. Logic-based Reward Shaping for Multi-Agent Reinforcement Learning. (arXiv:2206.08881v1 [cs.AI])
51. CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer. (arXiv:2206.08883v1 [cs.CV])
52. Fast Population-Based Reinforcement Learning on a Single Machine. (arXiv:2206.08888v1 [cs.LG])
53. Adapting the Linearised Laplace Model Evidence for Modern Deep Learning. (arXiv:2206.08900v1 [stat.ML])
54. Cross-task Attention Mechanism for Dense Multi-task Learning. (arXiv:2206.08927v1 [cs.CV])
55. TAVA: Template-free Animatable Volumetric Actors. (arXiv:2206.08929v1 [cs.CV])
56. Nudge: Accelerating Overdue Pull Requests Towards Completion. (arXiv:2011.12468v5 [cs.SE] UPDATED)
57. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. (arXiv:2101.03961v3 [cs.LG] UPDATED)
58. GrASP: A Library for Extracting and Exploring Human-Interpretable Textual Patterns. (arXiv:2104.03958v2 [cs.CL] UPDATED)
59. Learngene: From Open-World to Your Learning Task. (arXiv:2106.06788v3 [cs.LG] UPDATED)
60. Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers. (arXiv:2108.10752v2 [eess.AS] UPDATED)
61. On the Compression of Neural Networks Using $\ell_0$-Norm Regularization and Weight Pruning. (arXiv:2109.05075v2 [cs.LG] UPDATED)
62. Meta-brain Models: biologically-inspired cognitive agents. (arXiv:2109.11938v3 [q-bio.NC] UPDATED)
63. Unsolved Problems in ML Safety. (arXiv:2109.13916v5 [cs.LG] UPDATED)
64. SaDe: Learning Models that Provably Satisfy Domain Constraints. (arXiv:2112.00552v3 [cs.LG] UPDATED)
65. Decision-Focused Learning: Through the Lens of Learning to Rank. (arXiv:2112.03609v4 [cs.LG] UPDATED)
66. Grounded Language-Image Pre-training. (arXiv:2112.03857v2 [cs.CV] UPDATED)
67. PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning. (arXiv:2201.10029v2 [cs.CV] UPDATED)
68. Human Interpretation of Saliency-based Explanation Over Text. (arXiv:2201.11569v2 [cs.CL] UPDATED)
69. Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks. (arXiv:2201.11729v2 [cs.LG] UPDATED)
70. Constrained Variational Policy Optimization for Safe Reinforcement Learning. (arXiv:2201.11927v3 [cs.LG] UPDATED)
71. ROCK: Causal Inference Principles for Reasoning about Commonsense Causality. (arXiv:2202.00436v2 [cs.CL] UPDATED)
72. Meta-Learning Hypothesis Spaces for Sequential Decision-making. (arXiv:2202.00602v3 [stat.ML] UPDATED)
73. Too much information: why CDCL solvers need to forget learned clauses. (arXiv:2202.01030v2 [cs.AI] UPDATED)
74. Universal Hopfield Networks: A General Framework for Single-Shot Associative Memory Models. (arXiv:2202.04557v2 [cs.NE] UPDATED)
75. BED: A Real-Time Object Detection System for Edge Devices. (arXiv:2202.07503v2 [cs.CV] UPDATED)
76. Graph Neural Networks for Multimodal Single-Cell Data Integration. (arXiv:2203.01884v2 [cs.LG] UPDATED)
77. Reinforcement Learning with Action-Free Pre-Training from Videos. (arXiv:2203.13880v2 [cs.CV] UPDATED)
78. MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. (arXiv:2204.08582v2 [cs.CL] UPDATED)
79. Toward Compositional Generalization in Object-Oriented World Modeling. (arXiv:2204.13661v2 [cs.LG] UPDATED)
80. Intelligent Trajectory Design for RIS-NOMA aided Multi-robot Communications. (arXiv:2205.01647v3 [cs.RO] UPDATED)
81. Communication-Efficient Adaptive Federated Learning. (arXiv:2205.02719v2 [cs.LG] UPDATED)
82. BayesPCN: A Continually Learnable Predictive Coding Associative Memory. (arXiv:2205.09930v2 [cs.LG] UPDATED)
83. How Powerful are Spectral Graph Neural Networks. (arXiv:2205.11172v2 [cs.LG] UPDATED)
84. Penalized Proximal Policy Optimization for Safe Reinforcement Learning. (arXiv:2205.11814v2 [cs.LG] UPDATED)
85. Conceptual Design of the Memory System of the Robot Cognitive Architecture ArmarX. (arXiv:2206.02241v2 [cs.AI] UPDATED)
86. Generalized Data Distribution Iteration. (arXiv:2206.03192v3 [cs.LG] UPDATED)
87. SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v2 [cs.LG] UPDATED)
88. Geometrically Guided Integrated Gradients. (arXiv:2206.05903v2 [cs.CV] UPDATED)
89. Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning. (arXiv:2206.07842v2 [cs.LG] UPDATED)
90. Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v2 [cs.CV] UPDATED)
91. EGRU: Event-based GRU for activity-sparse inference and learning. (arXiv:2206.06178v1 [cs.LG] CROSS LISTED)

