# Your interest papers
---
## cs.CV
---
### RT-DNAS: **Real-time** Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v1 [eess.IV])
- Authors : Qing Lu, Xiaowei Xu, Shunjie Dong, Callie Hao, Lei Yang, Cheng Zhuo, Yiyu Shi
- Link : [http://arxiv.org/abs/2206.04682](http://arxiv.org/abs/2206.04682)
> ABSTRACT  :  Accurately segmenting temporal frames of cine magnetic resonance imaging (MRI) is a crucial step in various real-time MRI guided cardiac interventions. To achieve fast and accurate visual assistance, there are strict requirements on the maximum latency and minimum throughput of the segmentation framework. State-of-the-art neural networks on this task are mostly hand-crafted to satisfy these constraints while achieving high accuracy. On the other hand, while existing literature have demonstrated the power of neural architecture search (NAS) in automatically identifying the best neural architectures for various medical applications, they are mostly guided by accuracy, sometimes with computation complexity, and the importance of real-time constraints are overlooked. A major challenge is that such constraints are non-differentiable and are thus not compatible with the widely used differentiable NAS frameworks. In this paper, we present a strategy that directly handles real-time constraints in a differentiable NAS framework named RT-DNAS. Experiments on extended 2017 MICCAI ACDC dataset show that compared with state-of-the-art manually and automatically designed architectures, RT-DNAS is able to identify ones with better accuracy while satisfying the real-time constraints.  
### Structure-consistent **Restoration** Network for Cataract Fundus Image **Enhancement**. (arXiv:2206.04684v1 [eess.IV])
- Authors : Heng Li, Haofeng Liu, Huazhu Fu, Hai Shu, Yitian Zhao, Xiaoling Luo, Yan Hu, Jiang Liu
- Link : [http://arxiv.org/abs/2206.04684](http://arxiv.org/abs/2206.04684)
> ABSTRACT  :  Fundus photography is a routine examination in clinics to diagnose and monitor ocular diseases. However, for cataract patients, the fundus image always suffers quality degradation caused by the clouding lens. The degradation prevents reliable diagnosis by ophthalmologists or computer-aided systems. To improve the certainty in clinical diagnosis, **restoration** algorithms have been proposed to enhance the quality of fundus images. Unfortunately, challenges remain in the deployment of these algorithms, such as collecting sufficient training data and preserving retinal structures. In this paper, to circumvent the strict deployment requirement, a structure-consistent **restoration** network (SCR-Net) for cataract fundus images is developed from synthesized data that shares an identical structure. A cataract simulation model is firstly designed to collect synthesized cataract sets (SCS) formed by cataract fundus images sharing identical structures. Then high-frequency components (HFCs) are extracted from the SCS to constrain structure consistency such that the structure preservation in SCR-Net is enforced. The experiments demonstrate the effectiveness of SCR-Net in the comparison with state-of-the-art methods and the follow-up clinical applications. The code is available at https://github.com/liamheng/ArcNet-Medical-Image-**Enhancement**.  
### ReFace: **Real-time** Adversarial Attacks on Face Recognition Systems. (arXiv:2206.04783v1 [cs.CV])
- Authors : Shehzeen Hussain, Todd Huster, Chris Mesterharm, Paarth Neekhara, Kevin An, Malhar Jere, Harshvardhan Sikka, Farinaz Koushanfar
- Link : [http://arxiv.org/abs/2206.04783](http://arxiv.org/abs/2206.04783)
> ABSTRACT  :  Deep neural network based face recognition models have been shown to be vulnerable to adversarial examples. However, many of the past attacks require the adversary to solve an input-dependent optimization problem using gradient descent which makes the attack impractical in real-time. These adversarial examples are also tightly coupled to the attacked model and are not as successful in transferring to different models. In this work, we propose ReFace, a real-time, highly-transferable attack on face recognition models based on Adversarial Transformation Networks (ATNs). ATNs model adversarial example generation as a feed-forward neural network. We find that the white-box attack success rate of a pure U-Net ATN falls substantially short of gradient-based attacks like PGD on large face recognition datasets. We therefore propose a new architecture for ATNs that closes this gap while maintaining a 10000x speedup over PGD. Furthermore, we find that at a given perturbation magnitude, our ATN adversarial perturbations are more effective in transferring to new face recognition models than PGD. ReFace attacks can successfully deceive commercial face recognition services in a transfer attack setting and reduce face identification accuracy from 82% to 16.4% for AWS SearchFaces API and Azure face verification accuracy from 91% to 50.1%.  
### **NeRF**-In: Free-Form **NeRF** Inpainting with RGB-D Priors. (arXiv:2206.04901v1 [cs.CV])
- Authors : Kang Liu, Chao Shen, Yu Chen
- Link : [http://arxiv.org/abs/2206.04901](http://arxiv.org/abs/2206.04901)
> ABSTRACT  :  Though Neural Radiance Field (**NeRF**) demonstrates compelling novel view synthesis results, it is still unintuitive to edit a pre-trained **NeRF** because the neural network's parameters and the scene geometry/appearance are often not explicitly associated. In this paper, we introduce the first framework that enables users to remove unwanted objects or retouch undesired regions in a 3D scene represented by a pre-trained **NeRF** without any category-specific data and training. The user first draws a free-form mask to specify a region containing unwanted objects over a rendered view from the pre-trained **NeRF**. Our framework first transfers the user-provided mask to other rendered views and estimates guiding color and depth images within these transferred masked regions. Next, we formulate an optimization problem that jointly inpaints the image content in all masked regions across multiple views by updating the **NeRF** model's parameters. We demonstrate our framework on diverse scenes and show it obtained visual plausible and structurally consistent results across multiple views using shorter time and less user manual efforts.  
### Self-Supervised Deep Subspace Clustering with Entropy-norm. (arXiv:2206.04958v1 [cs.CV])
- Authors : Guangyi Zhao, Simin Kou, Xuesong Yin
- Link : [http://arxiv.org/abs/2206.04958](http://arxiv.org/abs/2206.04958)
> ABSTRACT  :  Auto-Encoder based deep subspace clustering (DSC) is widely used in computer vision, motion segmentation and image processing. However, it suffers from the following three issues in the self-expressive matrix learning process: the first one is less useful information for learning self-expressive weights due to the simple reconstruction loss; the second one is that the construction of the self-expression layer associated with the sample size requires high-computational cost; and the last one is the limited connectivity of the existing regularization terms. In order to address these issues, in this paper we propose a novel model named Self-Supervised deep Subspace Clustering with Entropy-norm (S$^{3}$CE). Specifically, S$^{3}$CE exploits a self-supervised contrastive network to gain a more effetive feature vector. The local structure and dense connectivity of the original data benefit from the self-expressive layer and additional entropy-norm constraint. Moreover, a new module with data **enhancement** is designed to help S$^{3}$CE focus on the key information of data, and improve the clustering performance of positive and negative instances through spectral clustering. Extensive experimental results demonstrate the superior performance of S$^{3}$CE in comparison to the state-of-the-art approaches.  
### Position Labels for Self-Supervised Vision Transformer. (arXiv:2206.04981v1 [cs.CV])
- Authors : Zhemin Zhang, Xun Gong, Jinyi Wu
- Link : [http://arxiv.org/abs/2206.04981](http://arxiv.org/abs/2206.04981)
> ABSTRACT  :  Position encoding is important for vision transformer (ViT) to capture the spatial structure of the input image. General efficacy has been proven in ViT. In our work we propose to train ViT to recognize the 2D position encoding of patches of the input image, this apparently simple task actually yields a meaningful self-supervisory task. Based on previous work on ViT position encoding, we propose two position labels dedicated to 2D images including absolute position and relative position. Our position labels can be easily plugged into transformer, combined with the various current ViT variants. It can work in two ways: 1.As an auxiliary training target for vanilla ViT (e.g., ViT-B and **Swin**-B) to improve model performance. 2. Combine the self-supervised ViT (e.g., MAE) to provide a more powerful self-supervised signal for semantic feature learning. Experiments demonstrate that solely due to the proposed self-supervised methods, **Swin**-B and ViT-B obtained improvements of 1.9% (top-1 Acc) and 5.6% (top-1 Acc) on Mini-ImageNet, respectively.  
### **Real-time** Hyper-Dimensional Reconfiguration at the Edge using Hardware Accelerators. (arXiv:2206.05128v1 [cs.CV])
- Authors : Indhumathi Kandaswamy, Saurabh Farkya, Zachary Daniels, Gooitzen van, der Wal, Aswin Raghavan, Yuzheng Zhang, Jun Hu, Michael Lomnitz, Michael Isnardi, David Zhang, Michael Piacentino
- Link : [http://arxiv.org/abs/2206.05128](http://arxiv.org/abs/2206.05128)
> ABSTRACT  :  In this paper we present Hyper-Dimensional Reconfigurable Analytics at the Tactical Edge (HyDRATE) using low-SWaP embedded hardware that can perform real-time reconfiguration at the edge leveraging non-MAC (free of floating-point MultiplyACcumulate operations) deep neural nets (DNN) combined with hyperdimensional (HD) computing accelerators. We describe the algorithm, trained quantized model generation, and simulated performance of a feature extractor free of multiply-accumulates feeding a hyperdimensional logic-based classifier. Then we show how performance increases with the number of hyperdimensions. We describe the realized low-SWaP FPGA hardware and embedded software system compared to traditional DNNs and detail the implemented hardware accelerators. We discuss the measured system latency and power, noise robustness due to use of learnable quantization and HD computing, actual versus simulated system performance for a video activity classification task and demonstration of reconfiguration on this same dataset. We show that reconfigurability in the field is achieved by retraining only the feed-forward HD classifier without gradient descent backpropagation (gradient-free), using few-shot learning of new classes at the edge. Initial work performed used LRCN DNN and is currently extended to use Two-stream DNN with improved performance.  
### GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v3 [cs.CV] UPDATED)
- Authors : Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Dacheng Tao
- Link : [http://arxiv.org/abs/2111.13680](http://arxiv.org/abs/2111.13680)
> ABSTRACT  :  Learning-based optical flow estimation has been dominated with the pipeline of cost volume with convolutions for flow regression, which is inherently limited to local correlations and thus is hard to address the long-standing challenge of large displacements. To alleviate this, the state-of-the-art framework RAFT gradually improves its prediction quality by using a large number of iterative refinements, achieving remarkable performance but introducing linearly increasing inference time. To enable both high accuracy and efficiency, we completely revamp the dominant flow regression pipeline by reformulating optical flow as a global matching problem, which identifies the correspondences by directly comparing feature similarities. Specifically, we propose a GMFlow framework, which consists of three main components: a customized Transformer for feature **enhancement**, a correlation and softmax layer for global feature matching, and a self-attention layer for flow propagation. We further introduce a refinement step that reuses GMFlow at higher feature resolution for residual flow prediction. Our new framework outperforms 31-refinements RAFT on the challenging Sintel benchmark, while using only one refinement and running faster, suggesting a new paradigm for accurate and efficient optical flow estimation. Code is available at https://github.com/haofeixu/gmflow.  
### Illumination Adaptive Transformer. (arXiv:2205.14871v2 [cs.CV] UPDATED)
- Authors : Ziteng Cui, Kunchang Li, Lin Gu, Shenghan Su, Peng Gao, Zhengkai Jiang, Yu Qiao, Tatsuya Harada
- Link : [http://arxiv.org/abs/2205.14871](http://arxiv.org/abs/2205.14871)
> ABSTRACT  :  Challenging illumination conditions (**low light**, under**exposure** and over**exposure**) in the real world not only cast an unpleasant visual appearance but also taint the computer vision tasks. Existing light adaptive methods often deal with each condition individually. What is more, most of them often operate on a RAW image or over-simplify the camera image signal processing (ISP) pipeline. By decomposing the light transformation pipeline into local and global ISP components, we propose a lightweight fast Illumination Adaptive Transformer (IAT) which comprises two transformer-style branches: local estimation branch and global ISP branch. While the local branch estimates the pixel-wise local components relevant to illumination, the global branch defines learnable quires that attend the whole image to decode the parameters. Our IAT could also conduct both object detection and semantic segmentation under various light conditions. We have extensively evaluated IAT on multiple real-world datasets on 2 low-level tasks and 3 high-level tasks. With only 90k parameters and 0.004s processing speed (excluding high-level module), our IAT has consistently achieved superior performance over SOTA. Code is available at https://github.com/cuiziteng/Illumination-Adaptive-Transformer  
### xView3-SAR: Detecting **Dark** Fishing Activity Using Synthetic Aperture Radar Imagery. (arXiv:2206.00897v2 [cs.CV] UPDATED)
- Authors : Fernando Paolo, ting Tim, Ritwik Gupta, Bryce Goodman, Nirav Patel, Daniel Kuster, David Kroodsma, Jared Dunnmon
- Link : [http://arxiv.org/abs/2206.00897](http://arxiv.org/abs/2206.00897)
> ABSTRACT  :  Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that evade monitoring systems -- known as "**dark** vessels" -- is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of **dark** vessels day or **night**, under all-weather conditions. SAR images, however, require domain-specific treatment and is not widely accessible to the ML community. Moreover, the objects (vessels) are small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels from SAR. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We provide an overview of the results from the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data (https://iuu.xview.us/) and code (https://github.com/DIUx-xView) to support ongoing development and evaluation of ML approaches for this important application.  
### PIDNet: A **Real-time** Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v2 [cs.CV] UPDATED)
- Authors : Jiacong Xu, Zixiang Xiong
- Link : [http://arxiv.org/abs/2206.02066](http://arxiv.org/abs/2206.02066)
> ABSTRACT  :  Two-branch network architecture has shown its efficiency and effectiveness for real-time semantic segmentation tasks. However, direct fusion of low-level details and high-level semantics will lead to a phenomenon that the detailed features are easily overwhelmed by surrounding contextual information, namely overshoot in this paper, which limits the improvement of the accuracy of existed two-branch models. In this paper, we bridge a connection between Convolutional Neural Network (CNN) and Proportional-Integral-Derivative (PID) controller and reveal that the two-branch network is nothing but a Proportional-Integral (PI) controller, which inherently suffers from the similar overshoot issue. To alleviate this issue, we propose a novel three-branch network architecture: PIDNet, which possesses three branches to parse the detailed, context and boundary information (derivative of semantics), respectively, and employs boundary attention to guide the fusion of detailed and context branches in final stage. The family of PIDNets achieve the best trade-off between inference speed and accuracy and their test accuracy surpasses all the existed models with similar inference speed on Cityscapes, CamVid and COCO-Stuff datasets. Especially, PIDNet-S achieves 78.6% mIOU with inference speed of 93.2 FPS on Cityscapes test set and 80.1% mIOU with speed of 153.7 FPS on CamVid test set.  
## eess.IV
---
### RT-DNAS: **Real-time** Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v1 [eess.IV])
- Authors : Qing Lu, Xiaowei Xu, Shunjie Dong, Callie Hao, Lei Yang, Cheng Zhuo, Yiyu Shi
- Link : [http://arxiv.org/abs/2206.04682](http://arxiv.org/abs/2206.04682)
> ABSTRACT  :  Accurately segmenting temporal frames of cine magnetic resonance imaging (MRI) is a crucial step in various real-time MRI guided cardiac interventions. To achieve fast and accurate visual assistance, there are strict requirements on the maximum latency and minimum throughput of the segmentation framework. State-of-the-art neural networks on this task are mostly hand-crafted to satisfy these constraints while achieving high accuracy. On the other hand, while existing literature have demonstrated the power of neural architecture search (NAS) in automatically identifying the best neural architectures for various medical applications, they are mostly guided by accuracy, sometimes with computation complexity, and the importance of real-time constraints are overlooked. A major challenge is that such constraints are non-differentiable and are thus not compatible with the widely used differentiable NAS frameworks. In this paper, we present a strategy that directly handles real-time constraints in a differentiable NAS framework named RT-DNAS. Experiments on extended 2017 MICCAI ACDC dataset show that compared with state-of-the-art manually and automatically designed architectures, RT-DNAS is able to identify ones with better accuracy while satisfying the real-time constraints.  
### Structure-consistent **Restoration** Network for Cataract Fundus Image **Enhancement**. (arXiv:2206.04684v1 [eess.IV])
- Authors : Heng Li, Haofeng Liu, Huazhu Fu, Hai Shu, Yitian Zhao, Xiaoling Luo, Yan Hu, Jiang Liu
- Link : [http://arxiv.org/abs/2206.04684](http://arxiv.org/abs/2206.04684)
> ABSTRACT  :  Fundus photography is a routine examination in clinics to diagnose and monitor ocular diseases. However, for cataract patients, the fundus image always suffers quality degradation caused by the clouding lens. The degradation prevents reliable diagnosis by ophthalmologists or computer-aided systems. To improve the certainty in clinical diagnosis, **restoration** algorithms have been proposed to enhance the quality of fundus images. Unfortunately, challenges remain in the deployment of these algorithms, such as collecting sufficient training data and preserving retinal structures. In this paper, to circumvent the strict deployment requirement, a structure-consistent **restoration** network (SCR-Net) for cataract fundus images is developed from synthesized data that shares an identical structure. A cataract simulation model is firstly designed to collect synthesized cataract sets (SCS) formed by cataract fundus images sharing identical structures. Then high-frequency components (HFCs) are extracted from the SCS to constrain structure consistency such that the structure preservation in SCR-Net is enforced. The experiments demonstrate the effectiveness of SCR-Net in the comparison with state-of-the-art methods and the follow-up clinical applications. The code is available at https://github.com/liamheng/ArcNet-Medical-Image-**Enhancement**.  
## cs.LG
---
### RT-DNAS: **Real-time** Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v1 [eess.IV])
- Authors : Qing Lu, Xiaowei Xu, Shunjie Dong, Callie Hao, Lei Yang, Cheng Zhuo, Yiyu Shi
- Link : [http://arxiv.org/abs/2206.04682](http://arxiv.org/abs/2206.04682)
> ABSTRACT  :  Accurately segmenting temporal frames of cine magnetic resonance imaging (MRI) is a crucial step in various real-time MRI guided cardiac interventions. To achieve fast and accurate visual assistance, there are strict requirements on the maximum latency and minimum throughput of the segmentation framework. State-of-the-art neural networks on this task are mostly hand-crafted to satisfy these constraints while achieving high accuracy. On the other hand, while existing literature have demonstrated the power of neural architecture search (NAS) in automatically identifying the best neural architectures for various medical applications, they are mostly guided by accuracy, sometimes with computation complexity, and the importance of real-time constraints are overlooked. A major challenge is that such constraints are non-differentiable and are thus not compatible with the widely used differentiable NAS frameworks. In this paper, we present a strategy that directly handles real-time constraints in a differentiable NAS framework named RT-DNAS. Experiments on extended 2017 MICCAI ACDC dataset show that compared with state-of-the-art manually and automatically designed architectures, RT-DNAS is able to identify ones with better accuracy while satisfying the real-time constraints.  
### ReFace: **Real-time** Adversarial Attacks on Face Recognition Systems. (arXiv:2206.04783v1 [cs.CV])
- Authors : Shehzeen Hussain, Todd Huster, Chris Mesterharm, Paarth Neekhara, Kevin An, Malhar Jere, Harshvardhan Sikka, Farinaz Koushanfar
- Link : [http://arxiv.org/abs/2206.04783](http://arxiv.org/abs/2206.04783)
> ABSTRACT  :  Deep neural network based face recognition models have been shown to be vulnerable to adversarial examples. However, many of the past attacks require the adversary to solve an input-dependent optimization problem using gradient descent which makes the attack impractical in real-time. These adversarial examples are also tightly coupled to the attacked model and are not as successful in transferring to different models. In this work, we propose ReFace, a real-time, highly-transferable attack on face recognition models based on Adversarial Transformation Networks (ATNs). ATNs model adversarial example generation as a feed-forward neural network. We find that the white-box attack success rate of a pure U-Net ATN falls substantially short of gradient-based attacks like PGD on large face recognition datasets. We therefore propose a new architecture for ATNs that closes this gap while maintaining a 10000x speedup over PGD. Furthermore, we find that at a given perturbation magnitude, our ATN adversarial perturbations are more effective in transferring to new face recognition models than PGD. ReFace attacks can successfully deceive commercial face recognition services in a transfer attack setting and reduce face identification accuracy from 82% to 16.4% for AWS SearchFaces API and Azure face verification accuracy from 91% to 50.1%.  
### Binarizing Split Learning for Data Privacy **Enhancement** and Computation Reduction. (arXiv:2206.04864v1 [cs.LG])
- Authors : Ngoc Duy, Alsharif Abuadbba, Yansong Gao, Tran Khoa, Naveen Chilamkurti
- Link : [http://arxiv.org/abs/2206.04864](http://arxiv.org/abs/2206.04864)
> ABSTRACT  :  Split learning (SL) enables data privacy preservation by allowing clients to collaboratively train a deep learning model with the server without sharing raw data. However, SL still has limitations such as potential data privacy leakage and high computation at clients. In this study, we propose to binarize the SL local layers for faster computation (up to 17.5 times less forward-propagation time in both training and inference phases on mobile devices) and reduced memory usage (up to 32 times less memory and bandwidth requirements). More importantly, the binarized SL (B-SL) model can reduce privacy leakage from SL smashed data with merely a small degradation in model accuracy. To further enhance the privacy preservation, we also propose two novel approaches: 1) training with additional local leak loss and 2) applying differential privacy, which could be integrated separately or concurrently into the B-SL model. Experimental results with different datasets have affirmed the advantages of the B-SL models compared with several benchmark models. The effectiveness of B-SL models against feature-space hijacking attack (FSHA) is also illustrated. Our results have demonstrated B-SL models are promising for lightweight IoT/mobile applications with high privacy-preservation requirements such as mobile healthcare applications.  
### Deep Learning-based Massive MIMO CSI Acquisition for 5G Evolution and 6G. (arXiv:2206.04967v1 [eess.SP])
- Authors : Xin Wang, Xiaolin Hou, Lan Chen, Yoshihisa Kishiyama, Takahiro Asai
- Link : [http://arxiv.org/abs/2206.04967](http://arxiv.org/abs/2206.04967)
> ABSTRACT  :  Recently, inspired by successful applications in many fields, deep learning (DL) technologies for CSI acquisition have received considerable research interest from both academia and industry. Considering the practical feedback mechanism of 5th generation (5G) New radio (NR) networks, we propose two implementation schemes for artificial intelligence for CSI (AI4CSI), the DL-based receiver and end-to-end design, respectively. The proposed AI4CSI schemes were evaluated in 5G NR networks in terms of spectrum efficiency (SE), feedback overhead, and computational complexity, and compared with legacy schemes. To demonstrate whether these schemes can be used in real-life scenarios, both the modeled-based channel data and practically measured channels were used in our investigations. When DL-based CSI acquisition is applied to the receiver only, which has little air interface impact, it provides approximately 25\% SE gain at a moderate feedback overhead level. It is feasible to deploy it in current 5G networks during 5G evolutions. For the end-to-end DL-based CSI **enhancement**s, the evaluations also demonstrated their additional performance gain on SE, which is 6% -- 26% compared with DL-based receivers and 33% -- 58% compared with legacy CSI schemes. Considering its large impact on air-interface design, it will be a candidate technology for 6th generation (6G) networks, in which an air interface designed by artificial intelligence can be used.  
### NeuroComb: Improving SAT Solving with Graph Neural Networks. (arXiv:2110.14053v3 [cs.AI] UPDATED)
- Authors : Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth McMillan, Risto Miikkulainen
- Link : [http://arxiv.org/abs/2110.14053](http://arxiv.org/abs/2110.14053)
> ABSTRACT  :  Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers by improving their variable branching heuristics through predictions generated by Graph Neural Networks(GNNs). However, so far this approach either has not made solving more effective, or has required online access to substantial GPU resources. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroComb, which builds on two insights: (1) predictions of important variables and clauses can be combined with dynamic branching into a more effective hybrid branching strategy, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. NeuroComb is implemented as an **enhancement** to a classic CDCL solver called MiniSat and a more recent CDCL solver called Glucose. As a result, it allowed MiniSat to solve 11% and Glucose 5% more problems on the recent SATCOMP-2021 competition problem set, with the computational resource requirement of only one GPU. NeuroComb is therefore a both effective and practical approach to improving SAT solving through machine learning.  
### Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence Model Tackles All SMAC Tasks. (arXiv:2112.02845v3 [cs.LG] UPDATED)
- Authors : Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, Bo Xu
- Link : [http://arxiv.org/abs/2112.02845](http://arxiv.org/abs/2112.02845)
> ABSTRACT  :  Offline reinforcement learning leverages previously-collected offline datasets to learn optimal policies with no necessity to access the real environment. Such a paradigm is also desirable for multi-agent reinforcement learning (MARL) tasks, given the increased interactions among agents and with the enviroment. Yet, in MARL, the paradigm of offline pre-training with online fine-tuning has not been studied, nor datasets or benchmarks for offline MARL research are available. In this paper, we facilitate the research by providing large-scale datasets, and use them to examine the usage of the Decision Transformer in the context of MARL. We investigate the generalisation of MARL offline pre-training in the following three aspects: 1) between single agents and multiple agents, 2) from offline pretraining to the online fine-tuning, and 3) to that of multiple downstream tasks with few-shot and zero-shot capabilities. We start by introducing the first offline MARL dataset with diverse quality levels based on the StarCraftII environment, and then propose the novel architecture of multi-agent decision transformer (MADT) for effective offline learning. MADT leverages transformer's modelling ability of sequence modelling and integrates it seamlessly with both offline and online MARL tasks. A crucial benefit of MADT is that it learns generalisable policies that can transfer between different types of agents under different task scenarios. On StarCraft II offline dataset, MADT outperforms the state-of-the-art offline RL baselines. When applied to online tasks, the pre-trained MADT significantly improves sample efficiency, and enjoys strong performance both few-short and zero-shot cases. To our best knowledge, this is the first work that studies and demonstrates the effectiveness of offline pre-trained models in terms of sample efficiency and generalisability **enhancement**s in MARL.  
### Predicting the Thermal Sunyaev-Zel'dovich Field using Modular and Equivariant Set-Based Neural Networks. (arXiv:2203.00026v2 [astro-ph.CO] UPDATED)
- Authors : Leander Thiele, Miles Cranmer, William Coulton, Shirley Ho
- Link : [http://arxiv.org/abs/2203.00026](http://arxiv.org/abs/2203.00026)
> ABSTRACT  :  Theoretical uncertainty limits our ability to extract cosmological information from baryonic fields such as the thermal Sunyaev-Zel'dovich (tSZ) effect. Being sourced by the electron pressure field, the tSZ effect depends on baryonic physics that is usually modeled by expensive hydrodynamic simulations. We train neural networks on the IllustrisTNG-300 cosmological simulation to predict the continuous electron pressure field in galaxy clusters from gravity-only simulations. Modeling clusters is challenging for neural networks as most of the gas pressure is concentrated in a handful of voxels and even the largest hydrodynamical simulations contain only a few hundred clusters that can be used for training. Instead of conventional convolutional neural net (CNN) architectures, we choose to employ a rotationally equivariant DeepSets architecture to operate directly on the set of **dark** matter particles. We argue that set-based architectures provide distinct advantages over CNNs. For example, we can enforce exact rotational and permutation equivariance, incorporate existing knowledge on the tSZ field, and work with sparse fields as are standard in cosmology. We compose our architecture with separate, physically meaningful modules, making it amenable to interpretation. For example, we can separately study the influence of local and cluster-scale environment, determine that cluster triaxiality has negligible impact, and train a module that corrects for mis-centering. Our model improves by 70 % on analytic profiles fit to the same simulation data. We argue that the electron pressure field, viewed as a function of a gravity-only simulation, has inherent stochasticity, and model this property through a conditional-VAE extension to the network. This modification yields further improvement by 7 %, it is limited by our small training set however. (abridged)  
## cs.AI
---
### NeuroComb: Improving SAT Solving with Graph Neural Networks. (arXiv:2110.14053v3 [cs.AI] UPDATED)
- Authors : Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth McMillan, Risto Miikkulainen
- Link : [http://arxiv.org/abs/2110.14053](http://arxiv.org/abs/2110.14053)
> ABSTRACT  :  Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers by improving their variable branching heuristics through predictions generated by Graph Neural Networks(GNNs). However, so far this approach either has not made solving more effective, or has required online access to substantial GPU resources. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroComb, which builds on two insights: (1) predictions of important variables and clauses can be combined with dynamic branching into a more effective hybrid branching strategy, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. NeuroComb is implemented as an **enhancement** to a classic CDCL solver called MiniSat and a more recent CDCL solver called Glucose. As a result, it allowed MiniSat to solve 11% and Glucose 5% more problems on the recent SATCOMP-2021 competition problem set, with the computational resource requirement of only one GPU. NeuroComb is therefore a both effective and practical approach to improving SAT solving through machine learning.  
### Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence Model Tackles All SMAC Tasks. (arXiv:2112.02845v3 [cs.LG] UPDATED)
- Authors : Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, Bo Xu
- Link : [http://arxiv.org/abs/2112.02845](http://arxiv.org/abs/2112.02845)
> ABSTRACT  :  Offline reinforcement learning leverages previously-collected offline datasets to learn optimal policies with no necessity to access the real environment. Such a paradigm is also desirable for multi-agent reinforcement learning (MARL) tasks, given the increased interactions among agents and with the enviroment. Yet, in MARL, the paradigm of offline pre-training with online fine-tuning has not been studied, nor datasets or benchmarks for offline MARL research are available. In this paper, we facilitate the research by providing large-scale datasets, and use them to examine the usage of the Decision Transformer in the context of MARL. We investigate the generalisation of MARL offline pre-training in the following three aspects: 1) between single agents and multiple agents, 2) from offline pretraining to the online fine-tuning, and 3) to that of multiple downstream tasks with few-shot and zero-shot capabilities. We start by introducing the first offline MARL dataset with diverse quality levels based on the StarCraftII environment, and then propose the novel architecture of multi-agent decision transformer (MADT) for effective offline learning. MADT leverages transformer's modelling ability of sequence modelling and integrates it seamlessly with both offline and online MARL tasks. A crucial benefit of MADT is that it learns generalisable policies that can transfer between different types of agents under different task scenarios. On StarCraft II offline dataset, MADT outperforms the state-of-the-art offline RL baselines. When applied to online tasks, the pre-trained MADT significantly improves sample efficiency, and enjoys strong performance both few-short and zero-shot cases. To our best knowledge, this is the first work that studies and demonstrates the effectiveness of offline pre-trained models in terms of sample efficiency and generalisability **enhancement**s in MARL.  
### PIDNet: A **Real-time** Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v2 [cs.CV] UPDATED)
- Authors : Jiacong Xu, Zixiang Xiong
- Link : [http://arxiv.org/abs/2206.02066](http://arxiv.org/abs/2206.02066)
> ABSTRACT  :  Two-branch network architecture has shown its efficiency and effectiveness for real-time semantic segmentation tasks. However, direct fusion of low-level details and high-level semantics will lead to a phenomenon that the detailed features are easily overwhelmed by surrounding contextual information, namely overshoot in this paper, which limits the improvement of the accuracy of existed two-branch models. In this paper, we bridge a connection between Convolutional Neural Network (CNN) and Proportional-Integral-Derivative (PID) controller and reveal that the two-branch network is nothing but a Proportional-Integral (PI) controller, which inherently suffers from the similar overshoot issue. To alleviate this issue, we propose a novel three-branch network architecture: PIDNet, which possesses three branches to parse the detailed, context and boundary information (derivative of semantics), respectively, and employs boundary attention to guide the fusion of detailed and context branches in final stage. The family of PIDNets achieve the best trade-off between inference speed and accuracy and their test accuracy surpasses all the existed models with similar inference speed on Cityscapes, CamVid and COCO-Stuff datasets. Especially, PIDNet-S achieves 78.6% mIOU with inference speed of 93.2 FPS on Cityscapes test set and 80.1% mIOU with speed of 153.7 FPS on CamVid test set.  
# Paper List
---
## cs.CV
---
**92** new papers in cs.CV:-) 
1. Extending Momentum Contrast with Cross Similarity Consistency Regularization. (arXiv:2206.04676v1 [cs.LG])
2. Can Backdoor Attacks Survive Time-Varying Models?. (arXiv:2206.04677v1 [cs.CR])
3. POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution Samples. (arXiv:2206.04679v1 [cs.LG])
4. Gaussian Fourier Pyramid for Local Laplacian Filter. (arXiv:2206.04681v1 [eess.IV])
5. RT-DNAS: **Real-time** Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v1 [eess.IV])
6. Structure-consistent **Restoration** Network for Cataract Fundus Image **Enhancement**. (arXiv:2206.04684v1 [eess.IV])
7. AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding Biomechanical Testing. (arXiv:2206.04689v1 [eess.IV])
8. AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging. (arXiv:2206.04732v1 [eess.IV])
9. An Empirical Study on Disentanglement of Negative-free Contrastive Learning. (arXiv:2206.04756v1 [cs.LG])
10. What should AI see? Using the Public's Opinion to Determine the Perception of an AI. (arXiv:2206.04776v1 [cs.LG])
11. Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v1 [cs.LG])
12. ReFace: **Real-time** Adversarial Attacks on Face Recognition Systems. (arXiv:2206.04783v1 [cs.CV])
13. Building Spatio-temporal Transformers for Egocentric 3D Pose Estimation. (arXiv:2206.04785v1 [cs.CV])
14. Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition. (arXiv:2206.04790v1 [cs.CV])
15. Stable and memory-efficient image recovery using monotone operator learning (MOL). (arXiv:2206.04797v1 [cs.CV])
16. R4D: Utilizing Reference Objects for Long-Range Distance Estimation. (arXiv:2206.04831v1 [cs.CV])
17. Masked Autoencoders are Robust Data Augmentors. (arXiv:2206.04846v1 [cs.CV])
18. Heterogeneous Face Recognition via Face Synthesis with Identity-Attribute Disentanglement. (arXiv:2206.04854v1 [cs.CV])
19. Symbolic image detection using scene and knowledge graphs. (arXiv:2206.04863v1 [cs.CV])
20. The Gender Gap in Face Recognition Accuracy Is a Hairy Problem. (arXiv:2206.04867v1 [cs.CV])
21. The 1st Data Science for Pavements Challenge. (arXiv:2206.04874v1 [cs.CV])
22. Efficient Per-Shot Convex Hull Prediction By Recurrent Learning. (arXiv:2206.04877v1 [eess.IV])
23. Unsupervised Foggy Scene Understanding via Self Spatial-Temporal Label Diffusion. (arXiv:2206.04879v1 [cs.CV])
24. Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers. (arXiv:2206.04881v1 [cs.CR])
25. AntPivot: Livestream Highlight Detection via Hierarchical Attention Mechanism. (arXiv:2206.04888v1 [cs.MM])
26. **NeRF**-In: Free-Form **NeRF** Inpainting with RGB-D Priors. (arXiv:2206.04901v1 [cs.CV])
27. Out of Sight, Out of Mind: A Source-View-Wise Feature Aggregation for Multi-View Image-Based Rendering. (arXiv:2206.04906v1 [cs.CV])
28. PatchComplete: Learning Multi-Resolution Patch Priors for 3D Shape Completion on Unseen Categories. (arXiv:2206.04916v1 [cs.CV])
29. Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose Estimation. (arXiv:2206.04927v1 [cs.CV])
30. Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes. (arXiv:2206.04942v1 [cs.CV])
31. Deep Multi-view Semi-supervised Clustering with Sample Pairwise Constraints. (arXiv:2206.04949v1 [cs.CV])
32. Self-Supervised Deep Subspace Clustering with Entropy-norm. (arXiv:2206.04958v1 [cs.CV])
33. NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression Recognition. (arXiv:2206.04975v1 [cs.CV])
34. Convolutional Layers Are Not Translation Equivariant. (arXiv:2206.04979v1 [cs.CV])
35. Position Labels for Self-Supervised Vision Transformer. (arXiv:2206.04981v1 [cs.CV])
36. Subjective Quality Assessment for Images Generated by Computer Graphics. (arXiv:2206.05008v1 [cs.GR])
37. Spatial Cross-Attention Improves Self-Supervised Visual Representation Learning. (arXiv:2206.05028v1 [cs.CV])
38. Image Generation with Multimodal Priors using Denoising Diffusion Probabilistic Models. (arXiv:2206.05039v1 [cs.CV])
39. A GPU-Accelerated Light-field Super-resolution Framework Based on Mixed Noise Model and Weighted Regularization. (arXiv:2206.05047v1 [eess.IV])
40. Denoising Generalized Expectation-Consistent Approximation for MRI Image Recovery. (arXiv:2206.05049v1 [eess.IV])
41. A No-reference Quality Assessment Metric for Point Cloud Based on Captured Video Sequences. (arXiv:2206.05054v1 [eess.IV])
42. Learning self-calibrated optic disc and cup segmentation from multi-rater annotations. (arXiv:2206.05092v1 [eess.IV])
43. Federated Momentum Contrastive Clustering. (arXiv:2206.05093v1 [cs.LG])
44. SimVP: Simpler yet Better Video Prediction. (arXiv:2206.05099v1 [cs.CV])
45. Saccade Mechanisms for Image Classification, Object Detection and Tracking. (arXiv:2206.05102v1 [cs.CV])
46. Globally-Optimal Contrast Maximisation for Event Cameras. (arXiv:2206.05127v1 [cs.CV])
47. **Real-time** Hyper-Dimensional Reconfiguration at the Edge using Hardware Accelerators. (arXiv:2206.05128v1 [cs.CV])
48. Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification. (arXiv:2206.05148v1 [eess.IV])
49. Referring Image Matting. (arXiv:2206.05149v1 [cs.CV])
50. MEAT: Maneuver Extraction from Agent Trajectories. (arXiv:2206.05158v1 [cs.CV])
51. An Image Processing Pipeline for Camera Trap Time-Lapse Recordings. (arXiv:2206.05159v1 [cs.CV])
52. Exploring Feature Self-relation for Self-supervised Transformer. (arXiv:2206.05184v1 [cs.CV])
53. Learning the Space of Deep Models. (arXiv:2206.05194v1 [cs.CV])
54. ClamNet: Using contrastive learning with variable depth Unets for medical image segmentation. (arXiv:2206.05225v1 [cs.CV])
55. Optical Diffraction Tomography based on 3D Physics-Inspired Neural Network (PINN). (arXiv:2206.05236v1 [physics.optics])
56. Lost in Transmission: On the Impact of Networking Corruptions on Video Machine Learning Models. (arXiv:2206.05252v1 [cs.CV])
57. Rethinking Spatial Invariance of Convolutional Networks for Object Counting. (arXiv:2206.05253v1 [cs.CV])
58. Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces. (arXiv:2206.05257v1 [cs.CV])
59. Is Self-Supervised Learning More Robust Than Supervised Learning?. (arXiv:2206.05259v1 [cs.CV])
60. Balanced Product of Experts for Long-Tailed Recognition. (arXiv:2206.05260v1 [cs.CV])
61. Causal Balancing for Domain Generalization. (arXiv:2206.05263v1 [cs.LG])
62. Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?. (arXiv:2206.05266v1 [cs.LG])
63. Street Crossing Aid Using Light-weight CNNs for the Visually Impaired. (arXiv:1909.09598v2 [cs.CV] UPDATED)
64. Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection. (arXiv:2010.14982v2 [cs.CV] UPDATED)
65. Object-Region Video Transformers. (arXiv:2110.06915v3 [cs.CV] UPDATED)
66. Grafting Transformer on Automatically Designed Convolutional Neural Network for Hyperspectral Image Classification. (arXiv:2110.11084v2 [cs.CV] UPDATED)
67. Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v3 [eess.IV] UPDATED)
68. MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection. (arXiv:2111.13336v4 [cs.CV] UPDATED)
69. GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v3 [cs.CV] UPDATED)
70. MC-Blur: A Comprehensive Benchmark for Image Deblurring. (arXiv:2112.00234v2 [cs.CV] UPDATED)
71. Lumbar Bone Mineral Density Estimation from Chest X-ray Images: Anatomy-aware Attentive Multi-ROI Modeling. (arXiv:2201.01838v2 [eess.IV] UPDATED)
72. Head and eye egocentric gesture recognition for human-robot interaction using eyewear cameras. (arXiv:2201.11500v2 [cs.CV] UPDATED)
73. One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v3 [cs.AI] UPDATED)
74. Image translation of Ultrasound to Pseudo Anatomical Display by CycleGAN. (arXiv:2202.08053v2 [eess.IV] UPDATED)
75. How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs. (arXiv:2203.12344v2 [cs.CV] UPDATED)
76. Exploring Plain Vision Transformer Backbones for Object Detection. (arXiv:2203.16527v2 [cs.CV] UPDATED)
77. Efficient Convolutional Neural Networks on Raspberry Pi for Image Classification. (arXiv:2204.00943v2 [cs.CV] UPDATED)
78. The 6th AI City Challenge. (arXiv:2204.10380v4 [cs.CV] UPDATED)
79. Contrastive Supervised Distillation for Continual Representation Learning. (arXiv:2205.05476v2 [cs.CV] UPDATED)
80. Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v3 [cs.LG] UPDATED)
81. Towards Model Generalization for Monocular 3D Object Detection. (arXiv:2205.11664v3 [cs.CV] UPDATED)
82. DGSVis: Visual Analysis of Hierarchical Snapshots in Dynamic Graph. (arXiv:2205.13220v2 [cs.HC] UPDATED)
83. Illumination Adaptive Transformer. (arXiv:2205.14871v2 [cs.CV] UPDATED)
84. CompleteDT: Point Cloud Completion with Dense Augment Inference Transformers. (arXiv:2205.14999v2 [cs.CV] UPDATED)
85. xView3-SAR: Detecting **Dark** Fishing Activity Using Synthetic Aperture Radar Imagery. (arXiv:2206.00897v2 [cs.CV] UPDATED)
86. PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images. (arXiv:2206.01256v2 [cs.CV] UPDATED)
87. PIDNet: A **Real-time** Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v2 [cs.CV] UPDATED)
88. Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v3 [cs.LG] UPDATED)
89. Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v2 [cs.CV] UPDATED)
90. Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v3 [cs.CV] UPDATED)
91. CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v2 [cs.CV] UPDATED)
92. Simple Cues Lead to a Strong Multi-Object Tracker. (arXiv:2206.04656v2 [cs.CV] UPDATED)
## eess.IV
---
**18** new papers in eess.IV:-) 
1. Gaussian Fourier Pyramid for Local Laplacian Filter. (arXiv:2206.04681v1 [eess.IV])
2. RT-DNAS: **Real-time** Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v1 [eess.IV])
3. Structure-consistent **Restoration** Network for Cataract Fundus Image **Enhancement**. (arXiv:2206.04684v1 [eess.IV])
4. AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding Biomechanical Testing. (arXiv:2206.04689v1 [eess.IV])
5. AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging. (arXiv:2206.04732v1 [eess.IV])
6. Dilated POCS: Minimax Convex Optimization. (arXiv:2206.04759v1 [eess.IV])
7. Efficient Per-Shot Convex Hull Prediction By Recurrent Learning. (arXiv:2206.04877v1 [eess.IV])
8. A GPU-Accelerated Light-field Super-resolution Framework Based on Mixed Noise Model and Weighted Regularization. (arXiv:2206.05047v1 [eess.IV])
9. Denoising Generalized Expectation-Consistent Approximation for MRI Image Recovery. (arXiv:2206.05049v1 [eess.IV])
10. Meta-data Study in Autism Spectrum Disorder Classification Based on Structural MRI. (arXiv:2206.05052v1 [cs.LG])
11. A No-reference Quality Assessment Metric for Point Cloud Based on Captured Video Sequences. (arXiv:2206.05054v1 [eess.IV])
12. Learning self-calibrated optic disc and cup segmentation from multi-rater annotations. (arXiv:2206.05092v1 [eess.IV])
13. A Proof of the Tree of Shapes in n-D. (arXiv:2206.05109v1 [cs.DM])
14. Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification. (arXiv:2206.05148v1 [eess.IV])
15. Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v3 [eess.IV] UPDATED)
16. Lumbar Bone Mineral Density Estimation from Chest X-ray Images: Anatomy-aware Attentive Multi-ROI Modeling. (arXiv:2201.01838v2 [eess.IV] UPDATED)
17. Image translation of Ultrasound to Pseudo Anatomical Display by CycleGAN. (arXiv:2202.08053v2 [eess.IV] UPDATED)
18. CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v2 [cs.CV] UPDATED)
## cs.LG
---
**201** new papers in cs.LG:-) 
1. Extending Momentum Contrast with Cross Similarity Consistency Regularization. (arXiv:2206.04676v1 [cs.LG])
2. Can Backdoor Attacks Survive Time-Varying Models?. (arXiv:2206.04677v1 [cs.CR])
3. ReCo: A Dataset for Residential Community Layout Planning. (arXiv:2206.04678v1 [cs.LG])
4. POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution Samples. (arXiv:2206.04679v1 [cs.LG])
5. RT-DNAS: **Real-time** Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v1 [eess.IV])
6. Predictive Exit: Prediction of Fine-Grained Early Exits for Computation- and Energy-Efficient Inference. (arXiv:2206.04685v1 [cs.LG])
7. Unsupervised Deep Discriminant Analysis Based Clustering. (arXiv:2206.04686v1 [cs.LG])
8. Swan: A Neural Engine for Efficient DNN Training on Smartphone SoCs. (arXiv:2206.04687v1 [cs.LG])
9. NNTrainer: Light-Weight On-Device Training Framework. (arXiv:2206.04688v1 [cs.LG])
10. AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding Biomechanical Testing. (arXiv:2206.04689v1 [eess.IV])
11. A Resilient Distributed Boosting Algorithm. (arXiv:2206.04713v1 [cs.LG])
12. On the Unreasonable Effectiveness of Federated Averaging with Heterogeneous Data. (arXiv:2206.04723v1 [cs.LG])
13. COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning. (arXiv:2206.04726v1 [cs.LG])
14. STNDT: Modeling Neural Population Activity with a Spatiotemporal Transformer. (arXiv:2206.04727v1 [q-bio.NC])
15. A Neural Network Architecture for Program Understanding Inspired by Human Behaviors. (arXiv:2206.04730v1 [cs.SE])
16. Leveraging Centric Data Federated Learning Using Blockchain For Integrity Assurance. (arXiv:2206.04731v1 [cs.LG])
17. AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging. (arXiv:2206.04732v1 [eess.IV])
18. Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel Recombination. (arXiv:2206.04734v1 [cs.LG])
19. A Novel Partitioned Approach for Reduced Order Model -- Finite Element Model (ROM-FEM) and ROM-ROM Coupling. (arXiv:2206.04736v1 [math.NA])
20. I'm Me, We're Us, and I'm Us: Tri-directional Contrastive Learning on Hypergraphs. (arXiv:2206.04739v1 [cs.LG])
21. A Learning-Theoretic Framework for Certified Auditing of Machine Learning Models. (arXiv:2206.04740v1 [cs.LG])
22. Quantum Policy Iteration via Amplitude Estimation and Grover Search -- Towards Quantum Advantage for Reinforcement Learning. (arXiv:2206.04741v1 [quant-ph])
23. Mobility Improves the Convergence of Asynchronous Federated Learning. (arXiv:2206.04742v1 [cs.LG])
24. Strong Memory Lower Bounds for Learning Natural Models. (arXiv:2206.04743v1 [cs.LG])
25. Mildly Conservative Q-Learning for Offline Reinforcement Learning. (arXiv:2206.04745v1 [cs.LG])
26. HDTorch: Accelerating Hyperdimensional Computing with GP-GPUs for Design Space Exploration. (arXiv:2206.04746v1 [cs.LG])
27. An Empirical Study on Disentanglement of Negative-free Contrastive Learning. (arXiv:2206.04756v1 [cs.LG])
28. Data-Efficient Double-Win Lottery Tickets from Robust Pre-training. (arXiv:2206.04762v1 [cs.LG])
29. Neural Bregman Divergences for Distance Learning. (arXiv:2206.04763v1 [cs.LG])
30. Joint Entropy Search For Maximally-Informed Bayesian Optimization. (arXiv:2206.04771v1 [cs.LG])
31. What should AI see? Using the Public's Opinion to Determine the Perception of an AI. (arXiv:2206.04776v1 [cs.LG])
32. Trimmed Maximum Likelihood Estimation for Robust Learning in Generalized Linear Models. (arXiv:2206.04777v1 [cs.LG])
33. Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v1 [cs.LG])
34. ReFace: **Real-time** Adversarial Attacks on Face Recognition Systems. (arXiv:2206.04783v1 [cs.CV])
35. On the Bias-Variance Characteristics of LIME and SHAP in High Sparsity Movie Recommendation Explanation Tasks. (arXiv:2206.04784v1 [cs.LG])
36. Building Spatio-temporal Transformers for Egocentric 3D Pose Estimation. (arXiv:2206.04785v1 [cs.CV])
37. Comprehensive Fair Meta-learned Recommender System. (arXiv:2206.04789v1 [cs.IR])
38. Adaptive Model Pooling for Online Deep Anomaly Detection from a Complex Evolving Data Stream. (arXiv:2206.04792v1 [cs.LG])
39. Stable and memory-efficient image recovery using monotone operator learning (MOL). (arXiv:2206.04797v1 [cs.CV])
40. Learning to Efficiently Propagate for Reasoning on Knowledge Graphs. (arXiv:2206.04798v1 [cs.AI])
41. Explainable Artificial Intelligence (XAI) for Internet of Things: A Survey. (arXiv:2206.04800v1 [cs.AI])
42. Learning Attention-based Representations from Multiple Patterns for Relation Prediction in Knowledge Graphs. (arXiv:2206.04801v1 [cs.AI])
43. Detecting Anomalous Cryptocurrency Transactions: an AML/CFT Application of Machine Learning-based Forensics. (arXiv:2206.04803v1 [cs.CR])
44. Theoretical Error Performance Analysis for Variational Quantum Circuit Based Functional Regression. (arXiv:2206.04804v1 [quant-ph])
45. Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022. (arXiv:2206.04805v1 [cs.SD])
46. Syntactic Inductive Biases for Deep Learning Methods. (arXiv:2206.04806v1 [cs.LG])
47. Deep learning-enhanced ensemble-based data assimilation for high-dimensional nonlinear dynamical systems. (arXiv:2206.04811v1 [cs.LG])
48. Empirical Bayes approach to Truth Discovery problems. (arXiv:2206.04816v1 [cs.LG])
49. The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the \emph{Grokking Phenomenon}. (arXiv:2206.04817v1 [cs.LG])
50. Training Neural Networks using SAT solvers. (arXiv:2206.04833v1 [cs.LG])
51. Communication Efficient Distributed Learning for Kernelized Contextual Bandits. (arXiv:2206.04835v1 [cs.LG])
52. In Defense of Core-set: A Density-aware Core-set Selection for Active Learning. (arXiv:2206.04838v1 [cs.LG])
53. Hierarchical mixtures of Gaussians for combined dimensionality reduction and clustering. (arXiv:2206.04841v1 [cs.LG])
54. Neural Laplace: Learning diverse classes of differential equations in the Laplace domain. (arXiv:2206.04843v1 [cs.LG])
55. Beyond the Gates of Euclidean Space: Temporal-Discrimination-Fusions and Attention-based Graph Neural Network for Human Activity Recognition. (arXiv:2206.04855v1 [cs.LG])
56. Mixed integer linear optimization formulations for learning optimal binary classification trees. (arXiv:2206.04857v1 [cs.LG])
57. Conformal Prediction Intervals for Markov Decision Process Trajectories. (arXiv:2206.04860v1 [cs.LG])
58. Symbolic image detection using scene and knowledge graphs. (arXiv:2206.04863v1 [cs.CV])
59. Binarizing Split Learning for Data Privacy **Enhancement** and Computation Reduction. (arXiv:2206.04864v1 [cs.LG])
60. Multi-fidelity Hierarchical Neural Processes. (arXiv:2206.04872v1 [cs.LG])
61. Imitation Learning via Differentiable Physics. (arXiv:2206.04873v1 [cs.LG])
62. Efficient Per-Shot Convex Hull Prediction By Recurrent Learning. (arXiv:2206.04877v1 [eess.IV])
63. $\mathsf{G^2Retro}$: Two-Step Graph Generative Models for Retrosynthesis Prediction. (arXiv:2206.04882v1 [cs.LG])
64. Deep Leakage from Model in Federated Learning. (arXiv:2206.04887v1 [cs.LG])
65. Adversarial Counterfactual Environment Model Learning. (arXiv:2206.04890v1 [cs.LG])
66. Explaining Neural Networks without Access to Training Data. (arXiv:2206.04891v1 [cs.LG])
67. Provable Guarantees for Sparsity Recovery with Deterministic Missing Data Patterns. (arXiv:2206.04893v1 [cs.LG])
68. Out of Sight, Out of Mind: A Source-View-Wise Feature Aggregation for Multi-View Image-Based Rendering. (arXiv:2206.04906v1 [cs.CV])
69. Efficient Heterogeneous Treatment Effect Estimation With Multiple Experiments and Multiple Outcomes. (arXiv:2206.04907v1 [cs.LG])
70. NAGphormer: Neighborhood Aggregation Graph Transformer for Node Classification in Large Graphs. (arXiv:2206.04910v1 [cs.LG])
71. Fisher SAM: Information Geometry and Sharpness Aware Minimisation. (arXiv:2206.04920v1 [cs.LG])
72. Offline Stochastic Shortest Path: Learning, Evaluation and Towards Optimality. (arXiv:2206.04921v1 [cs.LG])
73. A bio-inspired implementation of a sparse-learning spike-based hippocampus memory model. (arXiv:2206.04924v1 [cs.NE])
74. MAREO: Memory- and Attention- based visual REasOning. (arXiv:2206.04928v1 [cs.AI])
75. Response to: Significance and stability of deep learning-based identification of subtypes within major psychiatric disorders. Molecular Psychiatry (2022). (arXiv:2206.04934v1 [cs.LG])
76. Deep Multi-view Semi-supervised Clustering with Sample Pairwise Constraints. (arXiv:2206.04949v1 [cs.CV])
77. Evolutionary Echo State Network: evolving reservoirs in the Fourier space. (arXiv:2206.04951v1 [cs.NE])
78. Merak: A Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models. (arXiv:2206.04959v1 [cs.LG])
79. Deep Learning-based Massive MIMO CSI Acquisition for 5G Evolution and 6G. (arXiv:2206.04967v1 [eess.SP])
80. Refining neural network predictions using background knowledge. (arXiv:2206.04976v1 [cs.AI])
81. Convolutional Layers Are Not Translation Equivariant. (arXiv:2206.04979v1 [cs.CV])
82. Zero-Shot Audio Classification using Image Embeddings. (arXiv:2206.04984v1 [cs.SD])
83. The Generalized Eigenvalue Problem as a Nash Equilibrium. (arXiv:2206.04993v1 [cs.LG])
84. Weighted Ensembles for Active Learning with Adaptivity. (arXiv:2206.05009v1 [cs.LG])
85. Spatial Cross-Attention Improves Self-Supervised Visual Representation Learning. (arXiv:2206.05028v1 [cs.CV])
86. Explanation as Question Answering based on a Task Model of the Agent's Design. (arXiv:2206.05030v1 [cs.HC])
87. Scalable Deep Gaussian Markov Random Fields for General Graphs. (arXiv:2206.05032v1 [stat.ML])
88. Improved Approximation for Fair Correlation Clustering. (arXiv:2206.05050v1 [cs.LG])
89. Temporal Inductive Logic Reasoning. (arXiv:2206.05051v1 [cs.LG])
90. Meta-data Study in Autism Spectrum Disorder Classification Based on Structural MRI. (arXiv:2206.05052v1 [cs.LG])
91. Coswara: A website application enabling COVID-19 screening by analysing respiratory sound samples and health symptoms. (arXiv:2206.05053v1 [cs.HC])
92. On Neural Architecture Inductive Biases for Relational Tasks. (arXiv:2206.05056v1 [cs.NE])
93. We Cannot Guarantee Safety: The Undecidability of Graph Neural Network Verification. (arXiv:2206.05070v1 [cs.LG])
94. Diffeomorphic Counterfactuals with Generative Models. (arXiv:2206.05075v1 [cs.LG])
95. Tensor Train for Global Optimization Problems in Robotics. (arXiv:2206.05077v1 [cs.RO])
96. Muffliato: Peer-to-Peer Privacy Amplification for Decentralized Optimization and Averaging. (arXiv:2206.05091v1 [cs.CR])
97. Federated Momentum Contrastive Clustering. (arXiv:2206.05093v1 [cs.LG])
98. Saccade Mechanisms for Image Classification, Object Detection and Tracking. (arXiv:2206.05102v1 [cs.CV])
99. Deep Multi-Agent Reinforcement Learning with Hybrid Action Spaces based on Maximum Entropy. (arXiv:2206.05108v1 [cs.LG])
100. PAVI: Plate-Amortized Variational Inference. (arXiv:2206.05111v1 [cs.AI])
101. Stochastic Zeroth order Descent with Structured Directions. (arXiv:2206.05124v1 [math.OC])
102. Distributionally Robust End-to-End Portfolio Construction. (arXiv:2206.05134v1 [q-fin.CP])
103. Fast Deep Autoencoder for Federated learning. (arXiv:2206.05136v1 [cs.LG])
104. Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification. (arXiv:2206.05148v1 [eess.IV])
105. MEAT: Maneuver Extraction from Agent Trajectories. (arXiv:2206.05158v1 [cs.CV])
106. An Image Processing Pipeline for Camera Trap Time-Lapse Recordings. (arXiv:2206.05159v1 [cs.CV])
107. Multifidelity Reinforcement Learning with Control Variates. (arXiv:2206.05165v1 [cs.LG])
108. How Much is Enough? A Study on Diffusion Times in Score-based Generative Models. (arXiv:2206.05173v1 [stat.ML])
109. Lightweight Conditional Model Extrapolation for Streaming Data under Class-Prior Shift. (arXiv:2206.05181v1 [cs.LG])
110. Human-AI Interaction Design in Machine Teaching. (arXiv:2206.05182v1 [cs.HC])
111. GD-VAEs: Geometric Dynamic Variational Autoencoders for Learning Nonlinear Dynamics and Dimension Reductions. (arXiv:2206.05183v1 [cs.LG])
112. On Convergence of FedProx: Local Dissimilarity Invariant Bounds, Non-smoothness and Beyond. (arXiv:2206.05187v1 [stat.ML])
113. Learning the Space of Deep Models. (arXiv:2206.05194v1 [cs.CV])
114. Bayesian Estimation of Differential Privacy. (arXiv:2206.05199v1 [cs.LG])
115. Dynamic mean field programming. (arXiv:2206.05200v1 [stat.ML])
116. Hierarchical Federated Learning with Privacy. (arXiv:2206.05209v1 [cs.LG])
117. A new distance measurement and its application in K-Means Algorithm. (arXiv:2206.05215v1 [cs.LG])
118. Measuring the Carbon Intensity of AI in Cloud Instances. (arXiv:2206.05229v1 [cs.LG])
119. StructCoder: Structure-Aware Transformer for Code Generation. (arXiv:2206.05239v1 [cs.LG])
120. ROI Constrained Bidding via Curriculum-Guided Bayesian Reinforcement Learning. (arXiv:2206.05240v1 [cs.LG])
121. List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering. (arXiv:2206.05245v1 [cs.DS])
122. Accelerated Algorithms for Monotone Inclusions and Constrained Nonconvex-Nonconcave Min-Max Optimization. (arXiv:2206.05248v1 [math.OC])
123. Rethinking Spatial Invariance of Convolutional Networks for Object Counting. (arXiv:2206.05253v1 [cs.CV])
124. Interactively Learning Preference Constraints in Linear Bandits. (arXiv:2206.05255v1 [cs.LG])
125. Is Self-Supervised Learning More Robust Than Supervised Learning?. (arXiv:2206.05259v1 [cs.CV])
126. Balanced Product of Experts for Long-Tailed Recognition. (arXiv:2206.05260v1 [cs.CV])
127. Meta Optimal Transport. (arXiv:2206.05262v1 [cs.LG])
128. Causal Balancing for Domain Generalization. (arXiv:2206.05263v1 [cs.LG])
129. Tight Bounds for State Tomography with Incoherent Measurements. (arXiv:2206.05265v1 [quant-ph])
130. Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?. (arXiv:2206.05266v1 [cs.LG])
131. Street Crossing Aid Using Light-weight CNNs for the Visually Impaired. (arXiv:1909.09598v2 [cs.CV] UPDATED)
132. Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. (arXiv:1910.06151v3 [cs.DS] UPDATED)
133. Unifying mirror descent and dual averaging. (arXiv:1910.13742v4 [math.OC] UPDATED)
134. Trainability of Dissipative Perceptron-Based Quantum Neural Networks. (arXiv:2005.12458v2 [quant-ph] UPDATED)
135. CoCon: A Self-Supervised Approach for Controlled Text Generation. (arXiv:2006.03535v3 [cs.CL] UPDATED)
136. Learning Classifiers under Delayed Feedback with a Time Window Assumption. (arXiv:2009.13092v2 [cs.LG] UPDATED)
137. AxFormer: Accuracy-driven Approximation of Transformers for Faster, Smaller and more Accurate NLP Models. (arXiv:2010.03688v2 [cs.CL] UPDATED)
138. Low-Rank Tensor Recovery with Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization. (arXiv:2012.03436v3 [cs.LG] UPDATED)
139. Robust Factorization of Real-world Tensor Streams with Patterns, Missing Values, and Outliers. (arXiv:2102.08466v2 [cs.LG] UPDATED)
140. Linear Bandit Algorithms with Sublinear Time Complexity. (arXiv:2103.02729v2 [cs.LG] UPDATED)
141. Membership-Mappings for Data Representation Learning: Measure Theoretic Conceptualization. (arXiv:2104.07060v3 [cs.LG] UPDATED)
142. Domain Transformer: Predicting Samples of Unseen, Future Domains. (arXiv:2106.06057v2 [cs.LG] UPDATED)
143. Solving PDEs on Unknown Manifolds with Machine Learning. (arXiv:2106.06682v2 [math.NA] UPDATED)
144. Generalization Bounds with Minimal Dependency on Hypothesis Class via Distributionally Robust Optimization. (arXiv:2106.11180v3 [math.OC] UPDATED)
145. Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix Manifold. (arXiv:2107.09207v2 [math.OC] UPDATED)
146. Self-Correcting Neural Networks For Safe Classification. (arXiv:2107.11445v2 [cs.LG] UPDATED)
147. Anarchic Federated Learning. (arXiv:2108.09875v4 [cs.LG] UPDATED)
148. Popularity Adjusted Block Models are Generalized Random Dot Product Graphs. (arXiv:2109.04010v2 [stat.ML] UPDATED)
149. Projected State-action Balancing Weights for Offline Reinforcement Learning. (arXiv:2109.04640v2 [cs.LG] UPDATED)
150. Connecting Low-Loss Subspace for Personalized Federated Learning. (arXiv:2109.07628v2 [cs.LG] UPDATED)
151. Assemblies of neurons learn to classify well-separated distributions. (arXiv:2110.03171v2 [cs.NE] UPDATED)
152. Sparsity in Partially Controllable Linear Systems. (arXiv:2110.06150v2 [math.OC] UPDATED)
153. Looper: An end-to-end ML platform for product decisions. (arXiv:2110.07554v7 [cs.LG] UPDATED)
154. Integrated Conditional Estimation-Optimization. (arXiv:2110.12351v2 [stat.ML] UPDATED)
155. Topologically penalized regression on manifolds. (arXiv:2110.13749v2 [cs.LG] UPDATED)
156. NeuroComb: Improving SAT Solving with Graph Neural Networks. (arXiv:2110.14053v3 [cs.AI] UPDATED)
157. Recurrent Neural Network Training with Convex Loss and Regularization Functions by Extended Kalman Filtering. (arXiv:2111.02673v2 [cs.LG] UPDATED)
158. LassoBench: A High-Dimensional Hyperparameter Optimization Benchmark Suite for Lasso. (arXiv:2111.02790v3 [cs.LG] UPDATED)
159. Preference Communication in Multi-Objective Normal-Form Games. (arXiv:2111.09191v2 [cs.GT] UPDATED)
160. Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v3 [eess.IV] UPDATED)
161. Deep Auto-encoder with Neural Response. (arXiv:2111.15309v2 [cs.LG] UPDATED)
162. Gaussian Mixture Variational Autoencoder with Contrastive Learning for Multi-Label Classification. (arXiv:2112.00976v2 [cs.LG] UPDATED)
163. Multi-task Self-distillation for Graph-based Semi-Supervised Learning. (arXiv:2112.01174v3 [cs.LG] UPDATED)
164. Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence Model Tackles All SMAC Tasks. (arXiv:2112.02845v3 [cs.LG] UPDATED)
165. Encoding protein dynamic information in graph representation for functional residue identification. (arXiv:2112.12033v2 [q-bio.BM] UPDATED)
166. Stochastic Continuous Submodular Maximization: Boosting via Non-oblivious Function. (arXiv:2201.00703v3 [cs.LG] UPDATED)
167. Gradient Descent on Neurons and its Link to Approximate Second-Order Optimization. (arXiv:2201.12250v2 [cs.LG] UPDATED)
168. Meta-Reinforcement Learning with Self-Modifying Networks. (arXiv:2202.02363v2 [cs.LG] UPDATED)
169. Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets. (arXiv:2202.04736v2 [cs.LG] UPDATED)
170. Low-Rank Approximation with $1/\epsilon^{1/3}$ Matrix-Vector Products. (arXiv:2202.05120v3 [cs.DS] UPDATED)
171. Trace norm regularization for multi-task learning with scarce data. (arXiv:2202.06742v2 [stat.ML] UPDATED)
172. One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v3 [cs.AI] UPDATED)
173. A Free Lunch with Influence Functions? Improving Neural Network Estimates with Concepts from Semiparametric Statistics. (arXiv:2202.09096v2 [cs.LG] UPDATED)
174. Predicting the Thermal Sunyaev-Zel'dovich Field using Modular and Equivariant Set-Based Neural Networks. (arXiv:2203.00026v2 [astro-ph.CO] UPDATED)
175. Plan Your Target and Learn Your Skills: Transferable State-Only Imitation Learning via Decoupled Policy Optimization. (arXiv:2203.02214v3 [cs.LG] UPDATED)
176. Decision-Focused Learning without Decision-Making: Learning Locally Optimized Decision Losses. (arXiv:2203.16067v3 [cs.LG] UPDATED)
177. Refined Convergence and Topology Learning for Decentralized Optimization with Heterogeneous Data. (arXiv:2204.04452v2 [cs.LG] UPDATED)
178. Contrastive Supervised Distillation for Continual Representation Learning. (arXiv:2205.05476v2 [cs.CV] UPDATED)
179. Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v3 [cs.LG] UPDATED)
180. RIGID: Robust Linear Regression with Missing Data. (arXiv:2205.13635v2 [cs.LG] UPDATED)
181. Knowledge Graph - Deep Learning: A Case Study in Question Answering in Aviation Safety Domain. (arXiv:2205.15952v2 [cs.CL] UPDATED)
182. Bayesian Inference of Stochastic Dynamical Networks. (arXiv:2206.00858v2 [stat.ML] UPDATED)
183. Decentralized Training of Foundation Models in Heterogeneous Environments. (arXiv:2206.01288v2 [cs.DC] UPDATED)
184. Fine-tuning Language Models over Slow Networks using Activation Compression with Guarantees. (arXiv:2206.01299v2 [cs.LG] UPDATED)
185. Tackling covariate shift with node-based Bayesian neural networks. (arXiv:2206.02435v2 [stat.ML] UPDATED)
186. Crust Macrofracturing as the Evidence of the Last Deglaciation. (arXiv:2206.02652v2 [physics.geo-ph] UPDATED)
187. A Simple and Optimal Policy Design for Online Learning with Safety against Heavy-tailed Risk. (arXiv:2206.02969v2 [stat.ML] UPDATED)
188. Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage. (arXiv:2206.03426v2 [cs.LG] UPDATED)
189. A Benchmark for Federated Hetero-Task Learning. (arXiv:2206.03436v2 [cs.LG] UPDATED)
190. Neural Bandit with Arm Group Graph. (arXiv:2206.03644v2 [cs.LG] UPDATED)
191. pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning. (arXiv:2206.03655v2 [cs.LG] UPDATED)
192. Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v3 [cs.LG] UPDATED)
193. $p$-Sparsified Sketches for Fast Multiple Output Kernel Methods. (arXiv:2206.03827v2 [stat.ML] UPDATED)
194. FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization. (arXiv:2206.03966v2 [cs.LG] UPDATED)
195. Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v2 [cs.CV] UPDATED)
196. Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning. (arXiv:2206.03996v2 [cs.LG] UPDATED)
197. Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v3 [cs.CV] UPDATED)
198. CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v2 [cs.CV] UPDATED)
199. Explicit Regularization in Overparametrized Models via Noise Injection. (arXiv:2206.04613v2 [cs.LG] UPDATED)
200. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v2 [cs.CL] UPDATED)
201. Globally Optimal Algorithms for Fixed-Budget Best Arm Identification. (arXiv:2206.04646v2 [stat.ML] UPDATED)
## cs.AI
---
**97** new papers in cs.AI:-) 
1. Extending Momentum Contrast with Cross Similarity Consistency Regularization. (arXiv:2206.04676v1 [cs.LG])
2. ReCo: A Dataset for Residential Community Layout Planning. (arXiv:2206.04678v1 [cs.LG])
3. Molecular dynamics without molecules: searching the conformational space of proteins with generative neural networks. (arXiv:2206.04683v1 [q-bio.QM])
4. Swan: A Neural Engine for Efficient DNN Training on Smartphone SoCs. (arXiv:2206.04687v1 [cs.LG])
5. Modular design patterns for neural-symbolic integration: refinement and combination. (arXiv:2206.04724v1 [cs.AI])
6. COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning. (arXiv:2206.04726v1 [cs.LG])
7. Towards Target Sequential Rules. (arXiv:2206.04728v1 [cs.DB])
8. A Neural Network Architecture for Program Understanding Inspired by Human Behaviors. (arXiv:2206.04730v1 [cs.SE])
9. Leveraging Centric Data Federated Learning Using Blockchain For Integrity Assurance. (arXiv:2206.04731v1 [cs.LG])
10. Mobility Improves the Convergence of Asynchronous Federated Learning. (arXiv:2206.04742v1 [cs.LG])
11. Mildly Conservative Q-Learning for Offline Reinforcement Learning. (arXiv:2206.04745v1 [cs.LG])
12. An Empirical Study on Disentanglement of Negative-free Contrastive Learning. (arXiv:2206.04756v1 [cs.LG])
13. Data-Efficient Double-Win Lottery Tickets from Robust Pre-training. (arXiv:2206.04762v1 [cs.LG])
14. What should AI see? Using the Public's Opinion to Determine the Perception of an AI. (arXiv:2206.04776v1 [cs.LG])
15. Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v1 [cs.LG])
16. Speak Like a Dog: Human to Non-human creature Voice Conversion. (arXiv:2206.04780v1 [cs.SD])
17. Building Spatio-temporal Transformers for Egocentric 3D Pose Estimation. (arXiv:2206.04785v1 [cs.CV])
18. Learning Reduced Nonlinear State-Space Models: an Output-Error Based Canonical Approach. (arXiv:2206.04791v1 [eess.SY])
19. Adaptive Model Pooling for Online Deep Anomaly Detection from a Complex Evolving Data Stream. (arXiv:2206.04792v1 [cs.LG])
20. Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions. (arXiv:2206.04793v1 [cs.CR])
21. Optimization for Infrastructure Cyber-Physical Systems. (arXiv:2206.04794v1 [eess.SY])
22. Stable and memory-efficient image recovery using monotone operator learning (MOL). (arXiv:2206.04797v1 [cs.CV])
23. Learning to Efficiently Propagate for Reasoning on Knowledge Graphs. (arXiv:2206.04798v1 [cs.AI])
24. Explainable Artificial Intelligence (XAI) for Internet of Things: A Survey. (arXiv:2206.04800v1 [cs.AI])
25. Learning Attention-based Representations from Multiple Patterns for Relation Prediction in Knowledge Graphs. (arXiv:2206.04801v1 [cs.AI])
26. Theoretical Error Performance Analysis for Variational Quantum Circuit Based Functional Regression. (arXiv:2206.04804v1 [quant-ph])
27. Syntactic Inductive Biases for Deep Learning Methods. (arXiv:2206.04806v1 [cs.LG])
28. Empirical Bayes approach to Truth Discovery problems. (arXiv:2206.04816v1 [cs.LG])
29. In Defense of Core-set: A Density-aware Core-set Selection for Active Learning. (arXiv:2206.04838v1 [cs.LG])
30. Neural Laplace: Learning diverse classes of differential equations in the Laplace domain. (arXiv:2206.04843v1 [cs.LG])
31. Heterogeneous Face Recognition via Face Synthesis with Identity-Attribute Disentanglement. (arXiv:2206.04854v1 [cs.CV])
32. The Gender Gap in Face Recognition Accuracy Is a Hairy Problem. (arXiv:2206.04867v1 [cs.CV])
33. Imitation Learning via Differentiable Physics. (arXiv:2206.04873v1 [cs.LG])
34. Deep Leakage from Model in Federated Learning. (arXiv:2206.04887v1 [cs.LG])
35. Explaining Neural Networks without Access to Training Data. (arXiv:2206.04891v1 [cs.LG])
36. ABCDE: An Agent-Based Cognitive Development Environment. (arXiv:2206.04909v1 [cs.AI])
37. NAGphormer: Neighborhood Aggregation Graph Transformer for Node Classification in Large Graphs. (arXiv:2206.04910v1 [cs.LG])
38. Offline Stochastic Shortest Path: Learning, Evaluation and Towards Optimality. (arXiv:2206.04921v1 [cs.LG])
39. MAREO: Memory- and Attention- based visual REasOning. (arXiv:2206.04928v1 [cs.AI])
40. Evolutionary Echo State Network: evolving reservoirs in the Fourier space. (arXiv:2206.04951v1 [cs.NE])
41. Refining neural network predictions using background knowledge. (arXiv:2206.04976v1 [cs.AI])
42. The Generalized Eigenvalue Problem as a Nash Equilibrium. (arXiv:2206.04993v1 [cs.LG])
43. A Simple Yet Efficient Method for Adversarial Word-Substitute Attack. (arXiv:2206.05015v1 [cs.CL])
44. Empathetic Conversational Systems: A Review of Current Advances, Gaps, and Opportunities. (arXiv:2206.05017v1 [cs.CL])
45. Spatial Cross-Attention Improves Self-Supervised Visual Representation Learning. (arXiv:2206.05028v1 [cs.CV])
46. Explanation as Question Answering based on a Task Model of the Agent's Design. (arXiv:2206.05030v1 [cs.HC])
47. Temporal Inductive Logic Reasoning. (arXiv:2206.05051v1 [cs.LG])
48. Meta-data Study in Autism Spectrum Disorder Classification Based on Structural MRI. (arXiv:2206.05052v1 [cs.LG])
49. On Neural Architecture Inductive Biases for Relational Tasks. (arXiv:2206.05056v1 [cs.NE])
50. Social Network Structure Shapes Innovation: Experience-sharing in RL with SAPIENS. (arXiv:2206.05060v1 [cs.AI])
51. Diffeomorphic Counterfactuals with Generative Models. (arXiv:2206.05075v1 [cs.LG])
52. SimVP: Simpler yet Better Video Prediction. (arXiv:2206.05099v1 [cs.CV])
53. Deep Multi-Agent Reinforcement Learning with Hybrid Action Spaces based on Maximum Entropy. (arXiv:2206.05108v1 [cs.LG])
54. PAVI: Plate-Amortized Variational Inference. (arXiv:2206.05111v1 [cs.AI])
55. Referring Image Matting. (arXiv:2206.05149v1 [cs.CV])
56. Multifidelity Reinforcement Learning with Control Variates. (arXiv:2206.05165v1 [cs.LG])
57. Human-AI Interaction Design in Machine Teaching. (arXiv:2206.05182v1 [cs.HC])
58. A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction. (arXiv:2206.05224v1 [cs.CL])
59. ROI Constrained Bidding via Curriculum-Guided Bayesian Reinforcement Learning. (arXiv:2206.05240v1 [cs.LG])
60. Rethinking Spatial Invariance of Convolutional Networks for Object Counting. (arXiv:2206.05253v1 [cs.CV])
61. Interactively Learning Preference Constraints in Linear Bandits. (arXiv:2206.05255v1 [cs.LG])
62. Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces. (arXiv:2206.05257v1 [cs.CV])
63. Meta Optimal Transport. (arXiv:2206.05262v1 [cs.LG])
64. Causal Balancing for Domain Generalization. (arXiv:2206.05263v1 [cs.LG])
65. A multiple criteria methodology for priority based portfolio selection. (arXiv:1812.10410v5 [cs.CY] UPDATED)
66. Improving Automated Driving through POMDP Planning with Human Internal States. (arXiv:2005.14549v2 [cs.AI] UPDATED)
67. AxFormer: Accuracy-driven Approximation of Transformers for Faster, Smaller and more Accurate NLP Models. (arXiv:2010.03688v2 [cs.CL] UPDATED)
68. Low-Rank Tensor Recovery with Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization. (arXiv:2012.03436v3 [cs.LG] UPDATED)
69. Why scholars are diagramming neural network models. (arXiv:2104.14811v2 [cs.HC] UPDATED)
70. Domain Transformer: Predicting Samples of Unseen, Future Domains. (arXiv:2106.06057v2 [cs.LG] UPDATED)
71. Weighted majority tournaments and Kemeny ranking with 2-dimensional Euclidean preferences. (arXiv:2106.13054v2 [cs.DM] UPDATED)
72. Connecting Low-Loss Subspace for Personalized Federated Learning. (arXiv:2109.07628v2 [cs.LG] UPDATED)
73. Assemblies of neurons learn to classify well-separated distributions. (arXiv:2110.03171v2 [cs.NE] UPDATED)
74. Looper: An end-to-end ML platform for product decisions. (arXiv:2110.07554v7 [cs.LG] UPDATED)
75. Multiwave COVID-19 Prediction from Social Awareness using Web Search and Mobility Data. (arXiv:2110.11584v2 [cs.SI] UPDATED)
76. NeuroComb: Improving SAT Solving with Graph Neural Networks. (arXiv:2110.14053v3 [cs.AI] UPDATED)
77. Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance. (arXiv:2111.11755v4 [cs.SD] UPDATED)
78. Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence Model Tackles All SMAC Tasks. (arXiv:2112.02845v3 [cs.LG] UPDATED)
79. CGIBNet: Bandwidth-constrained Communication with Graph Information Bottleneck in Multi-Agent Reinforcement Learning. (arXiv:2112.10374v3 [cs.AI] UPDATED)
80. Meta-Reinforcement Learning with Self-Modifying Networks. (arXiv:2202.02363v2 [cs.LG] UPDATED)
81. Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets. (arXiv:2202.04736v2 [cs.LG] UPDATED)
82. One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v3 [cs.AI] UPDATED)
83. Image translation of Ultrasound to Pseudo Anatomical Display by CycleGAN. (arXiv:2202.08053v2 [eess.IV] UPDATED)
84. Plan Your Target and Learn Your Skills: Transferable State-Only Imitation Learning via Decoupled Policy Optimization. (arXiv:2203.02214v3 [cs.LG] UPDATED)
85. ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps. (arXiv:2203.09127v3 [cs.CL] UPDATED)
86. Decision-Focused Learning without Decision-Making: Learning Locally Optimized Decision Losses. (arXiv:2203.16067v3 [cs.LG] UPDATED)
87. Assessing Confidence with Assurance 2.0. (arXiv:2205.04522v2 [cs.AI] UPDATED)
88. Contrastive Supervised Distillation for Continual Representation Learning. (arXiv:2205.05476v2 [cs.CV] UPDATED)
89. Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v3 [cs.LG] UPDATED)
90. Knowledge Graph - Deep Learning: A Case Study in Question Answering in Aviation Safety Domain. (arXiv:2205.15952v2 [cs.CL] UPDATED)
91. PIDNet: A **Real-time** Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v2 [cs.CV] UPDATED)
92. Product safety idioms: a method for building causal Bayesian networks for product safety and risk assessment. (arXiv:2206.02144v2 [cs.AI] UPDATED)
93. A knowledge graph representation learning approach to predict novel kinase-substrate interactions. (arXiv:2206.02290v2 [q-bio.QM] UPDATED)
94. Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v2 [cs.CV] UPDATED)
95. CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v2 [cs.CV] UPDATED)
96. Open ERP System Data For Occupational Fraud Detection. (arXiv:2206.04460v2 [cs.AI] UPDATED)
97. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v2 [cs.CL] UPDATED)

