# Your interest papers
---
## cs.CV
---
### **Low-light** **Enhancement** Method Based on Attention Map Net. (arXiv:2208.09330v1 [cs.CV])
- Authors : Mengfei Wu, Xucheng Xue, Taiji Lan, Xinwei Xu
- Link : [http://arxiv.org/abs/2208.09330](http://arxiv.org/abs/2208.09330)
> ABSTRACT  :  **Low-light** image **enhancement** is a crucial preprocessing task for some complex vision tasks. Target detection, image segmentation, and image recognition outcomes are all directly impacted by the impact of image **enhancement**. However, the majority of the currently used image **enhancement** techniques do not produce satisfactory outcomes, and these enhanced networks have relatively weak robustness. We suggest an improved network called BrightenNet that uses U-Net as its primary structure and incorporates a number of different attention mechanisms as a solution to this issue. In a specific application, we employ the network as the generator and LSGAN as the training framework to achieve better **enhancement** results. We demonstrate the validity of the proposed network BrightenNet in the experiments that follow in this paper. The results it produced can both preserve image details and conform to human vision standards.  
### Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise. (arXiv:2208.09392v1 [cs.CV])
- Authors : Arpit Bansal, Eitan Borgnia, Min Chu, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein
- Link : [http://arxiv.org/abs/2208.09392](http://arxiv.org/abs/2208.09392)
> ABSTRACT  :  Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image **restoration** operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models  
### Neural Light Field Estimation for Street Scenes with Differentiable Virtual Object Insertion. (arXiv:2208.09480v1 [cs.CV])
- Authors : Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, Sanja Fidler
- Link : [http://arxiv.org/abs/2208.09480](http://arxiv.org/abs/2208.09480)
> ABSTRACT  :  We consider the challenging problem of outdoor lighting estimation for the goal of photorealistic virtual object insertion into photographs. Existing works on outdoor lighting estimation typically simplify the scene lighting into an environment map which cannot capture the spatially-varying lighting effects in outdoor scenes. In this work, we propose a neural approach that estimates the 5D **HDR** light field from a single image, and a differentiable object insertion formulation that enables end-to-end training with image-based losses that encourage realism. Specifically, we design a hybrid lighting representation tailored to outdoor scenes, which contains an **HDR** sky dome that handles the extreme intensity of the sun, and a volumetric lighting representation that models the spatially-varying appearance of the surrounding scene. With the estimated lighting, our shadow-aware object insertion is fully differentiable, which enables adversarial training over the composited image to provide additional supervisory signal to the lighting prediction. We experimentally demonstrate that our hybrid lighting representation is more performant than existing outdoor lighting estimation methods. We further show the benefits of our AR object insertion in an autonomous driving application, where we obtain performance gains for a 3D object detector when trained on our augmented data.  
### ARID: A New Dataset for Recognizing Action in the **Dark**. (arXiv:2006.03876v4 [cs.CV] UPDATED)
- Authors : Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianxiong Yin, Simon See
- Link : [http://arxiv.org/abs/2006.03876](http://arxiv.org/abs/2006.03876)
> ABSTRACT  :  The task of action recognition in **dark** videos is useful in various scenarios, e.g., **night** surveillance and self-driving at **night**. Though progress has been made in the action recognition task for videos in normal illumination, few have studied action recognition in the **dark**. This is partly due to the lack of sufficient datasets for such a task. In this paper, we explored the task of action recognition in **dark** videos. We bridge the gap of the lack of data for this task by collecting a new dataset: the Action Recognition in the **Dark** (ARID) dataset. It consists of over 3,780 video clips with 11 action categories. To the best of our knowledge, it is the first dataset focused on human actions in **dark** videos. To gain further understandings of our ARID dataset, we analyze the ARID dataset in detail and exhibited its necessity over synthetic **dark** videos. Additionally, we benchmarked the performance of several current action recognition models on our dataset and explored potential methods for increasing their performances. Our results show that current action recognition models and frame **enhancement** methods may not be effective solutions for the task of action recognition in **dark** videos.  
### Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v2 [eess.IV] UPDATED)
- Authors : Congpei An, Ning Wu, Xiaoming Yuan
- Link : [http://arxiv.org/abs/2201.02979](http://arxiv.org/abs/2201.02979)
> ABSTRACT  :  The total variation (TV) regularization has phenomenally boosted various variational models for image processing tasks. We propose to combine the backward diffusion process in the earlier literature of image **enhancement** with the TV regularization, and show that the resulting enhanced TV minimization model is particularly effective for reducing the loss of contrast. The main purpose of this paper is to establish stable reconstruction guarantees for the enhanced TV model from noisy subsampled measurements with two sampling strategies, non-adaptive sampling for general linear measurements and variable-density sampling for Fourier measurements. In particular, under some weaker restricted isometry property conditions, the enhanced TV minimization model is shown to have tighter reconstruction error bounds than various TV-based models for the scenario where the level of noise is significant and the amount of measurements is limited. Advantages of the enhanced TV model are also numerically validated by preliminary experiments on the reconstruction of some synthetic, natural, and medical images.  
### **Real-time** End-to-End Video Text Spotter with Contrastive Representation Learning. (arXiv:2207.08417v3 [cs.CV] UPDATED)
- Authors : Wejia Wu, Zhuang Li, Jiahong Li, Chunhua Shen, Hong Zhou, Size Li, Zhongyuan Wang, Ping Luo
- Link : [http://arxiv.org/abs/2207.08417](http://arxiv.org/abs/2207.08417)
> ABSTRACT  :  Video text spotting(VTS) is the task that requires simultaneously detecting, tracking and recognizing text in the video. Existing video text spotting methods typically develop sophisticated pipelines and multiple models, which is not friend for real-time applications. Here we propose a real-time end-to-end video text spotter with Contrastive Representation learning (CoText). Our contributions are three-fold: 1) CoText simultaneously address the three tasks (e.g., text detection, tracking, recognition) in a real-time end-to-end trainable framework. 2) With contrastive learning, CoText models long-range dependencies and learning temporal information across multiple frames. 3) A simple, lightweight architecture is designed for effective and accurate performance, including GPU-parallel detection post-processing, CTC-based recognition head with Masked RoI. Extensive experiments show the superiority of our method. Especially, CoText achieves an video text spotting IDF1 of 72.0% at 41.0 FPS on ICDAR2015video, with 10.5% and 32.0 FPS improvement the previous best method. The code can be found at github.com/weijiawu/CoText.  
### Blind-Spot Collision Detection System for Commercial Vehicles Using Multi Deep CNN Architecture. (arXiv:2208.08224v2 [cs.CV] UPDATED)
- Authors : Muhammad Muzammel, Mohd Zuki, Mohamad Naufal, Mohamad Saad, Faryal Sheikh, Muhammad Ahsan
- Link : [http://arxiv.org/abs/2208.08224](http://arxiv.org/abs/2208.08224)
> ABSTRACT  :  Buses and heavy vehicles have more blind spots compared to cars and other road vehicles due to their large sizes. Therefore, accidents caused by these heavy vehicles are more fatal and result in severe injuries to other road users. These possible blind-spot collisions can be identified early using vision-based object detection approaches. Yet, the existing state-of-the-art vision-based object detection models rely heavily on a single feature descriptor for making decisions. In this research, the design of two convolutional neural networks (CNNs) based on high-level feature descriptors and their integration with faster R-CNN is proposed to detect blind-spot collisions for heavy vehicles. Moreover, a fusion approach is proposed to integrate two pre-trained networks (i.e., Resnet 50 and Resnet 101) for extracting high level features for blind-spot vehicle detection. The fusion of features significantly improves the performance of faster R-CNN and outperformed the existing state-of-the-art methods. Both approaches are validated on a self-recorded blind-spot vehicle detection dataset for buses and an online LISA dataset for vehicle detection. For both proposed approaches, a false detection rate (FDR) of 3.05% and 3.49% are obtained for the self recorded dataset, making these approaches suitable for **real time** applications.  
## eess.IV
---
### **Low-light** **Enhancement** Method Based on Attention Map Net. (arXiv:2208.09330v1 [cs.CV])
- Authors : Mengfei Wu, Xucheng Xue, Taiji Lan, Xinwei Xu
- Link : [http://arxiv.org/abs/2208.09330](http://arxiv.org/abs/2208.09330)
> ABSTRACT  :  **Low-light** image **enhancement** is a crucial preprocessing task for some complex vision tasks. Target detection, image segmentation, and image recognition outcomes are all directly impacted by the impact of image **enhancement**. However, the majority of the currently used image **enhancement** techniques do not produce satisfactory outcomes, and these enhanced networks have relatively weak robustness. We suggest an improved network called BrightenNet that uses U-Net as its primary structure and incorporates a number of different attention mechanisms as a solution to this issue. In a specific application, we employ the network as the generator and LSGAN as the training framework to achieve better **enhancement** results. We demonstrate the validity of the proposed network BrightenNet in the experiments that follow in this paper. The results it produced can both preserve image details and conform to human vision standards.  
### Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v2 [eess.IV] UPDATED)
- Authors : Congpei An, Ning Wu, Xiaoming Yuan
- Link : [http://arxiv.org/abs/2201.02979](http://arxiv.org/abs/2201.02979)
> ABSTRACT  :  The total variation (TV) regularization has phenomenally boosted various variational models for image processing tasks. We propose to combine the backward diffusion process in the earlier literature of image **enhancement** with the TV regularization, and show that the resulting enhanced TV minimization model is particularly effective for reducing the loss of contrast. The main purpose of this paper is to establish stable reconstruction guarantees for the enhanced TV model from noisy subsampled measurements with two sampling strategies, non-adaptive sampling for general linear measurements and variable-density sampling for Fourier measurements. In particular, under some weaker restricted isometry property conditions, the enhanced TV minimization model is shown to have tighter reconstruction error bounds than various TV-based models for the scenario where the level of noise is significant and the amount of measurements is limited. Advantages of the enhanced TV model are also numerically validated by preliminary experiments on the reconstruction of some synthetic, natural, and medical images.  
### Tilt-then-Blur or Blur-then-Tilt? Clarifying the Atmospheric Turbulence Model. (arXiv:2207.06377v3 [eess.IV] UPDATED)
- Authors : 
- Link : [http://arxiv.org/abs/2207.06377](http://arxiv.org/abs/2207.06377)
> ABSTRACT  :  Imaging at a long distance often requires advanced image **restoration** algorithms to compensate for the distortions caused by atmospheric turbulence. However, unlike many standard **restoration** problems such as deconvolution, the forward image formation model of the atmospheric turbulence does not have a simple expression. Thanks to the Zernike representation of the phase, one can show that the forward model is a combination of tilt (pixel shifting due to the linear phase terms) and blur (image smoothing due to the high order aberrations).    Confusions then arise between the ordering of the two operators. Should the model be tilt-then-blur, or blur-then-tilt? Some papers in the literature say that the model is tilt-then-blur, whereas more papers say that it is blur-then-tilt. This paper clarifies the differences between the two and discusses why the tilt-then-blur is the correct model. Recommendations are given to the research community.  
### Blind-Spot Collision Detection System for Commercial Vehicles Using Multi Deep CNN Architecture. (arXiv:2208.08224v2 [cs.CV] UPDATED)
- Authors : Muhammad Muzammel, Mohd Zuki, Mohamad Naufal, Mohamad Saad, Faryal Sheikh, Muhammad Ahsan
- Link : [http://arxiv.org/abs/2208.08224](http://arxiv.org/abs/2208.08224)
> ABSTRACT  :  Buses and heavy vehicles have more blind spots compared to cars and other road vehicles due to their large sizes. Therefore, accidents caused by these heavy vehicles are more fatal and result in severe injuries to other road users. These possible blind-spot collisions can be identified early using vision-based object detection approaches. Yet, the existing state-of-the-art vision-based object detection models rely heavily on a single feature descriptor for making decisions. In this research, the design of two convolutional neural networks (CNNs) based on high-level feature descriptors and their integration with faster R-CNN is proposed to detect blind-spot collisions for heavy vehicles. Moreover, a fusion approach is proposed to integrate two pre-trained networks (i.e., Resnet 50 and Resnet 101) for extracting high level features for blind-spot vehicle detection. The fusion of features significantly improves the performance of faster R-CNN and outperformed the existing state-of-the-art methods. Both approaches are validated on a self-recorded blind-spot vehicle detection dataset for buses and an online LISA dataset for vehicle detection. For both proposed approaches, a false detection rate (FDR) of 3.05% and 3.49% are obtained for the self recorded dataset, making these approaches suitable for **real time** applications.  
## cs.LG
---
### Discovering Faint and High Apparent Motion Rate Near-Earth Asteroids Using A Deep Learning Program. (arXiv:2208.09098v1 [astro-ph.IM])
- Authors : Franklin Wang, Jian Ge, Kevin Willis
- Link : [http://arxiv.org/abs/2208.09098](http://arxiv.org/abs/2208.09098)
> ABSTRACT  :  Although many near-Earth objects have been found by ground-based telescopes, some fast-moving ones, especially those near detection limits, have been missed by observatories. We developed a convolutional neural network for detecting faint fast-moving near-Earth objects. It was trained with artificial streaks generated from simulations and was able to find these asteroid streaks with an accuracy of 98.7% and a false positive rate of 0.02% on simulated data. This program was used to search image data from the Zwicky Transient Facility (ZTF) in four **night**s in 2019, and it identified six previously undiscovered asteroids. The visual magnitudes of our detections range from ~19.0 - 20.3 and motion rates range from ~6.8 - 24 deg/day, which is very faint compared to other ZTF detections moving at similar motion rates. Our asteroids are also ~1 - 51 m diameter in size and ~5 - 60 lunar distances away at close approach, assuming their albedo values follow the albedo distribution function of known asteroids. The use of a purely simulated dataset to train our model enables the program to gain sensitivity in detecting faint and fast-moving objects while still being able to recover nearly all discoveries made by previously designed neural networks which used real detections to train neural networks. Our approach can be adopted by any observatory for detecting fast-moving asteroid streaks.  
### Feature Selection **Enhancement** and Feature Space Visualization for Speech-Based Emotion Recognition. (arXiv:2208.09269v1 [eess.SP])
- Authors : Sofia Kanwal, Sohail Asghar, Hazrat Ali
- Link : [http://arxiv.org/abs/2208.09269](http://arxiv.org/abs/2208.09269)
> ABSTRACT  :  Robust speech emotion recognition relies on the quality of the speech features. We present speech features **enhancement** strategy that improves speech emotion recognition. We used the INTERSPEECH 2010 challenge feature-set. We identified subsets from the features set and applied Principle Component Analysis to the subsets. Finally, the features are fused horizontally. The resulting feature set is analyzed using t-distributed neighbour embeddings (t-SNE) before the application of features for emotion recognition. The method is compared with the state-of-the-art methods used in the literature. The empirical evidence is drawn using two well-known datasets: Emotional Speech Dataset (EMO-DB) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) for two languages, German and English, respectively. Our method achieved an average recognition gain of 11.5\% for six out of seven emotions for the EMO-DB dataset, and 13.8\% for seven out of eight emotions for the RAVDESS dataset as compared to the baseline study.  
### Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise. (arXiv:2208.09392v1 [cs.CV])
- Authors : Arpit Bansal, Eitan Borgnia, Min Chu, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein
- Link : [http://arxiv.org/abs/2208.09392](http://arxiv.org/abs/2208.09392)
> ABSTRACT  :  Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image **restoration** operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models  
### Augmenting Message Passing by Retrieving Similar Graphs. (arXiv:2206.00362v2 [cs.LG] UPDATED)
- Authors : Dingmin Wang, Shengchao Liu, Hanchen Wang, Bernardo Cuenca, Linfeng Song, Jian Tang, Song Le, Qi Liu
- Link : [http://arxiv.org/abs/2206.00362](http://arxiv.org/abs/2206.00362)
> ABSTRACT  :  Graph Neural Networks~(GNNs) are effective tools for graph representation learning. Most GNNs rely on a recursive neighborhood aggregation scheme, named message passing, thereby their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). Motivated by the success of retrieval-based models and off-the-shelf high-performance retrieval systems, we propose a non-parametric and model-agnostic scheme called GraphRetrieval to boost existing GNN models. In GraphRetrieval, similar training graphs associated with their ground-truth labels are retrieved as an **enhancement** to be jointly utilized with the input graph representation to complete various graph property predictive tasks. In particular, to effectively "absorb" useful information from retrieved graphs and "ignore" possible noise, we introduce an adapter based on self-attention to explicitly learn the interaction between an input graph and its retrieved similar graphs. By experimenting with three classic GNN models on 12 different datasets, we have demonstrated GraphRetrieval is able to bring substantial improvements to existing GNN models without comprising the model size and the prediction efficiency. Our work also firstly validates the feasibility and effectiveness of retrieved-enhanced graph neural networks.  
### A Tutorial on the Spectral Theory of Markov Chains. (arXiv:2207.02296v2 [cs.LG] UPDATED)
- Authors : Eddie Seabrook, Laurenz Wiskott
- Link : [http://arxiv.org/abs/2207.02296](http://arxiv.org/abs/2207.02296)
> ABSTRACT  :  Markov chains are a class of probabilistic models that have achieved widespread application in the quantitative sciences. This is in part due to their versatility, but is compounded by the ease with which they can be probed analytically. This tutorial provides an in-depth introduction to Markov chains, and explores their connection to graphs and random walks. We utilize tools from linear algebra and graph theory to describe the transition matrices of different types of Markov chains, with a particular focus on exploring properties of the eigenvalues and eigenvectors corresponding to these matrices. The results presented are relevant to a number of methods in machine learning and data mining, which we describe at various stages. Rather than being a novel academic study in its own right, this text presents a collection of known results, together with some new concepts. Moreover, the tutorial focuses on offering intuition to readers rather than formal understanding, and only assumes basic **exposure** to concepts from linear algebra and probability theory. It is therefore accessible to students and researchers from a wide variety of disciplines.  
## cs.AI
---
### Feature Selection **Enhancement** and Feature Space Visualization for Speech-Based Emotion Recognition. (arXiv:2208.09269v1 [eess.SP])
- Authors : Sofia Kanwal, Sohail Asghar, Hazrat Ali
- Link : [http://arxiv.org/abs/2208.09269](http://arxiv.org/abs/2208.09269)
> ABSTRACT  :  Robust speech emotion recognition relies on the quality of the speech features. We present speech features **enhancement** strategy that improves speech emotion recognition. We used the INTERSPEECH 2010 challenge feature-set. We identified subsets from the features set and applied Principle Component Analysis to the subsets. Finally, the features are fused horizontally. The resulting feature set is analyzed using t-distributed neighbour embeddings (t-SNE) before the application of features for emotion recognition. The method is compared with the state-of-the-art methods used in the literature. The empirical evidence is drawn using two well-known datasets: Emotional Speech Dataset (EMO-DB) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) for two languages, German and English, respectively. Our method achieved an average recognition gain of 11.5\% for six out of seven emotions for the EMO-DB dataset, and 13.8\% for seven out of eight emotions for the RAVDESS dataset as compared to the baseline study.  
### Augmenting Message Passing by Retrieving Similar Graphs. (arXiv:2206.00362v2 [cs.LG] UPDATED)
- Authors : Dingmin Wang, Shengchao Liu, Hanchen Wang, Bernardo Cuenca, Linfeng Song, Jian Tang, Song Le, Qi Liu
- Link : [http://arxiv.org/abs/2206.00362](http://arxiv.org/abs/2206.00362)
> ABSTRACT  :  Graph Neural Networks~(GNNs) are effective tools for graph representation learning. Most GNNs rely on a recursive neighborhood aggregation scheme, named message passing, thereby their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). Motivated by the success of retrieval-based models and off-the-shelf high-performance retrieval systems, we propose a non-parametric and model-agnostic scheme called GraphRetrieval to boost existing GNN models. In GraphRetrieval, similar training graphs associated with their ground-truth labels are retrieved as an **enhancement** to be jointly utilized with the input graph representation to complete various graph property predictive tasks. In particular, to effectively "absorb" useful information from retrieved graphs and "ignore" possible noise, we introduce an adapter based on self-attention to explicitly learn the interaction between an input graph and its retrieved similar graphs. By experimenting with three classic GNN models on 12 different datasets, we have demonstrated GraphRetrieval is able to bring substantial improvements to existing GNN models without comprising the model size and the prediction efficiency. Our work also firstly validates the feasibility and effectiveness of retrieved-enhanced graph neural networks.  
### Shielding Federated Learning Systems against Inference Attacks with ARM TrustZone. (arXiv:2208.05895v3 [cs.CR] UPDATED)
- Authors : Aghiles Ait, Sonia Ben, Vlad Nitu, Valerio Schiavoni
- Link : [http://arxiv.org/abs/2208.05895](http://arxiv.org/abs/2208.05895)
> ABSTRACT  :  Federated Learning (FL) opens new perspectives for training machine learning models while keeping personal data on the users premises. Specifically, in FL, models are trained on the users devices and only model updates (i.e., gradients) are sent to a central server for aggregation purposes. However, the long list of inference attacks that leak private data from gradients, published in the recent years, have emphasized the need of devising effective protection mechanisms to incentivize the adoption of FL at scale. While there exist solutions to mitigate these attacks on the server side, little has been done to protect users from attacks performed on the client side. In this context, the use of Trusted Execution Environments (TEEs) on the client side are among the most proposing solutions. However, existing frameworks (e.g., **Dark**neTZ) require statically putting a large portion of the machine learning model into the TEE to effectively protect against complex attacks or a combination of attacks. We present GradSec, a solution that allows protecting in a TEE only sensitive layers of a machine learning model, either statically or dynamically, hence reducing both the TCB size and the overall training time by up to 30% and 56%, respectively compared to state-of-the-art competitors.  
# Paper List
---
## cs.CV
---
**84** new papers in cs.CV:-) 
1. Automated Detection of Acute Lymphoblastic Leukemia Subtypes from Microscopic Blood Smear Images using Deep Neural Networks. (arXiv:2208.08992v1 [eess.IV])
2. VAuLT: Augmenting the Vision-and-Language Transformer with the Propagation of Deep Language Representations. (arXiv:2208.09021v1 [cs.CV])
3. Single-Stage Open-world Instance Segmentation with Cross-task Consistency Regularization. (arXiv:2208.09023v1 [cs.CV])
4. A Multi-Modal Wildfire Prediction and Personalized Early-Warning System Based on a Novel Machine Learning Framework. (arXiv:2208.09079v1 [cs.LG])
5. Out-of-distribution Detection via Frequency-regularized Generative Models. (arXiv:2208.09083v1 [cs.LG])
6. Towards Unbiased Label Distribution Learning for Facial Pose Estimation Using Anisotropic Spherical Gaussian. (arXiv:2208.09122v1 [cs.CV])
7. Video Interpolation by Event-driven Anisotropic Adjustment of Optical Flow. (arXiv:2208.09127v1 [cs.CV])
8. Towards Daily High-resolution Inundation Observations using Deep Learning and EO. (arXiv:2208.09135v1 [physics.geo-ph])
9. Vector Quantized Diffusion Model with CodeUnet for Text-to-Sign Pose Sequences Generation. (arXiv:2208.09141v1 [cs.CV])
10. Part-aware Prototypical Graph Network for One-shot Skeleton-based Action Recognition. (arXiv:2208.09150v1 [cs.CV])
11. Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning. (arXiv:2208.09170v1 [cs.CV])
12. Improved Image Classification with Token Fusion. (arXiv:2208.09183v1 [cs.CV])
13. Synthetic Data in Human Analysis: A Survey. (arXiv:2208.09191v1 [cs.CV])
14. Real-Time Robust Video Object Detection System Against Physical-World Adversarial Attacks. (arXiv:2208.09195v1 [cs.CV])
15. EAA-Net: Rethinking the Autoencoder Architecture with Intra-class Features for Medical Image Segmentation. (arXiv:2208.09197v1 [cs.CV])
16. TTT-UCDR: Test-time Training for Universal Cross-Domain Retrieval. (arXiv:2208.09198v1 [cs.CV])
17. Towards Efficient Capsule Networks. (arXiv:2208.09203v1 [cs.CV])
18. Booster-SHOT: Boosting Stacked Homography Transformations for Multiview Pedestrian Detection with Attention. (arXiv:2208.09211v1 [cs.CV])
19. Ensemble uncertainty as a criterion for dataset expansion in distinct bone segmentation from upper-body CT images. (arXiv:2208.09216v1 [eess.IV])
20. SoMoFormer: Social-Aware Motion Transformer for Multi-Person Motion Prediction. (arXiv:2208.09224v1 [cs.CV])
21. Shift Variance in Scene Text Detection. (arXiv:2208.09231v1 [cs.CV])
22. Diverse Video Captioning by Adaptive Spatio-temporal Attention. (arXiv:2208.09266v1 [cs.CV])
23. Diagnose Like a Radiologist: Hybrid Neuro-Probabilistic Reasoning for Attribute-Based Medical Image Diagnosis. (arXiv:2208.09282v1 [cs.CV])
24. Reproducibility Report: Contrastive Learning of Socially-aware Motion Representations. (arXiv:2208.09284v1 [cs.CV])
25. Shadows Aren't So Dangerous After All: A Fast and Robust Defense Against Shadow-Based Adversarial Attacks. (arXiv:2208.09285v1 [cs.CV])
26. Background Invariance Testing According to Semantic Proximity. (arXiv:2208.09286v1 [cs.CV])
27. Self-Supervised Visual Place Recognition by Mining Temporal and Feature Neighborhoods. (arXiv:2208.09315v1 [cs.CV])
28. **Low-light** **Enhancement** Method Based on Attention Map Net. (arXiv:2208.09330v1 [cs.CV])
29. Text to Image Generation: Leaving no Language Behind. (arXiv:2208.09333v1 [cs.CL])
30. Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models. (arXiv:2208.09336v1 [cs.CV])
31. IPNET:Influential Prototypical Networks for Few Shot Learning. (arXiv:2208.09345v1 [cs.CV])
32. DCNNV-19: Uma rede neural convolucional profunda para detec\c{c}\~ao de COVID-19 em tomografias computadorizadas tor\'acicas. (arXiv:2208.09349v1 [eess.IV])
33. PyMIC: A deep learning toolkit for annotation-efficient medical image segmentation. (arXiv:2208.09350v1 [eess.IV])
34. VLMAE: Vision-Language Masked Autoencoder. (arXiv:2208.09374v1 [cs.CV])
35. Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise. (arXiv:2208.09392v1 [cs.CV])
36. PersDet: Monocular 3D Detection in Perspective Bird's-Eye-View. (arXiv:2208.09394v1 [cs.CV])
37. PrepNet: A Convolutional Auto-Encoder to Homogenize CT Scans for Cross-Dataset Medical Image Analysis. (arXiv:2208.09408v1 [eess.IV])
38. Wildfire Forecasting with Satellite Images and Deep Generative Model. (arXiv:2208.09411v1 [cs.CV])
39. ModSelect: Automatic Modality Selection for Synthetic-to-Real Domain Generalization. (arXiv:2208.09414v1 [cs.CV])
40. Aspect-based Sentiment Classification with Sequential Cross-modal Semantic Graph. (arXiv:2208.09417v1 [cs.CV])
41. Hierarchical Compositional Representations for Few-shot Action Recognition. (arXiv:2208.09424v1 [cs.CV])
42. Curbing Task Interference using Representation Similarity-Guided Multi-Task Feature Sharing. (arXiv:2208.09427v1 [cs.CV])
43. MonoPCNS: Monocular 3D Object Detection via Point Cloud Network Simulation. (arXiv:2208.09446v1 [cs.CV])
44. Guided-deconvolution for Correlative Light and Electron Microscopy. (arXiv:2208.09451v1 [cs.CV])
45. Temporal View Synthesis of Dynamic Scenes through 3D Object Motion Estimation with Multi-Plane Images. (arXiv:2208.09463v1 [cs.CV])
46. Neural Light Field Estimation for Street Scenes with Differentiable Virtual Object Insertion. (arXiv:2208.09480v1 [cs.CV])
47. ARID: A New Dataset for Recognizing Action in the **Dark**. (arXiv:2006.03876v4 [cs.CV] UPDATED)
48. BanglaWriting: A multi-purpose offline Bangla handwriting dataset. (arXiv:2011.07499v3 [cs.CV] UPDATED)
49. OpenCoS: Contrastive Semi-supervised Learning for Handling Open-set Unlabeled Data. (arXiv:2107.08943v3 [cs.CV] UPDATED)
50. ISyNet: Convolutional Neural Networks design for AI accelerator. (arXiv:2109.01932v2 [cs.CV] UPDATED)
51. WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation. (arXiv:2109.14196v2 [cs.CV] UPDATED)
52. Playing for 3D Human Recovery. (arXiv:2110.07588v2 [cs.CV] UPDATED)
53. "Sparse + Low-Rank'' Tensor Completion Approach for Recovering Images and Videos. (arXiv:2110.09298v2 [cs.CV] UPDATED)
54. Pixels2Pose: Super-Resolution Time-of-Flight Imaging for 3D Pose Estimation. (arXiv:2110.11414v2 [cs.CV] UPDATED)
55. Detecting Object States vs Detecting Objects: A New Dataset and a Quantitative Experimental Study. (arXiv:2112.08281v2 [cs.CV] UPDATED)
56. Contrastive Learning of Semantic and Visual Representations for Text Tracking. (arXiv:2112.14976v3 [cs.CV] UPDATED)
57. User Evaluation of Culture-to-Culture Image Translation with Generative Adversarial Nets. (arXiv:2201.01565v3 [cs.CV] UPDATED)
58. Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v2 [eess.IV] UPDATED)
59. MMNet: Muscle motion-guided network for micro-expression recognition. (arXiv:2201.05297v2 [cs.CV] UPDATED)
60. Optimizing Gradient-driven Criteria in Network Sparsity: Gradient is All You Need. (arXiv:2201.12826v2 [cs.CV] UPDATED)
61. Echofilter: A Deep Learning Segmentation Model Improves the Automation, Standardization, and Timeliness for Post-Processing Echosounder Data in Tidal Energy Streams. (arXiv:2202.09648v2 [cs.LG] UPDATED)
62. Transformers in Medical Image Analysis: A Review. (arXiv:2202.12165v3 [cs.CV] UPDATED)
63. Diffeomorphic Image Registration with Neural Velocity Field. (arXiv:2202.12498v3 [cs.CV] UPDATED)
64. Nuclei instance segmentation and classification in histopathology images with StarDist. (arXiv:2203.02284v3 [cs.CV] UPDATED)
65. Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks. (arXiv:2203.17030v2 [cs.CV] UPDATED)
66. Accelerated MRI With Deep Linear Convolutional Transform Learning. (arXiv:2204.07923v2 [eess.IV] UPDATED)
67. End-to-end Weakly-supervised Multiple 3D Hand Mesh Reconstruction from Single Image. (arXiv:2204.08154v2 [cs.CV] UPDATED)
68. A Survey on Unsupervised Visual Industrial Anomaly Detection Algorithms. (arXiv:2204.11161v3 [cs.CV] UPDATED)
69. Deeper Insights into the Robustness of ViTs towards Common Corruptions. (arXiv:2204.12143v3 [cs.CV] UPDATED)
70. How to Fine-tune Models with Few Samples: Update, Data Augmentation, and Test-time Augmentation. (arXiv:2205.07874v3 [cs.LG] UPDATED)
71. Adaptive Fine-Grained Sketch-Based Image Retrieval. (arXiv:2207.01723v3 [cs.CV] UPDATED)
72. SRRT: Search Region Regulation Tracking. (arXiv:2207.04438v2 [cs.CV] UPDATED)
73. Facilitated machine learning for image-based fruit quality assessment. (arXiv:2207.04523v2 [cs.CV] UPDATED)
74. Certified Adversarial Robustness via Anisotropic Randomized Smoothing. (arXiv:2207.05327v2 [cs.CV] UPDATED)
75. Water Surface Patch Classification Using Mixture Augmentation for River Scum Index. (arXiv:2207.06388v2 [cs.CV] UPDATED)
76. Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning. (arXiv:2207.06580v2 [cs.CV] UPDATED)
77. iColoriT: Towards Propagating Local Hint to the Right Region in Interactive Colorization by Leveraging Vision Transformer. (arXiv:2207.06831v3 [cs.CV] UPDATED)
78. Level Set-Based Camera Pose Estimation From Multiple 2D/3D Ellipse-Ellipsoid Correspondences. (arXiv:2207.07953v2 [cs.CV] UPDATED)
79. **Real-time** End-to-End Video Text Spotter with Contrastive Representation Learning. (arXiv:2207.08417v3 [cs.CV] UPDATED)
80. Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v3 [cs.CV] UPDATED)
81. HoW-3D: Holistic 3D Wireframe Perception from a Single Image. (arXiv:2208.06999v2 [cs.CV] UPDATED)
82. Blind-Spot Collision Detection System for Commercial Vehicles Using Multi Deep CNN Architecture. (arXiv:2208.08224v2 [cs.CV] UPDATED)
83. NeIF: Representing General Reflectance as Neural Intrinsics Fields for Uncalibrated Photometric Stereo. (arXiv:2208.08897v2 [cs.CV] UPDATED)
84. Language Guided Networks for Cross-modal Moment Retrieval. (arXiv:2006.10457v2 [cs.CV] CROSS LISTED)
## eess.IV
---
**10** new papers in eess.IV:-) 
1. Automated Detection of Acute Lymphoblastic Leukemia Subtypes from Microscopic Blood Smear Images using Deep Neural Networks. (arXiv:2208.08992v1 [eess.IV])
2. Ensemble uncertainty as a criterion for dataset expansion in distinct bone segmentation from upper-body CT images. (arXiv:2208.09216v1 [eess.IV])
3. **Low-light** **Enhancement** Method Based on Attention Map Net. (arXiv:2208.09330v1 [cs.CV])
4. DCNNV-19: Uma rede neural convolucional profunda para detec\c{c}\~ao de COVID-19 em tomografias computadorizadas tor\'acicas. (arXiv:2208.09349v1 [eess.IV])
5. PyMIC: A deep learning toolkit for annotation-efficient medical image segmentation. (arXiv:2208.09350v1 [eess.IV])
6. PrepNet: A Convolutional Auto-Encoder to Homogenize CT Scans for Cross-Dataset Medical Image Analysis. (arXiv:2208.09408v1 [eess.IV])
7. Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v2 [eess.IV] UPDATED)
8. Accelerated MRI With Deep Linear Convolutional Transform Learning. (arXiv:2204.07923v2 [eess.IV] UPDATED)
9. Tilt-then-Blur or Blur-then-Tilt? Clarifying the Atmospheric Turbulence Model. (arXiv:2207.06377v3 [eess.IV] UPDATED)
10. Blind-Spot Collision Detection System for Commercial Vehicles Using Multi Deep CNN Architecture. (arXiv:2208.08224v2 [cs.CV] UPDATED)
## cs.LG
---
**119** new papers in cs.LG:-) 
1. Automated Detection of Acute Lymphoblastic Leukemia Subtypes from Microscopic Blood Smear Images using Deep Neural Networks. (arXiv:2208.08992v1 [eess.IV])
2. Treeformer: Dense Gradient Trees for Efficient Attention Computation. (arXiv:2208.09015v1 [cs.CL])
3. Improving Small Molecule Generation using Mutual Information Machine. (arXiv:2208.09016v1 [cs.LG])
4. VAuLT: Augmenting the Vision-and-Language Transformer with the Propagation of Deep Language Representations. (arXiv:2208.09021v1 [cs.CV])
5. GraTO: Graph Neural Network Framework Tackling Over-smoothing with Neural Architecture Search. (arXiv:2208.09027v1 [cs.LG])
6. Communication-Efficient Collaborative Best Arm Identification. (arXiv:2208.09029v1 [cs.LG])
7. Quantitative Universal Approximation Bounds for Deep Belief Networks. (arXiv:2208.09033v1 [stat.ML])
8. Is Monte Carlo a bad sampling strategy for learning smooth functions in high dimensions?. (arXiv:2208.09045v1 [math.NA])
9. Self-Supervised Primal-Dual Learning for Constrained Optimization. (arXiv:2208.09046v1 [cs.LG])
10. Machine learning algorithms for three-dimensional mean-curvature computation in the level-set method. (arXiv:2208.09047v1 [cs.LG])
11. A Review of Uncertainty for Deep Reinforcement Learning. (arXiv:2208.09052v1 [cs.LG])
12. How important are socioeconomic factors for hurricane performance of power systems? An analysis of disparities through machine learning. (arXiv:2208.09063v1 [cs.LG])
13. Implicit Session Contexts for Next-Item Recommendations. (arXiv:2208.09076v1 [cs.IR])
14. A Multi-Modal Wildfire Prediction and Personalized Early-Warning System Based on a Novel Machine Learning Framework. (arXiv:2208.09079v1 [cs.LG])
15. Out-of-distribution Detection via Frequency-regularized Generative Models. (arXiv:2208.09083v1 [cs.LG])
16. Representation Learning for the Automatic Indexing of Sound Effects Libraries. (arXiv:2208.09096v1 [cs.SD])
17. Discovering Faint and High Apparent Motion Rate Near-Earth Asteroids Using A Deep Learning Program. (arXiv:2208.09098v1 [astro-ph.IM])
18. Scalable Multi-Agent Framework for Optimizing the Lab and Warehouse. (arXiv:2208.09099v1 [cs.MA])
19. A Causality-Based Learning Approach for Discovering the Underlying Dynamics of Complex Systems from Partial Observations with Stochastic Parameterization. (arXiv:2208.09104v1 [math.DS])
20. A Risk-Sensitive Approach to Policy Optimization. (arXiv:2208.09106v1 [cs.LG])
21. IAN: Iterated Adaptive Neighborhoods for manifold learning and dimensionality estimation. (arXiv:2208.09123v1 [cs.LG])
22. GraphTTA: Test Time Adaptation on Graph Neural Networks. (arXiv:2208.09126v1 [cs.LG])
23. Towards Daily High-resolution Inundation Observations using Deep Learning and EO. (arXiv:2208.09135v1 [physics.geo-ph])
24. DAFT: Distilling Adversarially Fine-tuned Models for Better OOD Generalization. (arXiv:2208.09139v1 [cs.LG])
25. Classification Performance Metric Elicitation and its Applications. (arXiv:2208.09142v1 [stat.ML])
26. Semi-analytic PINN methods for singularly perturbed boundary value problems. (arXiv:2208.09145v1 [math.NA])
27. Disentangled Representation with Causal Constraints for Counterfactual Fairness. (arXiv:2208.09147v1 [cs.LG])
28. Ginex: SSD-enabled Billion-scale Graph Neural Network Training on a Single Machine via Provably Optimal In-memory Caching. (arXiv:2208.09151v1 [cs.LG])
29. A Physics-informed Deep Learning Approach for Minimum Effort Stochastic Control of Colloidal Self-Assembly. (arXiv:2208.09182v1 [math.OC])
30. Cross-Domain Evaluation of a Deep Learning-Based Type Inference System. (arXiv:2208.09189v1 [cs.SE])
31. Real-Time Robust Video Object Detection System Against Physical-World Adversarial Attacks. (arXiv:2208.09195v1 [cs.CV])
32. Improving Post-Processing of Audio Event Detectors Using Reinforcement Learning. (arXiv:2208.09201v1 [cs.SD])
33. Almost Cost-Free Communication in Federated Best Arm Identification. (arXiv:2208.09215v1 [cs.LG])
34. Demystifying Randomly Initialized Networks for Evaluating Generative Models. (arXiv:2208.09218v1 [cs.LG])
35. FP8 Quantization: The Power of the Exponent. (arXiv:2208.09225v1 [cs.LG])
36. An Unsupervised Short- and Long-Term Mask Representation for Multivariate Time Series Anomaly Detection. (arXiv:2208.09240v1 [cs.LG])
37. Mitigating Disparity while Maximizing Reward: Tight Anytime Guarantee for Improving Bandits. (arXiv:2208.09254v1 [cs.LG])
38. Feature Selection **Enhancement** and Feature Space Visualization for Speech-Based Emotion Recognition. (arXiv:2208.09269v1 [eess.SP])
39. Atomistic structure search using local surrogate mode. (arXiv:2208.09273v1 [physics.chem-ph])
40. Reproducibility Report: Contrastive Learning of Socially-aware Motion Representations. (arXiv:2208.09284v1 [cs.CV])
41. Background Invariance Testing According to Semantic Proximity. (arXiv:2208.09286v1 [cs.CV])
42. Learn to Detect and Detect to Learn: Structure Learning and Decision Feedback for MIMO-OFDM Receive Processing. (arXiv:2208.09287v1 [eess.SP])
43. SimLDA: A tool for topic model evaluation. (arXiv:2208.09299v1 [cs.LG])
44. Expressing Multivariate Time Series as Graphs with Time Series Attention Transformer. (arXiv:2208.09300v1 [cs.LG])
45. Graph Convolutional Networks from the Perspective of Sheaves and the Neural Tangent Kernel. (arXiv:2208.09309v1 [cs.LG])
46. Entropy Augmented Reinforcement Learning. (arXiv:2208.09322v1 [cs.LG])
47. Deep Learning for Choice Modeling. (arXiv:2208.09325v1 [stat.ML])
48. Evaluating Explainability for Graph Neural Networks. (arXiv:2208.09339v1 [cs.LG])
49. Non-Stationary Dynamic Pricing Via Actor-Critic Information-Directed Pricing. (arXiv:2208.09372v1 [stat.ML])
50. Federated Learning with Noisy Labels. (arXiv:2208.09378v1 [cs.LG])
51. Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise. (arXiv:2208.09392v1 [cs.CV])
52. Nonlinear Optical Data Transformer for Machine Learning. (arXiv:2208.09398v1 [physics.optics])
53. Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models. (arXiv:2208.09399v1 [cs.LG])
54. Dance Style Transfer with Cross-modal Transformer. (arXiv:2208.09406v1 [cs.LG])
55. Learning in Stackelberg Games with Non-myopic Agents. (arXiv:2208.09407v1 [cs.GT])
56. SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v1 [cs.LG])
57. Federated Select: A Primitive for Communication- and Memory-Efficient Federated Learning. (arXiv:2208.09432v1 [cs.LG])
58. Estimating a potential without the agony of the partition function. (arXiv:2208.09433v1 [cs.LG])
59. Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes. (arXiv:2208.09437v1 [cs.CL])
60. Feature Selection for Fault Detection and Prediction based on Event Log Analysis. (arXiv:2208.09440v1 [cs.LG])
61. A Novel Plug-and-Play Approach for Adversarially Robust Generalization. (arXiv:2208.09449v1 [cs.LG])
62. Unified Policy Optimization for Continuous-action Reinforcement Learning in Non-stationary Tasks and Games. (arXiv:2208.09452v1 [cs.LG])
63. A Physics-based Domain Adaptation framework for modelling and forecasting building energy systems. (arXiv:2208.09456v1 [cs.LG])
64. Gender Bias and Universal Substitution Adversarial Attacks on Grammatical Error Correction Systems for Automated Assessment. (arXiv:2208.09466v1 [cs.CL])
65. Carefully choose the baseline: Lessons learned from applying XAI attribution methods for regression tasks in geoscience. (arXiv:2208.09473v1 [physics.geo-ph])
66. Communication Size Reduction of Federated Learning based on Neural ODE Model. (arXiv:2208.09478v1 [cs.LG])
67. Finding groups of cross-correlated features in bi-view data. (arXiv:2009.05079v3 [stat.ME] UPDATED)
68. Empirical or Invariant Risk Minimization? A Sample Complexity Perspective. (arXiv:2010.16412v2 [cs.LG] UPDATED)
69. BanglaWriting: A multi-purpose offline Bangla handwriting dataset. (arXiv:2011.07499v3 [cs.CV] UPDATED)
70. Unbox the Blackbox: Predict and Interpret YouTube Viewership Using Deep Learning. (arXiv:2101.01076v6 [cs.LG] UPDATED)
71. Max-Affine Spline Insights Into Deep Network Pruning. (arXiv:2101.02338v4 [cs.LG] UPDATED)
72. Local Calibration: Metrics and Recalibration. (arXiv:2102.10809v3 [cs.LG] UPDATED)
73. A Knowledge Graph-Enhanced Tensor Factorisation Model for Discovering Drug Targets. (arXiv:2105.10578v3 [q-bio.QM] UPDATED)
74. Projection-free Graph-based Classifier Learning using Gershgorin Disc Perfect Alignment. (arXiv:2106.01642v3 [cs.LG] UPDATED)
75. Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. (arXiv:2106.15013v3 [cs.LG] UPDATED)
76. OpenCoS: Contrastive Semi-supervised Learning for Handling Open-set Unlabeled Data. (arXiv:2107.08943v3 [cs.CV] UPDATED)
77. Deep Signature FBSDE Algorithm. (arXiv:2108.10504v2 [cs.LG] UPDATED)
78. Kernel PCA with the Nystr\"om method. (arXiv:2109.05578v3 [stat.ML] UPDATED)
79. Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning. (arXiv:2109.11978v3 [cs.RO] UPDATED)
80. SGDE: Secure Generative Data Exchange for Cross-Silo Federated Learning. (arXiv:2109.12062v2 [cs.LG] UPDATED)
81. ALBU: An approximate Loopy Belief message passing algorithm for LDA to improve performance on small data sets. (arXiv:2110.00635v2 [cs.LG] UPDATED)
82. A scalable and fast artificial neural network syndrome decoder for surface codes. (arXiv:2110.05854v3 [quant-ph] UPDATED)
83. GAETS: A Graph Autoencoder Time Series Approach Towards Battery Parameter Estimation. (arXiv:2111.09314v2 [cs.LG] UPDATED)
84. Risk and optimal policies in bandit experiments. (arXiv:2112.06363v11 [econ.EM] UPDATED)
85. Landslide Susceptibility Modeling by Interpretable Neural Network. (arXiv:2201.06837v2 [cs.LG] UPDATED)
86. Arrhythmia Classification using CGAN-augmented ECG Signals. (arXiv:2202.00569v2 [eess.SP] UPDATED)
87. CECILIA: Comprehensive Secure Machine Learning Framework. (arXiv:2202.03023v2 [cs.LG] UPDATED)
88. Domain Adversarial Spatial-Temporal Network: A Transferable Framework for Short-term Traffic Forecasting across Cities. (arXiv:2202.03630v2 [cs.LG] UPDATED)
89. Suboptimal Performance of the Bayes Optimal Algorithm in Frequentist Best Arm Identification. (arXiv:2202.05193v2 [stat.ML] UPDATED)
90. Echofilter: A Deep Learning Segmentation Model Improves the Automation, Standardization, and Timeliness for Post-Processing Echosounder Data in Tidal Energy Streams. (arXiv:2202.09648v2 [cs.LG] UPDATED)
91. Learning based Age of Information Minimization in UAV-relayed IoT Networks. (arXiv:2203.04227v2 [cs.IT] UPDATED)
92. Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks. (arXiv:2203.17030v2 [cs.CV] UPDATED)
93. Bi-fidelity Modeling of Uncertain and Partially Unknown Systems using DeepONets. (arXiv:2204.00997v2 [stat.ML] UPDATED)
94. NARX Identification using Derivative-Based Regularized Neural Networks. (arXiv:2204.05892v2 [eess.SY] UPDATED)
95. Accelerated MRI With Deep Linear Convolutional Transform Learning. (arXiv:2204.07923v2 [eess.IV] UPDATED)
96. Physics-informed neural networks for PDE-constrained optimization and control. (arXiv:2205.03377v2 [cs.LG] UPDATED)
97. How to Fine-tune Models with Few Samples: Update, Data Augmentation, and Test-time Augmentation. (arXiv:2205.07874v3 [cs.LG] UPDATED)
98. ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks. (arXiv:2205.08119v3 [cs.LG] UPDATED)
99. Deletion and Insertion Tests in Regression Models. (arXiv:2205.12423v2 [cs.LG] UPDATED)
100. Superposing Many Tickets into One: A Performance Booster for Sparse Neural Network Training. (arXiv:2205.15322v3 [cs.LG] UPDATED)
101. Bayesian Active Learning for Scanning Probe Microscopy: from Gaussian Processes to Hypothesis Learning. (arXiv:2205.15458v2 [cond-mat.mtrl-sci] UPDATED)
102. Augmenting Message Passing by Retrieving Similar Graphs. (arXiv:2206.00362v2 [cs.LG] UPDATED)
103. Pessimistic Off-Policy Optimization for Learning to Rank. (arXiv:2206.02593v2 [cs.LG] UPDATED)
104. Discovery and density estimation of latent confounders in Bayesian networks with evidence lower bound. (arXiv:2206.05490v3 [cs.LG] UPDATED)
105. On the Surprising Behaviour of node2vec. (arXiv:2206.08252v2 [cs.LG] UPDATED)
106. Parametric and Multivariate Uncertainty Calibration for Regression and Object Detection. (arXiv:2207.01242v2 [cs.LG] UPDATED)
107. A Tutorial on the Spectral Theory of Markov Chains. (arXiv:2207.02296v2 [cs.LG] UPDATED)
108. WeShort: Out-of-distribution Detection With Weak Shortcut structure. (arXiv:2207.05055v3 [cs.LG] UPDATED)
109. Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning. (arXiv:2207.06580v2 [cs.CV] UPDATED)
110. Journal Impact Factor and Peer Review Thoroughness and Helpfulness: A Supervised Machine Learning Study. (arXiv:2207.09821v3 [cs.DL] UPDATED)
111. Deep Learning-Based Acoustic Mosquito Detection in Noisy Conditions Using Trainable Kernels and Augmentations. (arXiv:2207.13843v2 [cs.SD] UPDATED)
112. Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v3 [cs.CV] UPDATED)
113. Spatial-Temporal Identity: A Simple yet Effective Baseline for Multivariate Time Series Forecasting. (arXiv:2208.05233v2 [cs.LG] UPDATED)
114. Rethinking Graph Neural Networks for the Graph Coloring Problem. (arXiv:2208.06975v2 [cs.LG] UPDATED)
115. Towards Informed Design and Validation Assistance in Computer Games Using Imitation Learning. (arXiv:2208.07811v2 [cs.SE] UPDATED)
116. On the generalization of learning algorithms that do not converge. (arXiv:2208.07951v2 [cs.LG] UPDATED)
117. A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages. (arXiv:2208.08227v2 [cs.LG] UPDATED)
118. Study of General Robust Subband Adaptive Filtering. (arXiv:2208.08856v2 [eess.SP] UPDATED)
119. Meta Sparse Principal Component Analysis. (arXiv:2208.08938v2 [stat.ML] UPDATED)
## cs.AI
---
**54** new papers in cs.AI:-) 
1. Automated Detection of Acute Lymphoblastic Leukemia Subtypes from Microscopic Blood Smear Images using Deep Neural Networks. (arXiv:2208.08992v1 [eess.IV])
2. Improving Small Molecule Generation using Mutual Information Machine. (arXiv:2208.09016v1 [cs.LG])
3. VAuLT: Augmenting the Vision-and-Language Transformer with the Propagation of Deep Language Representations. (arXiv:2208.09021v1 [cs.CV])
4. Mapping Husserlian phenomenology onto active inference. (arXiv:2208.09058v1 [q-bio.NC])
5. A Multi-Modal Wildfire Prediction and Personalized Early-Warning System Based on a Novel Machine Learning Framework. (arXiv:2208.09079v1 [cs.LG])
6. Towards Situation Awareness and Attention Guidance in a Multiplayer Environment using Augmented Reality and Carcassonne. (arXiv:2208.09094v1 [cs.HC])
7. IAN: Iterated Adaptive Neighborhoods for manifold learning and dimensionality estimation. (arXiv:2208.09123v1 [cs.LG])
8. Video Interpolation by Event-driven Anisotropic Adjustment of Optical Flow. (arXiv:2208.09127v1 [cs.CV])
9. Personalizing Intervened Network for Long-tailed Sequential User Behavior Modeling. (arXiv:2208.09130v1 [cs.IR])
10. GreenKGC: A Lightweight Knowledge Graph Completion Method. (arXiv:2208.09137v1 [cs.AI])
11. Classification Performance Metric Elicitation and its Applications. (arXiv:2208.09142v1 [stat.ML])
12. Disentangled Representation with Causal Constraints for Counterfactual Fairness. (arXiv:2208.09147v1 [cs.LG])
13. Atomist or Holist? A Diagnosis and Vision for More Productive Interdisciplinary AI Ethics Dialogue. (arXiv:2208.09174v1 [cs.CY])
14. Effective Transfer Learning for Low-Resource Natural Language Understanding. (arXiv:2208.09180v1 [cs.CL])
15. A Physics-informed Deep Learning Approach for Minimum Effort Stochastic Control of Colloidal Self-Assembly. (arXiv:2208.09182v1 [math.OC])
16. Improved Image Classification with Token Fusion. (arXiv:2208.09183v1 [cs.CV])
17. Crowdsourced Fact-Checking at Twitter: How Does the Crowd Compare With Experts?. (arXiv:2208.09214v1 [cs.IR])
18. An Unsupervised Short- and Long-Term Mask Representation for Multivariate Time Series Anomaly Detection. (arXiv:2208.09240v1 [cs.LG])
19. Mitigating Disparity while Maximizing Reward: Tight Anytime Guarantee for Improving Bandits. (arXiv:2208.09254v1 [cs.LG])
20. Feature Selection **Enhancement** and Feature Space Visualization for Speech-Based Emotion Recognition. (arXiv:2208.09269v1 [eess.SP])
21. UnCommonSense: Informative Negative Knowledge about Everyday. (arXiv:2208.09292v1 [cs.AI])
22. Expressing Multivariate Time Series as Graphs with Time Series Attention Transformer. (arXiv:2208.09300v1 [cs.LG])
23. Causal Intervention Improves Implicit Sentiment Analysis. (arXiv:2208.09329v1 [cs.CL])
24. Text to Image Generation: Leaving no Language Behind. (arXiv:2208.09333v1 [cs.CL])
25. Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models. (arXiv:2208.09336v1 [cs.CV])
26. Evaluating Explainability for Graph Neural Networks. (arXiv:2208.09339v1 [cs.LG])
27. Positive dependence in qualitative probabilistic networks. (arXiv:2208.09344v1 [cs.AI])
28. End-to-end Clinical Event Extraction from Chinese Electronic Health Record. (arXiv:2208.09354v1 [cs.CL])
29. Nonlinear Optical Data Transformer for Machine Learning. (arXiv:2208.09398v1 [physics.optics])
30. Aspect-based Sentiment Classification with Sequential Cross-modal Semantic Graph. (arXiv:2208.09417v1 [cs.CV])
31. SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v1 [cs.LG])
32. Curbing Task Interference using Representation Similarity-Guided Multi-Task Feature Sharing. (arXiv:2208.09427v1 [cs.CV])
33. Feature Selection for Fault Detection and Prediction based on Event Log Analysis. (arXiv:2208.09440v1 [cs.LG])
34. Unbox the Blackbox: Predict and Interpret YouTube Viewership Using Deep Learning. (arXiv:2101.01076v6 [cs.LG] UPDATED)
35. Max-Affine Spline Insights Into Deep Network Pruning. (arXiv:2101.02338v4 [cs.LG] UPDATED)
36. SGDE: Secure Generative Data Exchange for Cross-Silo Federated Learning. (arXiv:2109.12062v2 [cs.LG] UPDATED)
37. WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation. (arXiv:2109.14196v2 [cs.CV] UPDATED)
38. Contrastive Learning of Semantic and Visual Representations for Text Tracking. (arXiv:2112.14976v3 [cs.CV] UPDATED)
39. A Survey of Methods for Automated Algorithm Configuration. (arXiv:2202.01651v2 [cs.AI] UPDATED)
40. Domain Adversarial Spatial-Temporal Network: A Transferable Framework for Short-term Traffic Forecasting across Cities. (arXiv:2202.03630v2 [cs.LG] UPDATED)
41. ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks. (arXiv:2205.08119v3 [cs.LG] UPDATED)
42. Deletion and Insertion Tests in Regression Models. (arXiv:2205.12423v2 [cs.LG] UPDATED)
43. Superposing Many Tickets into One: A Performance Booster for Sparse Neural Network Training. (arXiv:2205.15322v3 [cs.LG] UPDATED)
44. Augmenting Message Passing by Retrieving Similar Graphs. (arXiv:2206.00362v2 [cs.LG] UPDATED)
45. Pessimistic Off-Policy Optimization for Learning to Rank. (arXiv:2206.02593v2 [cs.LG] UPDATED)
46. Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning. (arXiv:2207.06580v2 [cs.CV] UPDATED)
47. Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v3 [cs.CV] UPDATED)
48. Shielding Federated Learning Systems against Inference Attacks with ARM TrustZone. (arXiv:2208.05895v3 [cs.CR] UPDATED)
49. Rethinking Graph Neural Networks for the Graph Coloring Problem. (arXiv:2208.06975v2 [cs.LG] UPDATED)
50. C-Causal Blindness An experimental computational framework on the isomorphic relationship between biological computation, artificial computation, and logic using weighted hidden Markov models. (arXiv:2208.07143v2 [cs.AI] UPDATED)
51. Solving the Diffusion of Responsibility Problem in Multiagent Reinforcement Learning with a Policy Resonance Approach. (arXiv:2208.07753v2 [cs.AI] UPDATED)
52. Towards Informed Design and Validation Assistance in Computer Games Using Imitation Learning. (arXiv:2208.07811v2 [cs.SE] UPDATED)
53. NECE: Narrative Event Chain Extraction Toolkit. (arXiv:2208.08063v3 [cs.AI] UPDATED)
54. Language Guided Networks for Cross-modal Moment Retrieval. (arXiv:2006.10457v2 [cs.CV] CROSS LISTED)

