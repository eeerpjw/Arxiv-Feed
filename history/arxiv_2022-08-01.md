# Your interest papers
---
## cs.CV
---
### Neural Density-Distance Fields. (arXiv:2207.14455v1 [cs.CV])
- Authors : Itsuki Ueda, Yoshihiro Fukuhara, Hirokatsu Kataoka, Hiroaki Aizawa, Hidehiko Shishido, Itaru Kitahara
- Link : [http://arxiv.org/abs/2207.14455](http://arxiv.org/abs/2207.14455)
> ABSTRACT  :  The success of neural fields for 3D vision tasks is now indisputable. Following this trend, several methods aiming for visual localization (e.g., SLAM) have been proposed to estimate distance or density fields using neural fields. However, it is difficult to achieve high localization performance by only density fields-based methods such as Neural Radiance Field (**NeRF**) since they do not provide density gradient in most empty regions. On the other hand, distance field-based methods such as Neural Implicit Surface (NeuS) have limitations in objects' surface shapes. This paper proposes Neural Density-Distance Field (NeDDF), a novel 3D representation that reciprocally constrains the distance and density fields. We extend distance field formulation to shapes with no explicit boundary surface, such as fur or smoke, which enable explicit conversion from distance field to density field. Consistent distance and density fields realized by explicit conversion enable both robustness to initial values and high-quality registration. Furthermore, the consistency between fields allows fast convergence from sparse point clouds. Experiments show that NeDDF can achieve high localization performance while providing comparable results to **NeRF** on novel view synthesis. The code is available at https://github.com/ueda0319/neddf.  
### Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models. (arXiv:2207.14626v1 [cs.CV])
- Authors : Robert Legenstein
- Link : [http://arxiv.org/abs/2207.14626](http://arxiv.org/abs/2207.14626)
> ABSTRACT  :  Image **restoration** under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image **restoration** algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image **restoration** by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image **restoration**, and qualitatively show strong generalization to real-world test images.  
### Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling. (arXiv:2207.14663v1 [eess.IV])
- Authors : Dieuwertje Alblas, Christoph Brune, Kak Khee
- Link : [http://arxiv.org/abs/2207.14663](http://arxiv.org/abs/2207.14663)
> ABSTRACT  :  Personalised 3D vascular models are valuable for diagnosis, prognosis and treatment planning in patients with cardiovascular disease. Traditionally, such models have been constructed with explicit representations such as meshes and voxel masks, or implicit representations such as radial basis functions or atomic (tubular) shapes. Here, we propose to represent surfaces by the zero level set of their signed distance function (SDF) in a differentiable **implicit neural representation** (INR). This allows us to model complex vascular structures with a representation that is implicit, continuous, light-weight, and easy to integrate with deep learning algorithms. We here demonstrate the potential of this approach with three practical examples. First, we obtain an accurate and watertight surface for an abdominal aortic aneurysm (AAA) from CT images and show robust fitting from as little as 200 points on the surface. Second, we simultaneously fit nested vessel walls in a single INR without intersections. Third, we show how 3D models of individual arteries can be smoothly blended into a single watertight surface. Our results show that INRs are a flexible representation with potential for minimally interactive annotation and manipulation of complex vascular structures.  
### **High Dynamic Range** and Super-Resolution from Raw Image Bursts. (arXiv:2207.14671v1 [cs.CV])
- Authors : Bruno Lecouat, Thomas Eboli, Jean Ponce, Julien Mairal
- Link : [http://arxiv.org/abs/2207.14671](http://arxiv.org/abs/2207.14671)
> ABSTRACT  :  Photographs captured by smartphones and mid-range cameras have limited spatial resolution and dynamic range, with noisy response in underexposed regions and color artefacts in saturated areas. This paper introduces the first approach (to the best of our knowledge) to the reconstruction of high-resolution, high-dynamic range color images from raw photographic bursts captured by a handheld camera with **exposure** bracketing. This method uses a physically-accurate model of image formation to combine an iterative optimization algorithm for solving the corresponding inverse problem with a learned image representation for robust alignment and a learned natural image prior. The proposed algorithm is fast, with low memory requirements compared to state-of-the-art learning-based approaches to image **restoration**, and features that are learned end to end from synthetic yet realistic data. Extensive experiments demonstrate its excellent performance with super-resolution factors of up to $\times 4$ on real photographs taken in the wild with hand-held cameras, and high robustness to **low-light** conditions, noise, camera shake, and moderate object motion.  
### End-to-end View Synthesis via **NeRF** Attention. (arXiv:2207.14741v1 [cs.CV])
- Authors : Zelin Zhao, Jiaya Jia
- Link : [http://arxiv.org/abs/2207.14741](http://arxiv.org/abs/2207.14741)
> ABSTRACT  :  In this paper, we present a simple seq2seq formulation for view synthesis where we take a set of ray points as input and output colors corresponding to the rays. Directly applying a standard transformer on this seq2seq formulation has two limitations. First, the standard attention cannot successfully fit the volumetric rendering procedure, and therefore high-frequency components are missing in the synthesized views. Second, applying global attention to all rays and pixels is extremely inefficient. Inspired by the neural radiance field (**NeRF**), we propose the **NeRF** attention (**NeRF**A) to address the above problems. On the one hand, **NeRF**A considers the volumetric rendering equation as a soft feature modulation procedure. In this way, the feature modulation enhances the transformers with the **NeRF**-like inductive bias. On the other hand, **NeRF**A performs multi-stage attention to reduce the computational overhead. Furthermore, the **NeRF**A model adopts the ray and pixel transformers to learn the interactions between rays and pixels. **NeRF**A demonstrates superior performance over **NeRF** and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D. Besides, **NeRF**A establishes a new state-of-the-art under two settings: the single-scene view synthesis and the category-centric novel view synthesis. The code will be made publicly available.  
### Image Quality Assessment: Integrating Model-Centric and Data-Centric Approaches. (arXiv:2207.14769v1 [cs.CV])
- Authors : Peibei Cao, Dingquan Li, **Kede Ma**
- Link : [http://arxiv.org/abs/2207.14769](http://arxiv.org/abs/2207.14769)
> ABSTRACT  :  Learning-based image quality assessment (IQA) has made remarkable progress in the past decade, but nearly all consider the two key components - model and data - in relative isolation. Specifically, model-centric IQA focuses on developing "better" objective quality methods on fixed and extensively reused datasets, with a great danger of overfitting. Data-centric IQA involves conducting psychophysical experiments to construct "better" human-annotated datasets, which unfortunately ignores current IQA models during dataset creation. In this paper, we first design a series of experiments to probe computationally that such isolation of model and data impedes further progress of IQA. We then describe a computational framework that integrates model-centric and data-centric IQA. As a specific example, we design computational modules to quantify the sampling-worthiness of candidate images based on blind IQA (BIQA) model predictions and deep content-aware features. Experimental results show that the proposed sampling-worthiness module successfully spots diverse failures of the examined BIQA models, which are indeed worthy samples to be included in next-generation datasets.  
### StyleLight: **HDR** Panorama Generation for Lighting Estimation and Editing. (arXiv:2207.14811v1 [cs.CV])
- Authors : Guangcong Wang, Yinuo Yang, Chen Change, Ziwei Liu
- Link : [http://arxiv.org/abs/2207.14811](http://arxiv.org/abs/2207.14811)
> ABSTRACT  :  We present a new lighting estimation and editing framework to generate high-dynamic-range (**HDR**) indoor panorama lighting from a single limited field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras. Existing lighting estimation methods either directly regress lighting representation parameters or decompose this problem into LFOV-to-panorama and LDR-to-**HDR** lighting generation sub-tasks. However, due to the partial observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a scene, lighting estimation remains a challenging task. To tackle this problem, we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that integrates LDR and **HDR** panorama synthesis into a unified framework. The LDR and **HDR** panorama synthesis share a similar generator but have separate discriminators. During inference, given an LDR LFOV image, we propose a focal-masked GAN inversion method to find its latent code by the LDR panorama synthesis branch and then synthesize the **HDR** panorama by the **HDR** panorama synthesis branch. StyleLight takes LFOV-to-panorama and LDR-to-**HDR** lighting generation into a unified framework and thus greatly improves lighting estimation. Extensive experiments demonstrate that our framework achieves superior performance over state-of-the-art methods on indoor lighting estimation. Notably, StyleLight also enables intuitive lighting editing on indoor **HDR** panoramas, which is suitable for real-world applications. Code is available at https://style-light.github.io.  
### GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond. (arXiv:2207.14812v1 [cs.CV])
- Authors : Xiangyu Xu, Xintao Wang, **Jinwei Gu**, Chen Change
- Link : [http://arxiv.org/abs/2207.14812](http://arxiv.org/abs/2207.14812)
> ABSTRACT  :  We show that pre-trained Generative Adversarial Networks (GANs) such as StyleGAN and BigGAN can be used as a latent bank to improve the performance of image super-resolution. While most existing perceptual-oriented approaches attempt to generate realistic outputs through learning with adversarial loss, our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass for **restoration**. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Employing priors from different generative models allows GLEAN to be applied to diverse categories (\eg~human faces, cats, buildings, and cars). We further present a lightweight version of GLEAN, named LightGLEAN, which retains only the critical components in GLEAN. Notably, LightGLEAN consists of only 21% of parameters and 35% of FLOPs while achieving comparable image quality. We extend our method to different tasks including image colorization and blind image **restoration**, and extensive experiments show that our proposed models perform favorably in comparison to existing methods. Codes and models are available at https://github.com/open-mmlab/mmediting.  
### Effects of Image Size on Deep Learning. (arXiv:2101.11508v4 [cs.CV] UPDATED)
- Authors : Olivier Rukundo
- Link : [http://arxiv.org/abs/2101.11508](http://arxiv.org/abs/2101.11508)
> ABSTRACT  :  This paper presents the effects of late gadolinium **enhancement** (LGE) magnetic resonance imaging (MRI) image size on deep learning based fully automated quantification of myocardial infarction (MI). The main objective is to determine the best size for LGE MRI images in the training dataset to achieve optimal deep learning training outcomes. To determine the new size of LGE MRI images of the reference training dataset, non-extra pixel and extra pixel interpolation algorithms are used. A novel strategy based on thresholding, median filtering, and subtraction operations is introduced and applied to remove extra class labels in interpolated ground truth (GT) segmentation masks. Fully automated quantification is achieved using the expectation maximization, weighted intensity, a priori information (EWA) algorithm, and the outcome of automatic semantic segmentation of LGE-MRI images with the convolutional neural network (CNN). In the experiments, common class metrics are used to evaluate the quality of semantic segmentation with a CNN architecture of interest (U-net) against the GT segmentation. Arbitrary threshold, comparison of the sums, and sums of differences are used to estimate the relationship between semi-automatic and fully automated quantification of MI results. A close relationship between semi-automatic and fully automated quantification of MI results was more identified in the case involving the dataset of bigger LGE MRI images than in that of the dataset of smaller LGE MRI images, where quantification results based on the dataset of bigger LGE MRI images were 55.5% closer the manual or semi-automatic results while quantification results based on the dataset of smaller LGE MRI images were 22.2% closer the manual results  
### Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v4 [eess.IV] UPDATED)
- Authors : Amir Atapour
- Link : [http://arxiv.org/abs/2202.02832](http://arxiv.org/abs/2202.02832)
> ABSTRACT  :  Convolutional Neural Networks have demonstrated human-level performance in the classification of melanoma and other skin lesions, but evident performance disparities between differing skin tones should be addressed before widespread deployment. In this work, we propose an efficient yet effective algorithm for automatically labelling the skin tone of lesion images, and use this to annotate the benchmark ISIC dataset. We subsequently use these automated labels as the target for two leading bias unlearning techniques towards mitigating skin tone bias. Our experimental results provide evidence that our skin tone detection algorithm outperforms existing solutions and that unlearning skin tone may improve generalisation and can reduce the performance disparity between melanoma detection in lighter and **dark**er skin tones.  
### Exploring Event Camera-based Odometry for Planetary Robots. (arXiv:2204.05880v2 [cs.CV] UPDATED)
- Authors : Florian Mahlknecht, Daniel Gehrig, Jeremy Nash, Benjamin Morrell, Jeff Delaune, Davide Scaramuzza
- Link : [http://arxiv.org/abs/2204.05880](http://arxiv.org/abs/2204.05880)
> ABSTRACT  :  Due to their resilience to motion blur and high robustness in **low-light** and **high dynamic range** conditions, event cameras are poised to become enabling sensors for vision-based exploration on future Mars helicopter missions. However, existing event-based visual-inertial odometry (VIO) algorithms either suffer from high tracking errors or are brittle, since they cannot cope with significant depth uncertainties caused by an unforeseen loss of tracking or other effects. In this work, we introduce EKLT-VIO, which addresses both limitations by combining a state-of-the-art event-based frontend with a filter-based backend. This makes it both accurate and robust to uncertainties, outperforming event- and frame-based VIO algorithms on challenging benchmarks by 32%. In addition, we demonstrate accurate performance in hover-like conditions (outperforming existing event-based methods) as well as high robustness in newly collected Mars-like and high-dynamic-range sequences, where existing frame-based methods fail. In doing so, we show that event-based VIO is the way forward for vision-based exploration on Mars.  
### StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v2 [cs.CV] UPDATED)
- Authors : Minguk Kang, Joonghyuk Shin, Jaesik Park
- Link : [http://arxiv.org/abs/2206.09479](http://arxiv.org/abs/2206.09479)
> ABSTRACT  :  Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 13 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and **Swin** Transformer). Unlike other benchmarks used in the GAN community, we train representative GANs, including BigGAN, StyleGAN2, and StyleGAN3, in a unified training pipeline and quantify generation performance with 7 evaluation metrics. The benchmark evaluates other cutting-edge generative models(e.g., StyleGAN-XL, ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations, training, and evaluation scripts with the pre-trained weights. StudioGAN is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.  
## eess.IV
---
### Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling. (arXiv:2207.14663v1 [eess.IV])
- Authors : Dieuwertje Alblas, Christoph Brune, Kak Khee
- Link : [http://arxiv.org/abs/2207.14663](http://arxiv.org/abs/2207.14663)
> ABSTRACT  :  Personalised 3D vascular models are valuable for diagnosis, prognosis and treatment planning in patients with cardiovascular disease. Traditionally, such models have been constructed with explicit representations such as meshes and voxel masks, or implicit representations such as radial basis functions or atomic (tubular) shapes. Here, we propose to represent surfaces by the zero level set of their signed distance function (SDF) in a differentiable **implicit neural representation** (INR). This allows us to model complex vascular structures with a representation that is implicit, continuous, light-weight, and easy to integrate with deep learning algorithms. We here demonstrate the potential of this approach with three practical examples. First, we obtain an accurate and watertight surface for an abdominal aortic aneurysm (AAA) from CT images and show robust fitting from as little as 200 points on the surface. Second, we simultaneously fit nested vessel walls in a single INR without intersections. Third, we show how 3D models of individual arteries can be smoothly blended into a single watertight surface. Our results show that INRs are a flexible representation with potential for minimally interactive annotation and manipulation of complex vascular structures.  
### **High Dynamic Range** and Super-Resolution from Raw Image Bursts. (arXiv:2207.14671v1 [cs.CV])
- Authors : Bruno Lecouat, Thomas Eboli, Jean Ponce, Julien Mairal
- Link : [http://arxiv.org/abs/2207.14671](http://arxiv.org/abs/2207.14671)
> ABSTRACT  :  Photographs captured by smartphones and mid-range cameras have limited spatial resolution and dynamic range, with noisy response in underexposed regions and color artefacts in saturated areas. This paper introduces the first approach (to the best of our knowledge) to the reconstruction of high-resolution, high-dynamic range color images from raw photographic bursts captured by a handheld camera with **exposure** bracketing. This method uses a physically-accurate model of image formation to combine an iterative optimization algorithm for solving the corresponding inverse problem with a learned image representation for robust alignment and a learned natural image prior. The proposed algorithm is fast, with low memory requirements compared to state-of-the-art learning-based approaches to image **restoration**, and features that are learned end to end from synthetic yet realistic data. Extensive experiments demonstrate its excellent performance with super-resolution factors of up to $\times 4$ on real photographs taken in the wild with hand-held cameras, and high robustness to **low-light** conditions, noise, camera shake, and moderate object motion.  
### Effects of Image Size on Deep Learning. (arXiv:2101.11508v4 [cs.CV] UPDATED)
- Authors : Olivier Rukundo
- Link : [http://arxiv.org/abs/2101.11508](http://arxiv.org/abs/2101.11508)
> ABSTRACT  :  This paper presents the effects of late gadolinium **enhancement** (LGE) magnetic resonance imaging (MRI) image size on deep learning based fully automated quantification of myocardial infarction (MI). The main objective is to determine the best size for LGE MRI images in the training dataset to achieve optimal deep learning training outcomes. To determine the new size of LGE MRI images of the reference training dataset, non-extra pixel and extra pixel interpolation algorithms are used. A novel strategy based on thresholding, median filtering, and subtraction operations is introduced and applied to remove extra class labels in interpolated ground truth (GT) segmentation masks. Fully automated quantification is achieved using the expectation maximization, weighted intensity, a priori information (EWA) algorithm, and the outcome of automatic semantic segmentation of LGE-MRI images with the convolutional neural network (CNN). In the experiments, common class metrics are used to evaluate the quality of semantic segmentation with a CNN architecture of interest (U-net) against the GT segmentation. Arbitrary threshold, comparison of the sums, and sums of differences are used to estimate the relationship between semi-automatic and fully automated quantification of MI results. A close relationship between semi-automatic and fully automated quantification of MI results was more identified in the case involving the dataset of bigger LGE MRI images than in that of the dataset of smaller LGE MRI images, where quantification results based on the dataset of bigger LGE MRI images were 55.5% closer the manual or semi-automatic results while quantification results based on the dataset of smaller LGE MRI images were 22.2% closer the manual results  
### Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v4 [eess.IV] UPDATED)
- Authors : Amir Atapour
- Link : [http://arxiv.org/abs/2202.02832](http://arxiv.org/abs/2202.02832)
> ABSTRACT  :  Convolutional Neural Networks have demonstrated human-level performance in the classification of melanoma and other skin lesions, but evident performance disparities between differing skin tones should be addressed before widespread deployment. In this work, we propose an efficient yet effective algorithm for automatically labelling the skin tone of lesion images, and use this to annotate the benchmark ISIC dataset. We subsequently use these automated labels as the target for two leading bias unlearning techniques towards mitigating skin tone bias. Our experimental results provide evidence that our skin tone detection algorithm outperforms existing solutions and that unlearning skin tone may improve generalisation and can reduce the performance disparity between melanoma detection in lighter and **dark**er skin tones.  
### StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v2 [cs.CV] UPDATED)
- Authors : Minguk Kang, Joonghyuk Shin, Jaesik Park
- Link : [http://arxiv.org/abs/2206.09479](http://arxiv.org/abs/2206.09479)
> ABSTRACT  :  Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 13 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and **Swin** Transformer). Unlike other benchmarks used in the GAN community, we train representative GANs, including BigGAN, StyleGAN2, and StyleGAN3, in a unified training pipeline and quantify generation performance with 7 evaluation metrics. The benchmark evaluates other cutting-edge generative models(e.g., StyleGAN-XL, ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations, training, and evaluation scripts with the pre-trained weights. StudioGAN is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.  
## cs.LG
---
### A Recommender System for Equitable Public Art Curation and Installation. (arXiv:2207.14367v1 [cs.IR])
- Authors : Anna Haensch, Abiy Tasissa, Dina Deitsch
- Link : [http://arxiv.org/abs/2207.14367](http://arxiv.org/abs/2207.14367)
> ABSTRACT  :  The placement of art in public spaces can have a significant impact on who feels a sense of belonging. In cities, public art communicates whose interests and culture are being favored. In this paper, we propose a graph matching approach with local constraints to build a curatorial tool for selecting public art in a way that supports inclusive spaces. We develop a cost matrix by drawing on Schelling's model of segregation. Using the cost matrix as an input, the optimization problem is solved via projected gradient descent to obtain a soft assignment matrix. We discuss regularization terms to set curatorial constraints. Our optimization program allocates artwork to public spaces and walls in a way that de-prioritizes "in-group" preferences, by satisfying minimum representation and **exposure** criteria. We draw on existing literature to develop a fairness metric for our algorithmic output. Using Tufts University as a testbed, we assess the effectiveness of our approach and discuss its potential pitfalls from both a curatorial and equity standpoint.  
### Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models. (arXiv:2207.14626v1 [cs.CV])
- Authors : Robert Legenstein
- Link : [http://arxiv.org/abs/2207.14626](http://arxiv.org/abs/2207.14626)
> ABSTRACT  :  Image **restoration** under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image **restoration** algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image **restoration** by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image **restoration**, and qualitatively show strong generalization to real-world test images.  
### StyleLight: **HDR** Panorama Generation for Lighting Estimation and Editing. (arXiv:2207.14811v1 [cs.CV])
- Authors : Guangcong Wang, Yinuo Yang, Chen Change, Ziwei Liu
- Link : [http://arxiv.org/abs/2207.14811](http://arxiv.org/abs/2207.14811)
> ABSTRACT  :  We present a new lighting estimation and editing framework to generate high-dynamic-range (**HDR**) indoor panorama lighting from a single limited field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras. Existing lighting estimation methods either directly regress lighting representation parameters or decompose this problem into LFOV-to-panorama and LDR-to-**HDR** lighting generation sub-tasks. However, due to the partial observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a scene, lighting estimation remains a challenging task. To tackle this problem, we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that integrates LDR and **HDR** panorama synthesis into a unified framework. The LDR and **HDR** panorama synthesis share a similar generator but have separate discriminators. During inference, given an LDR LFOV image, we propose a focal-masked GAN inversion method to find its latent code by the LDR panorama synthesis branch and then synthesize the **HDR** panorama by the **HDR** panorama synthesis branch. StyleLight takes LFOV-to-panorama and LDR-to-**HDR** lighting generation into a unified framework and thus greatly improves lighting estimation. Extensive experiments demonstrate that our framework achieves superior performance over state-of-the-art methods on indoor lighting estimation. Notably, StyleLight also enables intuitive lighting editing on indoor **HDR** panoramas, which is suitable for real-world applications. Code is available at https://style-light.github.io.  
### Effects of Image Size on Deep Learning. (arXiv:2101.11508v4 [cs.CV] UPDATED)
- Authors : Olivier Rukundo
- Link : [http://arxiv.org/abs/2101.11508](http://arxiv.org/abs/2101.11508)
> ABSTRACT  :  This paper presents the effects of late gadolinium **enhancement** (LGE) magnetic resonance imaging (MRI) image size on deep learning based fully automated quantification of myocardial infarction (MI). The main objective is to determine the best size for LGE MRI images in the training dataset to achieve optimal deep learning training outcomes. To determine the new size of LGE MRI images of the reference training dataset, non-extra pixel and extra pixel interpolation algorithms are used. A novel strategy based on thresholding, median filtering, and subtraction operations is introduced and applied to remove extra class labels in interpolated ground truth (GT) segmentation masks. Fully automated quantification is achieved using the expectation maximization, weighted intensity, a priori information (EWA) algorithm, and the outcome of automatic semantic segmentation of LGE-MRI images with the convolutional neural network (CNN). In the experiments, common class metrics are used to evaluate the quality of semantic segmentation with a CNN architecture of interest (U-net) against the GT segmentation. Arbitrary threshold, comparison of the sums, and sums of differences are used to estimate the relationship between semi-automatic and fully automated quantification of MI results. A close relationship between semi-automatic and fully automated quantification of MI results was more identified in the case involving the dataset of bigger LGE MRI images than in that of the dataset of smaller LGE MRI images, where quantification results based on the dataset of bigger LGE MRI images were 55.5% closer the manual or semi-automatic results while quantification results based on the dataset of smaller LGE MRI images were 22.2% closer the manual results  
### The network signature of constellation line figures. (arXiv:2110.12329v3 [cs.SI] UPDATED)
- Authors : Doina Bucur
- Link : [http://arxiv.org/abs/2110.12329](http://arxiv.org/abs/2110.12329)
> ABSTRACT  :  In traditional astronomies across the world, groups of stars in the **night** sky were linked into constellations -- symbolic representations rich in meaning and with practical roles. In some sky cultures, constellations are represented as line (or connect-the-dot) figures, which are spatial networks drawn over the fixed background of stars. We analyse 1802 line figures from 56 sky cultures spanning all continents, in terms of their network, spatial, and brightness features, and ask what associations exist between these visual features and culture type or sky region. First, an embedded map of constellations is learnt, to show clusters of line figures. We then form the network of constellations (as linked by their similarity), to study how similar cultures are by computing their assortativity (or homophily) over the network. Finally, we measure the diversity (or entropy) index for the set of constellations drawn per sky region. Our results show distinct types of line figures, and that many folk astronomies with oral traditions have widespread similarities in constellation design, which do not align with cultural ancestry. In a minority of sky regions, certain line designs appear universal, but this is not the norm: in the majority of sky regions, the line geometries are diverse.  
### StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v2 [cs.CV] UPDATED)
- Authors : Minguk Kang, Joonghyuk Shin, Jaesik Park
- Link : [http://arxiv.org/abs/2206.09479](http://arxiv.org/abs/2206.09479)
> ABSTRACT  :  Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 13 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and **Swin** Transformer). Unlike other benchmarks used in the GAN community, we train representative GANs, including BigGAN, StyleGAN2, and StyleGAN3, in a unified training pipeline and quantify generation performance with 7 evaluation metrics. The benchmark evaluates other cutting-edge generative models(e.g., StyleGAN-XL, ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations, training, and evaluation scripts with the pre-trained weights. StudioGAN is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.  
## cs.AI
---
### End-to-end View Synthesis via **NeRF** Attention. (arXiv:2207.14741v1 [cs.CV])
- Authors : Zelin Zhao, Jiaya Jia
- Link : [http://arxiv.org/abs/2207.14741](http://arxiv.org/abs/2207.14741)
> ABSTRACT  :  In this paper, we present a simple seq2seq formulation for view synthesis where we take a set of ray points as input and output colors corresponding to the rays. Directly applying a standard transformer on this seq2seq formulation has two limitations. First, the standard attention cannot successfully fit the volumetric rendering procedure, and therefore high-frequency components are missing in the synthesized views. Second, applying global attention to all rays and pixels is extremely inefficient. Inspired by the neural radiance field (**NeRF**), we propose the **NeRF** attention (**NeRF**A) to address the above problems. On the one hand, **NeRF**A considers the volumetric rendering equation as a soft feature modulation procedure. In this way, the feature modulation enhances the transformers with the **NeRF**-like inductive bias. On the other hand, **NeRF**A performs multi-stage attention to reduce the computational overhead. Furthermore, the **NeRF**A model adopts the ray and pixel transformers to learn the interactions between rays and pixels. **NeRF**A demonstrates superior performance over **NeRF** and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D. Besides, **NeRF**A establishes a new state-of-the-art under two settings: the single-scene view synthesis and the category-centric novel view synthesis. The code will be made publicly available.  
### Enhance Incomplete Utterance **Restoration** by Joint Learning Token Extraction and Text Generation. (arXiv:2204.03958v3 [cs.CL] UPDATED)
- Authors : Shumpei Inoue, Tsungwei Liu, Nguyen Hong, Tien Nguyen
- Link : [http://arxiv.org/abs/2204.03958](http://arxiv.org/abs/2204.03958)
> ABSTRACT  :  This paper introduces a model for incomplete utterance **restoration** (IUR) called JET (\textbf{J}oint learning token \textbf{E}xtraction and \textbf{T}ext generation). Different from prior studies that only work on extraction or abstraction datasets, we design a simple but effective model, working for both scenarios of IUR. Our design simulates the nature of IUR, where omitted tokens from the context contribute to **restoration**. From this, we construct a Picker that identifies the omitted tokens. To support the picker, we design two label creation methods (soft and hard labels), which can work in cases of no annotation data for the omitted tokens. The **restoration** is done by using a Generator with the help of the Picker on joint learning. Promising results on four benchmark datasets in extraction and abstraction scenarios show that our model is better than the pretrained T5 and non-generative language model methods in both rich and limited training data settings.\footnote{The code is available at \url{https://github.com/shumpei19/JET}}  
# Paper List
---
## cs.CV
---
**100** new papers in cs.CV:-) 
1. Inverse Reinforcement Learning from Diverse Third-Person Videos via Graph Abstraction. (arXiv:2207.14299v1 [cs.LG])
2. SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation. (arXiv:2207.14315v1 [cs.CV])
3. Aztec curve: proposal for a new space-filling curve. (arXiv:2207.14345v1 [cs.CV])
4. Training a universal instance segmentation network for live cell images of various cell types and imaging modalities. (arXiv:2207.14347v1 [cs.CV])
5. Eye Gaze Estimation Model Analysis. (arXiv:2207.14373v1 [cs.CV])
6. Pro-tuning: Unified Prompt Tuning for Vision Tasks. (arXiv:2207.14381v1 [cs.CV])
7. A Deep Generative Approach to Oversampling in Ptychography. (arXiv:2207.14392v1 [eess.IV])
8. Low Cost Embedded Vision System For Location And Tracking Of A Color Object. (arXiv:2207.14396v1 [cs.CV])
9. Deep learning for understanding multilabel imbalanced Chest X-ray datasets. (arXiv:2207.14408v1 [eess.IV])
10. 3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image. (arXiv:2207.14425v1 [cs.CV])
11. Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval. (arXiv:2207.14428v1 [cs.CV])
12. Graph-Based Small Bowel Path Tracking with Cylindrical Constraints. (arXiv:2207.14436v1 [cs.CV])
13. Dataset and Evaluation algorithm design for GOALS Challenge. (arXiv:2207.14447v1 [cs.CV])
14. PC-GANs: Progressive Compensation Generative Adversarial Networks for Pan-sharpening. (arXiv:2207.14451v1 [eess.IV])
15. Deep Learning-based Occluded Person Re-identification: A Survey. (arXiv:2207.14452v1 [cs.CV])
16. Neural Density-Distance Fields. (arXiv:2207.14455v1 [cs.CV])
17. Low-Complexity Loeffler DCT Approximations for Image and Video Coding. (arXiv:2207.14463v1 [eess.IV])
18. Fine-grained Retrieval Prompt Tuning. (arXiv:2207.14465v1 [cs.CV])
19. Towards Domain-agnostic Depth Completion. (arXiv:2207.14466v1 [cs.CV])
20. Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation. (arXiv:2207.14472v1 [eess.IV])
21. Centrality and Consistency: Two-Stage Clean Samples Identification for Learning with Instance-Dependent Noisy Labels. (arXiv:2207.14476v1 [cs.CV])
22. FCSN: Global Context Aware Segmentation by Learning the Fourier Coefficients of Objects in Medical Images. (arXiv:2207.14477v1 [eess.IV])
23. StyleAM: Perception-Oriented Unsupervised Domain Adaption for Non-reference Image Quality Assessment. (arXiv:2207.14489v1 [cs.CV])
24. Conservative Generator, Progressive Discriminator: Coordination of Adversaries in Few-shot Incremental Image Synthesis. (arXiv:2207.14491v1 [cs.CV])
25. Reference-Guided Texture and Structure Inference for Image Inpainting. (arXiv:2207.14498v1 [cs.CV])
26. Class-Difficulty Based Methods for Long-Tailed Visual Recognition. (arXiv:2207.14499v1 [cs.CV])
27. A Transfer Learning-Based Approach to Marine Vessel Re-Identification. (arXiv:2207.14500v1 [cs.CV])
28. GPU-accelerated SIFT-aided source identification of stabilized videos. (arXiv:2207.14507v1 [cs.CV])
29. Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder. (arXiv:2207.14508v1 [cs.CV])
30. Uncertainty-Driven Action Quality Assessment. (arXiv:2207.14513v1 [cs.CV])
31. Evaluating the Practicality of Learned Image Compression. (arXiv:2207.14524v1 [eess.IV])
32. Curriculum Learning for Data-Efficient Vision-Language Alignment. (arXiv:2207.14525v1 [cs.CV])
33. Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings. (arXiv:2207.14539v1 [cs.CV])
34. A One-Shot Reparameterization Method for Reducing the Loss of Tile Pruning on DNNs. (arXiv:2207.14545v1 [cs.CV])
35. ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise Perspective for Medical Image Segmentation. (arXiv:2207.14552v1 [cs.CV])
36. Prompting for Multi-Modal Tracking. (arXiv:2207.14571v1 [cs.CV])
37. Image Augmentation for Satellite Images. (arXiv:2207.14580v1 [cs.CV])
38. Learning Prototype via Placeholder for Zero-shot Recognition. (arXiv:2207.14581v1 [cs.CV])
39. Deep Deformable 3D Caricatures with Learned Shape Control. (arXiv:2207.14593v1 [cs.CV])
40. WISE: Whitebox Image Stylization by Example-based Learning. (arXiv:2207.14606v1 [cs.CV])
41. Computational complexity reduction of deep neural networks. (arXiv:2207.14620v1 [cs.LG])
42. A Graph Theoretic Exploration of Coronary Vascular Trees. (arXiv:2207.14624v1 [cs.DM])
43. Content-Aware Differential Privacy with Conditional Invertible Neural Networks. (arXiv:2207.14625v1 [cs.CR])
44. Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models. (arXiv:2207.14626v1 [cs.CV])
45. SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v1 [eess.IV])
46. Multimodal SuperCon: Classifier for Drivers of Deforestation in Indonesia. (arXiv:2207.14656v1 [cs.CV])
47. Matching with AffNet based rectifications. (arXiv:2207.14660v1 [cs.CV])
48. Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling. (arXiv:2207.14663v1 [eess.IV])
49. **High Dynamic Range** and Super-Resolution from Raw Image Bursts. (arXiv:2207.14671v1 [cs.CV])
50. Enhanced Laser-Scan Matching with Online Error Estimation for Highway and Tunnel Driving. (arXiv:2207.14674v1 [cs.RO])
51. Global-Local Self-Distillation for Visual Representation Learning. (arXiv:2207.14676v1 [cs.CV])
52. AlphaVC: High-Performance and Efficient Learned Video Compression. (arXiv:2207.14678v1 [cs.CV])
53. Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks. (arXiv:2207.14682v1 [cs.SD])
54. Replacing the Framingham-based equation for prediction of cardiovascular disease risk and adverse outcome by using artificial intelligence and retinal imaging. (arXiv:2207.14685v1 [eess.IV])
55. Forensic License Plate Recognition with Compression-Informed Transformers. (arXiv:2207.14686v1 [cs.CV])
56. Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding. (arXiv:2207.14698v1 [cs.CV])
57. Improving Small Lesion Segmentation in CT Scans using Intensity Distribution Supervision: Application to Small Bowel Carcinoid Tumor. (arXiv:2207.14700v1 [eess.IV])
58. Robust Quantitative Susceptibility Mapping via Approximate Message Passing. (arXiv:2207.14709v1 [eess.IV])
59. End-to-end View Synthesis via **NeRF** Attention. (arXiv:2207.14741v1 [cs.CV])
60. ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval. (arXiv:2207.14757v1 [cs.CV])
61. Image Quality Assessment: Integrating Model-Centric and Data-Centric Approaches. (arXiv:2207.14769v1 [cs.CV])
62. Open-radiomics: A Research Protocol to Make Radiomics-based Machine Learning Pipelines Reproducible. (arXiv:2207.14776v1 [q-bio.QM])
63. Using Multi-modal Data for Improving Generalizability and Explainability of Disease Classification in Radiology. (arXiv:2207.14781v1 [cs.CV])
64. Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion. (arXiv:2207.14782v1 [cs.CV])
65. Recognition of Handwritten Chinese Text by Segmentation: A Segment-annotation-free Approach. (arXiv:2207.14801v1 [cs.CV])
66. Artifact Identification in X-ray Diffraction Data using Machine Learning Methods. (arXiv:2207.14804v1 [eess.IV])
67. PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition. (arXiv:2207.14807v1 [cs.CV])
68. StyleLight: **HDR** Panorama Generation for Lighting Estimation and Editing. (arXiv:2207.14811v1 [cs.CV])
69. GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond. (arXiv:2207.14812v1 [cs.CV])
70. Quantifying Data Augmentation for LiDAR based 3D Object Detection. (arXiv:2004.01643v2 [cs.CV] UPDATED)
71. Effects of Image Size on Deep Learning. (arXiv:2101.11508v4 [cs.CV] UPDATED)
72. Domain Generalization: A Survey. (arXiv:2103.02503v6 [cs.LG] UPDATED)
73. Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations. (arXiv:2103.06384v2 [eess.IV] UPDATED)
74. Cross-Subject Domain Adaptation for Classifying Working Memory Load with Multi-Frame EEG Images. (arXiv:2106.06769v2 [cs.LG] UPDATED)
75. Dive into Deep Learning. (arXiv:2106.11342v3 [cs.LG] UPDATED)
76. Deep Motion Prior for Weakly-Supervised Temporal Action Localization. (arXiv:2108.05607v2 [cs.CV] UPDATED)
77. Reproducible radiomics through automated machine learning validated on twelve clinical applications. (arXiv:2108.08618v2 [eess.IV] UPDATED)
78. Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v6 [cs.CV] UPDATED)
79. Robust Framework for COVID-19 Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v3 [eess.IV] UPDATED)
80. Weakly Supervised Visual-Auditory Fixation Prediction with Multigranularity Perception. (arXiv:2112.13697v4 [cs.CV] UPDATED)
81. Detecting Twenty-thousand Classes using Image-level Supervision. (arXiv:2201.02605v3 [cs.CV] UPDATED)
82. Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v4 [eess.IV] UPDATED)
83. ChimeraMix: Image Classification on Small Datasets via Masked Feature Mixing. (arXiv:2202.11616v2 [cs.CV] UPDATED)
84. P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose Estimation. (arXiv:2203.07628v2 [cs.CV] UPDATED)
85. CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images. (arXiv:2203.08138v3 [cs.CV] UPDATED)
86. Revisiting a kNN-based Image Classification System with High-capacity Storage. (arXiv:2204.01186v2 [cs.CV] UPDATED)
87. Exploring Event Camera-based Odometry for Planetary Robots. (arXiv:2204.05880v2 [cs.CV] UPDATED)
88. Language-Grounded Indoor 3D Semantic Segmentation in the Wild. (arXiv:2204.07761v2 [cs.CV] UPDATED)
89. Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing. (arXiv:2204.11573v3 [cs.CV] UPDATED)
90. Deep learning on rail profiles matching. (arXiv:2205.08687v2 [cs.CV] UPDATED)
91. StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v2 [cs.CV] UPDATED)
92. A Comparative Study of Graph Matching Algorithms in Computer Vision. (arXiv:2207.00291v2 [cs.CV] UPDATED)
93. Multiview Detection with Cardboard Human Modeling. (arXiv:2207.02013v3 [cs.CV] UPDATED)
94. Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v3 [cs.CV] UPDATED)
95. Learning-based Monocular 3D Reconstruction of Birds: A Contemporary Survey. (arXiv:2207.04512v2 [cs.CV] UPDATED)
96. Tackling Background Distraction in Video Object Segmentation. (arXiv:2207.06953v2 [cs.CV] UPDATED)
97. Prototype-Guided Continual Adaptation for Class-Incremental Unsupervised Domain Adaptation. (arXiv:2207.10856v2 [cs.CV] UPDATED)
98. Generative Steganography Network. (arXiv:2207.13867v2 [cs.CV] UPDATED)
99. Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v2 [cs.CV] UPDATED)
100. Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images. (arXiv:2207.14067v1 [cs.CV] CROSS LISTED)
## eess.IV
---
**27** new papers in eess.IV:-) 
1. A Deep Generative Approach to Oversampling in Ptychography. (arXiv:2207.14392v1 [eess.IV])
2. Low Cost Embedded Vision System For Location And Tracking Of A Color Object. (arXiv:2207.14396v1 [cs.CV])
3. Deep learning for understanding multilabel imbalanced Chest X-ray datasets. (arXiv:2207.14408v1 [eess.IV])
4. Graph-Based Small Bowel Path Tracking with Cylindrical Constraints. (arXiv:2207.14436v1 [cs.CV])
5. PC-GANs: Progressive Compensation Generative Adversarial Networks for Pan-sharpening. (arXiv:2207.14451v1 [eess.IV])
6. Low-Complexity Loeffler DCT Approximations for Image and Video Coding. (arXiv:2207.14463v1 [eess.IV])
7. Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation. (arXiv:2207.14472v1 [eess.IV])
8. FCSN: Global Context Aware Segmentation by Learning the Fourier Coefficients of Objects in Medical Images. (arXiv:2207.14477v1 [eess.IV])
9. GPU-accelerated SIFT-aided source identification of stabilized videos. (arXiv:2207.14507v1 [cs.CV])
10. Evaluating the Practicality of Learned Image Compression. (arXiv:2207.14524v1 [eess.IV])
11. Image Augmentation for Satellite Images. (arXiv:2207.14580v1 [cs.CV])
12. SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v1 [eess.IV])
13. Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling. (arXiv:2207.14663v1 [eess.IV])
14. **High Dynamic Range** and Super-Resolution from Raw Image Bursts. (arXiv:2207.14671v1 [cs.CV])
15. Replacing the Framingham-based equation for prediction of cardiovascular disease risk and adverse outcome by using artificial intelligence and retinal imaging. (arXiv:2207.14685v1 [eess.IV])
16. Improving Small Lesion Segmentation in CT Scans using Intensity Distribution Supervision: Application to Small Bowel Carcinoid Tumor. (arXiv:2207.14700v1 [eess.IV])
17. Robust Quantitative Susceptibility Mapping via Approximate Message Passing. (arXiv:2207.14709v1 [eess.IV])
18. Artifact Identification in X-ray Diffraction Data using Machine Learning Methods. (arXiv:2207.14804v1 [eess.IV])
19. Quantifying Data Augmentation for LiDAR based 3D Object Detection. (arXiv:2004.01643v2 [cs.CV] UPDATED)
20. Effects of Image Size on Deep Learning. (arXiv:2101.11508v4 [cs.CV] UPDATED)
21. Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations. (arXiv:2103.06384v2 [eess.IV] UPDATED)
22. Cross-Subject Domain Adaptation for Classifying Working Memory Load with Multi-Frame EEG Images. (arXiv:2106.06769v2 [cs.LG] UPDATED)
23. Reproducible radiomics through automated machine learning validated on twelve clinical applications. (arXiv:2108.08618v2 [eess.IV] UPDATED)
24. Robust Framework for COVID-19 Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v3 [eess.IV] UPDATED)
25. Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v4 [eess.IV] UPDATED)
26. StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v2 [cs.CV] UPDATED)
27. A Transformer-based Generative Adversarial Network for Brain Tumor Segmentation. (arXiv:2207.14134v2 [eess.IV] UPDATED)
## cs.LG
---
**135** new papers in cs.LG:-) 
1. Physics-Informed Neural Networks for Shell Structures. (arXiv:2207.14291v1 [cs.CE])
2. Image sensing with multilayer, nonlinear optical neural networks. (arXiv:2207.14293v1 [physics.optics])
3. Learning Personalized Representations using Graph Convolutional Network. (arXiv:2207.14298v1 [cs.LG])
4. Inverse Reinforcement Learning from Diverse Third-Person Videos via Graph Abstraction. (arXiv:2207.14299v1 [cs.LG])
5. Supplementing Recurrent Neural Network Wave Functions with Symmetry and Annealing to Improve Accuracy. (arXiv:2207.14314v1 [cond-mat.dis-nn])
6. Quantum Data Center: Theories and Applications. (arXiv:2207.14336v1 [quant-ph])
7. Training a universal instance segmentation network for live cell images of various cell types and imaging modalities. (arXiv:2207.14347v1 [cs.CV])
8. Bridging the Gap between Deep Learning and Hypothesis-Driven Analysis via Permutation Testing. (arXiv:2207.14349v1 [cs.LG])
9. Multiple Attribute Fairness: Application to Fraud Detection. (arXiv:2207.14355v1 [cs.LG])
10. Topological structure of complex predictions. (arXiv:2207.14358v1 [cs.LG])
11. Semi-supervised Learning of Partial Differential Operators and Dynamical Flows. (arXiv:2207.14366v1 [cs.LG])
12. A Recommender System for Equitable Public Art Curation and Installation. (arXiv:2207.14367v1 [cs.IR])
13. Model selection with Gini indices under auto-calibration. (arXiv:2207.14372v1 [cs.LG])
14. Expanding the class of global objective functions for dissimilarity-based hierarchical clustering. (arXiv:2207.14375v1 [cs.LG])
15. Latent Properties of Lifelong Learning Systems. (arXiv:2207.14378v1 [cs.LG])
16. Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v1 [cs.CL])
17. Model Reduction for Nonlinear Systems by Balanced Truncation of State and Gradient Covariance. (arXiv:2207.14387v1 [eess.SY])
18. Distributed Stochastic Bandit Learning with Context Distributions. (arXiv:2207.14391v1 [cs.LG])
19. A Deep Generative Approach to Oversampling in Ptychography. (arXiv:2207.14392v1 [eess.IV])
20. Sequential Models in the Synthetic Data Vault. (arXiv:2207.14406v1 [cs.LG])
21. Deep learning for understanding multilabel imbalanced Chest X-ray datasets. (arXiv:2207.14408v1 [eess.IV])
22. Sample-efficient Safe Learning for Online Nonlinear Control with Control Barrier Functions. (arXiv:2207.14419v1 [cs.RO])
23. Learning idempotent representation for subspace clustering. (arXiv:2207.14431v1 [cs.LG])
24. Lower bounds for learning quantum states with single-copy measurements. (arXiv:2207.14438v1 [quant-ph])
25. A Survey of Learning on Small Data. (arXiv:2207.14443v1 [cs.LG])
26. GTrans: Grouping and Fusing Transformer Layers for Neural Machine Translation. (arXiv:2207.14467v1 [cs.CL])
27. Significant changes in EEG neural oscillations during different phases of three-dimensional multiple object tracking task (3D-MOT) imply different roles for attention and working memory. (arXiv:2207.14470v1 [q-bio.NC])
28. Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation. (arXiv:2207.14472v1 [eess.IV])
29. Adaptive Gradient Methods at the Edge of Stability. (arXiv:2207.14484v1 [cs.LG])
30. SHAP for additively modeled features in a boosted trees model. (arXiv:2207.14490v1 [stat.ML])
31. Language Models Can Teach Themselves to Program Better. (arXiv:2207.14502v1 [cs.LG])
32. Factorizable Joint Shift in Multinomial Classification. (arXiv:2207.14514v1 [stat.ML])
33. Curriculum Learning for Data-Efficient Vision-Language Alignment. (arXiv:2207.14525v1 [cs.CV])
34. Leveraging Explanations in Interactive Machine Learning: An Overview. (arXiv:2207.14526v1 [cs.LG])
35. Conditioning Normalizing Flows for Rare Event Sampling. (arXiv:2207.14530v1 [physics.comp-ph])
36. Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings. (arXiv:2207.14539v1 [cs.CV])
37. Effectiveness of Transformer Models on IoT Security Detection in StackOverflow Discussions. (arXiv:2207.14542v1 [cs.CR])
38. A One-Shot Reparameterization Method for Reducing the Loss of Tile Pruning on DNNs. (arXiv:2207.14545v1 [cs.CV])
39. Best-of-Both-Worlds Algorithms for Partial Monitoring. (arXiv:2207.14550v1 [cs.LG])
40. Reweighted Manifold Learning of Collective Variables from Enhanced Sampling Simulations. (arXiv:2207.14554v1 [physics.chem-ph])
41. Cyclic Policy Distillation: Sample-Efficient Sim-to-Real Reinforcement Learning with Domain Randomization. (arXiv:2207.14561v1 [cs.RO])
42. Image Augmentation for Satellite Images. (arXiv:2207.14580v1 [cs.CV])
43. Decentralized Machine Learning for Intelligent Health Care Systems on the Computing Continuum. (arXiv:2207.14584v1 [cs.DC])
44. Stochastic Parallelizable Eigengap Dilation for Large Graph Clustering. (arXiv:2207.14589v1 [stat.ML])
45. Deep Reinforcement Learning for System-on-Chip: Myths and Realities. (arXiv:2207.14595v1 [cs.LG])
46. Archaeology of random recursive dags and Cooper-Frieze random networks. (arXiv:2207.14601v1 [math.PR])
47. KG-NSF: Knowledge Graph Completion with a Negative-Sample-Free Approach. (arXiv:2207.14617v1 [cs.LG])
48. Computational complexity reduction of deep neural networks. (arXiv:2207.14620v1 [cs.LG])
49. Content-Aware Differential Privacy with Conditional Invertible Neural Networks. (arXiv:2207.14625v1 [cs.CR])
50. Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models. (arXiv:2207.14626v1 [cs.CV])
51. Towards Communication-efficient Vertical Federated Learning Training via Cache-enabled Local Updates. (arXiv:2207.14628v1 [cs.LG])
52. Building Trust: Lessons from the Technion-Rambam Machine Learning in Healthcare Datathon Event. (arXiv:2207.14638v1 [cs.DB])
53. Subtype-Former: a deep learning approach for cancer subtype discovery with multi-omics data. (arXiv:2207.14639v1 [cs.LG])
54. EmoSens: Emotion Recognition based on Sensor data analysis using LightGBM. (arXiv:2207.14640v1 [cs.HC])
55. Active Distribution System Coordinated Control Method via Artificial Intelligence. (arXiv:2207.14642v1 [eess.SY])
56. Open World Learning Graph Convolution for Latency Estimation in Routing Networks. (arXiv:2207.14643v1 [cs.NI])
57. Using Graph Neural Networks for Program Termination. (arXiv:2207.14648v1 [cs.SE])
58. SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v1 [eess.IV])
59. Ensemble forecasts in reproducing kernel Hilbert space family: dynamical systems in Wonderland. (arXiv:2207.14653v1 [math-ph])
60. Multimodal SuperCon: Classifier for Drivers of Deforestation in Indonesia. (arXiv:2207.14656v1 [cs.CV])
61. Big Data and Analytics Implementation in Tertiary Institutions to Predict Students Performance in Nigeria. (arXiv:2207.14677v1 [cs.CY])
62. Replacing the Framingham-based equation for prediction of cardiovascular disease risk and adverse outcome by using artificial intelligence and retinal imaging. (arXiv:2207.14685v1 [eess.IV])
63. A Data-driven Latent Semantic Analysis for Automatic Text Summarization using LDA Topic Modelling. (arXiv:2207.14687v1 [cs.IR])
64. Design Methodology for Deep Out-of-Distribution Detectors in Real-Time Cyber-Physical Systems. (arXiv:2207.14694v1 [cs.LG])
65. BiFeat: Supercharge GNN Training via Graph Feature Quantization. (arXiv:2207.14696v1 [cs.LG])
66. Automatic Reward Design via Learning Motivation-Consistent Intrinsic Rewards. (arXiv:2207.14722v1 [cs.LG])
67. Meta Reinforcement Learning with Successor Feature Based Context. (arXiv:2207.14723v1 [cs.LG])
68. Tangential Wasserstein Projections. (arXiv:2207.14727v1 [stat.ML])
69. Graph Neural Networks for Channel Decoding. (arXiv:2207.14742v1 [cs.IT])
70. Open-radiomics: A Research Protocol to Make Radiomics-based Machine Learning Pipelines Reproducible. (arXiv:2207.14776v1 [q-bio.QM])
71. Using Multi-modal Data for Improving Generalizability and Explainability of Disease Classification in Radiology. (arXiv:2207.14781v1 [cs.CV])
72. Encoder-Decoder Architecture for 3D Seismic Inversion. (arXiv:2207.14789v1 [physics.geo-ph])
73. Personalized Promotion Decision Making Based on Direct and Enduring Effect Predictions. (arXiv:2207.14798v1 [cs.IR])
74. A Hybrid Complex-valued Neural Network Framework with Applications to Electroencephalogram (EEG). (arXiv:2207.14799v1 [cs.LG])
75. Contrastive UCB: Provably Efficient Contrastive Self-Supervised Learning in Online Reinforcement Learning. (arXiv:2207.14800v1 [cs.LG])
76. Artifact Identification in X-ray Diffraction Data using Machine Learning Methods. (arXiv:2207.14804v1 [eess.IV])
77. StyleLight: **HDR** Panorama Generation for Lighting Estimation and Editing. (arXiv:2207.14811v1 [cs.CV])
78. Quantifying Data Augmentation for LiDAR based 3D Object Detection. (arXiv:2004.01643v2 [cs.CV] UPDATED)
79. Conformal Prediction: a Unified Review of Theory and New Challenges. (arXiv:2005.07972v2 [cs.LG] UPDATED)
80. Can We Mitigate Backdoor Attack Using Adversarial Detection Methods?. (arXiv:2006.14871v2 [cs.LG] UPDATED)
81. Cluster-Specific Predictions with Multi-Task Gaussian Processes. (arXiv:2011.07866v3 [cs.LG] UPDATED)
82. Recursive Importance Sketching for Rank Constrained Least Squares: Algorithms and High-order Convergence. (arXiv:2011.08360v3 [math.OC] UPDATED)
83. Leveraging Expert Consistency to Improve Algorithmic Decision Support. (arXiv:2101.09648v2 [cs.LG] UPDATED)
84. Effects of Image Size on Deep Learning. (arXiv:2101.11508v4 [cs.CV] UPDATED)
85. Domain Generalization: A Survey. (arXiv:2103.02503v6 [cs.LG] UPDATED)
86. Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations. (arXiv:2103.06384v2 [eess.IV] UPDATED)
87. A deep learning approach to data-driven model-free pricing and to martingale optimal transport. (arXiv:2103.11435v2 [q-fin.CP] UPDATED)
88. Deep Learning for Bayesian Optimization of Scientific Problems with High-Dimensional Structure. (arXiv:2104.11667v3 [cs.LG] UPDATED)
89. Cross-Subject Domain Adaptation for Classifying Working Memory Load with Multi-Frame EEG Images. (arXiv:2106.06769v2 [cs.LG] UPDATED)
90. Spliced Binned-Pareto Distribution for Robust Modeling of Heavy-tailed Time Series. (arXiv:2106.10952v2 [stat.ML] UPDATED)
91. Dive into Deep Learning. (arXiv:2106.11342v3 [cs.LG] UPDATED)
92. POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems. (arXiv:2106.13867v4 [eess.SY] UPDATED)
93. Graphing else matters: exploiting aspect opinions and ratings in explainable graph-based recommendations. (arXiv:2107.03226v2 [cs.IR] UPDATED)
94. Rating and aspect-based opinion graph embeddings for explainable recommendations. (arXiv:2107.03385v2 [cs.IR] UPDATED)
95. Reservoir Computing with Diverse Timescales for Prediction of Multiscale Dynamics. (arXiv:2108.09446v2 [cs.LG] UPDATED)
96. Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v6 [cs.CV] UPDATED)
97. Robust Framework for COVID-19 Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v3 [eess.IV] UPDATED)
98. The network signature of constellation line figures. (arXiv:2110.12329v3 [cs.SI] UPDATED)
99. SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech. (arXiv:2111.10367v3 [cs.CL] UPDATED)
100. Blockchain-enabled Server-less Federated Learning. (arXiv:2112.07938v2 [cs.LG] UPDATED)
101. Continual Learning for Monolingual End-to-End Automatic Speech Recognition. (arXiv:2112.09427v3 [eess.AS] UPDATED)
102. Differentially Private SGDA for Minimax Problems. (arXiv:2201.09046v4 [cs.LG] UPDATED)
103. Validating Causal Inference Methods. (arXiv:2202.04208v5 [stat.ME] UPDATED)
104. Quantum Deep Reinforcement Learning for Robot Navigation Tasks. (arXiv:2202.12180v2 [cs.RO] UPDATED)
105. Cloud-Edge Training Architecture for Sim-to-Real Deep Reinforcement Learning. (arXiv:2203.02230v2 [cs.LG] UPDATED)
106. Regularized Deep Signed Distance Fields for Reactive Motion Generation. (arXiv:2203.04739v2 [cs.RO] UPDATED)
107. CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images. (arXiv:2203.08138v3 [cs.CV] UPDATED)
108. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v4 [cs.IR] UPDATED)
109. "FIJO": a French Insurance Soft Skill Detection Dataset. (arXiv:2204.05208v2 [cs.CL] UPDATED)
110. Email Spam Detection Using Hierarchical Attention Hybrid Deep Learning Method. (arXiv:2204.07390v2 [cs.CL] UPDATED)
111. A Learned Index for Exact Similarity Search in Metric Spaces. (arXiv:2204.10028v2 [cs.DB] UPDATED)
112. Learning Coulomb Diamonds in Large Quantum Dot Arrays. (arXiv:2205.01443v2 [cond-mat.mes-hall] UPDATED)
113. GreenDB: Toward a Product-by-Product Sustainability Database. (arXiv:2205.02908v2 [cs.LG] UPDATED)
114. Deep Learning-Based Synchronization for Uplink NB-IoT. (arXiv:2205.10805v2 [cs.IT] UPDATED)
115. Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods. (arXiv:2205.13602v2 [stat.ME] UPDATED)
116. Parameter Efficient Diff Pruning for Bias Mitigation. (arXiv:2205.15171v2 [cs.LG] UPDATED)
117. Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent. (arXiv:2206.02617v4 [cs.LG] UPDATED)
118. Port-Hamiltonian Neural Networks with State-Dependent Ports. (arXiv:2206.02660v2 [cs.LG] UPDATED)
119. Multi-channel neural networks for predicting influenza A virus hosts and antigenic types. (arXiv:2206.03823v3 [q-bio.QM] UPDATED)
120. StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v2 [cs.CV] UPDATED)
121. A geometric framework for outlier detection in high-dimensional data. (arXiv:2207.00367v2 [stat.ML] UPDATED)
122. Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v3 [cs.CV] UPDATED)
123. Interactive Recommendations for Optimal Allocations in Markets with Constraints. (arXiv:2207.04143v2 [cs.LG] UPDATED)
124. Federated Learning for Non-IID Data via Client Variance Reduction and Adaptive Server Update. (arXiv:2207.08391v2 [cs.LG] UPDATED)
125. Gauge-equivariant flow models for sampling in lattice field theories with pseudofermions. (arXiv:2207.08945v2 [hep-lat] UPDATED)
126. UniHPF : Universal Healthcare Predictive Framework with Zero Domain Knowledge. (arXiv:2207.09858v2 [cs.LG] UPDATED)
127. Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation. (arXiv:2207.09860v3 [cs.AI] UPDATED)
128. GreenDB -- A Dataset and Benchmark for Extraction of Sustainability Information of Consumer Goods. (arXiv:2207.10733v2 [cs.LG] UPDATED)
129. Multiple Robust Learning for Recommendation. (arXiv:2207.10796v2 [cs.IR] UPDATED)
130. Revisiting Parameter Reuse to Overcome Catastrophic Forgetting in Neural Networks. (arXiv:2207.11005v2 [cs.LG] UPDATED)
131. A novel Deep Learning approach for one-step Conformal Prediction approximation. (arXiv:2207.12377v3 [cs.LG] UPDATED)
132. VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations. (arXiv:2207.13091v3 [cs.GR] UPDATED)
133. Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v2 [cs.CV] UPDATED)
134. A Transformer-based Generative Adversarial Network for Brain Tumor Segmentation. (arXiv:2207.14134v2 [eess.IV] UPDATED)
135. Unsupervised Discovery of Inertial-Fusion Plasma Physics using Differentiable Kinetic Simulations and a Maximum Entropy Loss Function. (arXiv:2206.01637v2 [physics.plasm-ph] CROSS LISTED)
## cs.AI
---
**52** new papers in cs.AI:-) 
1. Knowledge-Driven Mechanistic Enrichment of the Preeclampsia Ignorome. (arXiv:2207.14294v1 [q-bio.GN])
2. Learning Personalized Representations using Graph Convolutional Network. (arXiv:2207.14298v1 [cs.LG])
3. Self-Supervised Hypergraph Transformer for Recommender Systems. (arXiv:2207.14338v1 [cs.IR])
4. Multiple Attribute Fairness: Application to Fraud Detection. (arXiv:2207.14355v1 [cs.LG])
5. Improving Few-shot News Recommendation via Cross-lingual Transfer. (arXiv:2207.14370v1 [cs.IR])
6. Latent Properties of Lifelong Learning Systems. (arXiv:2207.14378v1 [cs.LG])
7. Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v1 [cs.CL])
8. LAD: Language Models as Data for Zero-Shot Dialog. (arXiv:2207.14393v1 [cs.CL])
9. RCA: Ride Comfort-Aware Visual Navigation via Self-Supervised Learning. (arXiv:2207.14460v1 [cs.RO])
10. Class-Difficulty Based Methods for Long-Tailed Visual Recognition. (arXiv:2207.14499v1 [cs.CV])
11. Language Models Can Teach Themselves to Program Better. (arXiv:2207.14502v1 [cs.LG])
12. Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder. (arXiv:2207.14508v1 [cs.CV])
13. Curriculum Learning for Data-Efficient Vision-Language Alignment. (arXiv:2207.14525v1 [cs.CV])
14. SERCNN: Stacked Embedding Recurrent Convolutional Neural Network in Detecting Depression on Twitter. (arXiv:2207.14535v1 [cs.AI])
15. Image Augmentation for Satellite Images. (arXiv:2207.14580v1 [cs.CV])
16. Decentralized Machine Learning for Intelligent Health Care Systems on the Computing Continuum. (arXiv:2207.14584v1 [cs.DC])
17. Deep Reinforcement Learning for System-on-Chip: Myths and Realities. (arXiv:2207.14595v1 [cs.LG])
18. KG-NSF: Knowledge Graph Completion with a Negative-Sample-Free Approach. (arXiv:2207.14617v1 [cs.LG])
19. Detecting Spam Reviews on Vietnamese E-commerce Websites. (arXiv:2207.14636v1 [cs.CL])
20. Open World Learning Graph Convolution for Latency Estimation in Routing Networks. (arXiv:2207.14643v1 [cs.NI])
21. Using Graph Neural Networks for Program Termination. (arXiv:2207.14648v1 [cs.SE])
22. Big Data and Analytics Implementation in Tertiary Institutions to Predict Students Performance in Nigeria. (arXiv:2207.14677v1 [cs.CY])
23. Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks. (arXiv:2207.14682v1 [cs.SD])
24. Replacing the Framingham-based equation for prediction of cardiovascular disease risk and adverse outcome by using artificial intelligence and retinal imaging. (arXiv:2207.14685v1 [eess.IV])
25. Forensic License Plate Recognition with Compression-Informed Transformers. (arXiv:2207.14686v1 [cs.CV])
26. Automatic Reward Design via Learning Motivation-Consistent Intrinsic Rewards. (arXiv:2207.14722v1 [cs.LG])
27. Meta Reinforcement Learning with Successor Feature Based Context. (arXiv:2207.14723v1 [cs.LG])
28. Rating the Crisis of Online Public Opinion Using a Multi-Level Index System. (arXiv:2207.14740v1 [cs.SI])
29. End-to-end View Synthesis via **NeRF** Attention. (arXiv:2207.14741v1 [cs.CV])
30. Enhanced Methods for the Weight Constrained Shortest Path Problem: Constrained Path Finding Meets Bi-objective Search. (arXiv:2207.14744v1 [cs.AI])
31. ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval. (arXiv:2207.14757v1 [cs.CV])
32. SimCURL: Simple Contrastive User Representation Learning from Command Sequences. (arXiv:2207.14760v1 [cs.AI])
33. Combining Evolutionary Search with Behaviour Cloning for Procedurally Generated Content. (arXiv:2207.14772v1 [cs.AI])
34. Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion. (arXiv:2207.14782v1 [cs.CV])
35. Philosophy-Guided Mathematical Formalism for Complex Systems Modelling. (arXiv:2005.01192v5 [cs.NE] UPDATED)
36. Domain Generalization: A Survey. (arXiv:2103.02503v6 [cs.LG] UPDATED)
37. Dive into Deep Learning. (arXiv:2106.11342v3 [cs.LG] UPDATED)
38. Graphing else matters: exploiting aspect opinions and ratings in explainable graph-based recommendations. (arXiv:2107.03226v2 [cs.IR] UPDATED)
39. Rating and aspect-based opinion graph embeddings for explainable recommendations. (arXiv:2107.03385v2 [cs.IR] UPDATED)
40. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v4 [cs.IR] UPDATED)
41. Enhance Incomplete Utterance **Restoration** by Joint Learning Token Extraction and Text Generation. (arXiv:2204.03958v3 [cs.CL] UPDATED)
42. A First Runtime Analysis of the NSGA-II on a Multimodal Problem. (arXiv:2204.13750v3 [cs.NE] UPDATED)
43. Multi-Phase Multi-Objective Dexterous Manipulation with Adaptive Hierarchical Curriculum. (arXiv:2205.13441v2 [cs.RO] UPDATED)
44. Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v3 [cs.CV] UPDATED)
45. Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation. (arXiv:2207.09860v3 [cs.AI] UPDATED)
46. CheckINN: Wide Range Neural Network Verification in Imandra (Extended). (arXiv:2207.10562v2 [cs.LO] UPDATED)
47. Revisiting Parameter Reuse to Overcome Catastrophic Forgetting in Neural Networks. (arXiv:2207.11005v2 [cs.LG] UPDATED)
48. VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations. (arXiv:2207.13091v3 [cs.GR] UPDATED)
49. Knowing Where and What: Unified Word Block Pretraining for Document Understanding. (arXiv:2207.13979v2 [cs.CL] UPDATED)
50. Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v2 [cs.CV] UPDATED)
51. Entity Type Prediction Leveraging Graph Walks and Entity Descriptions. (arXiv:2207.14094v2 [cs.CL] UPDATED)
52. Lesion detection in contrast enhanced spectral mammography. (arXiv:2207.09692v1 [eess.IV] CROSS LISTED)

