# Your interest papers
---
## cs.CV
---
### OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation. (arXiv:2201.07309v1 [cs.CV])
- Authors : Qiao Gu, Brian Okorn, David Held
- Link : [http://arxiv.org/abs/2201.07309](http://arxiv.org/abs/2201.07309)
> ABSTRACT  :  **Real-time** object pose estimation is necessary for many robot manipulation algorithms. However, state-of-the-art methods for object pose estimation are trained for a specific set of objects; these methods thus need to be retrained to estimate the pose of each new object, often requiring tens of GPU-days of training for optimal performance. \revisef{In this paper, we propose the OSSID framework,} leveraging a slow zero-shot pose estimator to self-supervise the training of a fast detection algorithm. This fast detector can then be used to filter the input to the pose estimator, drastically improving its inference speed. We show that this self-supervised training exceeds the performance of existing zero-shot detection methods on two widely used object pose estimation and detection datasets, without requiring any human annotations. Further, we show that the resulting method for pose estimation has a significantly faster inference speed, due to the ability to filter out large parts of the image. Thus, our method for self-supervised online learning of a detector (trained using pseudo-labels from a slow pose estimator) leads to accurate pose estimation at real-time speeds, without requiring human annotations. Supplementary materials and code can be found at https://georgegu1997.github.io/OSSID/  
### **Swin**-Pose: **Swin** Transformer Based Human Pose Estimation. (arXiv:2201.07384v1 [cs.CV])
- Authors : Zinan Xiong, Chenxi Wang, Ying Li, Yan Luo, Yu Cao
- Link : [http://arxiv.org/abs/2201.07384](http://arxiv.org/abs/2201.07384)
> ABSTRACT  :  Convolutional neural networks (CNNs) have been widely utilized in many computer vision tasks. However, CNNs have a fixed reception field and lack the ability of long-range perception, which is crucial to human pose estimation. Due to its capability to capture long-range dependencies between pixels, transformer architecture has been adopted to computer vision applications recently and is proven to be a highly effective architecture. We are interested in exploring its capability in human pose estimation, and thus propose a novel model based on transformer architecture, enhanced with a feature pyramid fusion structure. More specifically, we use pre-trained **Swin** Transformer as our backbone and extract features from input images, we leverage a feature pyramid structure to extract feature maps from different stages. By fusing the features together, our model predicts the keypoint heatmap. The experiment results of our study have demonstrated that the proposed transformer-based model can achieve better performance compared to the state-of-the-art CNN-based models.  
### KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition. (arXiv:2201.07394v1 [cs.CV])
- Authors : Chingis Oinar
- Link : [http://arxiv.org/abs/2201.07394](http://arxiv.org/abs/2201.07394)
> ABSTRACT  :  Feature learning is a widely used method employed for large-scale face recognition. Recently, large-margin softmax loss methods have demonstrated significant **enhancement**s on deep face recognition. These methods propose fixed positive margins in order to enforce intra-class compactness and inter-class diversity. However, the majority of the proposed methods do not consider the class imbalance issue, which is a major challenge in practice for developing deep face recognition models. We hypothesize that it significantly affects the generalization ability of the deep face models. Inspired by this observation, we introduce a novel adaptive strategy, called KappaFace, to modulate the relative importance based on class difficultness and imbalance. With the support of the von Mises-Fisher distribution, our proposed KappaFace loss can intensify the margin's magnitude for hard learning or low concentration classes while relaxing it for counter classes. Experiments conducted on popular facial benchmarks demonstrate that our proposed method achieves superior performance to the state-of-the-art.  
### Self-Supervised Deep Blind Video Super-Resolution. (arXiv:2201.07422v1 [cs.CV])
- Authors : Haoran Bai, Jinshan Pan
- Link : [http://arxiv.org/abs/2201.07422](http://arxiv.org/abs/2201.07422)
> ABSTRACT  :  Existing deep learning-based video super-resolution (SR) methods usually depend on the supervised learning approach, where the training data is usually generated by the blurring operation with known or predefined kernels (e.g., Bicubic kernel) followed by a decimation operation. However, this does not hold for real applications as the degradation process is complex and cannot be approximated by these idea cases well. Moreover, obtaining high-resolution (HR) videos and the corresponding low-resolution (LR) ones in real-world scenarios is difficult. To overcome these problems, we propose a self-supervised learning method to solve the blind video SR problem, which simultaneously estimates blur kernels and HR videos from the LR videos. As directly using LR videos as supervision usually leads to trivial solutions, we develop a simple and effective method to generate auxiliary paired data from original LR videos according to the image formation of video SR, so that the networks can be better constrained by the generated paired data for both blur kernel estimation and latent HR video **restoration**. In addition, we introduce an optical flow estimation module to exploit the information from adjacent frames for HR video **restoration**. Experiments show that our method performs favorably against state-of-the-art ones on benchmarks and real-world videos.  
### TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning. (arXiv:2201.07451v1 [cs.CV])
- Authors : Linhao Qu, Shaolei Liu, Manning Wang, Shiman Li, Siqi Yin, Qin Qiao, Zhijian Song
- Link : [http://arxiv.org/abs/2201.07451](http://arxiv.org/abs/2201.07451)
> ABSTRACT  :  Image fusion is a technique to integrate information from multiple source images with complementary information to improve the richness of a single image. Due to insufficient task-specific training data and corresponding ground truth, most existing end-to-end image fusion methods easily fall into overfitting or tedious parameter optimization processes. Two-stage methods avoid the need of large amount of task-specific training data by training encoder-decoder network on large natural image datasets and utilizing the extracted features for fusion, but the domain gap between natural images and different fusion tasks results in limited performance. In this study, we design a novel encoder-decoder based image fusion framework and propose a destruction-reconstruction based self-supervised training scheme to encourage the network to learn task-specific features. Specifically, we propose three destruction-reconstruction self-supervised auxiliary tasks for multi-modal image fusion, multi-**exposure** image fusion and multi-focus image fusion based on pixel intensity non-linear transformation, brightness transformation and noise transformation, respectively. In order to encourage different fusion tasks to promote each other and increase the generalizability of the trained network, we integrate the three self-supervised auxiliary tasks by randomly choosing one of them to destroy a natural image in model training. In addition, we design a new encoder that combines CNN and Transformer for feature extraction, so that the trained model can exploit both local and global information. Extensive experiments on multi-modal image fusion, multi-**exposure** image fusion and multi-focus image fusion tasks demonstrate that our proposed method achieves the state-of-the-art performance in both subjective and objective evaluations. The code will be publicly available soon.  
### Virtual Coil Augmentation Technology for MRI via Deep Learning. (arXiv:2201.07540v1 [cs.CV])
- Authors : Cailian Yang, Xianghao Liao, Yuhao Wang, Minghui Zhang, Qiegen Liu
- Link : [http://arxiv.org/abs/2201.07540](http://arxiv.org/abs/2201.07540)
> ABSTRACT  :  Magnetic resonance imaging (MRI) is a widely used medical imaging modality. However, due to the limitations in hardware, scan time, and throughput, it is often clinically challenging to obtain high-quality MR images. In this article, we propose a method of using artificial intelligence to expand the channel to achieve the effect of increasing the virtual coil. The main feature of our work is utilizing dummy variable technology to expand the channel in both the image and k-space domains. The high-dimensional information formed by channel expansion is used as the prior information of parallel imaging to improve the reconstruction effect of parallel imaging. Two features are introduced, namely variable **enhancement** and sum of squares (SOS) objective function. Variable argumentation provides the network with more high-dimensional prior information, which is helpful for the network to extract the deep feature in-formation of the image. The SOS objective function is employed to solve the problem that k-space data is difficult to train while speeding up the convergence speed. Ablation studies and experimental results demonstrate that our method achieves significantly higher image reconstruction performance than current state-of-the-art techniques.  
### **Real-time** Recognition of Yoga Poses using computer Vision for Smart Health Care. (arXiv:2201.07594v1 [cs.CV])
- Authors : Abhishek Sharma, Yash Shah, Yash Agrawal, Prateek Jain
- Link : [http://arxiv.org/abs/2201.07594](http://arxiv.org/abs/2201.07594)
> ABSTRACT  :  Nowadays, yoga has become a part of life for many people. Exercises and sports technological assistance is implemented in yoga pose identification. In this work, a self-assistance based yoga posture identification technique is developed, which helps users to perform Yoga with the correction feature in **Real-time**. The work also presents Yoga-hand mudra (hand gestures) identification. The YOGI dataset has been developed which include 10 Yoga postures with around 400-900 images of each pose and also contain 5 mudras for identification of mudras postures. It contains around 500 images of each mudra. The feature has been extracted by making a skeleton on the body for yoga poses and hand for mudra poses. Two different algorithms have been used for creating a skeleton one for yoga poses and the second for hand mudras. Angles of the joints have been extracted as a features for different machine learning and deep learning models. among all the models XGBoost with RandomSearch CV is most accurate and gives 99.2\% accuracy. The complete design framework is described in the present paper.  
### Q-ViT: Fully Differentiable Quantization for Vision Transformer. (arXiv:2201.07703v1 [cs.CV])
- Authors : Zhexin Li, Tong Yang, Peisong Wang, Jian Cheng
- Link : [http://arxiv.org/abs/2201.07703](http://arxiv.org/abs/2201.07703)
> ABSTRACT  :  In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT, in which both of the quantization scales and bit-widths are learnable parameters. Specifically, based on our observation that heads in ViT display different quantization robustness, we leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. In addition, we propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop. Moreover, we analyze the quantization robustness of every architecture component of ViT and show that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. This study provides some insights for further research about ViT quantization. Extensive experiments on different ViT models, such as DeiT and **Swin** Transformer show the effectiveness of our quantization method. In particular, our method outperforms the state-of-the-art uniform quantization method by 1.5% on DeiT-Tiny.  
### Transformers in Vision: A Survey. (arXiv:2101.01169v5 [cs.CV] UPDATED)
- Authors : Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas, Fahad Shahbaz, Mubarak Shah
- Link : [http://arxiv.org/abs/2101.01169](http://arxiv.org/abs/2101.01169)
> ABSTRACT  :  Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image **enhancement**, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.  
### Wavelength-based Attributed Deep Neural Network for Underwater Image **Restoration**. (arXiv:2106.07910v3 [eess.IV] UPDATED)
- Authors : Prasen Kumar, Ira Bisht, Arijit Sur
- Link : [http://arxiv.org/abs/2106.07910](http://arxiv.org/abs/2106.07910)
> ABSTRACT  :  Background: Underwater images, in general, suffer from low contrast and high color distortions due to the non-uniform attenuation of the light as it propagates through the water. In addition, the degree of attenuation varies with the wavelength resulting in the asymmetric traversing of colors. Despite the prolific works for underwater image **restoration** (UIR) using deep learning, the above asymmetricity has not been addressed in the respective network engineering.    Contributions: As the first novelty, this paper shows that attributing the right receptive field size (context) based on the traversing range of the color channel may lead to a substantial performance gain for the task of UIR. Further, it is important to suppress the irrelevant multi-contextual features and increase the representational power of the model. Therefore, as a second novelty, we have incorporated an attentive skip mechanism to adaptively refine the learned multi-contextual features. The proposed framework, called Deep WaveNet, is optimized using the traditional pixel-wise and feature-based cost functions. An extensive set of experiments have been carried out to show the efficacy of the proposed scheme over existing best-published literature on benchmark datasets. More importantly, we have demonstrated a comprehensive validation of enhanced images across various high-level vision tasks, e.g., underwater image semantic segmentation, and diver's 2D pose estimation. A sample video to exhibit our real-world performance is available at \url{https://tinyurl.com/yzcrup9n}. Also, we have open-sourced our framework at \url{https://github.com/pksvision/Deep-WaveNet-UnderwaterImage-**Restoration**}.  
## eess.IV
---
### **Real-time** X-ray Phase-contrast Imaging Using SPINNet -- A Speckle-based Phase-contrast Imaging Neural Network. (arXiv:2201.07232v1 [eess.IV])
- Authors : Zhi Qiao, Xianbo Shi, Yudong Yao, Luca Rebuffi, Lahsen Assoufid
- Link : [http://arxiv.org/abs/2201.07232](http://arxiv.org/abs/2201.07232)
> ABSTRACT  :  X-ray phase-contrast imaging has become indispensable for visualizing samples with low absorption contrast. In this regard, speckle-based techniques have shown significant advantages in spatial resolution, phase sensitivity, and implementation flexibility compared with traditional methods. However, their computational cost has hindered their wider adoption. By exploiting the power of deep learning, we developed a new speckle-based phase-contrast imaging neural network (SPINNet) that boosts the phase retrieval speed by at least two orders of magnitude compared to existing methods. To achieve this performance, we combined SPINNet with a novel coded-mask-based technique, an enhanced version of the speckle-based method. Using this scheme, we demonstrate a simultaneous reconstruction of absorption and phase images on the order of 100 ms, where a traditional correlation-based analysis would take several minutes even with a cluster. In addition to significant improvement in speed, our experimental results show that the imaging resolution and phase retrieval quality of SPINNet outperform existing single-shot speckle-based methods. Furthermore, we successfully demonstrate its application in 3D X-ray phase-contrast tomography. Our result shows that SPINNet could enable many applications requiring high-resolution and fast data acquisition and processing, such as in-situ and in-operando 2D and 3D phase-contrast imaging and real-time at-wavelength metrology and wavefront sensing.  
### Accurate smartphone camera simulation using 3D scenes. (arXiv:2201.07411v1 [eess.IV])
- Authors : Zheng Lyu, Thomas Goossens, Brian Wandell, Joyce Farrell
- Link : [http://arxiv.org/abs/2201.07411](http://arxiv.org/abs/2201.07411)
> ABSTRACT  :  We assess the accuracy of a smartphone camera simulation. The simulation is an end-to-end analysis that begins with a physical description of a **high dynamic range** 3D scene and includes a specification of the optics and the image sensor. The simulation is compared to measurements of a physical version of the scene. The image system simulation accurately matched measurements of optical blur, depth of field, spectral quantum efficiency, scene inter-reflections, and sensor noise data. The results support the use of image systems simulation methods for soft prototyping cameras and for producing synthetic data in machine learning applications.  
### Wavelength-based Attributed Deep Neural Network for Underwater Image **Restoration**. (arXiv:2106.07910v3 [eess.IV] UPDATED)
- Authors : Prasen Kumar, Ira Bisht, Arijit Sur
- Link : [http://arxiv.org/abs/2106.07910](http://arxiv.org/abs/2106.07910)
> ABSTRACT  :  Background: Underwater images, in general, suffer from low contrast and high color distortions due to the non-uniform attenuation of the light as it propagates through the water. In addition, the degree of attenuation varies with the wavelength resulting in the asymmetric traversing of colors. Despite the prolific works for underwater image **restoration** (UIR) using deep learning, the above asymmetricity has not been addressed in the respective network engineering.    Contributions: As the first novelty, this paper shows that attributing the right receptive field size (context) based on the traversing range of the color channel may lead to a substantial performance gain for the task of UIR. Further, it is important to suppress the irrelevant multi-contextual features and increase the representational power of the model. Therefore, as a second novelty, we have incorporated an attentive skip mechanism to adaptively refine the learned multi-contextual features. The proposed framework, called Deep WaveNet, is optimized using the traditional pixel-wise and feature-based cost functions. An extensive set of experiments have been carried out to show the efficacy of the proposed scheme over existing best-published literature on benchmark datasets. More importantly, we have demonstrated a comprehensive validation of enhanced images across various high-level vision tasks, e.g., underwater image semantic segmentation, and diver's 2D pose estimation. A sample video to exhibit our real-world performance is available at \url{https://tinyurl.com/yzcrup9n}. Also, we have open-sourced our framework at \url{https://github.com/pksvision/Deep-WaveNet-UnderwaterImage-**Restoration**}.  
## cs.LG
---
### A Deep Learning Approach for Semantic Segmentation of Unbalanced Data in Electron Tomography of Catalytic Materials. (arXiv:2201.07342v1 [cond-mat.mtrl-sci])
- Authors : Arda Genc, Libor Kovarik
- Link : [http://arxiv.org/abs/2201.07342](http://arxiv.org/abs/2201.07342)
> ABSTRACT  :  Heterogeneous catalysts possess complex surface and bulk structures, relatively poor intrinsic contrast, and often a sparse distribution of the catalytic nanoparticles (NPs), posing a significant challenge for image segmentation, including the current state-of-the-art deep learning methods. To tackle this problem, we apply a deep learning-based approach for the multi-class semantic segmentation of a $\gamma$-Alumina/Pt catalytic material in a class imbalance situation. Specifically, we used the weighted focal loss as a loss function and attached it to the U-Net's fully convolutional network architecture. We assessed the accuracy of our results using Dice similarity coefficient (DSC), recall, precision, and Hausdorff distance (HD) metrics on the overlap between the ground-truth and predicted segmentations. Our adopted U-Net model with the weighted focal loss function achieved an average DSC score of 0.96 $\pm$ 0.003 in the $\gamma$-Alumina support material and 0.84 $\pm$ 0.03 in the Pt NPs segmentation tasks. We report an average boundary-overlap error of less than 2 nm at the 90th percentile of HD for $\gamma$-Alumina and Pt NPs segmentations. The complex surface morphology of the $\gamma$-Alumina and its relation to the Pt NPs were visualized in 3D by the deep learning-assisted automatic segmentation of a large data set of high-angle annular **dark**-field (HAADF) scanning transmission electron microscopy (STEM) tomography reconstructions.  
### ReGNL: Rapid Prediction of GDP during Disruptive Events using **Night**lights. (arXiv:2201.07612v1 [cs.LG])
- Authors : Rushabh Musthyala, Rudrajit Kargupta, Hritish Jain, Dipanjan Chakraborty
- Link : [http://arxiv.org/abs/2201.07612](http://arxiv.org/abs/2201.07612)
> ABSTRACT  :  Policy makers often make decisions based on parameters such as GDP, unemployment rate, industrial output, etc. The primary methods to obtain or even estimate such information are resource intensive and time consuming. In order to make timely and well-informed decisions, it is imperative to be able to come up with proxies for these parameters which can be sampled quickly and efficiently, especially during disruptive events, like the COVID-19 pandemic. Recently, there has been a lot of focus on using remote sensing data for this purpose. The data has become cheaper to collect compared to surveys, and can be available in **real time**. In this work, we present Regional GDP **Night**Light (ReGNL), a neural network based model which is trained on a custom dataset of historical **night**lights and GDP data along with the geographical coordinates of a place, and estimates the GDP of the place, given the other parameters. Taking the case of 50 US states, we find that ReGNL is disruption-agnostic and is able to predict the GDP for both normal years (2019) and for years with a disruptive event (2020). ReGNL outperforms timeseries ARIMA methods for prediction, even during the pandemic. Following from our findings, we make a case for building infrastructures to collect and make available granular data, especially in resource-poor geographies, so that these can be leveraged for policy making during disruptive events.  
### PECOS: Prediction for Enormous and Correlated Output Spaces. (arXiv:2010.05878v2 [cs.LG] UPDATED)
- Authors : Fu Yu, Kai Zhong, Jiong Zhang, Cheng Chang
- Link : [http://arxiv.org/abs/2010.05878](http://arxiv.org/abs/2010.05878)
> ABSTRACT  :  Many large-scale applications amount to finding relevant results from an enormous output space of potential candidates. For example, finding the best matching product from a large catalog or suggesting related search phrases on a search engine. The size of the output space for these problems can range from millions to billions, and can even be infinite in some applications. Moreover, training data is often limited for the long-tail items in the output space. Fortunately, items in the output space are often correlated thereby presenting an opportunity to alleviate the data sparsity issue. In this paper, we propose the Prediction for Enormous and Correlated Output Spaces (PECOS) framework, a versatile and modular machine learning framework for solving prediction problems for very large output spaces, and apply it to the eXtreme Multilabel Ranking (XMR) problem: given an input instance, find and rank the most relevant items from an enormous but fixed and finite output space. We propose a three phase framework for PECOS: (i) in the first phase, PECOS organizes the output space using a semantic indexing scheme, (ii) in the second phase, PECOS uses the indexing to narrow down the output space by orders of magnitude using a machine learned matching scheme, and (iii) in the third phase, PECOS ranks the matched items using a final ranking scheme. The versatility and modularity of PECOS allows for easy plug-and-play of various choices for the indexing, matching, and ranking phases. We also develop very fast inference procedures which allow us to perform XMR predictions in **real time**; for example, inference takes less than 1 millisecond per input on the dataset with 2.8 million labels. The PECOS software is available at https://libpecos.org.  
### Transformers in Vision: A Survey. (arXiv:2101.01169v5 [cs.CV] UPDATED)
- Authors : Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas, Fahad Shahbaz, Mubarak Shah
- Link : [http://arxiv.org/abs/2101.01169](http://arxiv.org/abs/2101.01169)
> ABSTRACT  :  Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image **enhancement**, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.  
## cs.AI
---
### OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation. (arXiv:2201.07309v1 [cs.CV])
- Authors : Qiao Gu, Brian Okorn, David Held
- Link : [http://arxiv.org/abs/2201.07309](http://arxiv.org/abs/2201.07309)
> ABSTRACT  :  **Real-time** object pose estimation is necessary for many robot manipulation algorithms. However, state-of-the-art methods for object pose estimation are trained for a specific set of objects; these methods thus need to be retrained to estimate the pose of each new object, often requiring tens of GPU-days of training for optimal performance. \revisef{In this paper, we propose the OSSID framework,} leveraging a slow zero-shot pose estimator to self-supervise the training of a fast detection algorithm. This fast detector can then be used to filter the input to the pose estimator, drastically improving its inference speed. We show that this self-supervised training exceeds the performance of existing zero-shot detection methods on two widely used object pose estimation and detection datasets, without requiring any human annotations. Further, we show that the resulting method for pose estimation has a significantly faster inference speed, due to the ability to filter out large parts of the image. Thus, our method for self-supervised online learning of a detector (trained using pseudo-labels from a slow pose estimator) leads to accurate pose estimation at real-time speeds, without requiring human annotations. Supplementary materials and code can be found at https://georgegu1997.github.io/OSSID/  
### Transformers in Vision: A Survey. (arXiv:2101.01169v5 [cs.CV] UPDATED)
- Authors : Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas, Fahad Shahbaz, Mubarak Shah
- Link : [http://arxiv.org/abs/2101.01169](http://arxiv.org/abs/2101.01169)
> ABSTRACT  :  Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image **enhancement**, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.  
# Paper List
---
## cs.CV
---
**64** new papers in cs.CV:-) 
1. Is Contrastive Learning Suitable for Left Ventricular Segmentation in Echocardiographic Images?. (arXiv:2201.07219v1 [eess.IV])
2. Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features. (arXiv:2201.07227v1 [eess.IV])
3. AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review. (arXiv:2201.07231v1 [eess.IV])
4. Exploring Kervolutional Neural Networks. (arXiv:2201.07264v1 [cs.CV])
5. OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation. (arXiv:2201.07309v1 [cs.CV])
6. Lung Swapping Autoencoder: Learning a Disentangled Structure-texture Representation of Chest Radiographs. (arXiv:2201.07344v1 [eess.IV])
7. Weakly Supervised Contrastive Learning for Better Severity Scoring of Lung Ultrasound. (arXiv:2201.07357v1 [eess.IV])
8. TriCoLo: Trimodal Contrastive Loss for Fine-grained Text to Shape Retrieval. (arXiv:2201.07366v1 [cs.CV])
9. The Role of Pleura and Adipose in Lung Ultrasound AI. (arXiv:2201.07368v1 [eess.IV])
10. Online Deep Learning based on Auto-Encoder. (arXiv:2201.07383v1 [cs.LG])
11. **Swin**-Pose: **Swin** Transformer Based Human Pose Estimation. (arXiv:2201.07384v1 [cs.CV])
12. KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition. (arXiv:2201.07394v1 [cs.CV])
13. Poseur: Direct Human Pose Regression with Transformers. (arXiv:2201.07412v1 [cs.CV])
14. Self-Supervised Deep Blind Video Super-Resolution. (arXiv:2201.07422v1 [cs.CV])
15. WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking. (arXiv:2201.07425v1 [cs.CV])
16. Variable Augmented Network for Invertible MR Coil Compression. (arXiv:2201.07428v1 [cs.CV])
17. Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth. (arXiv:2201.07436v1 [cs.CV])
18. TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning. (arXiv:2201.07451v1 [cs.CV])
19. Using Self-Supervised Pretext Tasks for Active Learning. (arXiv:2201.07459v1 [cs.CV])
20. High-fidelity 3D Model Compression based on Key Spheres. (arXiv:2201.07486v1 [cs.CV])
21. Weakly Supervised Semantic Segmentation of Remote Sensing Images for Tree Species Classification Based on Explanation Methods. (arXiv:2201.07495v1 [cs.CV])
22. Virtual Coil Augmentation Technology for MRI via Deep Learning. (arXiv:2201.07540v1 [cs.CV])
23. Simpler is better: spectral regularization and up-sampling techniques for variational autoencoders. (arXiv:2201.07544v1 [cs.LG])
24. Learned Cone-Beam CT Reconstruction Using Neural Ordinary Differential Equations. (arXiv:2201.07562v1 [eess.IV])
25. Superpixel Pre-Segmentation of HER2 Slides for Efficient Annotation. (arXiv:2201.07572v1 [cs.CV])
26. DMF-Net: Dual-Branch Multi-Scale Feature Fusion Network for copy forgery identification of anti-counterfeiting QR code. (arXiv:2201.07583v1 [cs.CV])
27. **Real-time** Recognition of Yoga Poses using computer Vision for Smart Health Care. (arXiv:2201.07594v1 [cs.CV])
28. A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo. (arXiv:2201.07609v1 [cs.CV])
29. Nonlinear Unknown Input Observability and Unknown Input Reconstruction: The General Analytical Solution. (arXiv:2201.07610v1 [math.OC])
30. CAST: Character labeling in Animation using Self-supervision by Tracking. (arXiv:2201.07619v1 [cs.CV])
31. A Survey on Training Challenges in Generative Adversarial Networks for Biomedical Image Analysis. (arXiv:2201.07646v1 [cs.LG])
32. Open Source Handwritten Text Recognition on Medieval Manuscripts using Mixed Models and Document-Specific Finetuning. (arXiv:2201.07661v1 [cs.CV])
33. Semi-automatic 3D Object Keypoint Annotation and Detection for the Masses. (arXiv:2201.07665v1 [cs.CV])
34. Neighborhood Spatial Aggregation MC Dropout for Efficient Uncertainty-aware Semantic Segmentation in Point Clouds. (arXiv:2201.07676v1 [cs.CV])
35. GroupGazer: A Tool to Compute the Gaze per Participant in Groups with integrated Calibration to Map the Gaze Online to a Screen or Beamer Projection. (arXiv:2201.07692v1 [cs.CV])
36. Visualization and Analysis of Wearable Health Data From COVID-19 Patients. (arXiv:2201.07698v1 [cs.HC])
37. Q-ViT: Fully Differentiable Quantization for Vision Transformer. (arXiv:2201.07703v1 [cs.CV])
38. Object Detection in Autonomous Vehicles: Status and Open Challenges. (arXiv:2201.07706v1 [cs.CV])
39. Towards holistic scene understanding: Semantic segmentation and beyond. (arXiv:2201.07734v1 [cs.CV])
40. A pipeline for automated processing of Corona KH-4 (1962-1972) stereo imagery. (arXiv:2201.07756v1 [cs.CV])
41. Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation. (arXiv:2201.07779v1 [cs.RO])
42. Towards a General Deep Feature Extractor for Facial Expression Recognition. (arXiv:2201.07781v1 [cs.CV])
43. ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes. (arXiv:2201.07788v1 [cs.CV])
44. GAMMA: A General Agent Motion Model for Autonomous Driving. (arXiv:1906.01566v5 [cs.RO] UPDATED)
45. Dilated Convolutions with Lateral Inhibitions for Semantic Image Segmentation. (arXiv:2006.03708v5 [cs.CV] UPDATED)
46. 3D Shape Reconstruction from Free-Hand Sketches. (arXiv:2006.09694v2 [cs.CV] UPDATED)
47. Towards a Plug-and-Play Fully AutomatedUnsupervised 360-Degree Deep Learning VisualDefect Detection System. (arXiv:2012.06737v2 [cs.CV] UPDATED)
48. Transformers in Vision: A Survey. (arXiv:2101.01169v5 [cs.CV] UPDATED)
49. Image Splicing Detection, Localization and Attribution via JPEG Primary Quantization Matrix Estimation and Clustering. (arXiv:2102.01439v2 [eess.IV] UPDATED)
50. Wavelength-based Attributed Deep Neural Network for Underwater Image **Restoration**. (arXiv:2106.07910v3 [eess.IV] UPDATED)
51. Self-Rule to Multi-Adapt: Generalized Multi-source Feature Learning Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection. (arXiv:2108.09178v2 [cs.CV] UPDATED)
52. Reconstructing Cosmic Polarization Rotation with ResUNet-CMB. (arXiv:2109.09715v2 [astro-ph.CO] UPDATED)
53. Automatic Map Update Using Dashcam Videos. (arXiv:2109.12131v2 [cs.CV] UPDATED)
54. Generation of microbial colonies dataset with deep learning style transfer. (arXiv:2111.03789v2 [cs.CV] UPDATED)
55. Medical Visual Question Answering: A Survey. (arXiv:2111.10056v2 [cs.CV] UPDATED)
56. Extrapolating from a Single Image to a Thousand Classes using Distillation. (arXiv:2112.00725v2 [cs.CV] UPDATED)
57. Overcoming the Domain Gap in Neural Action Representations. (arXiv:2112.01176v3 [cs.CV] UPDATED)
58. Active learning with MaskAL reduces annotation effort for training Mask R-CNN. (arXiv:2112.06586v2 [cs.CV] UPDATED)
59. When less is more: Simplifying inputs aids neural network understanding. (arXiv:2201.05610v2 [cs.LG] UPDATED)
60. GradTail: Learning Long-Tailed Data Using Gradient-based Sample Weighting. (arXiv:2201.05938v2 [cs.LG] UPDATED)
61. H&E-adversarial network: a convolutional neural network to learn stain-invariant features through Hematoxylin & Eosin regression. (arXiv:2201.06329v2 [eess.IV] UPDATED)
62. STURE: Spatial-Temporal Mutual Representation Learning for Robust Data Association in Online Multi-Object Tracking. (arXiv:2201.06824v2 [cs.CV] UPDATED)
63. RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training. (arXiv:2201.06857v2 [cs.CV] UPDATED)
64. Attentional Feature Refinement and Alignment Network for Aircraft Detection in SAR Imagery. (arXiv:2201.07124v2 [cs.CV] UPDATED)
## eess.IV
---
**15** new papers in eess.IV:-) 
1. Is Contrastive Learning Suitable for Left Ventricular Segmentation in Echocardiographic Images?. (arXiv:2201.07219v1 [eess.IV])
2. Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features. (arXiv:2201.07227v1 [eess.IV])
3. AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review. (arXiv:2201.07231v1 [eess.IV])
4. **Real-time** X-ray Phase-contrast Imaging Using SPINNet -- A Speckle-based Phase-contrast Imaging Neural Network. (arXiv:2201.07232v1 [eess.IV])
5. Lung Swapping Autoencoder: Learning a Disentangled Structure-texture Representation of Chest Radiographs. (arXiv:2201.07344v1 [eess.IV])
6. Weakly Supervised Contrastive Learning for Better Severity Scoring of Lung Ultrasound. (arXiv:2201.07357v1 [eess.IV])
7. The Role of Pleura and Adipose in Lung Ultrasound AI. (arXiv:2201.07368v1 [eess.IV])
8. Compressed Smooth Sparse Decomposition. (arXiv:2201.07404v1 [eess.IV])
9. Accurate smartphone camera simulation using 3D scenes. (arXiv:2201.07411v1 [eess.IV])
10. Cortical lesions, central vein sign, and paramagnetic rim lesions in multiple sclerosis: emerging machine learning techniques and future avenues. (arXiv:2201.07463v1 [eess.IV])
11. Learned Cone-Beam CT Reconstruction Using Neural Ordinary Differential Equations. (arXiv:2201.07562v1 [eess.IV])
12. A pipeline for automated processing of Corona KH-4 (1962-1972) stereo imagery. (arXiv:2201.07756v1 [cs.CV])
13. Image Splicing Detection, Localization and Attribution via JPEG Primary Quantization Matrix Estimation and Clustering. (arXiv:2102.01439v2 [eess.IV] UPDATED)
14. Wavelength-based Attributed Deep Neural Network for Underwater Image **Restoration**. (arXiv:2106.07910v3 [eess.IV] UPDATED)
15. H&E-adversarial network: a convolutional neural network to learn stain-invariant features through Hematoxylin & Eosin regression. (arXiv:2201.06329v2 [eess.IV] UPDATED)
## cs.LG
---
**133** new papers in cs.LG:-) 
1. Enhanced Self-Organizing Map Solution for the Traveling Salesman Problem. (arXiv:2201.07208v1 [cs.NE])
2. Efficient Training of Spiking Neural Networks with Temporally-Truncated Local Backpropagation through Time. (arXiv:2201.07210v1 [cs.NE])
3. Human-Level Control through Directly-Trained Deep Spiking Q-Networks. (arXiv:2201.07211v1 [cs.NE])
4. Layerwise Geo-Distributed Computing between Cloud and IoT. (arXiv:2201.07215v1 [cs.DC])
5. Is Contrastive Learning Suitable for Left Ventricular Segmentation in Echocardiographic Images?. (arXiv:2201.07219v1 [eess.IV])
6. Do not rug on me: Zero-dimensional Scam Detection. (arXiv:2201.07220v1 [cs.CR])
7. NSGZero: Efficiently Learning Non-Exploitable Policy in Large-Scale Network Security Games with Neural Monte Carlo Tree Search. (arXiv:2201.07224v1 [cs.CR])
8. Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features. (arXiv:2201.07227v1 [eess.IV])
9. AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review. (arXiv:2201.07231v1 [eess.IV])
10. EP-PQM: Efficient Parametric Probabilistic Quantum Memory with Fewer Qubits and Gates. (arXiv:2201.07265v1 [cs.ET])
11. TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data. (arXiv:2201.07284v1 [cs.LG])
12. Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v1 [cs.LG])
13. Sparsification of Decomposable Submodular Functions. (arXiv:2201.07289v1 [cs.DS])
14. Convergence of policy gradient for entropy regularized MDPs with neural network approximation in the mean-field regime. (arXiv:2201.07296v1 [math.OC])
15. Bregman Deviations of Generic Exponential Families. (arXiv:2201.07306v1 [cs.LG])
16. Towards Federated Clustering: A Federated Fuzzy $c$-Means Algorithm (FFCM). (arXiv:2201.07316v1 [cs.LG])
17. Interpretable Single-Cell Set Classification with Kernel Mean Embeddings. (arXiv:2201.07322v1 [cs.LG])
18. Learning grammar with a divide-and-concur neural network. (arXiv:2201.07341v1 [cs.CL])
19. A Deep Learning Approach for Semantic Segmentation of Unbalanced Data in Electron Tomography of Catalytic Materials. (arXiv:2201.07342v1 [cond-mat.mtrl-sci])
20. Learning Tensor Representations for Meta-Learning. (arXiv:2201.07348v1 [cs.LG])
21. Weakly Supervised Contrastive Learning for Better Severity Scoring of Lung Ultrasound. (arXiv:2201.07357v1 [eess.IV])
22. Sandbox Sample Classification Using Behavioral Indicators of Compromise. (arXiv:2201.07359v1 [cs.CR])
23. Prospective Learning: Back to the Future. (arXiv:2201.07372v1 [cs.LG])
24. Online Deep Learning based on Auto-Encoder. (arXiv:2201.07383v1 [cs.LG])
25. Variational Autoencoder Generative Adversarial Network for Synthetic Data Generation in Smart Home. (arXiv:2201.07387v1 [cs.LG])
26. Overview frequency principle/spectral bias in deep learning. (arXiv:2201.07395v1 [cs.LG])
27. Flexible Parallel Learning in Edge Scenarios: Communication, Computational and Energy Cost. (arXiv:2201.07402v1 [cs.NI])
28. Compressed Smooth Sparse Decomposition. (arXiv:2201.07404v1 [eess.IV])
29. Dual Space Graph Contrastive Learning. (arXiv:2201.07409v1 [cs.LG])
30. Lifted Primal-Dual Method for Bilinearly Coupled Smooth Minimax Optimization. (arXiv:2201.07427v1 [math.OC])
31. On the Convergence Rates of Policy Gradient Methods. (arXiv:2201.07443v1 [math.OC])
32. TourBERT: A pretrained language model for the tourism industry. (arXiv:2201.07449v1 [cs.CL])
33. The Enforcers: Consistent Sparse-Discrete Methods for Constraining Informative Emergent Communication. (arXiv:2201.07452v1 [cs.LG])
34. Cortical lesions, central vein sign, and paramagnetic rim lesions in multiple sclerosis: emerging machine learning techniques and future avenues. (arXiv:2201.07463v1 [eess.IV])
35. Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders. (arXiv:2201.07513v1 [cs.CR])
36. Privacy-Aware Human Mobility Prediction via Adversarial Networks. (arXiv:2201.07519v1 [cs.LG])
37. GNN-based Android Malware Detection with Jumping Knowledge. (arXiv:2201.07537v1 [cs.CR])
38. Simpler is better: spectral regularization and up-sampling techniques for variational autoencoders. (arXiv:2201.07544v1 [cs.LG])
39. Stability of Deep Neural Networks via discrete rough paths. (arXiv:2201.07566v1 [cs.LG])
40. Batch versus Sequential Active Learning for Recommender Systems. (arXiv:2201.07571v1 [cs.IR])
41. Superpixel Pre-Segmentation of HER2 Slides for Efficient Annotation. (arXiv:2201.07572v1 [cs.CV])
42. Models for information propagation on graphs. (arXiv:2201.07577v1 [math.NA])
43. Near-Optimal Sparse Allreduce for Distributed Deep Learning. (arXiv:2201.07598v1 [cs.DC])
44. Including STDP to eligibility propagation in multi-layer recurrent spiking neural networks. (arXiv:2201.07602v1 [cs.NE])
45. Semi-Supervised Clustering with Contrastive Learning for Discovering New Intents. (arXiv:2201.07604v1 [cs.LG])
46. ReGNL: Rapid Prediction of GDP during Disruptive Events using **Night**lights. (arXiv:2201.07612v1 [cs.LG])
47. Uncovering More Shallow Heuristics: Probing the Natural Language Inference Capacities of Transformer-Based Pre-Trained Language Models Using Syllogistic Patterns. (arXiv:2201.07614v1 [cs.CL])
48. A Survey on Training Challenges in Generative Adversarial Networks for Biomedical Image Analysis. (arXiv:2201.07646v1 [cs.LG])
49. Malware Classification Using Static Disassembly and Machine Learning. (arXiv:2201.07649v1 [cs.CR])
50. Comprehensive Efficiency Analysis of Machine Learning Algorithms for Developing Hardware-Based Cybersecurity Countermeasures. (arXiv:2201.07654v1 [cs.CR])
51. Multiblock ADMM for nonsmooth nonconvex optimization with nonlinear coupling constraints. (arXiv:2201.07657v1 [math.OC])
52. DSNet: Dynamic Skin Deformation Prediction by Recurrent Neural Network. (arXiv:2201.07660v1 [cs.GR])
53. Top-Down Influence? Predicting CEO Personality and Risk Impact from Speech Transcripts. (arXiv:2201.07670v1 [cs.CL])
54. Code Sophistication: From Code Recommendation to Logic Recommendation. (arXiv:2201.07674v1 [cs.SE])
55. Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows. (arXiv:2201.07677v1 [cs.LG])
56. Learning to Rank For Push Notifications Using Pairwise Expected Regret. (arXiv:2201.07681v1 [cs.IR])
57. Coupled Support Tensor Machine Classification for Multimodal Neuroimaging Data. (arXiv:2201.07683v1 [stat.ML])
58. On the Complexity of a Practical Primal-Dual Coordinate Method. (arXiv:2201.07684v1 [math.OC])
59. Anytime Optimal PSRO for Two-Player Zero-Sum Games. (arXiv:2201.07700v1 [cs.GT])
60. Object Detection in Autonomous Vehicles: Status and Open Challenges. (arXiv:2201.07706v1 [cs.CV])
61. Debiased Graph Neural Networks with Agnostic Label Selection Bias. (arXiv:2201.07708v1 [cs.LG])
62. Enhancing the Security & Privacy of Wearable Brain-Computer Interfaces. (arXiv:2201.07711v1 [cs.CR])
63. Visual Exploration of Machine Learning Model Behavior with Hierarchical Surrogate Rule Sets. (arXiv:2201.07724v1 [cs.HC])
64. Scotch: An Efficient Secure Computation Framework for Secure Aggregation. (arXiv:2201.07730v1 [cs.CR])
65. Towards holistic scene understanding: Semantic segmentation and beyond. (arXiv:2201.07734v1 [cs.CV])
66. Detection of Correlated Alarms Using Graph Embedding. (arXiv:2201.07748v1 [cs.LG])
67. Deep Capsule Encoder-Decoder Network for Surrogate Modeling and Uncertainty Quantification. (arXiv:2201.07753v1 [stat.ML])
68. Uncertainty Quantification in Scientific Machine Learning: Methods, Metrics, and Comparisons. (arXiv:2201.07766v1 [cs.LG])
69. Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation. (arXiv:2201.07779v1 [cs.RO])
70. Towards a General Deep Feature Extractor for Facial Expression Recognition. (arXiv:2201.07781v1 [cs.CV])
71. Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation. (arXiv:2201.07786v1 [cs.GR])
72. ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes. (arXiv:2201.07788v1 [cs.CV])
73. Input Selection for Bandwidth-Limited Neural Network Inference. (arXiv:1906.04673v2 [cs.LG] UPDATED)
74. Learning Interpretable Models Using an Oracle. (arXiv:1906.06852v3 [cs.LG] UPDATED)
75. On the Acceleration of Deep Learning Model Parallelism with Staleness. (arXiv:1909.02625v3 [cs.LG] UPDATED)
76. Scattering GCN: Overcoming Oversmoothness in Graph Convolutional Networks. (arXiv:2003.08414v4 [cs.LG] UPDATED)
77. Learning Inconsistent Preferences with Gaussian Processes. (arXiv:2006.03847v2 [stat.ML] UPDATED)
78. Physics Informed Deep Kernel Learning. (arXiv:2006.04976v2 [stat.ML] UPDATED)
79. Parameterized MDPs and Reinforcement Learning Problems -- A Maximum Entropy Principle Based Framework. (arXiv:2006.09646v3 [cs.LG] UPDATED)
80. Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v3 [cs.LG] UPDATED)
81. OrbNet: Deep Learning for Quantum Chemistry Using Symmetry-Adapted Atomic-Orbital Features. (arXiv:2007.08026v3 [physics.chem-ph] UPDATED)
82. Step-Ahead Error Feedback for Distributed Training with Compressed Gradient. (arXiv:2008.05823v2 [cs.LG] UPDATED)
83. Stein Variational Gaussian Processes. (arXiv:2009.12141v3 [stat.ML] UPDATED)
84. PECOS: Prediction for Enormous and Correlated Output Spaces. (arXiv:2010.05878v2 [cs.LG] UPDATED)
85. Deperturbation of Online Social Networks via Bayesian Label Transition. (arXiv:2010.14121v3 [cs.LG] UPDATED)
86. Geometric Scattering Attention Networks. (arXiv:2010.15010v2 [cs.LG] UPDATED)
87. KST-GCN: A Knowledge-Driven Spatial-Temporal Graph Convolutional Network for Traffic Forecasting. (arXiv:2011.14992v2 [cs.LG] UPDATED)
88. A Bayesian multiscale CNN framework to predict local stress fields in structures with microscale features. (arXiv:2012.11330v4 [cs.CE] UPDATED)
89. Transformers in Vision: A Survey. (arXiv:2101.01169v5 [cs.CV] UPDATED)
90. Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python. (arXiv:2101.04470v3 [cs.LG] UPDATED)
91. Coordinating Momenta for Cross-silo Federated Learning. (arXiv:2102.03970v2 [cs.LG] UPDATED)
92. The Elliptical Potential Lemma for General Distributions with an Application to Linear Thompson Sampling. (arXiv:2102.07987v3 [stat.ML] UPDATED)
93. PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains. (arXiv:2102.12206v3 [cs.CL] UPDATED)
94. Towards Personalized Federated Learning. (arXiv:2103.00710v2 [cs.LG] UPDATED)
95. Scalable Hypergraph Embedding System. (arXiv:2103.09660v3 [cs.SI] UPDATED)
96. Cooperative Multi-Agent Fairness and Equivariant Policies. (arXiv:2106.05727v3 [cs.AI] UPDATED)
97. Self-Supervised Metric Learning in Multi-View Data: A Downstream Task Perspective. (arXiv:2106.07138v3 [stat.ML] UPDATED)
98. Multi-Agent Curricula and Emergent Implicit Signaling. (arXiv:2106.11156v2 [cs.MA] UPDATED)
99. Optimality Inductive Biases and Agnostic Guidelines for Offline Reinforcement Learning. (arXiv:2107.01407v2 [cs.LG] UPDATED)
100. A Survey of Uncertainty in Deep Neural Networks. (arXiv:2107.03342v3 [cs.LG] UPDATED)
101. Linear Polytree Structural Equation Models: Structural Learning and Inverse Correlation Estimation. (arXiv:2107.10955v2 [stat.ML] UPDATED)
102. Decentralized Federated Learning: Balancing Communication and Computing Costs. (arXiv:2107.12048v3 [cs.LG] UPDATED)
103. Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training. (arXiv:2108.09373v2 [cs.DC] UPDATED)
104. Waveform Learning for Next-Generation Wireless Communication Systems. (arXiv:2109.00998v2 [cs.IT] UPDATED)
105. Generalized XGBoost Method. (arXiv:2109.07473v2 [cs.LG] UPDATED)
106. RAFT: A Real-World Few-Shot Text Classification Benchmark. (arXiv:2109.14076v3 [cs.CL] UPDATED)
107. FedTune: Automatic Tuning of Federated Learning Hyper-Parameters from System Perspective. (arXiv:2110.03061v3 [cs.LG] UPDATED)
108. One-Step Abductive Multi-Target Learning with Diverse Noisy Samples: An Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v3 [cs.LG] UPDATED)
109. Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning. (arXiv:2110.14049v2 [cs.LG] UPDATED)
110. Towards an Understanding of Default Policies in Multitask Policy Optimization. (arXiv:2111.02994v3 [cs.LG] UPDATED)
111. Dual Parameterization of Sparse Variational Gaussian Processes. (arXiv:2111.03412v2 [cs.LG] UPDATED)
112. Towards Green Automated Machine Learning: Status Quo and Future Directions. (arXiv:2111.05850v2 [cs.LG] UPDATED)
113. Building separable approximations for quantum states via neural networks. (arXiv:2112.08055v2 [quant-ph] UPDATED)
114. On Dynamic Pricing with Covariates. (arXiv:2112.13254v2 [cs.LG] UPDATED)
115. Bayesian Neural Hawkes Process for Event Uncertainty Prediction. (arXiv:2112.14474v2 [cs.LG] UPDATED)
116. GPS: A Policy-driven Sampling Approach for Graph Representation Learning. (arXiv:2112.14482v2 [cs.LG] UPDATED)
117. Jointly Efficient and Optimal Algorithms for Logistic Bandits. (arXiv:2201.01985v2 [cs.LG] UPDATED)
118. VGAER: graph neural network reconstruction based community detection. (arXiv:2201.04066v2 [cs.SI] UPDATED)
119. Dyna-T: Dyna-Q and Upper Confidence Bounds Applied to Trees. (arXiv:2201.04502v2 [cs.LG] UPDATED)
120. Direct Mutation and Crossover in Genetic Algorithms Applied to Reinforcement Learning Tasks. (arXiv:2201.04815v2 [cs.NE] UPDATED)
121. When less is more: Simplifying inputs aids neural network understanding. (arXiv:2201.05610v2 [cs.LG] UPDATED)
122. Automated causal inference in application to randomized controlled clinical trials. (arXiv:2201.05773v2 [stat.ME] UPDATED)
123. GradTail: Learning Long-Tailed Data Using Gradient-based Sample Weighting. (arXiv:2201.05938v2 [cs.LG] UPDATED)
124. Motion Inbetweening via Deep $\Delta$-Interpolator. (arXiv:2201.06701v2 [cs.LG] UPDATED)
125. Model Transferring Attacks to Backdoor HyperNetwork in Personalized Federated Learning. (arXiv:2201.07063v2 [cs.LG] UPDATED)
126. A Regularized Limited Memory BFGS method for Large-Scale Unconstrained Optimization and its Efficient Implementations. (arXiv:2101.04413v1 [math.OC] CROSS LISTED)
127. Model-Driven Deep Learning Based Channel Estimation and Feedback for Millimeter-Wave Massive Hybrid MIMO Systems. (arXiv:2104.11052v3 [cs.IT] CROSS LISTED)
128. Trajectory Design for UAV-Based Internet-of-Things Data Collection: A Deep Reinforcement Learning Approach. (arXiv:2107.11015v1 [cs.IT] CROSS LISTED)
129. Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition. (arXiv:2201.05554v1 [cs.SD] CROSS LISTED)
130. Investigation of Data Augmentation Techniques for Disordered Speech Recognition. (arXiv:2201.05562v1 [cs.SD] CROSS LISTED)
131. Lifelong Generative Learning via Knowledge Reconstruction. (arXiv:2201.06418v1 [cs.LG] CROSS LISTED)
132. Using machine learning to parametrize postmerger signals from binary neutron stars. (arXiv:2201.06461v1 [gr-qc] CROSS LISTED)
133. Data-Driven Deep Learning Based Hybrid Beamforming for Aerial Massive MIMO-OFDM Systems with Implicit CSI. (arXiv:2201.06778v1 [eess.SP] CROSS LISTED)
## cs.AI
---
**63** new papers in cs.AI:-) 
1. Enhanced Self-Organizing Map Solution for the Traveling Salesman Problem. (arXiv:2201.07208v1 [cs.NE])
2. Advancing Deep Residual Learning by Solving the Crux of Degradation in Spiking Neural Networks. (arXiv:2201.07209v1 [cs.NE])
3. Using Particle Swarm Optimization as Pathfinding Strategy in a Space with Obstacles. (arXiv:2201.07212v1 [cs.NE])
4. Layerwise Geo-Distributed Computing between Cloud and IoT. (arXiv:2201.07215v1 [cs.DC])
5. Wide Area Network Intelligence with Application to Multimedia Service. (arXiv:2201.07216v1 [cs.NI])
6. NSGZero: Efficiently Learning Non-Exploitable Policy in Large-Scale Network Security Games with Neural Monte Carlo Tree Search. (arXiv:2201.07224v1 [cs.CR])
7. AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review. (arXiv:2201.07231v1 [eess.IV])
8. Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v1 [cs.LG])
9. Convergence of policy gradient for entropy regularized MDPs with neural network approximation in the mean-field regime. (arXiv:2201.07296v1 [math.OC])
10. OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation. (arXiv:2201.07309v1 [cs.CV])
11. Prospective Learning: Back to the Future. (arXiv:2201.07372v1 [cs.LG])
12. Unveiling Project-Specific Bias in Neural Code Models. (arXiv:2201.07381v1 [cs.AI])
13. Neural Language Models are Effective Plagiarists. (arXiv:2201.07406v1 [cs.CL])
14. Cross-Language Binary-Source Code Matching with Intermediate Representations. (arXiv:2201.07420v1 [cs.SE])
15. Hiding Data in Colors: Secure and Lossless Deep Image Steganography via Conditional Invertible Neural Networks. (arXiv:2201.07444v1 [cs.CR])
16. TourBERT: A pretrained language model for the tourism industry. (arXiv:2201.07449v1 [cs.CL])
17. Mixed Nondeterministic-Probabilistic Automata: Blending graphical probabilistic models with nondeterminism. (arXiv:2201.07474v1 [cs.AI])
18. Development of Fake News Model using Machine Learning through Natural Language Processing. (arXiv:2201.07489v1 [cs.CL])
19. POPPINS : A Population-Based Digital Spiking Neuromorphic Processor with Integer Quadratic Integrate-and-Fire Neurons. (arXiv:2201.07490v1 [cs.NE])
20. Educational Timetabling: Problems, Benchmarks, and State-of-the-Art Results. (arXiv:2201.07525v1 [cs.AI])
21. Superpixel Pre-Segmentation of HER2 Slides for Efficient Annotation. (arXiv:2201.07572v1 [cs.CV])
22. Semi-Supervised Clustering with Contrastive Learning for Discovering New Intents. (arXiv:2201.07604v1 [cs.LG])
23. FAT: An In-Memory Accelerator with Fast Addition for Ternary Weight Neural Networks. (arXiv:2201.07634v1 [cs.AR])
24. Problem examination for AI methods in product design. (arXiv:2201.07642v1 [cs.AI])
25. DSNet: Dynamic Skin Deformation Prediction by Recurrent Neural Network. (arXiv:2201.07660v1 [cs.GR])
26. GEMEL: Model Merging for Memory-Efficient, Real-Time Video Analytics at the Edge. (arXiv:2201.07705v1 [cs.DC])
27. Object Detection in Autonomous Vehicles: Status and Open Challenges. (arXiv:2201.07706v1 [cs.CV])
28. Debiased Graph Neural Networks with Agnostic Label Selection Bias. (arXiv:2201.07708v1 [cs.LG])
29. Improving Behavioural Cloning with Human-Driven Dynamic Dataset Augmentation. (arXiv:2201.07719v1 [cs.AI])
30. Visual Exploration of Machine Learning Model Behavior with Hierarchical Surrogate Rule Sets. (arXiv:2201.07724v1 [cs.HC])
31. Towards holistic scene understanding: Semantic segmentation and beyond. (arXiv:2201.07734v1 [cs.CV])
32. Summarising and Comparing Agent Dynamics with Contrastive Spatiotemporal Abstraction. (arXiv:2201.07749v1 [cs.AI])
33. When Is It Acceptable to Break the Rules? Knowledge Representation of Moral Judgement Based on Empirical Data. (arXiv:2201.07763v1 [cs.AI])
34. ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes. (arXiv:2201.07788v1 [cs.CV])
35. Towards Intelligent Interactive Theatre: Drama Management as a way of Handling Performance. (arXiv:1909.10371v2 [cs.AI] UPDATED)
36. Parameterized MDPs and Reinforcement Learning Problems -- A Maximum Entropy Principle Based Framework. (arXiv:2006.09646v3 [cs.LG] UPDATED)
37. Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v3 [cs.LG] UPDATED)
38. Privacy and Robustness in Federated Learning: Attacks and Defenses. (arXiv:2012.06337v3 [cs.CR] UPDATED)
39. Transformers in Vision: A Survey. (arXiv:2101.01169v5 [cs.CV] UPDATED)
40. PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains. (arXiv:2102.12206v3 [cs.CL] UPDATED)
41. Towards Personalized Federated Learning. (arXiv:2103.00710v2 [cs.LG] UPDATED)
42. Cooperative Multi-Agent Fairness and Equivariant Policies. (arXiv:2106.05727v3 [cs.AI] UPDATED)
43. Multi-Agent Curricula and Emergent Implicit Signaling. (arXiv:2106.11156v2 [cs.MA] UPDATED)
44. Optimality Inductive Biases and Agnostic Guidelines for Offline Reinforcement Learning. (arXiv:2107.01407v2 [cs.LG] UPDATED)
45. Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v2 [cs.CL] UPDATED)
46. Automatic Map Update Using Dashcam Videos. (arXiv:2109.12131v2 [cs.CV] UPDATED)
47. RAFT: A Real-World Few-Shot Text Classification Benchmark. (arXiv:2109.14076v3 [cs.CL] UPDATED)
48. Generating User-Centred Explanations via Illocutionary Question Answering: From Philosophy to Interfaces. (arXiv:2110.00762v2 [cs.HC] UPDATED)
49. One-Step Abductive Multi-Target Learning with Diverse Noisy Samples: An Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v3 [cs.LG] UPDATED)
50. Generation of microbial colonies dataset with deep learning style transfer. (arXiv:2111.03789v2 [cs.CV] UPDATED)
51. Medical Visual Question Answering: A Survey. (arXiv:2111.10056v2 [cs.CV] UPDATED)
52. An Analytical Update Rule for General Policy Optimization. (arXiv:2112.02045v2 [cs.AI] UPDATED)
53. GPS: A Policy-driven Sampling Approach for Graph Representation Learning. (arXiv:2112.14482v2 [cs.LG] UPDATED)
54. VGAER: graph neural network reconstruction based community detection. (arXiv:2201.04066v2 [cs.SI] UPDATED)
55. Dyna-T: Dyna-Q and Upper Confidence Bounds Applied to Trees. (arXiv:2201.04502v2 [cs.LG] UPDATED)
56. Automated causal inference in application to randomized controlled clinical trials. (arXiv:2201.05773v2 [stat.ME] UPDATED)
57. Standby-Based Deadlock Avoidance Method for Multi-Agent Pickup and Delivery Tasks. (arXiv:2201.06014v2 [cs.MA] UPDATED)
58. Unintended Bias in Language Model-driven Conversational Recommendation. (arXiv:2201.06224v2 [cs.IR] UPDATED)
59. RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training. (arXiv:2201.06857v2 [cs.CV] UPDATED)
60. Model-Driven Deep Learning Based Channel Estimation and Feedback for Millimeter-Wave Massive Hybrid MIMO Systems. (arXiv:2104.11052v3 [cs.IT] CROSS LISTED)
61. Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition. (arXiv:2201.05554v1 [cs.SD] CROSS LISTED)
62. Investigation of Data Augmentation Techniques for Disordered Speech Recognition. (arXiv:2201.05562v1 [cs.SD] CROSS LISTED)
63. Lifelong Generative Learning via Knowledge Reconstruction. (arXiv:2201.06418v1 [cs.LG] CROSS LISTED)
