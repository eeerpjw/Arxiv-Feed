# Your interest papers
---
## cs.CV
---
### Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval. (arXiv:2209.15034v1 [cs.CV])
- Authors : lin Ristea, Andrei Anghel, Mihai Datcu, Bertrand Chapron
- Link : [http://arxiv.org/abs/2209.15034](http://arxiv.org/abs/2209.15034)
> ABSTRACT  :  Spaceborne synthetic aperture radar (SAR) can provide accurate images of the ocean surface roughness day-or-**night** in nearly all weather conditions, being an unique asset for many geophysical applications. Considering the huge amount of data daily acquired by satellites, automated techniques for physical features extraction are needed. Even if supervised deep learning methods attain state-of-the-art results, they require great amount of labeled data, which are difficult and excessively expensive to acquire for ocean SAR imagery. To this end, we use the subaperture decomposition (SD) algorithm to enhance the unsupervised learning retrieval on the ocean surface, empowering ocean researchers to search into large ocean databases. We empirically prove that SD improve the retrieval precision with over 20% for an unsupervised transformer auto-encoder network. Moreover, we show that SD brings important performance boost when Doppler centroid images are used as input data, leading the way to new unsupervised physics guided retrieval algorithms.  
### 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v1 [cs.CV])
- Authors : Ho Hin, Shunxing Bao, Yuankai Huo
- Link : [http://arxiv.org/abs/2209.15076](http://arxiv.org/abs/2209.15076)
> ABSTRACT  :  Vision transformers (ViTs) have quickly superseded convolutional networks (ConvNets) as the current state-of-the-art (SOTA) models for medical image segmentation. Hierarchical transformers (e.g., **Swin** Transformers) reintroduced several ConvNet priors and further enhanced the practical viability of adapting volumetric segmentation in 3D medical datasets. The effectiveness of hybrid approaches is largely credited to the large receptive field for non-local self-attention and the large number of model parameters. In this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the hierarchical transformer using ConvNet modules for robust volumetric segmentation. Specifically, we revisit volumetric depth-wise convolutions with large kernel size (e.g. starting from $7\times7\times7$) to enable the larger global receptive fields, inspired by **Swin** Transformer. We further substitute the multi-layer perceptron (MLP) in **Swin** Transformer blocks with pointwise depth convolutions and enhance model performances with fewer normalization and activation layers, thus reducing the number of model parameters. 3D UX-Net competes favorably with current SOTA transformers (e.g. **Swin**UNETR) using three challenging public datasets on volumetric brain and abdominal imaging: 1) MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI Challenge 2022 AMOS. 3D UX-Net consistently outperforms **Swin**UNETR with improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice (Feta2021). We further evaluate the transfer learning capability of 3D UX-Net with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880 to 0.900). The source code with our proposed model are available at https://github.com/MASILab/3DUX-Net.  
### Distilling Style from Image Pairs for Global Forward and Inverse **Tone Mapping**. (arXiv:2209.15165v1 [cs.CV])
- Authors : Aamir Mustafa, Param Hanji
- Link : [http://arxiv.org/abs/2209.15165](http://arxiv.org/abs/2209.15165)
> ABSTRACT  :  Many image **enhancement** or editing operations, such as forward and inverse tone mapping or color grading, do not have a unique solution, but instead a range of solutions, each representing a different style. Despite this, existing learning-based methods attempt to learn a unique mapping, disregarding this style. In this work, we show that information about the style can be distilled from collections of image pairs and encoded into a 2- or 3-dimensional vector. This gives us not only an efficient representation but also an interpretable latent space for editing the image style. We represent the global color mapping between a pair of images as a custom normalizing flow, conditioned on a polynomial basis of the pixel color. We show that such a network is more effective than PCA or VAE at encoding image style in low-dimensional space and lets us obtain an accuracy close to 40 dB, which is about 7-10 dB improvement over the state-of-the-art methods.  
### PointPillars Backbone Type Selection For Fast and Accurate LiDAR Object Detection. (arXiv:2209.15252v1 [cs.CV])
- Authors : Konrad Lis, Tomasz Kryjak
- Link : [http://arxiv.org/abs/2209.15252](http://arxiv.org/abs/2209.15252)
> ABSTRACT  :  3D object detection from LiDAR sensor data is an important topic in the context of autonomous cars and drones. In this paper, we present the results of experiments on the impact of backbone selection of a deep convolutional neural network on detection accuracy and computation speed. We chose the PointPillars network, which is characterised by a simple architecture, high speed, and modularity that allows for easy expansion. During the experiments, we paid particular attention to the change in detection efficiency (measured by the mAP metric) and the total number of multiply-addition operations needed to process one point cloud. We tested 10 different convolutional neural network architectures that are widely used in image-based detection problems. For a backbone like MobilenetV1, we obtained an almost 4x speedup at the cost of a 1.13% decrease in mAP. On the other hand, for CSP**Dark**net we got an acceleration of more than 1.5x at an increase in mAP of 0.33%. We have thus demonstrated that it is possible to significantly speed up a 3D object detector in LiDAR point clouds with a small decrease in detection efficiency. This result can be used when PointPillars or similar algorithms are implemented in embedded systems, including SoC FPGAs. The code is available at https://github.com/vision-agh/pointpillars\_backbone.  
### Melanoma Skin Cancer and Nevus Mole Classification using Intensity Value Estimation with Convolutional Neural Network. (arXiv:2209.15465v1 [cs.CV])
- Authors : Rafiqul Islam
- Link : [http://arxiv.org/abs/2209.15465](http://arxiv.org/abs/2209.15465)
> ABSTRACT  :  Melanoma skin cancer is one of the most dangerous and life-threatening cancer. **Exposure** to ultraviolet rays may damage the skin cell's DNA, which causes melanoma skin cancer. However, it is difficult to detect and classify melanoma and nevus mole at the immature stages. In this work, an automatic deep learning system is developed based on the intensity value estimation with a convolutional neural network model (CNN) to detect and classify melanoma and nevus mole more accurately. Since intensity levels are the most distinctive features for object or region of interest identification, the high-intensity pixel values are selected from the extracted lesion images. Incorporating those high-intensity features into the CNN improves the overall performance of the proposed model than the state-of-the-art methods for detecting melanoma skin cancer. To evaluate the system, we used 5-fold cross-validation. Experimental results show that a superior percentage of accuracy (92.58%), sensitivity (93.76%), specificity (91.56%), and precision (90.68%) are achieved.  
### Learning Second Order Local Anomaly for General Face Forgery Detection. (arXiv:2209.15490v1 [cs.CV])
- Authors : Jianwei Fei, Yunshu Dai, Peipeng Yu, Tianrun Shen, Zhihua Xia, Jian Weng
- Link : [http://arxiv.org/abs/2209.15490](http://arxiv.org/abs/2209.15490)
> ABSTRACT  :  In this work, we propose a novel method to improve the generalization ability of CNN-based face forgery detectors. Our method considers the feature anomalies of forged faces caused by the prevalent blending operations in face forgery algorithms. Specifically, we propose a weakly supervised Second Order Local Anomaly (SOLA) learning module to mine anomalies in local regions using deep feature maps. SOLA first decomposes the neighborhood of local features by different directions and distances and then calculates the first and second order local anomaly maps which provide more general forgery traces for the classifier. We also propose a Local **Enhancement** Module (LEM) to improve the discrimination between local features of real and forged regions, so as to ensure accuracy in calculating anomalies. Besides, an improved Adaptive Spatial Rich Model (ASRM) is introduced to help mine subtle noise features via learnable high pass filters. With neither pixel level annotations nor external synthetic data, our method using a simple ResNet18 backbone achieves competitive performances compared with state-of-the-art works when evaluated on unseen forgeries.  
### More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity. (arXiv:2207.03620v2 [cs.CV] UPDATED)
- Authors : Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu, Zhangyang Wang
- Link : [http://arxiv.org/abs/2207.03620](http://arxiv.org/abs/2207.03620)
> ABSTRACT  :  Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as **Swin** Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO.  
## eess.IV
---
### PointPillars Backbone Type Selection For Fast and Accurate LiDAR Object Detection. (arXiv:2209.15252v1 [cs.CV])
- Authors : Konrad Lis, Tomasz Kryjak
- Link : [http://arxiv.org/abs/2209.15252](http://arxiv.org/abs/2209.15252)
> ABSTRACT  :  3D object detection from LiDAR sensor data is an important topic in the context of autonomous cars and drones. In this paper, we present the results of experiments on the impact of backbone selection of a deep convolutional neural network on detection accuracy and computation speed. We chose the PointPillars network, which is characterised by a simple architecture, high speed, and modularity that allows for easy expansion. During the experiments, we paid particular attention to the change in detection efficiency (measured by the mAP metric) and the total number of multiply-addition operations needed to process one point cloud. We tested 10 different convolutional neural network architectures that are widely used in image-based detection problems. For a backbone like MobilenetV1, we obtained an almost 4x speedup at the cost of a 1.13% decrease in mAP. On the other hand, for CSP**Dark**net we got an acceleration of more than 1.5x at an increase in mAP of 0.33%. We have thus demonstrated that it is possible to significantly speed up a 3D object detector in LiDAR point clouds with a small decrease in detection efficiency. This result can be used when PointPillars or similar algorithms are implemented in embedded systems, including SoC FPGAs. The code is available at https://github.com/vision-agh/pointpillars\_backbone.  
## cs.LG
---
### 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v1 [cs.CV])
- Authors : Ho Hin, Shunxing Bao, Yuankai Huo
- Link : [http://arxiv.org/abs/2209.15076](http://arxiv.org/abs/2209.15076)
> ABSTRACT  :  Vision transformers (ViTs) have quickly superseded convolutional networks (ConvNets) as the current state-of-the-art (SOTA) models for medical image segmentation. Hierarchical transformers (e.g., **Swin** Transformers) reintroduced several ConvNet priors and further enhanced the practical viability of adapting volumetric segmentation in 3D medical datasets. The effectiveness of hybrid approaches is largely credited to the large receptive field for non-local self-attention and the large number of model parameters. In this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the hierarchical transformer using ConvNet modules for robust volumetric segmentation. Specifically, we revisit volumetric depth-wise convolutions with large kernel size (e.g. starting from $7\times7\times7$) to enable the larger global receptive fields, inspired by **Swin** Transformer. We further substitute the multi-layer perceptron (MLP) in **Swin** Transformer blocks with pointwise depth convolutions and enhance model performances with fewer normalization and activation layers, thus reducing the number of model parameters. 3D UX-Net competes favorably with current SOTA transformers (e.g. **Swin**UNETR) using three challenging public datasets on volumetric brain and abdominal imaging: 1) MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI Challenge 2022 AMOS. 3D UX-Net consistently outperforms **Swin**UNETR with improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice (Feta2021). We further evaluate the transfer learning capability of 3D UX-Net with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880 to 0.900). The source code with our proposed model are available at https://github.com/MASILab/3DUX-Net.  
### Accurate Long-term Air Temperature Prediction with a Fusion of Artificial Intelligence and Data Reduction Techniques. (arXiv:2209.15424v1 [physics.ao-ph])
- Authors : an Fister, sar Pel, Javier Del, Sancho Salcedo
- Link : [http://arxiv.org/abs/2209.15424](http://arxiv.org/abs/2209.15424)
> ABSTRACT  :  In this paper three customised Artificial Intelligence (AI) frameworks, considering Deep Learning (convolutional neural networks), Machine Learning algorithms and data reduction techniques are proposed, for a problem of long-term summer air temperature prediction. Specifically, the prediction of average air temperature in the first and second August fort**night**s, using input data from previous months, at two different locations, Paris (France) and C\'ordoba (Spain), is considered. The target variable, mainly in the first August fort**night**, can contain signals of extreme events such as heatwaves, like the mega-heatwave of 2003, which affected France and the Iberian Peninsula. Thus, an accurate prediction of long-term air temperature may be valuable also for different problems related to climate change, such as attribution of extreme events, and in other problems related to renewable energy. The analysis carried out this work is based on Reanalysis data, which are first processed by a correlation analysis among different prediction variables and the target (average air temperature in August first and second fort**night**s). An area with the largest correlation is located, and the variables within, after a feature selection process, are the input of different deep learning and ML algorithms. The experiments carried out show a very good prediction skill in the three proposed AI frameworks, both in Paris and C\'ordoba regions.  
## cs.AI
---
### Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval. (arXiv:2209.15034v1 [cs.CV])
- Authors : lin Ristea, Andrei Anghel, Mihai Datcu, Bertrand Chapron
- Link : [http://arxiv.org/abs/2209.15034](http://arxiv.org/abs/2209.15034)
> ABSTRACT  :  Spaceborne synthetic aperture radar (SAR) can provide accurate images of the ocean surface roughness day-or-**night** in nearly all weather conditions, being an unique asset for many geophysical applications. Considering the huge amount of data daily acquired by satellites, automated techniques for physical features extraction are needed. Even if supervised deep learning methods attain state-of-the-art results, they require great amount of labeled data, which are difficult and excessively expensive to acquire for ocean SAR imagery. To this end, we use the subaperture decomposition (SD) algorithm to enhance the unsupervised learning retrieval on the ocean surface, empowering ocean researchers to search into large ocean databases. We empirically prove that SD improve the retrieval precision with over 20% for an unsupervised transformer auto-encoder network. Moreover, we show that SD brings important performance boost when Doppler centroid images are used as input data, leading the way to new unsupervised physics guided retrieval algorithms.  
### Accurate Long-term Air Temperature Prediction with a Fusion of Artificial Intelligence and Data Reduction Techniques. (arXiv:2209.15424v1 [physics.ao-ph])
- Authors : an Fister, sar Pel, Javier Del, Sancho Salcedo
- Link : [http://arxiv.org/abs/2209.15424](http://arxiv.org/abs/2209.15424)
> ABSTRACT  :  In this paper three customised Artificial Intelligence (AI) frameworks, considering Deep Learning (convolutional neural networks), Machine Learning algorithms and data reduction techniques are proposed, for a problem of long-term summer air temperature prediction. Specifically, the prediction of average air temperature in the first and second August fort**night**s, using input data from previous months, at two different locations, Paris (France) and C\'ordoba (Spain), is considered. The target variable, mainly in the first August fort**night**, can contain signals of extreme events such as heatwaves, like the mega-heatwave of 2003, which affected France and the Iberian Peninsula. Thus, an accurate prediction of long-term air temperature may be valuable also for different problems related to climate change, such as attribution of extreme events, and in other problems related to renewable energy. The analysis carried out this work is based on Reanalysis data, which are first processed by a correlation analysis among different prediction variables and the target (average air temperature in August first and second fort**night**s). An area with the largest correlation is located, and the variables within, after a feature selection process, are the input of different deep learning and ML algorithms. The experiments carried out show a very good prediction skill in the three proposed AI frameworks, both in Paris and C\'ordoba regions.  
# Paper List
---
## cs.CV
---
**115** new papers in cs.CV:-) 
1. Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval. (arXiv:2209.15034v1 [cs.CV])
2. Large-Scale Spatial Cross-Calibration of Hinode/SOT-SP and SDO/HMI. (arXiv:2209.15036v1 [astro-ph.SR])
3. Generalizability of Adversarial Robustness Under Distribution Shifts. (arXiv:2209.15042v1 [cs.LG])
4. Graph Attention Network for Camera Relocalization on Dynamic Scenes. (arXiv:2209.15056v1 [cs.CV])
5. Partially calibrated semi-generalized pose from hybrid point correspondences. (arXiv:2209.15072v1 [cs.CV])
6. 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v1 [cs.CV])
7. Automatic satellite building construction monitoring. (arXiv:2209.15084v1 [cs.CV])
8. Zero-shot visual reasoning through probabilistic analogical mapping. (arXiv:2209.15087v1 [cs.CV])
9. Open-source tool for Airway Segmentation in Computed Tomography using 2.5D Modified EfficientDet: Contribution to the ATM22 Challenge. (arXiv:2209.15094v1 [eess.IV])
10. AICCA: AI-driven Cloud Classification Atlas. (arXiv:2209.15096v1 [physics.ao-ph])
11. Heterogeneous reconstruction of deformable atomic models in Cryo-EM. (arXiv:2209.15121v1 [q-bio.BM])
12. Embedded System Performance Analysis for Implementing a Portable Drowsiness Detection System for Drivers. (arXiv:2209.15148v1 [cs.CV])
13. MonoNeuralFusion: Online Monocular Neural 3D Reconstruction with Geometric Priors. (arXiv:2209.15153v1 [cs.CV])
14. MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features. (arXiv:2209.15159v1 [cs.CV])
15. Distilling Style from Image Pairs for Global Forward and Inverse **Tone Mapping**. (arXiv:2209.15165v1 [cs.CV])
16. Understanding Pure CLIP Guidance for Voxel Grid NeRF Models. (arXiv:2209.15172v1 [cs.CV])
17. Physical Adversarial Attack meets Computer Vision: A Decade Survey. (arXiv:2209.15179v1 [cs.CV])
18. SCI: A spectrum concentrated implicit neural compression for biomedical data. (arXiv:2209.15180v1 [eess.IV])
19. Multi-Prompt Alignment for Multi-source Unsupervised Domain Adaptation. (arXiv:2209.15210v1 [cs.CV])
20. Dual Progressive Transformations for Weakly Supervised Semantic Segmentation. (arXiv:2209.15211v1 [cs.CV])
21. INT: Towards Infinite-frames 3D Detection with An Efficient Framework. (arXiv:2209.15215v1 [cs.CV])
22. Your Out-of-Distribution Detection Method is Not Robust!. (arXiv:2209.15246v1 [cs.CV])
23. Hyperspectral and LiDAR data for the prediction via machine learning of tree species, volume and biomass: a possible contribution for updating forest management plans. (arXiv:2209.15248v1 [cs.CV])
24. Traffic Sign Classification Using Deep and Quantum Neural Networks. (arXiv:2209.15251v1 [cs.CV])
25. PointPillars Backbone Type Selection For Fast and Accurate LiDAR Object Detection. (arXiv:2209.15252v1 [cs.CV])
26. S2P: State-conditioned Image Synthesis for Data Augmentation in Offline Reinforcement Learning. (arXiv:2209.15256v1 [cs.LG])
27. Energy Efficient Hardware Acceleration of Neural Networks with Power-of-Two Quantisation. (arXiv:2209.15257v1 [cs.CV])
28. Transformers for Object Detection in Large Point Clouds. (arXiv:2209.15258v1 [cs.CV])
29. Minimalistic Unsupervised Learning with the Sparse Manifold Transform. (arXiv:2209.15261v1 [cs.LG])
30. Diffusion-based Image Translation using Disentangled Style and Content Representation. (arXiv:2209.15264v1 [cs.CV])
31. Generative Model Watermarking Based on Human Visual System. (arXiv:2209.15268v1 [cs.CV])
32. ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training. (arXiv:2209.15270v1 [cs.CV])
33. Application-Driven AI Paradigm for Human Action Recognition. (arXiv:2209.15271v1 [cs.CV])
34. Learning Transferable Spatiotemporal Representations from Natural Script Knowledge. (arXiv:2209.15280v1 [cs.CV])
35. Verifiable and Energy Efficient Medical Image Analysis with Quantised Self-attentive Deep Neural Networks. (arXiv:2209.15287v1 [cs.CV])
36. Visual Privacy Protection Based on Type-I Adversarial Attack. (arXiv:2209.15304v1 [cs.CV])
37. Effective Early Stopping of Point Cloud Neural Networks. (arXiv:2209.15308v1 [cs.CV])
38. Did You Get What You Paid For? Rethinking Annotation Cost of Deep Learning Based Computer Aided Detection in Chest Radiographs. (arXiv:2209.15314v1 [cs.CV])
39. Convolutional Neural Networks Quantization with Attention. (arXiv:2209.15317v1 [cs.AI])
40. SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation. (arXiv:2209.15323v1 [cs.CV])
41. Towards End-to-end Handwritten Document Recognition. (arXiv:2209.15362v1 [cs.CV])
42. Inharmonious Region Localization by Magnifying Domain Discrepancy. (arXiv:2209.15368v1 [cs.CV])
43. Automatic Context-Driven Inference of Engagement in HMI: A Survey. (arXiv:2209.15370v1 [cs.HC])
44. Viewpoint Planning based on Shape Completion for Fruit Mapping and Reconstruction. (arXiv:2209.15376v1 [cs.RO])
45. DELAD: Deep Landweber-guided deconvolution with Hessian and sparse prior. (arXiv:2209.15377v1 [eess.IV])
46. Semi-Supervised Single-View 3D Reconstruction via Prototype Shape Priors. (arXiv:2209.15383v1 [cs.CV])
47. Evaluation of importance estimators in deep learning classifiers for Computed Tomography. (arXiv:2209.15398v1 [cs.CV])
48. Rethinking the Learning Paradigm for Facial Expression Recognition. (arXiv:2209.15402v1 [cs.CV])
49. An information-theoretic approach to unsupervised keypoint representation learning. (arXiv:2209.15404v1 [cs.CV])
50. Spikformer: When Spiking Neural Network Meets Transformer. (arXiv:2209.15425v1 [cs.NE])
51. Exploiting Instance-based Mixed Sampling via Auxiliary Source Domain Supervision for Domain-adaptive Action Detection. (arXiv:2209.15439v1 [cs.CV])
52. Semi-Supervised Domain Generalization for Cardiac Magnetic Resonance Image Segmentation with High Quality Pseudo Labels. (arXiv:2209.15451v1 [eess.IV])
53. Road Network Deterioration Monitoring Using Aerial Images and Computer Vision. (arXiv:2209.15455v1 [cs.CV])
54. Towards General-Purpose Representation Learning of Polygonal Geometries. (arXiv:2209.15458v1 [cs.CV])
55. Melanoma Skin Cancer and Nevus Mole Classification using Intensity Value Estimation with Convolutional Neural Network. (arXiv:2209.15465v1 [cs.CV])
56. Two-headed eye-segmentation approach for biometric identification. (arXiv:2209.15471v1 [cs.CV])
57. Reliable Face Morphing Attack Detection in On-The-Fly Border Control Scenario with Variation in Image Resolution and Capture Distance. (arXiv:2209.15474v1 [cs.CV])
58. Point Cloud Quality Assessment using 3D Saliency Maps. (arXiv:2209.15475v1 [cs.CV])
59. Impact of Face Image Quality Estimation on Presentation Attack Detection. (arXiv:2209.15489v1 [cs.CV])
60. Learning Second Order Local Anomaly for General Face Forgery Detection. (arXiv:2209.15490v1 [cs.CV])
61. A Closer Look at Temporal Ordering in the Segmentation of Instructional Videos. (arXiv:2209.15501v1 [cs.CV])
62. Sphere-Guided Training of Neural Implicit Surfaces. (arXiv:2209.15511v1 [cs.CV])
63. Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study. (arXiv:2209.15517v1 [cs.CV])
64. Slimmable Networks for Contrastive Self-supervised Learning. (arXiv:2209.15525v1 [cs.CV])
65. TT-NF: Tensor Train Neural Fields. (arXiv:2209.15529v1 [cs.LG])
66. The More Secure, The Less Equally Usable: Gender and Ethnicity (Un)fairness of Deep Face Recognition along Security Thresholds. (arXiv:2209.15550v1 [cs.CV])
67. Towards a Unified View of Affinity-Based Knowledge Distillation. (arXiv:2209.15555v1 [cs.CV])
68. Automated Characterization of Catalytically Active Inclusion Body Production in Biotechnological Screening Systems. (arXiv:2209.15584v1 [q-bio.QM])
69. Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods. (arXiv:2209.15589v1 [cs.CV])
70. Bias Mimicking: A Simple Sampling Approach for Bias Mitigation. (arXiv:2209.15605v1 [cs.CV])
71. Towards Multi-spatiotemporal-scale Generalized PDE Modeling. (arXiv:2209.15616v1 [cs.LG])
72. Point Normal Orientation and Surface Reconstruction by Incorporating Isovalue Constraints to Poisson Equation. (arXiv:2209.15619v1 [cs.GR])
73. Anomaly localization for copy detection patterns through print estimations. (arXiv:2209.15625v1 [cs.CV])
74. ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing. (arXiv:2209.15632v1 [cs.CV])
75. Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator. (arXiv:2209.15637v1 [cs.CV])
76. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models. (arXiv:2209.15639v1 [cs.CV])
77. TransCenter: Transformers with Dense Representations for Multiple-Object Tracking. (arXiv:2103.15145v4 [cs.CV] UPDATED)
78. Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. (arXiv:2110.01052v5 [cs.LG] UPDATED)
79. Fusion of complementary 2D and 3D mesostructural datasets using generative adversarial networks. (arXiv:2110.11281v3 [cs.CV] UPDATED)
80. AutoMTL: A Programming Framework for Automating Efficient Multi-Task Learning. (arXiv:2110.13076v3 [cs.LG] UPDATED)
81. Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup. (arXiv:2111.15454v2 [cs.CV] UPDATED)
82. From Coarse to Fine-grained Concept based Discrimination for Phrase Detection. (arXiv:2112.03237v3 [cs.CV] UPDATED)
83. Explainable Medical Imaging AI Needs Human-Centered Design: Guidelines and Evidence from a Systematic Review. (arXiv:2112.12596v4 [cs.HC] UPDATED)
84. Patch-Based Stochastic Attention for Image Editing. (arXiv:2202.03163v3 [cs.CV] UPDATED)
85. One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v3 [cs.CV] UPDATED)
86. End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks. (arXiv:2204.01681v3 [physics.ins-det] UPDATED)
87. AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images. (arXiv:2204.12077v2 [eess.IV] UPDATED)
88. Revisiting Classical Multiclass Linear Discriminant Analysis with a Novel Prototype-based Interpretable Solution. (arXiv:2205.00668v2 [cs.CV] UPDATED)
89. An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v3 [cs.LG] UPDATED)
90. Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v3 [cs.CV] UPDATED)
91. Re-parameterizing Your Optimizers rather than Architectures. (arXiv:2205.15242v2 [cs.LG] UPDATED)
92. Sparse Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v4 [cs.CV] UPDATED)
93. Learning to Estimate Shapley Values with Vision Transformers. (arXiv:2206.05282v2 [cs.CV] UPDATED)
94. The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation. (arXiv:2206.06487v2 [cs.CV] UPDATED)
95. Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?. (arXiv:2206.07959v2 [cs.CV] UPDATED)
96. DecisioNet: A Binary-Tree Structured Neural Network. (arXiv:2207.01127v4 [cs.CV] UPDATED)
97. I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference. (arXiv:2207.01405v2 [cs.CV] UPDATED)
98. DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation. (arXiv:2207.01971v2 [cs.CV] UPDATED)
99. More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity. (arXiv:2207.03620v2 [cs.CV] UPDATED)
100. Learning Depth from Focus in the Wild. (arXiv:2207.09658v2 [cs.CV] UPDATED)
101. Provable Defense Against Geometric Transformations. (arXiv:2207.11177v2 [cs.LG] UPDATED)
102. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v2 [cs.LG] UPDATED)
103. Pyramidal Denoising Diffusion Probabilistic Models. (arXiv:2208.01864v3 [cs.CV] UPDATED)
104. Shadows Aren't So Dangerous After All: A Fast and Robust Defense Against Shadow-Based Adversarial Attacks. (arXiv:2208.09285v2 [cs.CV] UPDATED)
105. Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models. (arXiv:2208.14133v2 [cs.LG] UPDATED)
106. Consistent Targets Provide Better Supervision in Semi-supervised Object Detection. (arXiv:2209.01589v2 [cs.CV] UPDATED)
107. Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v2 [eess.IV] UPDATED)
108. A Continual Development Methodology for Large-scale Multitask Dynamic ML Systems. (arXiv:2209.07326v2 [cs.LG] UPDATED)
109. NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes. (arXiv:2209.08776v5 [cs.CV] UPDATED)
110. VToonify: Controllable High-Resolution Portrait Video Style Transfer. (arXiv:2209.11224v3 [cs.CV] UPDATED)
111. Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models. (arXiv:2209.12104v2 [eess.IV] UPDATED)
112. Evaluation of Medical Image Segmentation Models for Uncertain, Small or Empty Reference Annotations. (arXiv:2209.13008v2 [eess.IV] UPDATED)
113. GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v2 [cs.LG] UPDATED)
114. Dataset Distillation for Medical Dataset Sharing. (arXiv:2209.14603v2 [cs.CR] UPDATED)
115. Creative Painting with Latent Diffusion Models. (arXiv:2209.14697v2 [cs.CV] UPDATED)
## eess.IV
---
**20** new papers in eess.IV:-) 
1. Ghost translation. (arXiv:2209.15012v1 [eess.IV])
2. Open-source tool for Airway Segmentation in Computed Tomography using 2.5D Modified EfficientDet: Contribution to the ATM22 Challenge. (arXiv:2209.15094v1 [eess.IV])
3. Heterogeneous reconstruction of deformable atomic models in Cryo-EM. (arXiv:2209.15121v1 [q-bio.BM])
4. Low-Dose CT Using Denoising Diffusion Probabilistic Model for 20$\times$ Speedup. (arXiv:2209.15136v1 [eess.IV])
5. Embedded System Performance Analysis for Implementing a Portable Drowsiness Detection System for Drivers. (arXiv:2209.15148v1 [cs.CV])
6. SCI: A spectrum concentrated implicit neural compression for biomedical data. (arXiv:2209.15180v1 [eess.IV])
7. PointPillars Backbone Type Selection For Fast and Accurate LiDAR Object Detection. (arXiv:2209.15252v1 [cs.CV])
8. Energy Efficient Hardware Acceleration of Neural Networks with Power-of-Two Quantisation. (arXiv:2209.15257v1 [cs.CV])
9. DELAD: Deep Landweber-guided deconvolution with Hessian and sparse prior. (arXiv:2209.15377v1 [eess.IV])
10. Sweet Streams are Made of This: The System Engineer's View on Energy Efficiency in Video Communications. (arXiv:2209.15405v1 [eess.IV])
11. XR-RF Imaging Enabled by Software-Defined Metasurfaces and Machine Learning: Foundational Vision, Technologies and Challenges. (arXiv:2209.15436v1 [eess.SP])
12. Semi-Supervised Domain Generalization for Cardiac Magnetic Resonance Image Segmentation with High Quality Pseudo Labels. (arXiv:2209.15451v1 [eess.IV])
13. Prediction of motion induced magnetic fields for human brain MRI at 3T. (arXiv:2209.15470v1 [physics.med-ph])
14. Point Cloud Quality Assessment using 3D Saliency Maps. (arXiv:2209.15475v1 [cs.CV])
15. Explainable Medical Imaging AI Needs Human-Centered Design: Guidelines and Evidence from a Systematic Review. (arXiv:2112.12596v4 [cs.HC] UPDATED)
16. AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images. (arXiv:2204.12077v2 [eess.IV] UPDATED)
17. Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v2 [eess.IV] UPDATED)
18. Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models. (arXiv:2209.12104v2 [eess.IV] UPDATED)
19. Evaluation of Medical Image Segmentation Models for Uncertain, Small or Empty Reference Annotations. (arXiv:2209.13008v2 [eess.IV] UPDATED)
20. Dataset Distillation for Medical Dataset Sharing. (arXiv:2209.14603v2 [cs.CR] UPDATED)
## cs.LG
---
**219** new papers in cs.LG:-) 
1. A deep learning approach to the probabilistic numerical solution of path-dependent partial differential equations. (arXiv:2209.15010v1 [cs.LG])
2. Automatic Data Augmentation via Invariance-Constrained Learning. (arXiv:2209.15031v1 [cs.LG])
3. Generalizability of Adversarial Robustness Under Distribution Shifts. (arXiv:2209.15042v1 [cs.LG])
4. Start Small: Training Game Level Generators from Nothing by Learning at Multiple Sizes. (arXiv:2209.15052v1 [cs.LG])
5. Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v1 [stat.ML])
6. Graph Attention Network for Camera Relocalization on Dynamic Scenes. (arXiv:2209.15056v1 [cs.CV])
7. Provably expressive temporal graph networks. (arXiv:2209.15059v1 [cs.LG])
8. Few-shot Text Classification with Dual Contrastive Consistency. (arXiv:2209.15069v1 [cs.CL])
9. 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v1 [cs.CV])
10. Online Weighted Q-Ensembles for Reduced Hyperparameter Tuning in Reinforcement Learning. (arXiv:2209.15078v1 [cs.LG])
11. Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments. (arXiv:2209.15090v1 [eess.SY])
12. Improving Generative Flow Networks with Path Regularization. (arXiv:2209.15092v1 [cs.LG])
13. Likelihood adjusted semidefinite programs for clustering heterogeneous data. (arXiv:2209.15097v1 [stat.ML])
14. Improving Molecular Pretraining with Complementary Featurizations. (arXiv:2209.15101v1 [cs.LG])
15. Restricted Strong Convexity of Deep Learning Models with Smooth Activations. (arXiv:2209.15106v1 [cs.LG])
16. How to tackle an emerging topic? Combining strong and weak labels for Covid news NER. (arXiv:2209.15108v1 [cs.CL])
17. Understanding Interventional TreeSHAP : How and Why it Works. (arXiv:2209.15123v1 [cs.LG])
18. Nonconvex Matrix Factorization is Geodesically Convex: Global Landscape Analysis for Fixed-rank Matrix Optimization From a Riemannian Perspective. (arXiv:2209.15130v1 [math.OC])
19. Low-Dose CT Using Denoising Diffusion Probabilistic Model for 20$\times$ Speedup. (arXiv:2209.15136v1 [eess.IV])
20. Augmentation Backdoors. (arXiv:2209.15139v1 [cs.LG])
21. On Convergence of Average-Reward Off-Policy Control Algorithms in Weakly-Communicating MDPs. (arXiv:2209.15141v1 [cs.LG])
22. Double Graphs Regularized Multi-view Subspace Clustering. (arXiv:2209.15143v1 [cs.LG])
23. Batch Multivalid Conformal Prediction. (arXiv:2209.15145v1 [cs.LG])
24. Ensemble Machine Learning Model Trained on a New Synthesized Dataset Generalizes Well for Stress Prediction Using Wearable Devices. (arXiv:2209.15146v1 [cs.LG])
25. Variable-Based Calibration for Machine Learning Classifiers. (arXiv:2209.15154v1 [cs.LG])
26. Rethinking and Recomputing the Value of ML Models. (arXiv:2209.15157v1 [cs.LG])
27. MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features. (arXiv:2209.15159v1 [cs.CV])
28. Linearly Mapping from Image to Text Space. (arXiv:2209.15162v1 [cs.CL])
29. Reward Shaping for User Satisfaction in a REINFORCE Recommender. (arXiv:2209.15166v1 [cs.IR])
30. Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient Classification. (arXiv:2209.15168v1 [cs.CL])
31. Dynamic-Backbone Protein-Ligand Structure Prediction with Multiscale Generative Diffusion Models. (arXiv:2209.15171v1 [q-bio.QM])
32. Understanding Pure CLIP Guidance for Voxel Grid NeRF Models. (arXiv:2209.15172v1 [cs.CV])
33. Music Source Separation with Band-split RNN. (arXiv:2209.15174v1 [eess.AS])
34. Domain Generalization -- A Causal Perspective. (arXiv:2209.15177v1 [cs.LG])
35. RL-MD: A Novel Reinforcement Learning Approach for DNA Motif Discovery. (arXiv:2209.15181v1 [cs.LG])
36. Neural Integral Equations. (arXiv:2209.15190v1 [cs.LG])
37. An efficient encoder-decoder architecture with top-down attention for speech separation. (arXiv:2209.15200v1 [cs.SD])
38. Downlink Compression Improves TopK Sparsification. (arXiv:2209.15203v1 [cs.LG])
39. ASPiRe:Adaptive Skill Priors for Reinforcement Learning. (arXiv:2209.15205v1 [cs.LG])
40. Mixture of experts models for multilevel data: modelling framework and approximation theory. (arXiv:2209.15207v1 [math.ST])
41. Scale-invariant Bayesian Neural Networks with Connectivity Tangent Kernel. (arXiv:2209.15208v1 [cs.LG])
42. GM-VAE: Representation Learning with VAE on Gaussian Manifold. (arXiv:2209.15217v1 [cs.LG])
43. Optimal Query Complexities for Dynamic Trace Estimation. (arXiv:2209.15219v1 [cs.DS])
44. Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. (arXiv:2209.15224v1 [stat.ML])
45. The Replicator Dynamic, Chain Components and the Response Graph. (arXiv:2209.15230v1 [cs.GT])
46. Efficient Graph based Recommender System with Weighted Averaging of Messages. (arXiv:2209.15238v1 [cs.LG])
47. Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v1 [cs.LG])
48. Fed-CBS: A Heterogeneity-Aware Client Sampling Mechanism for Federated Learning via Class-Imbalance Reduction. (arXiv:2209.15245v1 [cs.LG])
49. Your Out-of-Distribution Detection Method is Not Robust!. (arXiv:2209.15246v1 [cs.CV])
50. Experts in the Loop: Conditional Variable Selection for Accelerating Post-Silicon Analysis Based on Deep Learning. (arXiv:2209.15249v1 [cs.LG])
51. S2P: State-conditioned Image Synthesis for Data Augmentation in Offline Reinforcement Learning. (arXiv:2209.15256v1 [cs.LG])
52. Energy Efficient Hardware Acceleration of Neural Networks with Power-of-Two Quantisation. (arXiv:2209.15257v1 [cs.CV])
53. Transformers for Object Detection in Large Point Clouds. (arXiv:2209.15258v1 [cs.CV])
54. SoK: On the Impossible Security of Very Large Foundation Models. (arXiv:2209.15259v1 [cs.LG])
55. Minimalistic Unsupervised Learning with the Sparse Manifold Transform. (arXiv:2209.15261v1 [cs.LG])
56. Diffusion-based Image Translation using Disentangled Style and Content Representation. (arXiv:2209.15264v1 [cs.CV])
57. ReLU Neural Networks Learn the Simplest Models: Neural Isometry and Exact Recovery. (arXiv:2209.15265v1 [cs.LG])
58. Data Poisoning Attacks Against Multimodal Encoders. (arXiv:2209.15266v1 [cs.CR])
59. Machine Unlearning Method Based On Projection Residual. (arXiv:2209.15276v1 [cs.LG])
60. Rethinking skip connection model as a learnable Markov chain. (arXiv:2209.15278v1 [cs.LG])
61. Sparse tree-based initialization for neural networks. (arXiv:2209.15283v1 [stat.ML])
62. The Minority Matters: A Diversity-Promoting Collaborative Metric Learning Algorithm. (arXiv:2209.15292v1 [cs.IR])
63. A Survey: Credit Sentiment Score Prediction. (arXiv:2209.15293v1 [cs.CE])
64. Effective Early Stopping of Point Cloud Neural Networks. (arXiv:2209.15308v1 [cs.CV])
65. Metro: Memory-Enhanced Transformer for Retrosynthetic Planning via Reaction Tree. (arXiv:2209.15315v1 [cs.LG])
66. Observational Robustness and Invariances in Reinforcement Learning via Lexicographic Objectives. (arXiv:2209.15320v1 [cs.LG])
67. Leveraging variational autoencoders for multiple data imputation. (arXiv:2209.15321v1 [stat.ML])
68. Sparse Random Networks for Communication-Efficient Federated Learning. (arXiv:2209.15328v1 [cs.LG])
69. Many-Body Approximation for Tensors. (arXiv:2209.15338v1 [stat.ML])
70. AudioGen: Textually Guided Audio Generation. (arXiv:2209.15352v1 [cs.SD])
71. Efficient computation of the Knowledge Gradient for Bayesian Optimization. (arXiv:2209.15367v1 [cs.LG])
72. Improve learning combining crowdsourced labels by weighting Areas Under the Margin. (arXiv:2209.15380v1 [cs.LG])
73. Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization. (arXiv:2209.15382v1 [cs.LG])
74. Evaluation of importance estimators in deep learning classifiers for Computed Tomography. (arXiv:2209.15398v1 [cs.CV])
75. Parea: multi-view ensemble clustering for cancer subtype discovery. (arXiv:2209.15399v1 [cs.LG])
76. An information-theoretic approach to unsupervised keypoint representation learning. (arXiv:2209.15404v1 [cs.CV])
77. Equivariant Energy-Guided SDE for Inverse Molecular Design. (arXiv:2209.15408v1 [physics.chem-ph])
78. Higher-order Neural Additive Models: An Interpretable Machine Learning Model with Feature Interactions. (arXiv:2209.15409v1 [cs.LG])
79. Predicting the power grid frequency of European islands. (arXiv:2209.15414v1 [stat.AP])
80. DynImp: Dynamic Imputation for Wearable Sensing Data Through Sensory and Temporal Relatedness. (arXiv:2209.15415v1 [eess.SP])
81. Optimal Efficiency-Envy Trade-Off via Optimal Transport. (arXiv:2209.15416v1 [cs.GT])
82. Equitable Marketplace Mechanism Design. (arXiv:2209.15418v1 [cs.GT])
83. Ensemble-based gradient inference for particle methods in optimization and sampling. (arXiv:2209.15420v1 [stat.ML])
84. TabDDPM: Modelling Tabular Data with Diffusion Models. (arXiv:2209.15421v1 [cs.LG])
85. Accurate Long-term Air Temperature Prediction with a Fusion of Artificial Intelligence and Data Reduction Techniques. (arXiv:2209.15424v1 [physics.ao-ph])
86. Spikformer: When Spiking Neural Network Meets Transformer. (arXiv:2209.15425v1 [cs.NE])
87. Tuning of Mixture-of-Experts Mixed-Precision Neural Networks. (arXiv:2209.15427v1 [cs.LG])
88. Relative representations enable zero-shot latent space communication. (arXiv:2209.15430v1 [cs.LG])
89. Empowering the trustworthiness of ML-based critical systems through engineering activities. (arXiv:2209.15438v1 [cs.SE])
90. Blessing from Experts: Super Reinforcement Learning in Confounded Environments. (arXiv:2209.15448v1 [cs.LG])
91. End-to-End Label Uncertainty Modeling in Speech Emotion Recognition using Bayesian Neural Networks and Label Distribution Learning. (arXiv:2209.15449v1 [eess.AS])
92. Explainable Censored Learning: Finding Critical Features with Long Term Prognostic Values for Survival Prediction. (arXiv:2209.15450v1 [cs.LG])
93. Safe Exploration Method for Reinforcement Learning under Existence of Disturbance. (arXiv:2209.15452v1 [cs.LG])
94. GPNet: Simplifying Graph Neural Networks via Multi-channel Geometric Polynomials. (arXiv:2209.15454v1 [cs.LG])
95. Scheduling for Urban Air Mobility using Safe Learning. (arXiv:2209.15457v1 [cs.LG])
96. Towards General-Purpose Representation Learning of Polygonal Geometries. (arXiv:2209.15458v1 [cs.CV])
97. Sparsity-Constrained Optimal Transport. (arXiv:2209.15466v1 [stat.ML])
98. Two-headed eye-segmentation approach for biometric identification. (arXiv:2209.15471v1 [cs.CV])
99. On The Robustness of Self-Supervised Representations for Spoken Language Modeling. (arXiv:2209.15483v1 [cs.CL])
100. Graph Neural Networks for Link Prediction with Subgraph Sketching. (arXiv:2209.15486v1 [cs.LG])
101. Using Knowledge Distillation to improve interpretable models in a retail banking context. (arXiv:2209.15496v1 [cs.LG])
102. Efficient LSTM Training with Eligibility Traces. (arXiv:2209.15502v1 [cs.LG])
103. Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2209.15505v1 [cs.LG])
104. Learning with MISELBO: The Mixture Cookbook. (arXiv:2209.15514v1 [cs.LG])
105. TT-NF: Tensor Train Neural Fields. (arXiv:2209.15529v1 [cs.LG])
106. Bayesian Neural Networks for Geothermal Resource Assessment: Prediction with Uncertainty. (arXiv:2209.15543v1 [physics.geo-ph])
107. Designing and Training of Lightweight Neural Networks on Edge Devices using Early Halting in Knowledge Distillation. (arXiv:2209.15560v1 [cs.LG])
108. On the optimization and generalization of overparameterized implicit neural networks. (arXiv:2209.15562v1 [cs.LG])
109. Holographic-(V)AE: an end-to-end SO(3)-Equivariant (Variational) Autoencoder in Fourier Space. (arXiv:2209.15567v1 [cs.LG])
110. Fault Prognosis in Particle Accelerator Power Electronics Using Ensemble Learning. (arXiv:2209.15570v1 [physics.acc-ph])
111. Building Normalizing Flows with Stochastic Interpolants. (arXiv:2209.15571v1 [cs.LG])
112. Convergence of weak-SINDy Surrogate Models. (arXiv:2209.15573v1 [math.NA])
113. Match to Win: Analysing Sequences Lengths for Efficient Self-supervised Learning in Speech and Audio. (arXiv:2209.15575v1 [cs.SD])
114. Physically Meaningful Uncertainty Quantification in Probabilistic Wind Turbine Power Curve Models as a Damage Sensitive Feature. (arXiv:2209.15579v1 [cs.LG])
115. Cloud Classification with Unsupervised Deep Learning. (arXiv:2209.15585v1 [physics.ao-ph])
116. New Metric Formulas that Include Measurement Errors in Machine Learning for Natural Sciences. (arXiv:2209.15588v1 [cs.LG])
117. Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods. (arXiv:2209.15589v1 [cs.CV])
118. Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability. (arXiv:2209.15594v1 [cs.LG])
119. Rethinking Data Heterogeneity in Federated Learning: Introducing a New Notion and Standard Benchmarks. (arXiv:2209.15595v1 [cs.LG])
120. Individual Privacy Accounting with Gaussian Differential Privacy. (arXiv:2209.15596v1 [cs.CR])
121. MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction. (arXiv:2209.15597v1 [cs.AI])
122. Shuffled linear regression through graduated convex relaxation. (arXiv:2209.15608v1 [stat.CO])
123. $\Phi$-DVAE: Learning Physically Interpretable Representations with Nonlinear Filtering. (arXiv:2209.15609v1 [stat.ML])
124. TinyTurbo: Efficient Turbo Decoders on Edge. (arXiv:2209.15614v1 [cs.IT])
125. Towards Multi-spatiotemporal-scale Generalized PDE Modeling. (arXiv:2209.15616v1 [cs.LG])
126. Beyond Bayes-optimality: meta-learning what you know you don't know. (arXiv:2209.15618v1 [cs.AI])
127. Family-Based Fingerprint Analysis: A Position Paper. (arXiv:2209.15620v1 [cs.CR])
128. Neural Unbalanced Optimal Transport via Cycle-Consistent Semi-Couplings. (arXiv:2209.15621v1 [cs.LG])
129. Finding NEEMo: Geometric Fitting using Neural Estimation of the Energy Mover's Distance. (arXiv:2209.15624v1 [stat.ML])
130. Anomaly localization for copy detection patterns through print estimations. (arXiv:2209.15625v1 [cs.CV])
131. B2RL: An open-source Dataset for Building Batch Reinforcement Learning. (arXiv:2209.15626v1 [cs.LG])
132. A General Framework for Sample-Efficient Function Approximation in Reinforcement Learning. (arXiv:2209.15634v1 [cs.LG])
133. Vertical Semi-Federated Learning for Efficient Online Advertising. (arXiv:2209.15635v1 [cs.LG])
134. Learning Accurate Decision Trees with Bandit Feedback via Quantized Gradient Descent. (arXiv:2102.07567v3 [cs.LG] UPDATED)
135. Deep Recurrent Encoder: A scalable end-to-end network to model brain signals. (arXiv:2103.02339v3 [q-bio.NC] UPDATED)
136. Provable Guarantees against Data Poisoning Using Self-Expansion and Compatibility. (arXiv:2105.03692v2 [cs.LG] UPDATED)
137. Smooth Bilevel Programming for Sparse Regularization. (arXiv:2106.01429v2 [stat.ML] UPDATED)
138. Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. (arXiv:2110.01052v5 [cs.LG] UPDATED)
139. Probabilistic Metamodels for an Efficient Characterization of Complex Driving Scenarios. (arXiv:2110.02892v3 [cs.LG] UPDATED)
140. DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-based Systems. (arXiv:2110.11155v2 [cs.SE] UPDATED)
141. Fusion of complementary 2D and 3D mesostructural datasets using generative adversarial networks. (arXiv:2110.11281v3 [cs.CV] UPDATED)
142. AutoMTL: A Programming Framework for Automating Efficient Multi-Task Learning. (arXiv:2110.13076v3 [cs.LG] UPDATED)
143. Application of the Multi-label Residual Convolutional Neural Network text classifier using Content-Based Routing process. (arXiv:2110.15801v3 [cs.CL] UPDATED)
144. Adaptive Discretization in Online Reinforcement Learning. (arXiv:2110.15843v2 [stat.ML] UPDATED)
145. A transformer-based model for default prediction in mid-cap corporate markets. (arXiv:2111.09902v2 [q-fin.GN] UPDATED)
146. Explainable Medical Imaging AI Needs Human-Centered Design: Guidelines and Evidence from a Systematic Review. (arXiv:2112.12596v4 [cs.HC] UPDATED)
147. Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders. (arXiv:2202.09671v3 [stat.ML] UPDATED)
148. Plan Your Target and Learn Your Skills: Transferable State-Only Imitation Learning via Decoupled Policy Optimization. (arXiv:2203.02214v5 [cs.LG] UPDATED)
149. Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies. (arXiv:2203.07338v2 [cs.LG] UPDATED)
150. End-to-end P300 BCI using Bayesian accumulation of Riemannian probabilities. (arXiv:2203.07807v2 [cs.LG] UPDATED)
151. POETREE: Interpretable Policy Learning with Adaptive Decision Trees. (arXiv:2203.08057v2 [cs.LG] UPDATED)
152. One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v3 [cs.CV] UPDATED)
153. PACE: A Parallelizable Computation Encoder for Directed Acyclic Graphs. (arXiv:2203.10304v2 [cs.LG] UPDATED)
154. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. (arXiv:2203.13474v4 [cs.LG] UPDATED)
155. Flexible risk design using bi-directional dispersion. (arXiv:2203.14434v2 [stat.ML] UPDATED)
156. End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks. (arXiv:2204.01681v3 [physics.ins-det] UPDATED)
157. GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets. (arXiv:2204.02782v3 [cs.LG] UPDATED)
158. Improved Group Robustness via Classifier Retraining on Independent Splits. (arXiv:2204.09583v2 [cs.LG] UPDATED)
159. Hierarchical Label-wise Attention Transformer Model for Explainable ICD Coding. (arXiv:2204.10716v2 [cs.LG] UPDATED)
160. AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images. (arXiv:2204.12077v2 [eess.IV] UPDATED)
161. Transfer Learning with Pre-trained Conditional Generative Models. (arXiv:2204.12833v2 [cs.LG] UPDATED)
162. Formulating Robustness Against Unforeseen Attacks. (arXiv:2204.13779v3 [cs.LG] UPDATED)
163. Sequential Importance Sampling for Hybrid Model Bayesian Inference to Support Bioprocess Mechanism Learning and Robust Control. (arXiv:2205.02410v4 [stat.ML] UPDATED)
164. Risk Control for Online Learning Models. (arXiv:2205.09095v6 [cs.LG] UPDATED)
165. Riemannian Metric Learning via Optimal Transport. (arXiv:2205.09244v2 [cs.LG] UPDATED)
166. Toward Discovering Options that Achieve Faster Planning. (arXiv:2205.12515v2 [cs.LG] UPDATED)
167. An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v3 [cs.LG] UPDATED)
168. Approximate Conditional Coverage via Neural Model Approximations. (arXiv:2205.14310v2 [cs.LG] UPDATED)
169. Re-parameterizing Your Optimizers rather than Architectures. (arXiv:2205.15242v2 [cs.LG] UPDATED)
170. Contextual Bandits with Knapsacks for a Conversion Model. (arXiv:2206.00314v2 [cs.LG] UPDATED)
171. Neuro-Symbolic Causal Language Planning with Commonsense Prompting. (arXiv:2206.02928v3 [cs.CL] UPDATED)
172. Look Back When Surprised: Stabilizing Reverse Experience Replay for Neural Approximation. (arXiv:2206.03171v2 [cs.LG] UPDATED)
173. Sparse Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v4 [cs.CV] UPDATED)
174. Learning to Estimate Shapley Values with Vision Transformers. (arXiv:2206.05282v2 [cs.CV] UPDATED)
175. Local Distance Preserving Auto-encoders using Continuous k-Nearest Neighbours Graphs. (arXiv:2206.05909v2 [cs.LG] UPDATED)
176. The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation. (arXiv:2206.06487v2 [cs.CV] UPDATED)
177. FixEval: Execution-based Evaluation of Program Fixes for Programming Problems. (arXiv:2206.07796v3 [cs.SE] UPDATED)
178. Beyond Real-world Benchmark Datasets: An Empirical Study of Node Classification with GNNs. (arXiv:2206.09144v4 [cs.LG] UPDATED)
179. DecisioNet: A Binary-Tree Structured Neural Network. (arXiv:2207.01127v4 [cs.CV] UPDATED)
180. On the Subspace Structure of Gradient-Based Meta-Learning. (arXiv:2207.03804v2 [cs.LG] UPDATED)
181. FLOWGEN: Fast and slow graph generation. (arXiv:2207.07656v5 [cs.LG] UPDATED)
182. Amplitude Scintillation Forecasting Using Bagged Trees. (arXiv:2207.08745v2 [cs.LG] UPDATED)
183. A Deep Reinforcement Learning Approach for Finding Non-Exploitable Strategies in Two-Player Atari Games. (arXiv:2207.08894v2 [cs.LG] UPDATED)
184. Controllable Data Generation by Deep Learning: A Review. (arXiv:2207.09542v4 [cs.LG] UPDATED)
185. Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for Adversarial Robustness. (arXiv:2207.10283v2 [cs.LG] UPDATED)
186. Detecting Small Query Graphs in A Large Graph via Neural Subgraph Search. (arXiv:2207.10305v2 [cs.LG] UPDATED)
187. Provable Defense Against Geometric Transformations. (arXiv:2207.11177v2 [cs.LG] UPDATED)
188. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v2 [cs.LG] UPDATED)
189. Language Models Can Teach Themselves to Program Better. (arXiv:2207.14502v2 [cs.LG] UPDATED)
190. Pyramidal Denoising Diffusion Probabilistic Models. (arXiv:2208.01864v3 [cs.CV] UPDATED)
191. Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v3 [cs.LG] UPDATED)
192. Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training. (arXiv:2208.06102v2 [cs.LG] UPDATED)
193. The Final Ascent: When Bigger Models Generalize Worse on Noisy-Labeled Data. (arXiv:2208.08003v2 [cs.LG] UPDATED)
194. Retrieval-based Controllable Molecule Generation. (arXiv:2208.11126v2 [q-bio.QM] UPDATED)
195. Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models. (arXiv:2208.14133v2 [cs.LG] UPDATED)
196. Identifying Weight-Variant Latent Causal Models. (arXiv:2208.14153v2 [cs.LG] UPDATED)
197. Identifying Latent Causal Content for Multi-Source Domain Adaptation. (arXiv:2208.14161v2 [cs.LG] UPDATED)
198. A Comprehensive Review of Digital Twin -- Part 1: Modeling and Twinning Enabling Technologies. (arXiv:2208.14197v2 [cs.CE] UPDATED)
199. Evolutionary Deep Reinforcement Learning for Dynamic Slice Management in O-RAN. (arXiv:2208.14394v2 [eess.SY] UPDATED)
200. Hermes: Accelerating Long-Latency Load Requests via Perceptron-Based Off-Chip Load Prediction. (arXiv:2209.00188v3 [cs.AR] UPDATED)
201. FP8 Formats for Deep Learning. (arXiv:2209.05433v2 [cs.LG] UPDATED)
202. Identification of Cognitive Workload during Surgical Tasks with Multimodal Deep Learning. (arXiv:2209.06208v2 [cs.LG] UPDATED)
203. Optimizing Connectivity through Network Gradients for the Restricted Boltzmann Machine. (arXiv:2209.06932v2 [cs.LG] UPDATED)
204. Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v2 [eess.IV] UPDATED)
205. A Continual Development Methodology for Large-scale Multitask Dynamic ML Systems. (arXiv:2209.07326v2 [cs.LG] UPDATED)
206. Boosting as Frank-Wolfe. (arXiv:2209.10831v2 [cs.LG] UPDATED)
207. VToonify: Controllable High-Resolution Portrait Video Style Transfer. (arXiv:2209.11224v3 [cs.CV] UPDATED)
208. PL-kNN: A Parameterless Nearest Neighbors Classifier. (arXiv:2209.12647v2 [cs.LG] UPDATED)
209. Evaluation of Medical Image Segmentation Models for Uncertain, Small or Empty Reference Annotations. (arXiv:2209.13008v2 [eess.IV] UPDATED)
210. Accelerating hypersonic reentry simulations using deep learning-based hybridization (with guarantees). (arXiv:2209.13434v2 [stat.ML] UPDATED)
211. Hierarchical Sliced Wasserstein Distance. (arXiv:2209.13570v4 [stat.ML] UPDATED)
212. Online Subset Selection using $\alpha$-Core with no Augmented Regret. (arXiv:2209.14222v2 [cs.LG] UPDATED)
213. A Closer Look at Evaluating the Bit-Flip Attack Against Deep Neural Networks. (arXiv:2209.14243v2 [cs.CR] UPDATED)
214. Neighborhood Gradient Clustering: An Efficient Decentralized Learning Method for Non-IID Data Distributions. (arXiv:2209.14390v2 [cs.LG] UPDATED)
215. GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v2 [cs.LG] UPDATED)
216. Label driven Knowledge Distillation for Federated Learning with non-IID Data. (arXiv:2209.14520v2 [cs.LG] UPDATED)
217. Dataset Distillation for Medical Dataset Sharing. (arXiv:2209.14603v2 [cs.CR] UPDATED)
218. Creative Painting with Latent Diffusion Models. (arXiv:2209.14697v2 [cs.CV] UPDATED)
219. Statistical Learning and Inverse Problems: An Stochastic Gradient Approach. (arXiv:2209.14967v2 [stat.ML] UPDATED)
## cs.AI
---
**93** new papers in cs.AI:-) 
1. Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval. (arXiv:2209.15034v1 [cs.CV])
2. Generalizability of Adversarial Robustness Under Distribution Shifts. (arXiv:2209.15042v1 [cs.LG])
3. Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v1 [stat.ML])
4. Graph Attention Network for Camera Relocalization on Dynamic Scenes. (arXiv:2209.15056v1 [cs.CV])
5. Reasoning about Complex Networks: A Logic Programming Approach. (arXiv:2209.15067v1 [cs.AI])
6. Few-shot Text Classification with Dual Contrastive Consistency. (arXiv:2209.15069v1 [cs.CL])
7. Zero-shot visual reasoning through probabilistic analogical mapping. (arXiv:2209.15087v1 [cs.CV])
8. OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence for Digital Agriculture. (arXiv:2209.15104v1 [cs.AI])
9. How to tackle an emerging topic? Combining strong and weak labels for Covid news NER. (arXiv:2209.15108v1 [cs.CL])
10. A Quantitative Account of Harm. (arXiv:2209.15111v1 [cs.AI])
11. Modeling driver's evasive behavior during safety-critical lane changes:Two-dimensional time-to-collision and deep reinforcement learning. (arXiv:2209.15133v1 [cs.AI])
12. Machine Learning for Stress Monitoring from Wearable Devices: A Systematic Literature Review. (arXiv:2209.15137v1 [cs.AI])
13. Ensemble Machine Learning Model Trained on a New Synthesized Dataset Generalizes Well for Stress Prediction Using Wearable Devices. (arXiv:2209.15146v1 [cs.LG])
14. Rethinking and Recomputing the Value of ML Models. (arXiv:2209.15157v1 [cs.LG])
15. MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features. (arXiv:2209.15159v1 [cs.CV])
16. Blur the Linguistic Boundary: Interpreting Chinese Buddhist Sutra in English via Neural Machine Translation. (arXiv:2209.15164v1 [cs.CL])
17. Reward Shaping for User Satisfaction in a REINFORCE Recommender. (arXiv:2209.15166v1 [cs.IR])
18. Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition. (arXiv:2209.15176v1 [cs.CL])
19. RL-MD: A Novel Reinforcement Learning Approach for DNA Motif Discovery. (arXiv:2209.15181v1 [cs.LG])
20. Learning by Distilling Context. (arXiv:2209.15189v1 [cs.CL])
21. Evaluation of taxonomic and neural embedding methods for calculating semantic similarity. (arXiv:2209.15197v1 [cs.CL])
22. Synonym Detection Using Syntactic Dependency And Neural Embeddings. (arXiv:2209.15202v1 [cs.CL])
23. ASPiRe:Adaptive Skill Priors for Reinforcement Learning. (arXiv:2209.15205v1 [cs.LG])
24. What Makes Pre-trained Language Models Better Zero/Few-shot Learners?. (arXiv:2209.15206v1 [cs.CL])
25. Scale-invariant Bayesian Neural Networks with Connectivity Tangent Kernel. (arXiv:2209.15208v1 [cs.LG])
26. Construction and Applications of Open Business Knowledge Graph. (arXiv:2209.15214v1 [cs.AI])
27. GM-VAE: Representation Learning with VAE on Gaussian Manifold. (arXiv:2209.15217v1 [cs.LG])
28. Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v1 [cs.LG])
29. SoK: On the Impossible Security of Very Large Foundation Models. (arXiv:2209.15259v1 [cs.LG])
30. A Multiple Criteria Decision Analysis based Approach to Remove Uncertainty in SMP Models. (arXiv:2209.15260v1 [cs.SE])
31. Diffusion-based Image Translation using Disentangled Style and Content Representation. (arXiv:2209.15264v1 [cs.CV])
32. Application-Driven AI Paradigm for Human Action Recognition. (arXiv:2209.15271v1 [cs.CV])
33. Online Multi-Agent Decentralized Byzantine-robust Gradient Estimation. (arXiv:2209.15274v1 [cs.AI])
34. A Multivariate Complexity Analysis of Qualitative Reasoning Problems. (arXiv:2209.15275v1 [cs.CC])
35. Machine Unlearning Method Based On Projection Residual. (arXiv:2209.15276v1 [cs.LG])
36. Rethinking skip connection model as a learnable Markov chain. (arXiv:2209.15278v1 [cs.LG])
37. Learning Transferable Spatiotemporal Representations from Natural Script Knowledge. (arXiv:2209.15280v1 [cs.CV])
38. Verifiable and Energy Efficient Medical Image Analysis with Quantised Self-attentive Deep Neural Networks. (arXiv:2209.15287v1 [cs.CV])
39. Effective Early Stopping of Point Cloud Neural Networks. (arXiv:2209.15308v1 [cs.CV])
40. Convolutional Neural Networks Quantization with Attention. (arXiv:2209.15317v1 [cs.AI])
41. Observational Robustness and Invariances in Reinforcement Learning via Lexicographic Objectives. (arXiv:2209.15320v1 [cs.LG])
42. SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data. (arXiv:2209.15329v1 [cs.CL])
43. Automatic Context-Driven Inference of Engagement in HMI: A Survey. (arXiv:2209.15370v1 [cs.HC])
44. Programmable Control of Ultrasound Swarmbots through Reinforcement Learning. (arXiv:2209.15393v1 [cs.RO])
45. Evaluation of importance estimators in deep learning classifiers for Computed Tomography. (arXiv:2209.15398v1 [cs.CV])
46. Parea: multi-view ensemble clustering for cancer subtype discovery. (arXiv:2209.15399v1 [cs.LG])
47. Accurate Long-term Air Temperature Prediction with a Fusion of Artificial Intelligence and Data Reduction Techniques. (arXiv:2209.15424v1 [physics.ao-ph])
48. Tuning of Mixture-of-Experts Mixed-Precision Neural Networks. (arXiv:2209.15427v1 [cs.LG])
49. Relative representations enable zero-shot latent space communication. (arXiv:2209.15430v1 [cs.LG])
50. Safe Exploration Method for Reinforcement Learning under Existence of Disturbance. (arXiv:2209.15452v1 [cs.LG])
51. GPNet: Simplifying Graph Neural Networks via Multi-channel Geometric Polynomials. (arXiv:2209.15454v1 [cs.LG])
52. Scheduling for Urban Air Mobility using Safe Learning. (arXiv:2209.15457v1 [cs.LG])
53. Towards General-Purpose Representation Learning of Polygonal Geometries. (arXiv:2209.15458v1 [cs.CV])
54. Using Knowledge Distillation to improve interpretable models in a retail banking context. (arXiv:2209.15496v1 [cs.LG])
55. Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions. (arXiv:2209.15565v1 [cs.CL])
56. Rethinking Data Heterogeneity in Federated Learning: Introducing a New Notion and Standard Benchmarks. (arXiv:2209.15595v1 [cs.LG])
57. MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction. (arXiv:2209.15597v1 [cs.AI])
58. Protein structure generation via folding diffusion. (arXiv:2209.15611v1 [q-bio.BM])
59. Beyond Bayes-optimality: meta-learning what you know you don't know. (arXiv:2209.15618v1 [cs.AI])
60. A General Framework for Sample-Efficient Function Approximation in Reinforcement Learning. (arXiv:2209.15634v1 [cs.LG])
61. Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. (arXiv:2110.01052v5 [cs.LG] UPDATED)
62. The AI Triplet: Computational, Conceptual, and Mathematical Knowledge in AI Education. (arXiv:2110.09290v2 [cs.CY] UPDATED)
63. Application of the Multi-label Residual Convolutional Neural Network text classifier using Content-Based Routing process. (arXiv:2110.15801v3 [cs.CL] UPDATED)
64. Consistency and Consensus Driven for Hesitant Fuzzy Linguistic Decision Making with Pairwise Comparisons. (arXiv:2111.04092v2 [cs.AI] UPDATED)
65. Plan Your Target and Learn Your Skills: Transferable State-Only Imitation Learning via Decoupled Policy Optimization. (arXiv:2203.02214v5 [cs.LG] UPDATED)
66. One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v3 [cs.CV] UPDATED)
67. Multimodal Hate Speech Detection from Bengali Memes and Texts. (arXiv:2204.10196v2 [cs.CL] UPDATED)
68. Transfer Learning with Pre-trained Conditional Generative Models. (arXiv:2204.12833v2 [cs.LG] UPDATED)
69. DagSim: Combining DAG-based model structure with unconstrained data types and relations for flexible, transparent, and modularized data simulation. (arXiv:2205.11234v2 [cs.AI] UPDATED)
70. Toward Discovering Options that Achieve Faster Planning. (arXiv:2205.12515v2 [cs.LG] UPDATED)
71. An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v3 [cs.LG] UPDATED)
72. Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v3 [cs.CV] UPDATED)
73. Re-parameterizing Your Optimizers rather than Architectures. (arXiv:2205.15242v2 [cs.LG] UPDATED)
74. Neuro-Symbolic Causal Language Planning with Commonsense Prompting. (arXiv:2206.02928v3 [cs.CL] UPDATED)
75. Sparse Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v4 [cs.CV] UPDATED)
76. FLOWGEN: Fast and slow graph generation. (arXiv:2207.07656v5 [cs.LG] UPDATED)
77. A Deep Reinforcement Learning Approach for Finding Non-Exploitable Strategies in Two-Player Atari Games. (arXiv:2207.08894v2 [cs.LG] UPDATED)
78. Controllable Data Generation by Deep Learning: A Review. (arXiv:2207.09542v4 [cs.LG] UPDATED)
79. Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for Adversarial Robustness. (arXiv:2207.10283v2 [cs.LG] UPDATED)
80. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v2 [cs.LG] UPDATED)
81. Language Models Can Teach Themselves to Program Better. (arXiv:2207.14502v2 [cs.LG] UPDATED)
82. Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v3 [cs.LG] UPDATED)
83. Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training. (arXiv:2208.06102v2 [cs.LG] UPDATED)
84. K-MHaS: A Multi-label Hate Speech Detection Dataset in Korean Online News Comment. (arXiv:2208.10684v3 [cs.CL] UPDATED)
85. A Comprehensive Review of Digital Twin -- Part 1: Modeling and Twinning Enabling Technologies. (arXiv:2208.14197v2 [cs.CE] UPDATED)
86. Evolutionary Deep Reinforcement Learning for Dynamic Slice Management in O-RAN. (arXiv:2208.14394v2 [eess.SY] UPDATED)
87. Identification of Cognitive Workload during Surgical Tasks with Multimodal Deep Learning. (arXiv:2209.06208v2 [cs.LG] UPDATED)
88. A Continual Development Methodology for Large-scale Multitask Dynamic ML Systems. (arXiv:2209.07326v2 [cs.LG] UPDATED)
89. Online Subset Selection using $\alpha$-Core with no Augmented Regret. (arXiv:2209.14222v2 [cs.LG] UPDATED)
90. GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v2 [cs.LG] UPDATED)
91. Label driven Knowledge Distillation for Federated Learning with non-IID Data. (arXiv:2209.14520v2 [cs.LG] UPDATED)
92. Creative Painting with Latent Diffusion Models. (arXiv:2209.14697v2 [cs.CV] UPDATED)
93. Compositional Semantic Parsing with Large Language Models. (arXiv:2209.15003v2 [cs.CL] UPDATED)

