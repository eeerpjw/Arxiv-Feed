# Your interest papers
---
## cs.CV
---
### Towards Interpretable Video Super-Resolution via Alternating Optimization. (arXiv:2207.10765v1 [cs.CV])
- Authors : Jiezhang Cao, Jingyun Liang, Kai Zhang, Wenguan Wang, Qin Wang, Yulun Zhang, Hao Tang, Luc Van
- Link : [http://arxiv.org/abs/2207.10765](http://arxiv.org/abs/2207.10765)
> ABSTRACT  :  In this paper, we study a practical space-time video super-resolution (STVSR) problem which aims at generating a high-framerate high-resolution sharp video from a low-framerate low-resolution blurry video. Such problem often occurs when recording a fast dynamic event with a low-framerate and low-resolution camera, and the captured video would suffer from three typical issues: i) motion blur occurs due to object/camera motions during **exposure** time; ii) motion aliasing is unavoidable when the event temporal frequency exceeds the Nyquist limit of temporal sampling; iii) high-frequency details are lost because of the low spatial sampling rate. These issues can be alleviated by a cascade of three separate sub-tasks, including video deblurring, frame interpolation, and super-resolution, which, however, would fail to capture the spatial and temporal correlations among video sequences. To address this, we propose an interpretable STVSR framework by leveraging both model-based and learning-based methods. Specifically, we formulate STVSR as a joint video deblurring, frame interpolation, and super-resolution problem, and solve it as two sub-problems in an alternate way. For the first sub-problem, we derive an interpretable analytical solution and use it as a Fourier data transform layer. Then, we propose a recurrent video **enhancement** layer for the second sub-problem to further recover high-frequency details. Extensive experiments demonstrate the superiority of our method in terms of quantitative metrics and visual quality.  
### Auto-regressive Image Synthesis with Integrated Quantization. (arXiv:2207.10776v1 [cs.CV])
- Authors : Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Kaiwen Cui, Changgong Zhang, Shijian Lu
- Link : [http://arxiv.org/abs/2207.10776](http://arxiv.org/abs/2207.10776)
> ABSTRACT  :  Deep generative models have achieved conspicuous progress in realistic image synthesis with multifarious conditional inputs, while generating diverse yet high-fidelity images remains a grand challenge in conditional image generation. This paper presents a versatile framework for conditional image generation which incorporates the inductive bias of CNNs and powerful sequence modeling of auto-regression that naturally leads to diverse image generation. Instead of independently quantizing the features of multiple domains as in prior research, we design an integrated quantization scheme with a variational regularizer that mingles the feature discretization in multiple domains, and markedly boosts the auto-regressive modeling performance. Notably, the variational regularizer enables to regularize feature distributions in incomparable latent spaces by penalizing the intra-domain variations of distributions. In addition, we design a Gumbel sampling strategy that allows to incorporate distribution uncertainty into the auto-regressive training procedure. The Gumbel sampling substantially mitigates the **exposure** bias that often incurs misalignment between the training and inference stages and severely impairs the inference performance. Extensive experiments over multiple conditional image generation tasks show that our method achieves superior diverse image generation performance qualitatively and quantitatively as compared with the state-of-the-art.  
### Spatio-Temporal Deformable Attention Network for Video Deblurring. (arXiv:2207.10852v1 [cs.CV])
- Authors : Huicong Zhang, Haozhe Xie, Hongxun Yao
- Link : [http://arxiv.org/abs/2207.10852](http://arxiv.org/abs/2207.10852)
> ABSTRACT  :  The key success factor of the video deblurring methods is to compensate for the blurry pixels of the mid-frame with the sharp pixels of the adjacent video frames. Therefore, mainstream methods align the adjacent frames based on the estimated optical flows and fuse the alignment frames for **restoration**. However, these methods sometimes generate unsatisfactory results because they rarely consider the blur levels of pixels, which may introduce blurry pixels from video frames. Actually, not all the pixels in the video frames are sharp and beneficial for deblurring. To address this problem, we propose the spatio-temporal deformable attention network (STDANet) for video delurring, which extracts the information of sharp pixels by considering the pixel-wise blur levels of the video frames. Specifically, STDANet is an encoder-decoder network combined with the motion estimator and spatio-temporal deformable attention (STDA) module, where motion estimator predicts coarse optical flows that are used as base offsets to find the corresponding sharp pixels in STDA module. Experimental results indicate that the proposed STDANet performs favorably against state-of-the-art methods on the GoPro, DVD, and BSD datasets.  
### Cost Aggregation with 4D Convolutional **Swin** Transformer for Few-Shot Segmentation. (arXiv:2207.10866v1 [cs.CV])
- Authors : Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, Seungryong Kim
- Link : [http://arxiv.org/abs/2207.10866](http://arxiv.org/abs/2207.10866)
> ABSTRACT  :  This paper presents a novel cost aggregation network, called Volumetric Aggregation with Transformers (VAT), for few-shot segmentation. The use of transformers can benefit correlation map aggregation through self-attention over a global receptive field. However, the tokenization of a correlation map for transformer processing can be detrimental, because the discontinuity at token boundaries reduces the local context available near the token edges and decreases inductive bias. To address this problem, we propose a 4D Convolutional **Swin** Transformer, where a high-dimensional **Swin** Transformer is preceded by a series of small-kernel convolutions that impart local context to all pixels and introduce convolutional inductive bias. We additionally boost aggregation performance by applying transformers within a pyramidal structure, where aggregation at a coarser level guides aggregation at a finer level. Noise in the transformer output is then filtered in the subsequent decoder with the help of the query's appearance embedding. With this model, a new state-of-the-art is set for all the standard benchmarks in few-shot segmentation. It is shown that VAT attains state-of-the-art performance for semantic correspondence as well, where cost aggregation also plays a central role.  
### Visible and Near Infrared Image Fusion Based on Texture Information. (arXiv:2207.10953v1 [cs.CV])
- Authors : Guanyu Zhang, Beichen Sun, Yuehan Qi, Yang Liu
- Link : [http://arxiv.org/abs/2207.10953](http://arxiv.org/abs/2207.10953)
> ABSTRACT  :  Multi-sensor fusion is widely used in the environment perception system of the autonomous vehicle. It solves the interference caused by environmental changes and makes the whole driving system safer and more reliable. In this paper, a novel visible and near-infrared fusion method based on texture information is proposed to enhance unstructured environmental images. It aims at the problems of artifact, information loss and noise in traditional visible and near infrared image fusion methods. Firstly, the structure information of the visible image (RGB) and the near infrared image (NIR) after texture removal is obtained by relative total variation (RTV) calculation as the base layer of the fused image; secondly, a Bayesian classification model is established to calculate the noise weight and the noise information and the noise information in the visible image is adaptively filtered by joint **bilateral** filter; finally, the fused image is acquired by color space conversion. The experimental results demonstrate that the proposed algorithm can preserve the spectral characteristics and the unique information of visible and near-infrared images without artifacts and color distortion, and has good robustness as well as preserving the unique texture.  
### Faster VoxelPose: **Real-time** 3D Human Pose Estimation by Orthographic Projection. (arXiv:2207.10955v1 [cs.CV])
- Authors : Hang Ye, Wentao Zhu, Chunyu Wang, Rujie Wu, Yizhou Wang
- Link : [http://arxiv.org/abs/2207.10955](http://arxiv.org/abs/2207.10955)
> ABSTRACT  :  While the voxel-based methods have achieved promising results for multi-person 3D pose estimation from multi-cameras, they suffer from heavy computation burdens, especially for large scenes. We present Faster VoxelPose to address the challenge by re-projecting the feature volume to the three two-dimensional coordinate planes and estimating X, Y, Z coordinates from them separately. To that end, we first localize each person by a 3D bounding box by estimating a 2D box and its height based on the volume features projected to the xy-plane and z-axis, respectively. Then for each person, we estimate partial joint coordinates from the three coordinate planes separately which are then fused to obtain the final 3D pose. The method is free from costly 3D-CNNs and improves the speed of VoxelPose by ten times and meanwhile achieves competitive accuracy as the state-of-the-art methods, proving its potential in real-time applications.  
### NeurAR: Neural Uncertainty for Autonomous 3D Reconstruction. (arXiv:2207.10985v1 [cs.CV])
- Authors : Yunlong Ran, Jing Zeng, Shibo He, Lincheng Li, Yingfeng Chen, Gimhee Lee, Jiming Chen, Qi Ye
- Link : [http://arxiv.org/abs/2207.10985](http://arxiv.org/abs/2207.10985)
> ABSTRACT  :  Implicit neural representations have shown compelling results in offline 3D reconstruction and also recently demonstrated the potential for online SLAM systems. However, applying them to autonomous 3D reconstruction, where robots are required to explore a scene and plan a view path for the reconstruction, has not been studied. In this paper, we explore for the first time the possibility of using **implicit neural representation**s for autonomous 3D scene reconstruction by addressing two key challenges: 1) seeking a criterion to measure the quality of the candidate viewpoints for the view planning based on the new representations, and 2) learning the criterion from data that can generalize to different scenes instead of hand-crafting one. For the first challenge, a proxy of Peak Signal-to-Noise Ratio (PSNR) is proposed to quantify a viewpoint quality. The proxy is acquired by treating the color of a spatial point in a scene as a random variable under a Gaussian distribution rather than a deterministic one; the variance of the distribution quantifies the uncertainty of the reconstruction and composes the proxy. For the second challenge, the proxy is optimized jointly with the parameters of an implicit neural network for the scene. With the proposed view quality criterion, we can then apply the new representations to autonomous 3D reconstruction. Our method demonstrates significant improvements on various metrics for the rendered image quality and the geometry quality of the reconstructed 3D models when compared with variants using TSDF or reconstruction without view planning.  
### 3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal. (arXiv:2207.11061v1 [cs.CV])
- Authors : Hao Meng, Sheng Jin, Wentao Liu, Chen Qian, Mengxiang Lin, Wanli Ouyang, Ping Luo
- Link : [http://arxiv.org/abs/2207.11061](http://arxiv.org/abs/2207.11061)
> ABSTRACT  :  Estimating 3D interacting hand pose from a single RGB image is essential for understanding human actions. Unlike most previous works that directly predict the 3D poses of two interacting hands simultaneously, we propose to decompose the challenging interacting hand pose estimation task and estimate the pose of each hand separately. In this way, it is straightforward to take advantage of the latest research progress on the single-hand pose estimation system. However, hand pose estimation in interacting scenarios is very challenging, due to (1) severe hand-hand occlusion and (2) ambiguity caused by the homogeneous appearance of hands. To tackle these two challenges, we propose a novel Hand De-occlusion and Removal (**HDR**) framework to perform hand de-occlusion and distractor removal. We also propose the first large-scale synthetic amodal hand dataset, termed Amodal InterHand Dataset (AIH), to facilitate model training and promote the development of the related research. Experiments show that the proposed method significantly outperforms previous state-of-the-art interacting hand pose estimation approaches. Codes and data are available at https://github.com/MengHao666/**HDR**.  
### Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v1 [eess.IV])
- Authors : Emanuele Dalsasso, my Abergel, Florence Tupin
- Link : [http://arxiv.org/abs/2207.11095](http://arxiv.org/abs/2207.11095)
> ABSTRACT  :  Speckle filtering is generally a prerequisite to the analysis of synthetic aperture radar (SAR) images. Tremendous progress has been achieved in the domain of single-image despeckling. Latest techniques rely on deep neural networks to restore the various structures and textures peculiar to SAR images. The availability of time series of SAR images offers the possibility of improving speckle filtering by combining different speckle realizations over the same area. The supervised training of deep neural networks requires ground-truth speckle-free images. Such images can only be obtained indirectly through some form of averaging, by spatial or temporal integration, and are imperfect. Given the potential of very high quality **restoration** reachable by multi-temporal speckle filtering, the limitations of ground-truth images need to be circumvented. We extend a recent self-supervised training strategy for single-look complex SAR images, called MERLIN, to the case of multi-temporal filtering. This requires modeling the sources of statistical dependencies in the spatial and temporal dimensions as well as between the real and imaginary components of the complex amplitudes. Quantitative analysis on datasets with simulated speckle indicates a clear improvement of speckle reduction when additional SAR images are included. Our method is then applied to stacks of TerraSAR-X images and shown to outperform competing multi-temporal speckle filtering approaches. The code of the trained models is made freely available on the $\href{https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/}{\text{GitLab}}$ of the IMAGES team of the LTCI Lab, T\'el\'ecom Paris Institut Polytechnique de Paris.  
### Fast strategies for multi-temporal speckle reduction of Sentinel-1 GRD images. (arXiv:2207.11111v1 [eess.IV])
- Authors : Emanuele Dalsasso, Florence Tupin
- Link : [http://arxiv.org/abs/2207.11111](http://arxiv.org/abs/2207.11111)
> ABSTRACT  :  Reducing speckle and limiting the variations of the physical parameters in Synthetic Aperture Radar (SAR) images is often a key-step to fully exploit the potential of such data. Nowadays, deep learning approaches produce state of the art results in single-image SAR **restoration**. Nevertheless, huge multi-temporal stacks are now often available and could be efficiently exploited to further improve image quality. This paper explores two fast strategies employing a single-image despeckling algorithm, namely SAR2SAR, in a multi-temporal framework. The first one is based on Quegan filter and replaces the local reflectivity pre-estimation by SAR2SAR. The second one uses SAR2SAR to suppress speckle from a ratio image encoding the multi-temporal information under the form of a "super-image", i.e. the temporal arithmetic mean of a time series. Experimental results on Sentinel-1 GRD data show that these two multi-temporal strategies provide improved filtering results while adding a limited computational cost.  
### Improved $\alpha$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning. (arXiv:2207.11223v1 [eess.IV])
- Authors : Sanaz Mohammadjafari, Mucahit Cevik, Ayse Basar
- Link : [http://arxiv.org/abs/2207.11223](http://arxiv.org/abs/2207.11223)
> ABSTRACT  :  Generative Adversarial Networks (GANs) have gained significant attention in several computer vision tasks for generating high-quality synthetic data. Various medical applications including diagnostic imaging and radiation therapy can benefit greatly from synthetic data generation due to data scarcity in the domain. However, medical image data is typically kept in 3D space, and generative models suffer from the curse of dimensionality issues in generating such synthetic data. In this paper, we investigate the potential of GANs for generating connected 3D volumes. We propose an improved version of 3D $\alpha$-GAN by incorporating various architectural **enhancement**s. On a synthetic dataset of connected 3D spheres and ellipsoids, our model can generate fully connected 3D shapes with similar geometrical characteristics to that of training data. We also show that our 3D GAN model can successfully generate high-quality 3D tumor volumes and associated treatment specifications (e.g., isocenter locations). Similar moment invariants to the training data as well as fully connected 3D shapes confirm that improved 3D $\alpha$-GAN implicitly learns the training data distribution, and generates realistic-looking samples. The capability of improved 3D $\alpha$-GAN makes it a valuable source for generating synthetic medical image data that can help future research in this domain.  
### FoV-**NeRF**: Foveated Neural Radiance Fields for Virtual Reality. (arXiv:2103.16365v2 [cs.GR] UPDATED)
- Authors : Nianchen Deng, Zhenyi He, Jiannan Ye, Budmonde Duinkharjav, Praneeth Chakravarthula, Xubo Yang, Qi Sun
- Link : [http://arxiv.org/abs/2103.16365](http://arxiv.org/abs/2103.16365)
> ABSTRACT  :  Virtual Reality (VR) is becoming ubiquitous with the rise of consumer displays and commercial VR platforms. Such displays require low latency and high quality rendering of synthetic imagery with reduced compute overheads. Recent advances in neural rendering showed promise of unlocking new possibilities in 3D computer graphics via image-based representations of virtual or physical environments. Specifically, the neural radiance fields (**NeRF**) demonstrated that photo-realistic quality and continuous view changes of 3D scenes can be achieved without loss of view-dependent effects. While **NeRF** can significantly benefit rendering for VR applications, it faces unique challenges posed by high field-of-view, high resolution, and stereoscopic/egocentric viewing, typically causing low quality and high latency of the rendered images. In VR, this not only harms the interaction experience but may also cause sickness. To tackle these problems toward six-degrees-of-freedom, egocentric, and stereo **NeRF** in VR, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. We then jointly optimize the latency/performance and visual quality while mutually bridging human perception and neural scene synthesis to achieve perceptually high-quality immersive interaction. We conducted both objective analysis and subjective studies to evaluate the effectiveness of our approach. We find that our method significantly reduces latency (up to 99% time reduction compared with **NeRF**) without loss of high-fidelity rendering (perceptually identical to full-resolution ground truth). The presented approach may serve as the first step toward future VR/AR systems that capture, teleport, and visualize remote environments in real-time.  
### MoFa**NeRF**: Morphable Facial Neural Radiance Field. (arXiv:2112.02308v2 [cs.CV] UPDATED)
- Authors : Yiyu Zhuang, Hao Zhu, Xusen Sun, Xun Cao
- Link : [http://arxiv.org/abs/2112.02308](http://arxiv.org/abs/2112.02308)
> ABSTRACT  :  We propose a parametric model that maps free-view images into a vector space of coded facial shape, expression and appearance with a neural radiance field, namely Morphable Facial **NeRF**. Specifically, MoFa**NeRF** takes the coded facial shape, expression and appearance along with space coordinate and view direction as input to an MLP, and outputs the radiance of the space point for photo-realistic image synthesis. Compared with conventional 3D morphable models (3DMM), MoFa**NeRF** shows superiority in directly synthesizing photo-realistic facial details even for eyes, mouths, and beards. Also, continuous face morphing can be easily achieved by interpolating the input shape, expression and appearance codes. By introducing identity-specific modulation and texture encoder, our model synthesizes accurate photometric details and shows strong representation ability. Our model shows strong ability on multiple applications including image-based fitting, random generation, face rigging, face editing, and novel view synthesis. Experiments show that our method achieves higher representation ability than previous parametric models, and achieves competitive performance in several applications. To the best of our knowledge, our work is the first facial parametric model built upon a neural radiance field that can be used in fitting, generation and manipulation. The code and data is available at https://github.com/zhuhao-nju/mofanerf.  
### Formulating Event-based Image Reconstruction as a Linear Inverse Problem using Optical Flow. (arXiv:2112.06242v2 [cs.CV] UPDATED)
- Authors : Zelin Zhang, Anthony Yezzi, Guillermo Gallego
- Link : [http://arxiv.org/abs/2112.06242](http://arxiv.org/abs/2112.06242)
> ABSTRACT  :  Event cameras are novel bio-inspired sensors that measure per-pixel brightness differences asynchronously. Recovering brightness from events is appealing since the reconstructed images inherit the **high dynamic range** (**HDR**) and high-speed properties of events; hence they can be used in many robotic vision applications and to generate slow-motion **HDR** videos. However, state-of-the-art methods tackle this problem by training an event-to-image recurrent neural network (RNN), which lacks explainability and is difficult to tune. In this work we show, for the first time, how tackling the joint problem of motion and brightness estimation leads us to formulate event-based image reconstruction as a linear inverse problem that can be solved without training an image reconstruction RNN. Instead, classical and learning-based image priors can be used to solve the problem and remove artifacts from the reconstructed images. The experiments show that the proposed approach generates images with visual quality on par with state-of-the-art methods despite only using data from a short time interval. The proposed linear formulation and solvers have a unifying character because they can be applied also to reconstruct brightness from the second derivative. Additionally, the linear formulation is attractive because it can be naturally combined with super-resolution, motion-segmentation and color demosaicing.  
### An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images. (arXiv:2204.01341v3 [cs.CV] UPDATED)
- Authors : Jiawei Zhang, Xin Zhao, Tao Jiang, Md Mamunur, Yudong Yao, Hao Lin, Jinghua Zhang, Ao Pan, Marcin Grzegorzek, Chen Li
- Link : [http://arxiv.org/abs/2204.01341](http://arxiv.org/abs/2204.01341)
> ABSTRACT  :  This paper proposes a novel pixel interval down-sampling network (PID-Net) for dense tiny object (yeast cells) counting tasks with higher accuracy. The PID-Net is an end-to-end convolutional neural network (CNN) model with an encoder--decoder architecture. The pixel interval down-sampling operations are concatenated with max-pooling operations to combine the sparse and dense features. This addresses the limitation of contour conglutination of dense objects while counting. The evaluation was conducted using classical segmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as counting metrics. The experimental results show that the proposed PID-Net had the best performance and potential for dense tiny object counting tasks, which achieved 96.97\% counting accuracy on the dataset with 2448 yeast cell images. By comparing with the state-of-the-art approaches, such as Attention U-Net, **Swin** U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects with clearer boundaries and fewer incorrect debris, which shows the great potential of PID-Net in the task of accurate counting.  
### Learned Video Compression via Heterogeneous Deformable Compensation Network. (arXiv:2207.04589v2 [eess.IV] UPDATED)
- Authors : Huairui Wang, Zhenzhong Chen, Chang Wen
- Link : [http://arxiv.org/abs/2207.04589](http://arxiv.org/abs/2207.04589)
> ABSTRACT  :  Learned video compression has recently emerged as an essential research topic in developing advanced video compression technologies, where motion compensation is considered one of the most challenging issues. In this paper, we propose a learned video compression framework via heterogeneous deformable compensation strategy (HDCVC) to tackle the problems of unstable compression performance caused by single-size deformable kernels in downsampled feature domain. More specifically, instead of utilizing optical flow warping or single-size-kernel deformable alignment, the proposed algorithm extracts features from the two adjacent frames to estimate content-adaptive heterogeneous deformable (HetDeform) kernel offsets. Then we transform the reference features with the HetDeform convolution to accomplish motion compensation. Moreover, we design a Spatial-Neighborhood-Conditioned Divisive Normalization (SNCDN) to achieve more effective data Gaussianization combined with the Generalized Divisive Normalization. Furthermore, we propose a multi-frame enhanced reconstruction module for exploiting context and temporal information for final quality **enhancement**. Experimental results indicate that HDCVC achieves superior performance than the recent state-of-the-art learned video compression approaches.  
## eess.IV
---
### Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v1 [eess.IV])
- Authors : Emanuele Dalsasso, my Abergel, Florence Tupin
- Link : [http://arxiv.org/abs/2207.11095](http://arxiv.org/abs/2207.11095)
> ABSTRACT  :  Speckle filtering is generally a prerequisite to the analysis of synthetic aperture radar (SAR) images. Tremendous progress has been achieved in the domain of single-image despeckling. Latest techniques rely on deep neural networks to restore the various structures and textures peculiar to SAR images. The availability of time series of SAR images offers the possibility of improving speckle filtering by combining different speckle realizations over the same area. The supervised training of deep neural networks requires ground-truth speckle-free images. Such images can only be obtained indirectly through some form of averaging, by spatial or temporal integration, and are imperfect. Given the potential of very high quality **restoration** reachable by multi-temporal speckle filtering, the limitations of ground-truth images need to be circumvented. We extend a recent self-supervised training strategy for single-look complex SAR images, called MERLIN, to the case of multi-temporal filtering. This requires modeling the sources of statistical dependencies in the spatial and temporal dimensions as well as between the real and imaginary components of the complex amplitudes. Quantitative analysis on datasets with simulated speckle indicates a clear improvement of speckle reduction when additional SAR images are included. Our method is then applied to stacks of TerraSAR-X images and shown to outperform competing multi-temporal speckle filtering approaches. The code of the trained models is made freely available on the $\href{https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/}{\text{GitLab}}$ of the IMAGES team of the LTCI Lab, T\'el\'ecom Paris Institut Polytechnique de Paris.  
### Fast strategies for multi-temporal speckle reduction of Sentinel-1 GRD images. (arXiv:2207.11111v1 [eess.IV])
- Authors : Emanuele Dalsasso, Florence Tupin
- Link : [http://arxiv.org/abs/2207.11111](http://arxiv.org/abs/2207.11111)
> ABSTRACT  :  Reducing speckle and limiting the variations of the physical parameters in Synthetic Aperture Radar (SAR) images is often a key-step to fully exploit the potential of such data. Nowadays, deep learning approaches produce state of the art results in single-image SAR **restoration**. Nevertheless, huge multi-temporal stacks are now often available and could be efficiently exploited to further improve image quality. This paper explores two fast strategies employing a single-image despeckling algorithm, namely SAR2SAR, in a multi-temporal framework. The first one is based on Quegan filter and replaces the local reflectivity pre-estimation by SAR2SAR. The second one uses SAR2SAR to suppress speckle from a ratio image encoding the multi-temporal information under the form of a "super-image", i.e. the temporal arithmetic mean of a time series. Experimental results on Sentinel-1 GRD data show that these two multi-temporal strategies provide improved filtering results while adding a limited computational cost.  
### Improved $\alpha$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning. (arXiv:2207.11223v1 [eess.IV])
- Authors : Sanaz Mohammadjafari, Mucahit Cevik, Ayse Basar
- Link : [http://arxiv.org/abs/2207.11223](http://arxiv.org/abs/2207.11223)
> ABSTRACT  :  Generative Adversarial Networks (GANs) have gained significant attention in several computer vision tasks for generating high-quality synthetic data. Various medical applications including diagnostic imaging and radiation therapy can benefit greatly from synthetic data generation due to data scarcity in the domain. However, medical image data is typically kept in 3D space, and generative models suffer from the curse of dimensionality issues in generating such synthetic data. In this paper, we investigate the potential of GANs for generating connected 3D volumes. We propose an improved version of 3D $\alpha$-GAN by incorporating various architectural **enhancement**s. On a synthetic dataset of connected 3D spheres and ellipsoids, our model can generate fully connected 3D shapes with similar geometrical characteristics to that of training data. We also show that our 3D GAN model can successfully generate high-quality 3D tumor volumes and associated treatment specifications (e.g., isocenter locations). Similar moment invariants to the training data as well as fully connected 3D shapes confirm that improved 3D $\alpha$-GAN implicitly learns the training data distribution, and generates realistic-looking samples. The capability of improved 3D $\alpha$-GAN makes it a valuable source for generating synthetic medical image data that can help future research in this domain.  
### Learned Video Compression via Heterogeneous Deformable Compensation Network. (arXiv:2207.04589v2 [eess.IV] UPDATED)
- Authors : Huairui Wang, Zhenzhong Chen, Chang Wen
- Link : [http://arxiv.org/abs/2207.04589](http://arxiv.org/abs/2207.04589)
> ABSTRACT  :  Learned video compression has recently emerged as an essential research topic in developing advanced video compression technologies, where motion compensation is considered one of the most challenging issues. In this paper, we propose a learned video compression framework via heterogeneous deformable compensation strategy (HDCVC) to tackle the problems of unstable compression performance caused by single-size deformable kernels in downsampled feature domain. More specifically, instead of utilizing optical flow warping or single-size-kernel deformable alignment, the proposed algorithm extracts features from the two adjacent frames to estimate content-adaptive heterogeneous deformable (HetDeform) kernel offsets. Then we transform the reference features with the HetDeform convolution to accomplish motion compensation. Moreover, we design a Spatial-Neighborhood-Conditioned Divisive Normalization (SNCDN) to achieve more effective data Gaussianization combined with the Generalized Divisive Normalization. Furthermore, we propose a multi-frame enhanced reconstruction module for exploiting context and temporal information for final quality **enhancement**. Experimental results indicate that HDCVC achieves superior performance than the recent state-of-the-art learned video compression approaches.  
## cs.LG
---
### TaDaa: **real time** Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems. (arXiv:2207.11187v1 [cs.IR])
- Authors : Leon Feng, Jnana Senapati, Bill Liu
- Link : [http://arxiv.org/abs/2207.11187](http://arxiv.org/abs/2207.11187)
> ABSTRACT  :  This paper proposes TaDaa: Ticket Assignment Deep learning Auto Advisor, which leverages the latest Transformers models and machine learning techniques quickly assign issues within an organization, like customer support, help desk and alike issue ticketing systems. The project provides functionality to 1) assign an issue to the correct group, 2) assign an issue to the best resolver, and 3) provide the most relevant previously solved tickets to resolvers. We leverage one ticketing system sample dataset, with over 3k+ groups and over 10k+ resolvers to obtain a 95.2% top 3 accuracy on group suggestions and a 79.0% top 5 accuracy on resolver suggestions. We hope this research will greatly improve average issue resolution time on customer support, help desk, and issue ticketing systems.  
### Improved $\alpha$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning. (arXiv:2207.11223v1 [eess.IV])
- Authors : Sanaz Mohammadjafari, Mucahit Cevik, Ayse Basar
- Link : [http://arxiv.org/abs/2207.11223](http://arxiv.org/abs/2207.11223)
> ABSTRACT  :  Generative Adversarial Networks (GANs) have gained significant attention in several computer vision tasks for generating high-quality synthetic data. Various medical applications including diagnostic imaging and radiation therapy can benefit greatly from synthetic data generation due to data scarcity in the domain. However, medical image data is typically kept in 3D space, and generative models suffer from the curse of dimensionality issues in generating such synthetic data. In this paper, we investigate the potential of GANs for generating connected 3D volumes. We propose an improved version of 3D $\alpha$-GAN by incorporating various architectural **enhancement**s. On a synthetic dataset of connected 3D spheres and ellipsoids, our model can generate fully connected 3D shapes with similar geometrical characteristics to that of training data. We also show that our 3D GAN model can successfully generate high-quality 3D tumor volumes and associated treatment specifications (e.g., isocenter locations). Similar moment invariants to the training data as well as fully connected 3D shapes confirm that improved 3D $\alpha$-GAN implicitly learns the training data distribution, and generates realistic-looking samples. The capability of improved 3D $\alpha$-GAN makes it a valuable source for generating synthetic medical image data that can help future research in this domain.  
### Formulating Event-based Image Reconstruction as a Linear Inverse Problem using Optical Flow. (arXiv:2112.06242v2 [cs.CV] UPDATED)
- Authors : Zelin Zhang, Anthony Yezzi, Guillermo Gallego
- Link : [http://arxiv.org/abs/2112.06242](http://arxiv.org/abs/2112.06242)
> ABSTRACT  :  Event cameras are novel bio-inspired sensors that measure per-pixel brightness differences asynchronously. Recovering brightness from events is appealing since the reconstructed images inherit the **high dynamic range** (**HDR**) and high-speed properties of events; hence they can be used in many robotic vision applications and to generate slow-motion **HDR** videos. However, state-of-the-art methods tackle this problem by training an event-to-image recurrent neural network (RNN), which lacks explainability and is difficult to tune. In this work we show, for the first time, how tackling the joint problem of motion and brightness estimation leads us to formulate event-based image reconstruction as a linear inverse problem that can be solved without training an image reconstruction RNN. Instead, classical and learning-based image priors can be used to solve the problem and remove artifacts from the reconstructed images. The experiments show that the proposed approach generates images with visual quality on par with state-of-the-art methods despite only using data from a short time interval. The proposed linear formulation and solvers have a unifying character because they can be applied also to reconstruct brightness from the second derivative. Additionally, the linear formulation is attractive because it can be naturally combined with super-resolution, motion-segmentation and color demosaicing.  
### Neighbour Interaction based Click-Through Rate Prediction via Graph-masked Transformer. (arXiv:2201.13311v2 [cs.IR] UPDATED)
- Authors : Erxue Min, Yu Rong, Tingyang Xu, Yatao Bian, Peilin Zhao, Junzhou Huang, Da Luo, Kangyi Lin, Sophia Ananiadou
- Link : [http://arxiv.org/abs/2201.13311](http://arxiv.org/abs/2201.13311)
> ABSTRACT  :  Click-Through Rate (CTR) prediction, which aims to estimate the probability that a user will click an item, is an essential component of online advertising. Existing methods mainly attempt to mine user interests from users' historical behaviours, which contain users' directly interacted items. Although these methods have made great progress, they are often limited by the recommender system's direct **exposure** and inactive interactions, and thus fail to mine all potential user interests. To tackle these problems, we propose Neighbor-Interaction based CTR prediction (NI-CTR), which considers this task under a Heterogeneous Information Network (HIN) setting. In short, Neighbor-Interaction based CTR prediction involves the local neighborhood of the target user-item pair in the HIN to predict their linkage. In order to guide the representation learning of the local neighbourhood, we further consider different kinds of interactions among the local neighborhood nodes from both explicit and implicit perspective, and propose a novel Graph-Masked Transformer (GMT) to effectively incorporates these kinds of interactions to produce highly representative embeddings for the target user-item pair. Moreover, in order to improve model robustness against neighbour sampling, we enforce a consistency regularization loss over the neighbourhood embedding.    We conduct extensive experiments on two real-world datasets with millions of instances and the experimental results show that our proposed method outperforms state-of-the-art CTR models significantly. Meanwhile, the comprehensive ablation studies verify the effectiveness of every component of our model. Furthermore, we have deployed this framework on the WeChat Official Account Platform with billions of users. The online A/B tests demonstrate an average CTR improvement of 21.9 against all online baselines.  
## cs.AI
---
### TaDaa: **real time** Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems. (arXiv:2207.11187v1 [cs.IR])
- Authors : Leon Feng, Jnana Senapati, Bill Liu
- Link : [http://arxiv.org/abs/2207.11187](http://arxiv.org/abs/2207.11187)
> ABSTRACT  :  This paper proposes TaDaa: Ticket Assignment Deep learning Auto Advisor, which leverages the latest Transformers models and machine learning techniques quickly assign issues within an organization, like customer support, help desk and alike issue ticketing systems. The project provides functionality to 1) assign an issue to the correct group, 2) assign an issue to the best resolver, and 3) provide the most relevant previously solved tickets to resolvers. We leverage one ticketing system sample dataset, with over 3k+ groups and over 10k+ resolvers to obtain a 95.2% top 3 accuracy on group suggestions and a 79.0% top 5 accuracy on resolver suggestions. We hope this research will greatly improve average issue resolution time on customer support, help desk, and issue ticketing systems.  
### Data Augmentation in Natural Language Processing: A Novel Text Generation Approach for Long and Short Text Classifiers. (arXiv:2103.14453v2 [cs.CL] UPDATED)
- Authors : Markus Bayer, rn Buchhold, Marcel Keller, rg Dallmeyer, Christian Reuter
- Link : [http://arxiv.org/abs/2103.14453](http://arxiv.org/abs/2103.14453)
> ABSTRACT  :  In many cases of machine learning, research suggests that the development of training data might have a higher relevance than the choice and modelling of classifiers themselves. Thus, data augmentation methods have been developed to improve classifiers by artificially created training data. In NLP, there is the challenge of establishing universal rules for text transformations which provide new linguistic patterns. In this paper, we present and evaluate a text generation method suitable to increase the performance of classifiers for long and short texts. We achieved promising improvements when evaluating short as well as long text tasks with the **enhancement** by our text generation method. Especially with regard to small data analytics, additive accuracy gains of up to 15.53% and 3.56% are achieved within a constructed low data regime, compared to the no augmentation baseline and another data augmentation technique. As the current track of these constructed regimes is not universally applicable, we also show major improvements in several real world low data tasks (up to +4.84 F1-score). Since we are evaluating the method from many perspectives (in total 11 datasets), we also observe situations where the method might not be suitable. We discuss implications and patterns for the successful application of our approach on different types of datasets.  
### Neighbour Interaction based Click-Through Rate Prediction via Graph-masked Transformer. (arXiv:2201.13311v2 [cs.IR] UPDATED)
- Authors : Erxue Min, Yu Rong, Tingyang Xu, Yatao Bian, Peilin Zhao, Junzhou Huang, Da Luo, Kangyi Lin, Sophia Ananiadou
- Link : [http://arxiv.org/abs/2201.13311](http://arxiv.org/abs/2201.13311)
> ABSTRACT  :  Click-Through Rate (CTR) prediction, which aims to estimate the probability that a user will click an item, is an essential component of online advertising. Existing methods mainly attempt to mine user interests from users' historical behaviours, which contain users' directly interacted items. Although these methods have made great progress, they are often limited by the recommender system's direct **exposure** and inactive interactions, and thus fail to mine all potential user interests. To tackle these problems, we propose Neighbor-Interaction based CTR prediction (NI-CTR), which considers this task under a Heterogeneous Information Network (HIN) setting. In short, Neighbor-Interaction based CTR prediction involves the local neighborhood of the target user-item pair in the HIN to predict their linkage. In order to guide the representation learning of the local neighbourhood, we further consider different kinds of interactions among the local neighborhood nodes from both explicit and implicit perspective, and propose a novel Graph-Masked Transformer (GMT) to effectively incorporates these kinds of interactions to produce highly representative embeddings for the target user-item pair. Moreover, in order to improve model robustness against neighbour sampling, we enforce a consistency regularization loss over the neighbourhood embedding.    We conduct extensive experiments on two real-world datasets with millions of instances and the experimental results show that our proposed method outperforms state-of-the-art CTR models significantly. Meanwhile, the comprehensive ablation studies verify the effectiveness of every component of our model. Furthermore, we have deployed this framework on the WeChat Official Account Platform with billions of users. The online A/B tests demonstrate an average CTR improvement of 21.9 against all online baselines.  
### An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images. (arXiv:2204.01341v3 [cs.CV] UPDATED)
- Authors : Jiawei Zhang, Xin Zhao, Tao Jiang, Md Mamunur, Yudong Yao, Hao Lin, Jinghua Zhang, Ao Pan, Marcin Grzegorzek, Chen Li
- Link : [http://arxiv.org/abs/2204.01341](http://arxiv.org/abs/2204.01341)
> ABSTRACT  :  This paper proposes a novel pixel interval down-sampling network (PID-Net) for dense tiny object (yeast cells) counting tasks with higher accuracy. The PID-Net is an end-to-end convolutional neural network (CNN) model with an encoder--decoder architecture. The pixel interval down-sampling operations are concatenated with max-pooling operations to combine the sparse and dense features. This addresses the limitation of contour conglutination of dense objects while counting. The evaluation was conducted using classical segmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as counting metrics. The experimental results show that the proposed PID-Net had the best performance and potential for dense tiny object counting tasks, which achieved 96.97\% counting accuracy on the dataset with 2448 yeast cell images. By comparing with the state-of-the-art approaches, such as Attention U-Net, **Swin** U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects with clearer boundaries and fewer incorrect debris, which shows the great potential of PID-Net in the task of accurate counting.  
# Paper List
---
## cs.CV
---
**155** new papers in cs.CV:-) 
1. R2P: A Deep Learning Model from mmWave Radar to Point Cloud. (arXiv:2207.10690v1 [cs.CV])
2. Synthetic Dataset Generation for Adversarial Machine Learning Research. (arXiv:2207.10719v1 [cs.CV])
3. Fusing Frame and Event Vision for High-speed Optical Flow for Edge Application. (arXiv:2207.10720v1 [cs.CV])
4. Irrelevant Pixels are Everywhere: Find and Exclude Them for More Efficient Computer Vision. (arXiv:2207.10741v1 [cs.CV])
5. DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection. (arXiv:2207.10758v1 [cs.CV])
6. TIDEE: Tidying Up Novel Rooms using Visuo-Semantic Commonsense Priors. (arXiv:2207.10761v1 [cs.CV])
7. MeshLoc: Mesh-Based Visual Localization. (arXiv:2207.10762v1 [cs.CV])
8. Towards Interpretable Video Super-Resolution via Alternating Optimization. (arXiv:2207.10765v1 [cs.CV])
9. Focused Decoding Enables 3D Anatomical Detection by Transformers. (arXiv:2207.10774v1 [cs.CV])
10. Auto-regressive Image Synthesis with Integrated Quantization. (arXiv:2207.10776v1 [cs.CV])
11. An advanced combination of semi-supervised Normalizing Flow & Yolo (YoloNF) to detect and recognize vehicle license plates. (arXiv:2207.10777v1 [cs.CV])
12. Strategising template-guided needle placement for MR-targeted prostate biopsy. (arXiv:2207.10784v1 [cs.LG])
13. Inductive and Transductive Few-Shot Video Classification via Appearance and Temporal Alignments. (arXiv:2207.10785v1 [cs.CV])
14. Test-Time Adaptation via Self-Training with Nearest Neighbor Information. (arXiv:2207.10792v1 [cs.CV])
15. Neuroimaging Feature Extraction using a Neural Network Classifier for Imaging Genetics. (arXiv:2207.10794v1 [q-bio.QM])
16. Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation. (arXiv:2207.10825v1 [cs.CV])
17. Few-shot Image Generation Using Discrete Content Representation. (arXiv:2207.10833v1 [cs.CV])
18. Uncertainty-aware Multi-modal Learning via Cross-modal Random Network Prediction. (arXiv:2207.10851v1 [cs.CV])
19. Spatio-Temporal Deformable Attention Network for Video Deblurring. (arXiv:2207.10852v1 [cs.CV])
20. Prototype-Guided Continual Adaptation for Class-Incremental Unsupervised Domain Adaptation. (arXiv:2207.10856v1 [cs.CV])
21. Geodesic-Former: a Geodesic-Guided Few-shot 3D Point Cloud Instance Segmenter. (arXiv:2207.10859v1 [cs.CV])
22. On Higher Adversarial Susceptibility of Contrastive Self-Supervised Learning. (arXiv:2207.10862v1 [cs.CV])
23. Cost Aggregation with 4D Convolutional **Swin** Transformer for Few-Shot Segmentation. (arXiv:2207.10866v1 [cs.CV])
24. Optimizing Image Compression via Joint Learning with Denoising. (arXiv:2207.10869v1 [eess.IV])
25. An Ensemble Approach for Multiple Emotion Descriptors Estimation Using Multi-task Learning. (arXiv:2207.10878v1 [cs.CV])
26. My View is the Best View: Procedure Learning from Egocentric Videos. (arXiv:2207.10883v1 [cs.CV])
27. XAI based Performance Preserving Adaptive Image Compression for Efficient Satellite Communication. (arXiv:2207.10885v1 [eess.IV])
28. FairGRAPE: Fairness-aware GRAdient Pruning mEthod for Face Attribute Classification. (arXiv:2207.10888v1 [cs.CV])
29. Bi-directional Contrastive Learning for Domain Adaptive Semantic Segmentation. (arXiv:2207.10892v1 [cs.CV])
30. 3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization. (arXiv:2207.10895v1 [cs.CV])
31. Efficient Modeling of Future Context for Image Captioning. (arXiv:2207.10897v1 [cs.CV])
32. Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness. (arXiv:2207.10899v1 [cs.CV])
33. DBQ-SSD: Dynamic Ball Query for Efficient 3D Object Detection. (arXiv:2207.10909v1 [cs.CV])
34. Optimization of Forcemyography Sensor Placement for Arm Movement Recognition. (arXiv:2207.10915v1 [cs.CV])
35. PLD-SLAM: A Real-Time Visual SLAM Using Points and Line Segments in Dynamic Scenes. (arXiv:2207.10916v1 [cs.CV])
36. Long-tailed Instance Segmentation using Gumbel Optimized Loss. (arXiv:2207.10936v1 [cs.CV])
37. Dense RGB-D-Inertial SLAM with Map Deformations. (arXiv:2207.10940v1 [cs.RO])
38. Dynamic Local Aggregation Network with Adaptive Clusterer for Anomaly Detection. (arXiv:2207.10948v1 [cs.CV])
39. Scale dependant layer for self-supervised nuclei encoding. (arXiv:2207.10950v1 [cs.CV])
40. Vision-based Human Fall Detection Systems using Deep Learning: A Review. (arXiv:2207.10952v1 [cs.CV])
41. Visible and Near Infrared Image Fusion Based on Texture Information. (arXiv:2207.10953v1 [cs.CV])
42. Faster VoxelPose: **Real-time** 3D Human Pose Estimation by Orthographic Projection. (arXiv:2207.10955v1 [cs.CV])
43. QueryProp: Object Query Propagation for High-Performance Video Object Detection. (arXiv:2207.10959v1 [cs.CV])
44. Principal Geodesic Analysis of Merge Trees (and Persistence Diagrams). (arXiv:2207.10960v1 [cs.GR])
45. Opportunistic hip fracture risk prediction in Men from X-ray: Findings from the Osteoporosis in Men (MrOS) Study. (arXiv:2207.10970v1 [cs.CV])
46. Learning Human Kinematics by Modeling Temporal Correlations between Joints for Video-based Human Pose Estimation. (arXiv:2207.10971v1 [cs.CV])
47. NeurAR: Neural Uncertainty for Autonomous 3D Reconstruction. (arXiv:2207.10985v1 [cs.CV])
48. Few-shot Object Counting and Detection. (arXiv:2207.10988v1 [cs.CV])
49. Taguchi based Design of Sequential Convolution Neural Network for Classification of Defective Fasteners. (arXiv:2207.10992v1 [cs.CV])
50. Learning Generalized Non-Rigid Multimodal Biomedical Image Registration from Generic Point Set Data. (arXiv:2207.10994v1 [cs.CV])
51. Meta-Registration: Learning Test-Time Optimization for Single-Pair Image Registration. (arXiv:2207.10996v1 [cs.CV])
52. Rapid Lung Ultrasound COVID-19 Severity Scoring with Resource-Efficient Deep Feature Extraction. (arXiv:2207.10998v1 [eess.IV])
53. POP: Mining POtential Performance of new fashion products via webly cross-modal query expansion. (arXiv:2207.11001v1 [cs.CV])
54. Fact sheet: Automatic Self-Reported Personality Recognition Track. (arXiv:2207.11012v1 [cs.CV])
55. Open video data sharing in developmental and behavioural science. (arXiv:2207.11020v1 [cs.CV])
56. Custom Structure Preservation in Face Aging. (arXiv:2207.11025v1 [cs.CV])
57. MobileDenseNet: A new approach to object detection on mobile devices. (arXiv:2207.11031v1 [cs.CV])
58. GesSure -- A Robust Face-Authentication enabled Dynamic Gesture Recognition GUI Application. (arXiv:2207.11033v1 [cs.HC])
59. Graph Spatio-Spectral Total Variation Model for Hyperspectral Image Denoising. (arXiv:2207.11050v1 [eess.IV])
60. 3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal. (arXiv:2207.11061v1 [cs.CV])
61. RealFlow: EM-based Realistic Optical Flow Dataset Generation from Videos. (arXiv:2207.11075v1 [cs.CV])
62. Facial Expression Recognition using Vanilla ViT backbones with MAE Pretraining. (arXiv:2207.11081v1 [cs.CV])
63. Visual Speech-Aware Perceptual 3D Facial Expression Reconstruction from Videos. (arXiv:2207.11094v1 [cs.CV])
64. Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v1 [eess.IV])
65. Zero-Shot Video Captioning with Evolving Pseudo-Tokens. (arXiv:2207.11100v1 [cs.CV])
66. Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs. (arXiv:2207.11102v1 [eess.IV])
67. DeVIS: Making Deformable Transformers Work for Video Instance Segmentation. (arXiv:2207.11103v1 [cs.CV])
68. Fast strategies for multi-temporal speckle reduction of Sentinel-1 GRD images. (arXiv:2207.11111v1 [eess.IV])
69. Rethinking the Reference-based Distinctive Image Captioning. (arXiv:2207.11118v1 [cs.CV])
70. Adaptive Graph-Based Feature Normalization for Facial Expression Recognition. (arXiv:2207.11123v1 [cs.CV])
71. VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and Pooled Vehicle Information. (arXiv:2207.11146v1 [cs.CV])
72. InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images. (arXiv:2207.11148v1 [cs.CV])
73. Adaptive Soft Contrastive Learning. (arXiv:2207.11163v1 [cs.CV])
74. METER-ML: A Multi-sensor Earth Observation Benchmark for Automated Methane Source Mapping. (arXiv:2207.11166v1 [cs.CV])
75. Rethinking Few-Shot Object Detection on a Multi-Domain Benchmark. (arXiv:2207.11169v1 [cs.CV])
76. Training Certifiably Robust Neural Networks Against Semantic Perturbations. (arXiv:2207.11177v1 [cs.CV])
77. Multi-Faceted Distillation of Base-Novel Commonality for Few-shot Object Detection. (arXiv:2207.11184v1 [cs.CV])
78. Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images. (arXiv:2207.11186v1 [cs.CV])
79. Self-Supervised-RCNN for Medical Image Segmentation with Limited Data Annotation. (arXiv:2207.11191v1 [cs.CV])
80. Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis. (arXiv:2207.11192v1 [cs.CV])
81. Target-Driven Structured Transformer Planner for Vision-Language Navigation. (arXiv:2207.11201v1 [cs.CV])
82. Human Treelike Tubular Structure Segmentation: A Comprehensive Review and Future Perspectives. (arXiv:2207.11203v1 [eess.IV])
83. Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise Binarization. (arXiv:2207.11209v1 [cs.CV])
84. Improving Predictive Performance and Calibration by Weight Fusion in Semantic Segmentation. (arXiv:2207.11211v1 [cs.CV])
85. Target Identification and Bayesian Model Averaging with Probabilistic Hierarchical Factor Probabilities. (arXiv:2207.11212v1 [cs.CV])
86. Few-Shot Class-Incremental Learning via Entropy-Regularized Data-Free Replay. (arXiv:2207.11213v1 [cs.CV])
87. Domain Generalization for Activity Recognition via Adaptive Feature Fusion. (arXiv:2207.11221v1 [cs.CV])
88. Forest and Water Bodies Segmentation Through Satellite Images Using U-Net. (arXiv:2207.11222v1 [cs.CV])
89. Improved $\alpha$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning. (arXiv:2207.11223v1 [eess.IV])
90. Large-Kernel Attention for 3D Medical Image Segmentation. (arXiv:2207.11225v1 [eess.IV])
91. FewGAN: Generating from the Joint Distribution of a Few Images. (arXiv:2207.11226v1 [cs.CV])
92. Face editing with GAN -- A Review. (arXiv:2207.11227v1 [cs.CV])
93. Classifying Crop Types using Gaussian Bayesian Models and Neural Networks on GHISACONUS USGS data from NASA Hyperspectral Satellite Imagery. (arXiv:2207.11228v1 [cs.CV])
94. You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine. (arXiv:2207.11230v1 [cs.CV])
95. Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement. (arXiv:2207.11232v1 [cs.CV])
96. A System-driven Automatic Ground Truth Generation Method for DL Inner-City Driving Corridor Detectors. (arXiv:2207.11234v1 [cs.CV])
97. Improved lightweight identification of agricultural diseases based on MobileNetV3. (arXiv:2207.11238v1 [cs.CV])
98. Multiface: A Dataset for Neural Face Rendering. (arXiv:2207.11243v1 [cs.CV])
99. Deep Learning Hyperparameter Optimization for Breast Mass Detection in Mammograms. (arXiv:2207.11244v1 [cs.CV])
100. Panoptic Scene Graph Generation. (arXiv:2207.11247v1 [cs.CV])
101. NASA: Neural Articulated Shape Approximation. (arXiv:1912.03207v5 [cs.CV] UPDATED)
102. How Trustworthy are Performance Evaluations for Basic Vision Tasks?. (arXiv:2008.03533v4 [cs.CV] UPDATED)
103. Learning Energy-Based Models With Adversarial Training. (arXiv:2012.06568v3 [cs.LG] UPDATED)
104. FoV-**NeRF**: Foveated Neural Radiance Fields for Virtual Reality. (arXiv:2103.16365v2 [cs.GR] UPDATED)
105. The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v5 [cs.CV] UPDATED)
106. Bi-level Feature Alignment for Versatile Image Translation and Manipulation. (arXiv:2107.03021v2 [cs.CV] UPDATED)
107. Learning to Predict Diverse Human Motions from a Single Image via Mixture Density Networks. (arXiv:2109.05776v2 [cs.CV] UPDATED)
108. Scale-aware direct monocular odometry. (arXiv:2109.10077v2 [cs.RO] UPDATED)
109. Bounding-box deep calibration for high performance face detection. (arXiv:2110.03892v2 [cs.CV] UPDATED)
110. MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v3 [cs.CV] UPDATED)
111. Understanding the Dynamics of DNNs Using Graph Modularity. (arXiv:2111.12485v3 [cs.CV] UPDATED)
112. The Shape Part Slot Machine: Contact-based Reasoning for Generating 3D Shapes from Parts. (arXiv:2112.00584v2 [cs.GR] UPDATED)
113. D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding. (arXiv:2112.01551v2 [cs.CV] UPDATED)
114. MoFa**NeRF**: Morphable Facial Neural Radiance Field. (arXiv:2112.02308v2 [cs.CV] UPDATED)
115. 4DContrast: Contrastive Learning with Dynamic Correspondences for 3D Scene Understanding. (arXiv:2112.02990v2 [cs.CV] UPDATED)
116. NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector. (arXiv:2112.06102v3 [cs.NE] UPDATED)
117. Formulating Event-based Image Reconstruction as a Linear Inverse Problem using Optical Flow. (arXiv:2112.06242v2 [cs.CV] UPDATED)
118. SAGA: Stochastic Whole-Body Grasping with Contact. (arXiv:2112.10103v2 [cs.CV] UPDATED)
119. RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-guided Disease Classification. (arXiv:2202.11781v2 [cs.CV] UPDATED)
120. Fusing Local Similarities for Retrieval-based 3D Orientation Estimation of Unseen Objects. (arXiv:2203.08472v2 [cs.CV] UPDATED)
121. A workflow for segmenting soil and plant X-ray CT images with deep learning in Googles Colaboratory. (arXiv:2203.09674v2 [eess.IV] UPDATED)
122. Domain Generalization by Mutual-Information Regularization with Pre-trained Models. (arXiv:2203.10789v2 [cs.LG] UPDATED)
123. Deep Portrait Delighting. (arXiv:2203.12088v5 [cs.CV] UPDATED)
124. What to Hide from Your Students: Attention-Guided Masked Image Modeling. (arXiv:2203.12719v2 [cs.CV] UPDATED)
125. Self-supervised Video-centralised Transformer for Video Face Clustering. (arXiv:2203.13166v2 [cs.CV] UPDATED)
126. mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v3 [cs.CV] UPDATED)
127. An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images. (arXiv:2204.01341v3 [cs.CV] UPDATED)
128. CHORE: Contact, Human and Object REconstruction from a single RGB image. (arXiv:2204.02445v2 [cs.CV] UPDATED)
129. Stripformer: Strip Transformer for Fast Image Deblurring. (arXiv:2204.04627v2 [cs.CV] UPDATED)
130. TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v2 [cs.CV] UPDATED)
131. Multimodal Detection of Unknown Objects on Roads for Autonomous Driving. (arXiv:2205.01414v3 [cs.CV] UPDATED)
132. EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers. (arXiv:2205.03436v2 [cs.CV] UPDATED)
133. Few-shot Class-incremental Learning for 3D Point Cloud Objects. (arXiv:2205.15225v2 [cs.CV] UPDATED)
134. EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v4 [cs.CV] UPDATED)
135. OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v5 [cs.LG] UPDATED)
136. Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2206.10878v2 [cs.CV] UPDATED)
137. Deep Learning approach for Classifying Trusses and Runners of Strawberries. (arXiv:2207.02721v2 [cs.CV] UPDATED)
138. Learned Video Compression via Heterogeneous Deformable Compensation Network. (arXiv:2207.04589v2 [eess.IV] UPDATED)
139. Hierarchical Average Precision Training for Pertinent Image Retrieval. (arXiv:2207.04873v2 [cs.CV] UPDATED)
140. Pyramid Transformer for Traffic Sign Detection. (arXiv:2207.06067v2 [cs.CV] UPDATED)
141. Adversarially-Aware Robust Object Detector. (arXiv:2207.06202v3 [cs.CV] UPDATED)
142. Sample-dependent Adaptive Temperature Scaling for Improved Calibration. (arXiv:2207.06211v2 [cs.CV] UPDATED)
143. SHREC 2022 Track on Online Detection of Heterogeneous Gestures. (arXiv:2207.06706v2 [cs.CV] UPDATED)
144. MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask. (arXiv:2207.08046v2 [cs.CV] UPDATED)
145. DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection. (arXiv:2207.08531v2 [cs.CV] UPDATED)
146. Latency-Aware Collaborative Perception. (arXiv:2207.08560v3 [cs.CV] UPDATED)
147. AiATrack: Attention in Attention for Transformer Visual Tracking. (arXiv:2207.09603v2 [cs.CV] UPDATED)
148. ERA: Expert Retrieval and Assembly for Early Action Prediction. (arXiv:2207.09675v3 [cs.CV] UPDATED)
149. AU-Supervised Convolutional Vision Transformers for Synthetic Facial Expression Recognition. (arXiv:2207.09777v2 [cs.CV] UPDATED)
150. Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v3 [cs.CV] UPDATED)
151. World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v2 [cs.RO] UPDATED)
152. BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis. (arXiv:2207.10120v2 [cs.CV] UPDATED)
153. Visual Knowledge Tracing. (arXiv:2207.10157v2 [cs.CV] UPDATED)
154. Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles. (arXiv:2207.10172v2 [cs.CV] UPDATED)
155. Semantic-Aware Fine-Grained Correspondence. (arXiv:2207.10456v2 [cs.CV] UPDATED)
## eess.IV
---
**25** new papers in eess.IV:-) 
1. Retinex-qDPC: automatic background rectified quantitative differential phase contrast imaging. (arXiv:2207.10669v1 [eess.IV])
2. Fusing Frame and Event Vision for High-speed Optical Flow for Edge Application. (arXiv:2207.10720v1 [cs.CV])
3. Hardware-Efficient Template-Based Deep CNNs Accelerator Design. (arXiv:2207.10723v1 [cs.AR])
4. Strategising template-guided needle placement for MR-targeted prostate biopsy. (arXiv:2207.10784v1 [cs.LG])
5. Optimizing Image Compression via Joint Learning with Denoising. (arXiv:2207.10869v1 [eess.IV])
6. XAI based Performance Preserving Adaptive Image Compression for Efficient Satellite Communication. (arXiv:2207.10885v1 [eess.IV])
7. Rapid Lung Ultrasound COVID-19 Severity Scoring with Resource-Efficient Deep Feature Extraction. (arXiv:2207.10998v1 [eess.IV])
8. Graph Spatio-Spectral Total Variation Model for Hyperspectral Image Denoising. (arXiv:2207.11050v1 [eess.IV])
9. Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v1 [eess.IV])
10. Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs. (arXiv:2207.11102v1 [eess.IV])
11. Fast strategies for multi-temporal speckle reduction of Sentinel-1 GRD images. (arXiv:2207.11111v1 [eess.IV])
12. Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images. (arXiv:2207.11186v1 [cs.CV])
13. Human Treelike Tubular Structure Segmentation: A Comprehensive Review and Future Perspectives. (arXiv:2207.11203v1 [eess.IV])
14. Forest and Water Bodies Segmentation Through Satellite Images Using U-Net. (arXiv:2207.11222v1 [cs.CV])
15. Improved $\alpha$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning. (arXiv:2207.11223v1 [eess.IV])
16. Large-Kernel Attention for 3D Medical Image Segmentation. (arXiv:2207.11225v1 [eess.IV])
17. Face editing with GAN -- A Review. (arXiv:2207.11227v1 [cs.CV])
18. Improved lightweight identification of agricultural diseases based on MobileNetV3. (arXiv:2207.11238v1 [cs.CV])
19. Point Cloud Quality Assessment: Dataset Construction and Learning-based No-Reference Metric. (arXiv:2012.11895v4 [eess.IV] UPDATED)
20. NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector. (arXiv:2112.06102v3 [cs.NE] UPDATED)
21. Physics-informed neural networks to learn cardiac fiber orientation from multiple electroanatomical maps. (arXiv:2201.12362v3 [eess.IV] UPDATED)
22. A workflow for segmenting soil and plant X-ray CT images with deep learning in Googles Colaboratory. (arXiv:2203.09674v2 [eess.IV] UPDATED)
23. Differential invariants for SE(2)-equivariant networks. (arXiv:2206.13279v2 [eess.IV] UPDATED)
24. Learned Video Compression via Heterogeneous Deformable Compensation Network. (arXiv:2207.04589v2 [eess.IV] UPDATED)
25. Hierarchical Average Precision Training for Pertinent Image Retrieval. (arXiv:2207.04873v2 [cs.CV] UPDATED)
## cs.LG
---
**163** new papers in cs.LG:-) 
1. Improved Generalization Guarantees in Restricted Data Models. (arXiv:2207.10668v1 [cs.CR])
2. ME-GAN: Learning Panoptic Electrocardio Representations for Multi-view ECG Synthesis Conditioned on Heart Diseases. (arXiv:2207.10670v1 [cs.LG])
3. Correcting Model Bias with Sparse Implicit Processes. (arXiv:2207.10673v1 [stat.ML])
4. A machine learning based approach to gravitational lens identification with the International LOFAR Telescope. (arXiv:2207.10698v1 [astro-ph.GA])
5. Efficient model compression with Random Operation Access Specific Tile (ROAST) hashing. (arXiv:2207.10702v1 [cs.LG])
6. Learning Physics from the Machine: An Interpretable Boosted Decision Tree Analysis for the Majorana Demonstrator. (arXiv:2207.10710v1 [physics.data-an])
7. JAWS: Predictive Inference Under Covariate Shift. (arXiv:2207.10716v1 [cs.LG])
8. Synthetic Dataset Generation for Adversarial Machine Learning Research. (arXiv:2207.10719v1 [cs.CV])
9. Heterogeneous Ensemble Learning for Enhanced Crash Forecasts -- A Frequentest and Machine Learning based Stacking Framework. (arXiv:2207.10721v1 [cs.LG])
10. Federated Semi-Supervised Domain Adaptation via Knowledge Transfer. (arXiv:2207.10727v1 [cs.LG])
11. The trade-offs of model size in large recommendation models : A 10000 $\times$ compressed criteo-tb DLRM model (100 GB parameters to mere 10MB). (arXiv:2207.10731v1 [cs.LG])
12. Explainable AI Algorithms for Vibration Data-based Fault Detection: Use Case-adadpted Methods and Critical Evaluation. (arXiv:2207.10732v1 [eess.SP])
13. GreenDB -- A Dataset and Benchmark for Extraction of Sustainability Information of Consumer Goods. (arXiv:2207.10733v1 [cs.LG])
14. BigIssue: A Realistic Bug Localization Benchmark. (arXiv:2207.10739v1 [cs.LG])
15. A Transferable Recommender Approach for Selecting the Best Density Functional Approximations in Chemical Discovery. (arXiv:2207.10747v1 [physics.chem-ph])
16. Federated Learning on Adaptively Weighted Nodes by Bilevel Optimization. (arXiv:2207.10751v1 [cs.LG])
17. DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection. (arXiv:2207.10758v1 [cs.CV])
18. Modeling User Behavior With Interaction Networks for Spam Detection. (arXiv:2207.10767v1 [cs.LG])
19. Deep Sufficient Representation Learning via Mutual Information. (arXiv:2207.10772v1 [stat.ML])
20. Data-Driven Stochastic AC-OPF using Gaussian Processes. (arXiv:2207.10781v1 [stat.ML])
21. Strategising template-guided needle placement for MR-targeted prostate biopsy. (arXiv:2207.10784v1 [cs.LG])
22. Delayed Feedback in Generalised Linear Bandits Revisited. (arXiv:2207.10786v1 [cs.LG])
23. Neuroimaging Feature Extraction using a Neural Network Classifier for Imaging Genetics. (arXiv:2207.10794v1 [q-bio.QM])
24. Multiple Robust Learning for Recommendation. (arXiv:2207.10796v1 [cs.IR])
25. IDPS Signature Classification with a Reject Option and the Incorporation of Expert Knowledge. (arXiv:2207.10797v1 [cs.CR])
26. Understanding High Dimensional Spaces through Visual Means Employing Multidimensional Projections. (arXiv:2207.10800v1 [cs.HC])
27. PhishSim: Aiding Phishing Website Detection with a Feature-Free Tool. (arXiv:2207.10801v1 [cs.CR])
28. Active Data Pattern Extraction Attacks on Generative Language Models. (arXiv:2207.10802v1 [cs.CR])
29. NFDLM: A Lightweight Network Flow based Deep Learning Model for DDoS Attack Detection in IoT Domains. (arXiv:2207.10803v1 [cs.CR])
30. Suppressing Poisoning Attacks on Federated Learning for Medical Imaging. (arXiv:2207.10804v1 [cs.CR])
31. PowerFDNet: Deep Learning-Based Stealthy False Data Injection Attack Detection for AC-model Transmission Systems. (arXiv:2207.10805v1 [cs.CR])
32. A Machine Learning Approach for Driver Identification Based on CAN-BUS Sensor Data. (arXiv:2207.10807v1 [cs.LG])
33. A Convolutional Attention Based Deep Network Solution for UAV Network Attack Recognition over Fading Channels and Interference. (arXiv:2207.10810v1 [cs.CR])
34. Supervised Contrastive ResNet and Transfer Learning for the In-vehicle Intrusion Detection System. (arXiv:2207.10814v1 [cs.CR])
35. End-to-End and Self-Supervised Learning for ComParE 2022 Stuttering Sub-Challenge. (arXiv:2207.10817v1 [cs.SD])
36. Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation. (arXiv:2207.10825v1 [cs.CV])
37. Automated Dilated Spatio-Temporal Synchronous Graph Modeling for Traffic Prediction. (arXiv:2207.10830v1 [cs.LG])
38. Characterizing Coherent Integrated Photonic Neural Networks under Imperfections. (arXiv:2207.10835v1 [cs.ET])
39. Robust Knowledge Adaptation for Dynamic Graph Neural Networks. (arXiv:2207.10839v1 [cs.LG])
40. Uncertainty-aware Multi-modal Learning via Cross-modal Random Network Prediction. (arXiv:2207.10851v1 [cs.CV])
41. Transformer with Implicit Edges for Particle-based Physics Simulation. (arXiv:2207.10860v1 [cs.LG])
42. Assessing mortality prediction through different representation models based on concepts extracted from clinical notes. (arXiv:2207.10872v1 [cs.CL])
43. FairGRAPE: Fairness-aware GRAdient Pruning mEthod for Face Attribute Classification. (arXiv:2207.10888v1 [cs.CV])
44. Privacy and Transparency in Graph Machine Learning: A Unified Perspective. (arXiv:2207.10896v1 [cs.LG])
45. Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness. (arXiv:2207.10899v1 [cs.CV])
46. What's in the laundromat? Mapping and characterising offshore owned domestic property in London. (arXiv:2207.10931v1 [cs.LG])
47. Statistical Hypothesis Testing Based on Machine Learning: Large Deviations Analysis. (arXiv:2207.10939v1 [stat.ML])
48. Respecting Time Series Properties Makes Deep Time Series Forecasting Perfect. (arXiv:2207.10941v1 [cs.LG])
49. Multilabel Prototype Generation for Data Reduction in k-Nearest Neighbour classification. (arXiv:2207.10947v1 [cs.LG])
50. Hyper-Representations for Pre-Training and Transfer Learning. (arXiv:2207.10951v1 [cs.LG])
51. Principal Geodesic Analysis of Merge Trees (and Persistence Diagrams). (arXiv:2207.10960v1 [cs.GR])
52. Learning Generalized Non-Rigid Multimodal Biomedical Image Registration from Generic Point Set Data. (arXiv:2207.10994v1 [cs.CV])
53. Revisiting Parameter Reuse to Overcome Catastrophic Forgetting in Neural Networks. (arXiv:2207.11005v1 [cs.LG])
54. Layer-Wise Partitioning and Merging for Efficient and Scalable Deep Learning. (arXiv:2207.11019v1 [cs.DC])
55. Custom Structure Preservation in Face Aging. (arXiv:2207.11025v1 [cs.CV])
56. A Transferable Intersection Reconstruction Network for Traffic Speed Prediction. (arXiv:2207.11030v1 [cs.LG])
57. MobileDenseNet: A new approach to object detection on mobile devices. (arXiv:2207.11031v1 [cs.CV])
58. Spatial-Temporal Feature Extraction and Evaluation Network for Citywide Traffic Condition Prediction. (arXiv:2207.11034v1 [cs.LG])
59. Quantized Sparse Weight Decomposition for Neural Network Compression. (arXiv:2207.11048v1 [cs.LG])
60. Context-aware controller inference for stabilizing dynamical systems from scarce data. (arXiv:2207.11049v1 [math.OC])
61. Latent Space Unsupervised Semantic Segmentation. (arXiv:2207.11067v1 [cs.LG])
62. Do Artificial Intelligence Systems Understand?. (arXiv:2207.11089v1 [cs.AI])
63. Classification via score-based generative modelling. (arXiv:2207.11091v1 [cs.LG])
64. DeVIS: Making Deformable Transformers Work for Video Instance Segmentation. (arXiv:2207.11103v1 [cs.CV])
65. Near Real-Time Distributed State Estimation via AI/ML-Empowered 5G Networks. (arXiv:2207.11117v1 [cs.LG])
66. On Controller Tuning with Time-Varying Bayesian Optimization. (arXiv:2207.11120v1 [cs.LG])
67. Optimism in Face of a Context: Regret Guarantees for Stochastic Contextual MDP. (arXiv:2207.11126v1 [cs.LG])
68. Deep learning of diffeomorphisms for optimal reparametrizations of shapes. (arXiv:2207.11141v1 [math.OC])
69. Towards Global Optimality in Cooperative MARL with Sequential Transformation. (arXiv:2207.11143v1 [cs.MA])
70. VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and Pooled Vehicle Information. (arXiv:2207.11146v1 [cs.CV])
71. Learn Continuously, Act Discretely: Hybrid Action-Space Reinforcement Learning For Optimal Execution. (arXiv:2207.11152v1 [q-fin.TR])
72. SPRT-based Efficient Best Arm Identification in Stochastic Bandits. (arXiv:2207.11158v1 [stat.ML])
73. Fairness-aware Network Revenue Management with Demand Learning. (arXiv:2207.11159v1 [stat.ML])
74. Lagrangian Method for Q-Function Learning (with Applications to Machine Translation). (arXiv:2207.11161v1 [cs.LG])
75. Generalized Identifiability Bounds for Mixture Models with Grouped Samples. (arXiv:2207.11164v1 [math.ST])
76. High dimensional stochastic linear contextual bandit with missing covariates. (arXiv:2207.11165v1 [stat.ML])
77. Decentralized scheduling through an adaptive, trading-based multi-agent system. (arXiv:2207.11172v1 [cs.AI])
78. Verifying Fairness in Quantum Machine Learning. (arXiv:2207.11173v1 [quant-ph])
79. Low cost prediction of probability distributions of molecular properties for early virtual screening. (arXiv:2207.11174v1 [q-bio.BM])
80. Explaining Dynamic Graph Neural Networks via Relevance Back-propagation. (arXiv:2207.11175v1 [cs.LG])
81. Training Certifiably Robust Neural Networks Against Semantic Perturbations. (arXiv:2207.11177v1 [cs.CV])
82. Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images. (arXiv:2207.11186v1 [cs.CV])
83. TaDaa: **real time** Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems. (arXiv:2207.11187v1 [cs.IR])
84. Self-Supervised-RCNN for Medical Image Segmentation with Limited Data Annotation. (arXiv:2207.11191v1 [cs.CV])
85. Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis. (arXiv:2207.11192v1 [cs.CV])
86. Target-Driven Structured Transformer Planner for Vision-Language Navigation. (arXiv:2207.11201v1 [cs.CV])
87. Human Treelike Tubular Structure Segmentation: A Comprehensive Review and Future Perspectives. (arXiv:2207.11203v1 [eess.IV])
88. Statistical and Computational Trade-offs in Variational Inference: A Case Study in Inferential Model Selection. (arXiv:2207.11208v1 [stat.ML])
89. Target Identification and Bayesian Model Averaging with Probabilistic Hierarchical Factor Probabilities. (arXiv:2207.11212v1 [cs.CV])
90. Domain Generalization for Activity Recognition via Adaptive Feature Fusion. (arXiv:2207.11221v1 [cs.CV])
91. Improved $\alpha$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning. (arXiv:2207.11223v1 [eess.IV])
92. Large-Kernel Attention for 3D Medical Image Segmentation. (arXiv:2207.11225v1 [eess.IV])
93. FewGAN: Generating from the Joint Distribution of a Few Images. (arXiv:2207.11226v1 [cs.CV])
94. Face editing with GAN -- A Review. (arXiv:2207.11227v1 [cs.CV])
95. Flow Moods: Recommending Music by Moods on Deezer. (arXiv:2207.11229v1 [cs.IR])
96. Learning Unsupervised Hierarchies of Audio Concepts. (arXiv:2207.11231v1 [cs.SD])
97. Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement. (arXiv:2207.11232v1 [cs.CV])
98. E2N: Error Estimation Networks for Goal-Oriented Mesh Adaptation. (arXiv:2207.11233v1 [cs.LG])
99. Twitmo: A Twitter Data Topic Modeling and Visualization Package for R. (arXiv:2207.11236v1 [cs.IR])
100. Defending Substitution-Based Profile Pollution Attacks on Sequential Recommenders. (arXiv:2207.11237v1 [cs.IR])
101. Improved lightweight identification of agricultural diseases based on MobileNetV3. (arXiv:2207.11238v1 [cs.CV])
102. Machine learning approach in the development of building occupant personas. (arXiv:2207.11239v1 [cs.LG])
103. Discrete Key-Value Bottleneck. (arXiv:2207.11240v1 [cs.LG])
104. Panoptic Scene Graph Generation. (arXiv:2207.11247v1 [cs.CV])
105. Composing Neural Learning and Symbolic Reasoning with an Application to Visual Discrimination. (arXiv:1907.05878v2 [cs.LG] UPDATED)
106. NASA: Neural Articulated Shape Approximation. (arXiv:1912.03207v5 [cs.CV] UPDATED)
107. CoLES: Contrastive Learning for Event Sequences with Self-Supervision. (arXiv:2002.08232v3 [cs.LG] UPDATED)
108. Relaxing the I.I.D. Assumption: Adaptively Minimax Optimal Regret via Root-Entropic Regularization. (arXiv:2007.06552v3 [stat.ML] UPDATED)
109. Learning Energy-Based Models With Adversarial Training. (arXiv:2012.06568v3 [cs.LG] UPDATED)
110. Learning for MPC with Stability & Safety Guarantees. (arXiv:2012.07369v2 [cs.LG] UPDATED)
111. Automatic Termination for Hyperparameter Optimization. (arXiv:2104.08166v4 [cs.LG] UPDATED)
112. Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks. (arXiv:2107.01809v2 [cs.LG] UPDATED)
113. From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers. (arXiv:2107.07999v7 [cs.LG] UPDATED)
114. Analyzing and Mitigating Interference in Neural Architecture Search. (arXiv:2108.12821v3 [cs.CL] UPDATED)
115. BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition. (arXiv:2109.13226v3 [eess.AS] UPDATED)
116. Sign and Relevance learning. (arXiv:2110.07292v2 [cs.LG] UPDATED)
117. Function-space Inference with Sparse Implicit Processes. (arXiv:2110.07618v3 [stat.ML] UPDATED)
118. Sound and Complete Neural Network Repair with Minimality and Locality Guarantees. (arXiv:2110.07682v3 [cs.LG] UPDATED)
119. NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework. (arXiv:2111.04130v2 [cs.CL] UPDATED)
120. Stronger Generalization Guarantees for Robot Learning by Combining Generative Models and Real-World Data. (arXiv:2111.08761v2 [cs.RO] UPDATED)
121. Understanding the Dynamics of DNNs Using Graph Modularity. (arXiv:2111.12485v3 [cs.CV] UPDATED)
122. The Shape Part Slot Machine: Contact-based Reasoning for Generating 3D Shapes from Parts. (arXiv:2112.00584v2 [cs.GR] UPDATED)
123. Generative Adversarial Networks for Labeled Acceleration Data Augmentation for Structural Damage Detection. (arXiv:2112.03478v6 [cs.LG] UPDATED)
124. Formulating Event-based Image Reconstruction as a Linear Inverse Problem using Optical Flow. (arXiv:2112.06242v2 [cs.CV] UPDATED)
125. Prompt Tuning GPT-2 language model for parameter-efficient domain adaptation of ASR systems. (arXiv:2112.08718v3 [cs.CL] UPDATED)
126. Self-attention Presents Low-dimensional Knowledge Graph Embeddings for Link Prediction. (arXiv:2112.10644v2 [cs.LG] UPDATED)
127. Doubly-Valid/Doubly-Sharp Sensitivity Analysis for Causal Inference with Unmeasured Confounding. (arXiv:2112.11449v2 [stat.ME] UPDATED)
128. Optimal Model Averaging of Support Vector Machines in Diverging Model Spaces. (arXiv:2112.12961v3 [stat.ML] UPDATED)
129. Improving Nonparametric Classification via Local Radial Regression with an Application to Stock Prediction. (arXiv:2112.13951v2 [stat.ML] UPDATED)
130. Deriving discriminative classifiers from generative models. (arXiv:2201.00844v2 [stat.ML] UPDATED)
131. DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale. (arXiv:2201.05596v2 [cs.LG] UPDATED)
132. Differential Geometry for Neural Implicit Models. (arXiv:2201.09263v3 [cs.GR] UPDATED)
133. STOPS: Short-Term-based Volatility-controlled Policy Search and its Global Convergence. (arXiv:2201.09857v5 [cs.LG] UPDATED)
134. Post-training Quantization for Neural Networks with Provable Guarantees. (arXiv:2201.11113v2 [cs.LG] UPDATED)
135. Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks. (arXiv:2201.11729v4 [cs.LG] UPDATED)
136. Rapid protein assignments and structures from raw NMR spectra with the deep learning technique ARTINA. (arXiv:2201.12041v4 [q-bio.BM] UPDATED)
137. Physics-informed neural networks to learn cardiac fiber orientation from multiple electroanatomical maps. (arXiv:2201.12362v3 [eess.IV] UPDATED)
138. Neighbour Interaction based Click-Through Rate Prediction via Graph-masked Transformer. (arXiv:2201.13311v2 [cs.IR] UPDATED)
139. Flat Latent Manifolds for Human-machine Co-creation of Music. (arXiv:2202.12243v2 [cs.SD] UPDATED)
140. On the sample complexity of stabilizing linear dynamical systems from data. (arXiv:2203.00474v2 [math.OC] UPDATED)
141. Fast Bayesian Coresets via Subsampling and Quasi-Newton Refinement. (arXiv:2203.09675v2 [stat.ML] UPDATED)
142. Domain Generalization by Mutual-Information Regularization with Pre-trained Models. (arXiv:2203.10789v2 [cs.LG] UPDATED)
143. Deep Portrait Delighting. (arXiv:2203.12088v5 [cs.CV] UPDATED)
144. Tight bounds on the hardness of learning simple nonparametric mixtures. (arXiv:2203.15150v2 [cs.LG] UPDATED)
145. Improved Relation Networks for End-to-End Speaker Verification and Identification. (arXiv:2203.17218v2 [eess.AS] UPDATED)
146. Provable concept learning for interpretable predictions using variational autoencoders. (arXiv:2204.00492v3 [cs.LG] UPDATED)
147. An Extensive Data Processing Pipeline for MIMIC-IV. (arXiv:2204.13841v2 [cs.LG] UPDATED)
148. Controlled Generation of Unseen Faults for Partial and Open-Partial Domain Adaptation. (arXiv:2204.14068v2 [cs.LG] UPDATED)
149. Multimodal Detection of Unknown Objects on Roads for Autonomous Driving. (arXiv:2205.01414v3 [cs.CV] UPDATED)
150. Efficient Automated Deep Learning for Time Series Forecasting. (arXiv:2205.05511v3 [cs.LG] UPDATED)
151. Learning Dialogue Representations from Consecutive Utterances. (arXiv:2205.13568v2 [cs.CL] UPDATED)
152. OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v5 [cs.LG] UPDATED)
153. Flexible Group Fairness Metrics for Survival Analysis. (arXiv:2206.03256v3 [cs.CY] UPDATED)
154. X-Risk Analysis for AI Research. (arXiv:2206.05862v6 [cs.CY] UPDATED)
155. Concept Identification for Complex Engineering Datasets. (arXiv:2206.06155v2 [cs.LG] UPDATED)
156. Reinforcement Learning Approaches for the Orienteering Problem with Stochastic and Dynamic Release Dates. (arXiv:2207.00885v2 [math.OC] UPDATED)
157. Hierarchical Average Precision Training for Pertinent Image Retrieval. (arXiv:2207.04873v2 [cs.CV] UPDATED)
158. Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v3 [cs.CV] UPDATED)
159. World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v2 [cs.RO] UPDATED)
160. Improving Privacy-Preserving Vertical Federated Learning by Efficient Communication with ADMM. (arXiv:2207.10226v2 [cs.LG] UPDATED)
161. Unsupervised Legendre-Galerkin Neural Network for Stiff Partial Differential Equations. (arXiv:2207.10241v2 [cs.LG] UPDATED)
162. ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v2 [cs.LG] UPDATED)
163. Quantum Metropolis Solver: A Quantum Walks Approach to Optimization Problems. (arXiv:2207.06462v1 [quant-ph] CROSS LISTED)
## cs.AI
---
**82** new papers in cs.AI:-) 
1. ME-GAN: Learning Panoptic Electrocardio Representations for Multi-view ECG Synthesis Conditioned on Heart Diseases. (arXiv:2207.10670v1 [cs.LG])
2. R2P: A Deep Learning Model from mmWave Radar to Point Cloud. (arXiv:2207.10690v1 [cs.CV])
3. Synthetic Dataset Generation for Adversarial Machine Learning Research. (arXiv:2207.10719v1 [cs.CV])
4. A Proposal for Foley Sound Synthesis Challenge. (arXiv:2207.10760v1 [cs.SD])
5. Focused Decoding Enables 3D Anatomical Detection by Transformers. (arXiv:2207.10774v1 [cs.CV])
6. An advanced combination of semi-supervised Normalizing Flow & Yolo (YoloNF) to detect and recognize vehicle license plates. (arXiv:2207.10777v1 [cs.CV])
7. Heuristic Rating Estimation Method for the incomplete pairwise comparisons matrices. (arXiv:2207.10783v1 [cs.AI])
8. Test-Time Adaptation via Self-Training with Nearest Neighbor Information. (arXiv:2207.10792v1 [cs.CV])
9. PhishSim: Aiding Phishing Website Detection with a Feature-Free Tool. (arXiv:2207.10801v1 [cs.CR])
10. PowerFDNet: Deep Learning-Based Stealthy False Data Injection Attack Detection for AC-model Transmission Systems. (arXiv:2207.10805v1 [cs.CR])
11. WordSig: QR streams enabling platform-independent self-identification that's impossible to deepfake. (arXiv:2207.10806v1 [cs.CR])
12. Security and Safety Aspects of AI in Industry Applications. (arXiv:2207.10809v1 [cs.CR])
13. Supervised Contrastive ResNet and Transfer Learning for the In-vehicle Intrusion Detection System. (arXiv:2207.10814v1 [cs.CR])
14. An Ensemble Approach for Multiple Emotion Descriptors Estimation Using Multi-task Learning. (arXiv:2207.10878v1 [cs.CV])
15. My View is the Best View: Procedure Learning from Egocentric Videos. (arXiv:2207.10883v1 [cs.CV])
16. FairGRAPE: Fairness-aware GRAdient Pruning mEthod for Face Attribute Classification. (arXiv:2207.10888v1 [cs.CV])
17. Impact of RoCE Congestion Control Policies on Distributed Training of DNNs. (arXiv:2207.10898v1 [cs.NI])
18. Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness. (arXiv:2207.10899v1 [cs.CV])
19. Statistical Hypothesis Testing Based on Machine Learning: Large Deviations Analysis. (arXiv:2207.10939v1 [stat.ML])
20. Respecting Time Series Properties Makes Deep Time Series Forecasting Perfect. (arXiv:2207.10941v1 [cs.LG])
21. Efficient Testing of Deep Neural Networks via Decision Boundary Analysis. (arXiv:2207.10942v1 [cs.SE])
22. Vision-based Human Fall Detection Systems using Deep Learning: A Review. (arXiv:2207.10952v1 [cs.CV])
23. WRHT: Efficient All-reduce for Distributed DNN Training in Optical Interconnect System. (arXiv:2207.10982v1 [cs.DC])
24. Algorithmic Fairness in Business Analytics: Directions for Research and Practice. (arXiv:2207.10991v1 [cs.AI])
25. Taguchi based Design of Sequential Convolution Neural Network for Classification of Defective Fasteners. (arXiv:2207.10992v1 [cs.CV])
26. Gradual Drift Detection in Process Models Using Conformance Metrics. (arXiv:2207.11007v1 [cs.AI])
27. Open video data sharing in developmental and behavioural science. (arXiv:2207.11020v1 [cs.CV])
28. Latent Space Unsupervised Semantic Segmentation. (arXiv:2207.11067v1 [cs.LG])
29. Do Artificial Intelligence Systems Understand?. (arXiv:2207.11089v1 [cs.AI])
30. Classification via score-based generative modelling. (arXiv:2207.11091v1 [cs.LG])
31. Solving the Batch Stochastic Bin Packing Problem in Cloud: A Chance-constrained Optimization Approach. (arXiv:2207.11122v1 [math.OC])
32. Proactive Distributed Constraint Optimization of Heterogeneous Incident Vehicle Teams. (arXiv:2207.11132v1 [eess.SY])
33. Motion Planning and Control for Multi Vehicle Autonomous Racing at High Speeds. (arXiv:2207.11136v1 [cs.RO])
34. Towards Global Optimality in Cooperative MARL with Sequential Transformation. (arXiv:2207.11143v1 [cs.MA])
35. VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and Pooled Vehicle Information. (arXiv:2207.11146v1 [cs.CV])
36. CQE in OWL 2 QL: A "Longest Honeymoon" Approach (extended version). (arXiv:2207.11155v1 [cs.DB])
37. Lagrangian Method for Q-Function Learning (with Applications to Machine Translation). (arXiv:2207.11161v1 [cs.LG])
38. Decentralized scheduling through an adaptive, trading-based multi-agent system. (arXiv:2207.11172v1 [cs.AI])
39. Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images. (arXiv:2207.11186v1 [cs.CV])
40. TaDaa: **real time** Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems. (arXiv:2207.11187v1 [cs.IR])
41. Self-Supervised-RCNN for Medical Image Segmentation with Limited Data Annotation. (arXiv:2207.11191v1 [cs.CV])
42. Target-Driven Structured Transformer Planner for Vision-Language Navigation. (arXiv:2207.11201v1 [cs.CV])
43. Toward a Generic Mapping Language for Transformations between RDF and Data Interchange Formats. (arXiv:2207.11205v1 [cs.DB])
44. Domain Generalization for Activity Recognition via Adaptive Feature Fusion. (arXiv:2207.11221v1 [cs.CV])
45. Classifying Crop Types using Gaussian Bayesian Models and Neural Networks on GHISACONUS USGS data from NASA Hyperspectral Satellite Imagery. (arXiv:2207.11228v1 [cs.CV])
46. You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine. (arXiv:2207.11230v1 [cs.CV])
47. Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement. (arXiv:2207.11232v1 [cs.CV])
48. A System-driven Automatic Ground Truth Generation Method for DL Inner-City Driving Corridor Detectors. (arXiv:2207.11234v1 [cs.CV])
49. Defending Substitution-Based Profile Pollution Attacks on Sequential Recommenders. (arXiv:2207.11237v1 [cs.IR])
50. Improved lightweight identification of agricultural diseases based on MobileNetV3. (arXiv:2207.11238v1 [cs.CV])
51. Discrete Key-Value Bottleneck. (arXiv:2207.11240v1 [cs.LG])
52. Panoptic Scene Graph Generation. (arXiv:2207.11247v1 [cs.CV])
53. Composing Neural Learning and Symbolic Reasoning with an Application to Visual Discrimination. (arXiv:1907.05878v2 [cs.LG] UPDATED)
54. Towards Socially Intelligent Agents with Mental State Transition and Human Utility. (arXiv:2103.07011v2 [cs.CL] UPDATED)
55. Data Augmentation in Natural Language Processing: A Novel Text Generation Approach for Long and Short Text Classifiers. (arXiv:2103.14453v2 [cs.CL] UPDATED)
56. Automatic Termination for Hyperparameter Optimization. (arXiv:2104.08166v4 [cs.LG] UPDATED)
57. The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v5 [cs.CV] UPDATED)
58. Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks. (arXiv:2107.01809v2 [cs.LG] UPDATED)
59. A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v5 [cs.CL] UPDATED)
60. From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers. (arXiv:2107.07999v7 [cs.LG] UPDATED)
61. Generative Adversarial Networks for Labeled Acceleration Data Augmentation for Structural Damage Detection. (arXiv:2112.03478v6 [cs.LG] UPDATED)
62. Self-attention Presents Low-dimensional Knowledge Graph Embeddings for Link Prediction. (arXiv:2112.10644v2 [cs.LG] UPDATED)
63. Improving Nonparametric Classification via Local Radial Regression with an Application to Stock Prediction. (arXiv:2112.13951v2 [stat.ML] UPDATED)
64. DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale. (arXiv:2201.05596v2 [cs.LG] UPDATED)
65. Post-training Quantization for Neural Networks with Provable Guarantees. (arXiv:2201.11113v2 [cs.LG] UPDATED)
66. Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks. (arXiv:2201.11729v4 [cs.LG] UPDATED)
67. Modeling Complex Dependencies for Session-based Recommendations via Graph Neural Networks. (arXiv:2201.12532v2 [cs.IR] UPDATED)
68. Neighbour Interaction based Click-Through Rate Prediction via Graph-masked Transformer. (arXiv:2201.13311v2 [cs.IR] UPDATED)
69. An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images. (arXiv:2204.01341v3 [cs.CV] UPDATED)
70. ELECRec: Training Sequential Recommenders as Discriminators. (arXiv:2204.02011v4 [cs.AI] UPDATED)
71. OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v5 [cs.LG] UPDATED)
72. X-Risk Analysis for AI Research. (arXiv:2206.05862v6 [cs.CY] UPDATED)
73. Predicting Stock Price Movement after Disclosure of Corporate Annual Reports: A Case Study of 2021 China CSI 300 Stocks. (arXiv:2206.12528v2 [q-fin.ST] UPDATED)
74. Modeling Randomly Walking Volatility with Chained Gamma Distributions. (arXiv:2207.01151v2 [q-fin.CP] UPDATED)
75. Deep Learning approach for Classifying Trusses and Runners of Strawberries. (arXiv:2207.02721v2 [cs.CV] UPDATED)
76. Hierarchical Average Precision Training for Pertinent Image Retrieval. (arXiv:2207.04873v2 [cs.CV] UPDATED)
77. Magpie: Automatically Tuning Static Parameters for Distributed File Systems using Deep Reinforcement Learning. (arXiv:2207.09298v2 [cs.DC] UPDATED)
78. World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v2 [cs.RO] UPDATED)
79. STOP: A dataset for Spoken Task Oriented Semantic Parsing. (arXiv:2207.10643v2 [cs.CL] UPDATED)
80. Ich wei{\ss}, was du n\"achsten Sommer getan haben wirst: Predictive Policing in \"Osterreich. (arXiv:1907.00934v2 [cs.CY] CROSS LISTED)
81. "Part Man, Part Machine, All Cop": Automation in Policing. (arXiv:2106.12794v1 [cs.CY] CROSS LISTED)
82. Quantum Metropolis Solver: A Quantum Walks Approach to Optimization Problems. (arXiv:2207.06462v1 [quant-ph] CROSS LISTED)

