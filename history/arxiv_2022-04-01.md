# Your interest papers
---
## cs.CV
---
### DD**NeRF**: Depth Distribution Neural Radiance Fields. (arXiv:2203.16626v1 [cs.CV])
- Authors : David Dadon, Ohad Fried, Yacov Hel
- Link : [http://arxiv.org/abs/2203.16626](http://arxiv.org/abs/2203.16626)
> ABSTRACT  :  In recent years, the field of **implicit neural representation** has progressed significantly. Models such as neural radiance fields (**NeRF**), which uses relatively small neural networks, can represent high-quality scenes and achieve state-of-the-art results for novel view synthesis. Training these types of networks, however, is still computationally very expensive. We present depth distribution neural radiance field (DD**NeRF**), a new method that significantly increases sampling efficiency along rays during training while achieving superior results for a given sampling budget. DD**NeRF** achieves this by learning a more accurate representation of the density distribution along rays. More specifically, we train a coarse model to predict the internal distribution of the transparency of an input volume in addition to the volume's total density. This finer distribution then guides the sampling procedure of the fine model. This method allows us to use fewer samples during training while reducing computational resources.  
### MPS-**NeRF**: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v1 [cs.CV])
- Authors : Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng, Zicheng Liu, Xin Tong
- Link : [http://arxiv.org/abs/2203.16875](http://arxiv.org/abs/2203.16875)
> ABSTRACT  :  There has been rapid progress recently on 3D human rendering, including novel view synthesis and pose animation, based on the advances of neural radiance fields (**NeRF**). However, most existing methods focus on person-specific training and their training typically requires multi-view videos. This paper deals with a new challenging task -- rendering novel views and novel poses for a person unseen in training, using only multiview images as input. For this task, we propose a simple yet effective method to train a generalizable **NeRF** with multiview images as conditional input. The key ingredient is a dedicated representation combining a canonical **NeRF** and a volume deformation scheme. Using a canonical space enables our method to learn shared properties of human and easily generalize to different people. Volume deformation is used to connect the canonical space with input and target images and query image features for radiance and density prediction. We leverage the parametric 3D human model fitted on the input images to derive the deformation, which works quite well in practice when combined with our canonical **NeRF**. The experiments on both real and synthetic data with the novel view synthesis and pose animation tasks collectively demonstrate the efficacy of our method.  
### Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond. (arXiv:2203.16931v1 [cs.CV])
- Authors : Yi Yu, **Wenhan Yang**, Peng Tan
- Link : [http://arxiv.org/abs/2203.16931](http://arxiv.org/abs/2203.16931)
> ABSTRACT  :  Rain removal aims to remove rain streaks from images/videos and reduce the disruptive effects caused by rain. It not only enhances image/video visibility but also allows many computer vision algorithms to function properly. This paper makes the first attempt to conduct a comprehensive study on the robustness of deep learning-based rain removal methods against adversarial attacks. Our study shows that, when the image/video is highly degraded, rain removal methods are more vulnerable to the adversarial attacks as small distortions/perturbations become less noticeable or detectable. In this paper, we first present a comprehensive empirical evaluation of various methods at different levels of attacks and with various losses/targets to generate the perturbations from the perspective of human perception and machine analysis tasks. A systematic evaluation of key modules in existing methods is performed in terms of their robustness against adversarial attacks. From the insights of our analysis, we construct a more robust deraining method by integrating these effective modules. Finally, we examine various types of adversarial attacks that are specific to deraining problems and their effects on both human and machine vision tasks, including 1) rain region attacks, adding perturbations only in the rain regions to make the perturbations in the attacked rain images less visible; 2) object-sensitive attacks, adding perturbations only in regions near the given objects. Code is available at https://github.com/yuyi-sd/Robust_Rain_Removal.  
### SimVQA: Exploring Simulated Environments for Visual Question Answering. (arXiv:2203.17219v1 [cs.CV])
- Authors : Paola Cascante, Hui Wu, Letao Wang, Rogerio Feris, Vicente Ordonez
- Link : [http://arxiv.org/abs/2203.17219](http://arxiv.org/abs/2203.17219)
> ABSTRACT  :  Existing work on VQA explores data augmentation to achieve better generalization by perturbing the images in the dataset or modifying the existing questions and answers. While these methods exhibit good performance, the diversity of the questions and answers are constrained by the available image set. In this work we explore using synthetic computer-generated data to fully control the visual and language space, allowing us to provide more diverse scenarios. We quantify the effect of synthetic data in real-world VQA benchmarks and to which extent it produces results that generalize to real data. By exploiting 3D and physics simulation platforms, we provide a pipeline to generate synthetic data to expand and replace type-specific questions and answers without risking the **exposure** of sensitive or personal data that might be present in real images. We offer a comprehensive analysis while expanding existing hyper-realistic datasets to be used for VQA. We also propose Feature Swapping (F-SWAP) -- where we randomly switch object-level features during training to make a VQA model more domain invariant. We show that F-SWAP is effective for enhancing a currently existing VQA dataset of real images without compromising on the accuracy to answer existing questions in the dataset.  
### R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])
- Authors : Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, Sergey Tulyakov
- Link : [http://arxiv.org/abs/2203.17261](http://arxiv.org/abs/2203.17261)
> ABSTRACT  :  Recent research explosion on Neural Radiance Field (**NeRF**) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of **NeRF** is its prohibitive inference time: Rendering a single pixel requires querying the **NeRF** network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over **NeRF** in novel view synthesis -- the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained **NeRF** model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average PSNR improvement) rendering quality than **NeRF** without any customized implementation tricks.  
### Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech **Enhancement** by Re-Synthesis. (arXiv:2203.17263v1 [cs.CV])
- Authors : Karren Yang, Dejan Markovic, Steven Krenn, Vasu Agrawal, Alexander Richard
- Link : [http://arxiv.org/abs/2203.17263](http://arxiv.org/abs/2203.17263)
> ABSTRACT  :  Since facial actions such as lip movements contain significant information about speech content, it is not surprising that audio-visual speech **enhancement** methods are more accurate than their audio-only counterparts. Yet, state-of-the-art approaches still struggle to generate clean, realistic speech without noise artifacts and unnatural distortions in challenging acoustic environments. In this paper, we propose a novel audio-visual speech **enhancement** framework for high-fidelity telecommunications in AR/VR. Our approach leverages audio-visual speech cues to generate the codes of a neural speech codec, enabling efficient synthesis of clean, realistic speech from noisy signals. Given the importance of speaker-specific cues in speech, we focus on developing personalized models that work well for individual speakers. We demonstrate the efficacy of our approach on a new audio-visual speech dataset collected in an unconstrained, large vocabulary setting, as well as existing audio-visual datasets, outperforming speech **enhancement** baselines on both quantitative metrics and human evaluation studies. Please see the supplemental video for qualitative results at https://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.  
### MyStyle: A Personalized Generative Prior. (arXiv:2203.17272v1 [cs.CV])
- Authors : Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, Daniel Cohen
- Link : [http://arxiv.org/abs/2203.17272](http://arxiv.org/abs/2203.17272)
> ABSTRACT  :  We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (~100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image **enhancement** problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.  
### Bringing Old Films Back to Life. (arXiv:2203.17276v1 [cs.CV])
- Authors : Ziyu Wan, Bo Zhang, Dongdong Chen, Jing Liao
- Link : [http://arxiv.org/abs/2203.17276](http://arxiv.org/abs/2203.17276)
> ABSTRACT  :  We present a learning-based framework, recurrent transformer network (RTN), to restore heavily degraded old films. Instead of performing frame-wise **restoration**, our method is based on the hidden knowledge learned from adjacent frames that contain abundant information about the occlusion, which is beneficial to restore challenging artifacts of each frame while ensuring temporal coherency. Moreover, contrasting the representation of the current frame and the hidden knowledge makes it possible to infer the scratch position in an unsupervised manner, and such defect localization generalizes well to real-world degradations. To better resolve mixed degradation and compensate for the flow estimation error during frame alignment, we propose to leverage more expressive transformer blocks for spatial **restoration**. Experiments on both synthetic dataset and real-world old films demonstrate the significant superiority of the proposed RTN over existing solutions. In addition, the same framework can effectively propagate the color from keyframes to the whole video, ultimately yielding compelling restored films. The implementation and model will be released at https://github.com/raywzy/Bringing-Old-Films-Back-to-Life.  
### Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v3 [cs.CV] UPDATED)
- Authors : Jue Wang, Gedas Bertasius, Du Tran, Lorenzo Torresani
- Link : [http://arxiv.org/abs/2106.09212](http://arxiv.org/abs/2106.09212)
> ABSTRACT  :  Video transformers have recently emerged as a competitive alternative to 3D CNNs for video understanding. However, due to their large number of parameters and reduced inductive biases, these models require supervised pretraining on large-scale image datasets to achieve top performance. In this paper, we empirically demonstrate that self-supervised pretraining of video transformers on video-only datasets can lead to action recognition results that are on par or better than those obtained with supervised pretraining on large-scale image datasets, even massive ones such as ImageNet-21K. Since transformer-based models are effective at capturing dependencies over extended temporal spans, we propose a simple learning procedure that forces the model to match a long-term view to a short-term view of the same video. Our approach, named Long-Short Temporal Contrastive Learning (LSTCL), enables video transformers to learn an effective clip-level representation by predicting temporal context captured from a longer temporal extent. To demonstrate the generality of our findings, we implement and validate our approach under three different self-supervised contrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct video-transformer architectures, including an improved variant of the **Swin** Transformer augmented with space-time attention. We conduct a thorough ablation study and show that LSTCL achieves competitive performance on multiple video benchmarks and represents a convincing alternative to supervised image-based pretraining.  
### DiGS : Divergence guided shape **implicit neural representation** for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)
- Authors : Yizhak Ben, Chamin Hewa, Stephen Gould
- Link : [http://arxiv.org/abs/2106.10811](http://arxiv.org/abs/2106.10811)
> ABSTRACT  :  Shape **implicit neural representation**s (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and shape space learning and show SOTA performance compared to other unoriented methods. Code and model parameters available at our project page https://chumbyte.github.io/DiGS-Site/.  
### **HDR**-**NeRF**: **High Dynamic Range** Neural Radiance Fields. (arXiv:2111.14451v3 [cs.CV] UPDATED)
- Authors : Xin Huang, Qi Zhang, Ying Feng, Hongdong Li, Xuan Wang, Qing Wang
- Link : [http://arxiv.org/abs/2111.14451](http://arxiv.org/abs/2111.14451)
> ABSTRACT  :  We present **High Dynamic Range** Neural Radiance Fields (**HDR**-**NeRF**) to recover an **HDR** radiance field from a set of low dynamic range (LDR) views with different **exposure**s. Using the **HDR**-**NeRF**, we are able to generate both novel **HDR** views and novel LDR views under different **exposure**s. The key to our method is to model the physical imaging process, which dictates that the radiance of a scene point transforms to a pixel value in the LDR image with two implicit functions: a radiance field and a tone mapper. The radiance field encodes the scene radiance (values vary from 0 to +infty), which outputs the density and radiance of a ray by giving corresponding ray origin and ray direction. The tone mapper models the mapping process that a ray hitting on the camera sensor becomes a pixel value. The color of the ray is predicted by feeding the radiance and the corresponding **exposure** time into the tone mapper. We use the classic volume rendering technique to project the output radiance, colors, and densities into **HDR** and LDR images, while only the input LDR images are used as the supervision. We collect a new forward-facing **HDR** dataset to evaluate the proposed method. Experimental results on synthetic and real-world scenes validate that our method can not only accurately control the **exposure**s of synthesized views but also render views with a **high dynamic range**.  
### AdaMixer: A Fast-Converging Query-Based Object Detector. (arXiv:2203.16507v2 [cs.CV] UPDATED)
- Authors : Ziteng Gao, Limin Wang, Bing Han, Sheng Guo
- Link : [http://arxiv.org/abs/2203.16507](http://arxiv.org/abs/2203.16507)
> ABSTRACT  :  Traditional object detectors employ the dense paradigm of scanning over locations and scales in an image. The recent query-based object detectors break this convention by decoding image features with a set of learnable queries. However, this paradigm still suffers from slow convergence, limited performance, and design complexity of extra networks between backbone and decoder. In this paper, we find that the key to these issues is the adaptability of decoders for casting queries to varying objects. Accordingly, we propose a fast-converging query-based detector, named AdaMixer, by improving the adaptability of query-based decoding processes in two aspects. First, each query adaptively samples features over space and scales based on estimated offsets, which allows AdaMixer to efficiently attend to the coherent regions of objects. Then, we dynamically decode these sampled features with an adaptive MLP-Mixer under the guidance of each query. Thanks to these two critical designs, AdaMixer enjoys architectural simplicity without requiring dense attentional encoders or explicit pyramid networks. On the challenging MS COCO benchmark, AdaMixer with ResNet-50 as the backbone, with 12 training epochs, reaches up to 45.0 AP on the validation set along with 27.9 APs in detecting small objects. With the longer training scheme, AdaMixer with ResNeXt-101-DCN and **Swin**-S reaches 49.5 and 51.3 AP. Our work sheds light on a simple, accurate, and fast converging architecture for query-based object detectors. The code is made available at https://github.com/MCG-NJU/AdaMixer  
## eess.IV
---
## cs.LG
---
### Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load. (arXiv:2203.16637v1 [cs.SD])
- Authors : Gasser Elbanna, Alice Biryukov, Neil Scheidwasser, Lara Orlandic, Pablo Mainar, Mikolaj Kegler, Pierre Beckmann, Milos Cernak
- Link : [http://arxiv.org/abs/2203.16637](http://arxiv.org/abs/2203.16637)
> ABSTRACT  :  As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained **exposure**. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches.  
### SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping. (arXiv:2203.16749v1 [eess.AS])
- Authors : Yuma Koizumi, Heiga Zen, Kohei Yatabe, Nanxin Chen, Michiel Bacchiani
- Link : [http://arxiv.org/abs/2203.16749](http://arxiv.org/abs/2203.16749)
> ABSTRACT  :  Neural vocoder using denoising diffusion probabilistic model (DDPM) has been improved by adaptation of the diffusion noise distribution to given acoustic features. In this study, we propose SpecGrad that adapts the diffusion noise so that its time-varying spectral envelope becomes close to the conditioning log-mel spectrogram. This adaptation by time-varying filtering improves the sound quality especially in the high-frequency bands. It is processed in the time-frequency domain to keep the computational cost almost the same as the conventional DDPM-based neural vocoders. Experimental results showed that SpecGrad generates higher-fidelity speech waveform than conventional DDPM-based neural vocoders in both analysis-synthesis and speech **enhancement** scenarios. Audio demos are available at wavegrad.github.io/specgrad/.  
### Speech **Enhancement** with Score-Based Generative Models in the Complex STFT Domain. (arXiv:2203.17004v1 [eess.AS])
- Authors : Simon Welker, Julius Richter, Timo Gerkmann
- Link : [http://arxiv.org/abs/2203.17004](http://arxiv.org/abs/2203.17004)
> ABSTRACT  :  Score-based generative models (SGMs) have recently shown impressive results for difficult generative tasks such as the unconditional and conditional generation of natural images and audio signals. In this work, we extend these models to the complex short-time Fourier transform (STFT) domain, proposing a novel training task for speech **enhancement** using a complex-valued deep neural network. We derive this training task within the formalism of stochastic differential equations, thereby enabling the use of predictor-corrector samplers. We provide alternative formulations inspired by previous publications on using SGMs for speech **enhancement**, avoiding the need for any prior assumptions on the noise distribution and making the training task purely generative which, as we show, results in improved **enhancement** performance.  
### CatIss: An Intelligent Tool for Categorizing Issues Reports using Transformers. (arXiv:2203.17196v1 [cs.SE])
- Authors : Maliheh Izadi
- Link : [http://arxiv.org/abs/2203.17196](http://arxiv.org/abs/2203.17196)
> ABSTRACT  :  Users use Issue Tracking Systems to keep track and manage issue reports in their repositories. An issue is a rich source of software information that contains different reports including a problem, a request for new features, or merely a question about the software product. As the number of these issues increases, it becomes harder to manage them manually. Thus, automatic approaches are proposed to help facilitate the management of issue reports.    This paper describes CatIss, an automatic CATegorizer of ISSue reports which is built upon the Transformer-based pre-trained RoBERTa model. CatIss classifies issue reports into three main categories of Bug reports, **Enhancement**/feature requests, and Questions. First, the datasets provided for the NLBSE tool competition are cleaned and preprocessed. Then, the pre-trained RoBERTa model is fine-tuned on the preprocessed dataset. Evaluating CatIss on about 80 thousand issue reports from GitHub, indicates that it performs very well surpassing the competition baseline, TicketTagger, and achieving 87.2% F1-score (micro average). Additionally, as CatIss is trained on a wide set of repositories, it is a generic prediction model, hence applicable for any unseen software project or projects with little historical data. Scripts for cleaning the datasets, training CatIss, and evaluating the model are publicly available.  
### R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])
- Authors : Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, Sergey Tulyakov
- Link : [http://arxiv.org/abs/2203.17261](http://arxiv.org/abs/2203.17261)
> ABSTRACT  :  Recent research explosion on Neural Radiance Field (**NeRF**) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of **NeRF** is its prohibitive inference time: Rendering a single pixel requires querying the **NeRF** network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over **NeRF** in novel view synthesis -- the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained **NeRF** model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average PSNR improvement) rendering quality than **NeRF** without any customized implementation tricks.  
### Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech **Enhancement** by Re-Synthesis. (arXiv:2203.17263v1 [cs.CV])
- Authors : Karren Yang, Dejan Markovic, Steven Krenn, Vasu Agrawal, Alexander Richard
- Link : [http://arxiv.org/abs/2203.17263](http://arxiv.org/abs/2203.17263)
> ABSTRACT  :  Since facial actions such as lip movements contain significant information about speech content, it is not surprising that audio-visual speech **enhancement** methods are more accurate than their audio-only counterparts. Yet, state-of-the-art approaches still struggle to generate clean, realistic speech without noise artifacts and unnatural distortions in challenging acoustic environments. In this paper, we propose a novel audio-visual speech **enhancement** framework for high-fidelity telecommunications in AR/VR. Our approach leverages audio-visual speech cues to generate the codes of a neural speech codec, enabling efficient synthesis of clean, realistic speech from noisy signals. Given the importance of speaker-specific cues in speech, we focus on developing personalized models that work well for individual speakers. We demonstrate the efficacy of our approach on a new audio-visual speech dataset collected in an unconstrained, large vocabulary setting, as well as existing audio-visual datasets, outperforming speech **enhancement** baselines on both quantitative metrics and human evaluation studies. Please see the supplemental video for qualitative results at https://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.  
### MyStyle: A Personalized Generative Prior. (arXiv:2203.17272v1 [cs.CV])
- Authors : Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, Daniel Cohen
- Link : [http://arxiv.org/abs/2203.17272](http://arxiv.org/abs/2203.17272)
> ABSTRACT  :  We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (~100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image **enhancement** problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.  
### Using IPA-Based Tacotron for Data Efficient Cross-Lingual Speaker Adaptation and Pronunciation **Enhancement**. (arXiv:2011.06392v2 [cs.SD] UPDATED)
- Authors : Hamed Hemati, Damian Borth
- Link : [http://arxiv.org/abs/2011.06392](http://arxiv.org/abs/2011.06392)
> ABSTRACT  :  Recent neural Text-to-Speech (TTS) models have been shown to perform very well when enough data is available. However, fine-tuning them for new speakers or languages is not straightforward in a low-resource setup. In this paper, we show that by applying minor modifications to a Tacotron model, one can transfer an existing TTS model for new speakers from the same or a different language using only 20 minutes of data. For this purpose, we first introduce a base multi-lingual Tacotron with language-agnostic input, then demonstrate how transfer learning is done for different scenarios of speaker adaptation without exploiting any pre-trained speaker encoder or code-switching technique. We evaluate the transferred model in both subjective and objective ways.  
### DiGS : Divergence guided shape **implicit neural representation** for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)
- Authors : Yizhak Ben, Chamin Hewa, Stephen Gould
- Link : [http://arxiv.org/abs/2106.10811](http://arxiv.org/abs/2106.10811)
> ABSTRACT  :  Shape **implicit neural representation**s (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and shape space learning and show SOTA performance compared to other unoriented methods. Code and model parameters available at our project page https://chumbyte.github.io/DiGS-Site/.  
### Rerunning OCR: A Machine Learning Approach to Quality Assessment and **Enhancement** Prediction. (arXiv:2110.01661v4 [cs.CL] UPDATED)
- Authors : Pit Schneider, Yves Maurer
- Link : [http://arxiv.org/abs/2110.01661](http://arxiv.org/abs/2110.01661)
> ABSTRACT  :  Iterating with new and improved OCR solutions enforces decision making when it comes to targeting the right candidates for reprocessing. This especially applies when the underlying data collection is of considerable size and rather diverse in terms of fonts, languages, periods of publication and consequently OCR quality. This article captures the efforts of the National Library of Luxembourg to support those targeting decisions. They are crucial in order to guarantee low computational overhead and reduced quality degradation risks, combined with a more quantifiable OCR improvement. In particular, this work explains the methodology of the library with respect to text block level quality assessment. Through extension of this technique, a regression model, that is able to take into account the **enhancement** potential of a new OCR engine, is also presented. They both mark promising approaches, especially for cultural institutions dealing with historical data of lower quality.  
## cs.AI
---
### Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load. (arXiv:2203.16637v1 [cs.SD])
- Authors : Gasser Elbanna, Alice Biryukov, Neil Scheidwasser, Lara Orlandic, Pablo Mainar, Mikolaj Kegler, Pierre Beckmann, Milos Cernak
- Link : [http://arxiv.org/abs/2203.16637](http://arxiv.org/abs/2203.16637)
> ABSTRACT  :  As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained **exposure**. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches.  
### MPS-**NeRF**: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v1 [cs.CV])
- Authors : Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng, Zicheng Liu, Xin Tong
- Link : [http://arxiv.org/abs/2203.16875](http://arxiv.org/abs/2203.16875)
> ABSTRACT  :  There has been rapid progress recently on 3D human rendering, including novel view synthesis and pose animation, based on the advances of neural radiance fields (**NeRF**). However, most existing methods focus on person-specific training and their training typically requires multi-view videos. This paper deals with a new challenging task -- rendering novel views and novel poses for a person unseen in training, using only multiview images as input. For this task, we propose a simple yet effective method to train a generalizable **NeRF** with multiview images as conditional input. The key ingredient is a dedicated representation combining a canonical **NeRF** and a volume deformation scheme. Using a canonical space enables our method to learn shared properties of human and easily generalize to different people. Volume deformation is used to connect the canonical space with input and target images and query image features for radiance and density prediction. We leverage the parametric 3D human model fitted on the input images to derive the deformation, which works quite well in practice when combined with our canonical **NeRF**. The experiments on both real and synthetic data with the novel view synthesis and pose animation tasks collectively demonstrate the efficacy of our method.  
### R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])
- Authors : Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, Sergey Tulyakov
- Link : [http://arxiv.org/abs/2203.17261](http://arxiv.org/abs/2203.17261)
> ABSTRACT  :  Recent research explosion on Neural Radiance Field (**NeRF**) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of **NeRF** is its prohibitive inference time: Rendering a single pixel requires querying the **NeRF** network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over **NeRF** in novel view synthesis -- the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained **NeRF** model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average PSNR improvement) rendering quality than **NeRF** without any customized implementation tricks.  
### Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v3 [cs.CV] UPDATED)
- Authors : Jue Wang, Gedas Bertasius, Du Tran, Lorenzo Torresani
- Link : [http://arxiv.org/abs/2106.09212](http://arxiv.org/abs/2106.09212)
> ABSTRACT  :  Video transformers have recently emerged as a competitive alternative to 3D CNNs for video understanding. However, due to their large number of parameters and reduced inductive biases, these models require supervised pretraining on large-scale image datasets to achieve top performance. In this paper, we empirically demonstrate that self-supervised pretraining of video transformers on video-only datasets can lead to action recognition results that are on par or better than those obtained with supervised pretraining on large-scale image datasets, even massive ones such as ImageNet-21K. Since transformer-based models are effective at capturing dependencies over extended temporal spans, we propose a simple learning procedure that forces the model to match a long-term view to a short-term view of the same video. Our approach, named Long-Short Temporal Contrastive Learning (LSTCL), enables video transformers to learn an effective clip-level representation by predicting temporal context captured from a longer temporal extent. To demonstrate the generality of our findings, we implement and validate our approach under three different self-supervised contrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct video-transformer architectures, including an improved variant of the **Swin** Transformer augmented with space-time attention. We conduct a thorough ablation study and show that LSTCL achieves competitive performance on multiple video benchmarks and represents a convincing alternative to supervised image-based pretraining.  
### DiGS : Divergence guided shape **implicit neural representation** for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)
- Authors : Yizhak Ben, Chamin Hewa, Stephen Gould
- Link : [http://arxiv.org/abs/2106.10811](http://arxiv.org/abs/2106.10811)
> ABSTRACT  :  Shape **implicit neural representation**s (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and shape space learning and show SOTA performance compared to other unoriented methods. Code and model parameters available at our project page https://chumbyte.github.io/DiGS-Site/.  
### Rerunning OCR: A Machine Learning Approach to Quality Assessment and **Enhancement** Prediction. (arXiv:2110.01661v4 [cs.CL] UPDATED)
- Authors : Pit Schneider, Yves Maurer
- Link : [http://arxiv.org/abs/2110.01661](http://arxiv.org/abs/2110.01661)
> ABSTRACT  :  Iterating with new and improved OCR solutions enforces decision making when it comes to targeting the right candidates for reprocessing. This especially applies when the underlying data collection is of considerable size and rather diverse in terms of fonts, languages, periods of publication and consequently OCR quality. This article captures the efforts of the National Library of Luxembourg to support those targeting decisions. They are crucial in order to guarantee low computational overhead and reduced quality degradation risks, combined with a more quantifiable OCR improvement. In particular, this work explains the methodology of the library with respect to text block level quality assessment. Through extension of this technique, a regression model, that is able to take into account the **enhancement** potential of a new OCR engine, is also presented. They both mark promising approaches, especially for cultural institutions dealing with historical data of lower quality.  
### BI-RADS BERT & Using Section Segmentation to Understand Radiology Reports. (arXiv:2110.07552v2 [cs.CL] UPDATED)
- Authors : Grey Kuling, Belinda Curpen
- Link : [http://arxiv.org/abs/2110.07552](http://arxiv.org/abs/2110.07552)
> ABSTRACT  :  Radiology reports are one of the main forms of communication between radiologists and other clinicians and contain important information for patient care. In order to use this information for research and automated patient care programs, it is necessary to convert the raw text into structured data suitable for analysis. State-of-the-art natural language processing (NLP) domain-specific contextual word embeddings have been shown to achieve impressive accuracy for these tasks in medicine, but have yet to be utilized for section structure segmentation. In this work, we pre-trained a contextual embedding BERT model using breast radiology reports and developed a classifier that incorporated the embedding with auxiliary global textual features in order to perform section segmentation. This model achieved a 98% accuracy at segregating free text reports sentence by sentence into sections of information outlined in the Breast Imaging Reporting and Data System (BI-RADS) lexicon, a significant improvement over the Classic BERT model without auxiliary information. We then evaluated whether using section segmentation improved the downstream extraction of clinically relevant information such as modality/procedure, previous cancer, menopausal status, the purpose of the exam, breast density, and breast MRI background parenchymal **enhancement**. Using the BERT model pre-trained on breast radiology reports combined with section segmentation resulted in an overall accuracy of 95.9% in the field extraction tasks. This is a 17% improvement compared to an overall accuracy of 78.9% for field extraction with models using Classic BERT embeddings and not using section segmentation. Our work shows the strength of using BERT in radiology report analysis and the advantages of section segmentation in identifying key features of patient factors recorded in breast radiology reports.  
### Marvin: an Innovative Omni-Directional Robotic Assistant for Domestic Environments. (arXiv:2112.05597v2 [cs.RO] UPDATED)
- Authors : Andrea Eirale, Mauro Martini, Luigi Tagliavini, Dario Gandini, Marcello Chiaberge, Giuseppe Quaglia
- Link : [http://arxiv.org/abs/2112.05597](http://arxiv.org/abs/2112.05597)
> ABSTRACT  :  Technology is progressively reshaping the domestic environment as we know it, enhancing home security and the overall people well-being through smart connected devices. However, population ageing and pandemics recently demonstrate to cause isolation of elderly people in their houses, generating the need for a reliable assistive figure. Robotic assistants are the new frontier of innovation for domestic welfare, and elderly monitoring is only one of the possible services an intelligent robotic platform can handle for collective well-being. Despite these emerging needs, the actual landscape of robotic assistants lacks a platform which successfully combine a reliable and agile mobility in cluttered domestic spaces, with lightweight and offline Artificial Intelligence (AI) solutions for perception and interaction. In this work, we present Marvin, a novel assistive robotic platform we developed with a modular layer-based architecture, merging a flexible mechanical design with cutting-edge AI for perception and vocal control. We focus the design of Marvin on three service functions: monitoring of elderly and reduced-mobility subjects, remote presence and connectivity, and **night** assistance. With respect to previous works, we propose a tiny size omnidirectional platform, which enable a more agile mobility and an effective obstacle avoidance. Moreover, we introduce a controllable positioning device, which easily allows the user to access the visual interface, and, at the same time, it can physically extend the field of view of the camera sensor. Nonetheless, we delicately consider the privacy issues arising from private data collection on cloud services, a critical aspect of commercial AI-based assistants. To this end, we demonstrate how lightweight deep learning solutions for visual perception and vocal command can be adopted, completely running offline on the embedded hardware of the robot.  
# Paper List
---
## cs.CV
---
**147** new papers in cs.CV:-) 
1. Towards Multimodal Depth Estimation from Light Fields. (arXiv:2203.16542v1 [cs.CV])
2. COSMOS: Cross-Modality Unsupervised Domain Adaptation for 3D Medical Image Segmentation based on Target-aware Domain Translation and Iterative Self-Training. (arXiv:2203.16557v1 [eess.IV])
3. Counterfactual Cycle-Consistent Learning for Instruction Following and Generation in Vision-Language Navigation. (arXiv:2203.16586v1 [cs.CV])
4. Constrained Few-shot Class-incremental Learning. (arXiv:2203.16588v1 [cs.CV])
5. Learning Local Displacements for Point Cloud Completion. (arXiv:2203.16600v1 [cs.CV])
6. Enhancing Cancer Prediction in Challenging Screen-Detected Incident Lung Nodules Using Time-Series Deep Learning. (arXiv:2203.16606v1 [eess.IV])
7. Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v1 [cs.AI])
8. End-to-end Document Recognition and Understanding with Dessurt. (arXiv:2203.16618v1 [cs.CV])
9. TR-MOT: Multi-Object Tracking by Reference. (arXiv:2203.16621v1 [cs.CV])
10. Federated Learning for the Classification of Tumor Infiltrating Lymphocytes. (arXiv:2203.16622v1 [eess.IV])
11. DD**NeRF**: Depth Distribution Neural Radiance Fields. (arXiv:2203.16626v1 [cs.CV])
12. Controllable Augmentations for Video Representation Learning. (arXiv:2203.16632v1 [cs.CV])
13. FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations. (arXiv:2203.16639v1 [cs.CV])
14. Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination. (arXiv:2203.16669v1 [cs.CV])
15. PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition. (arXiv:2203.16670v1 [cs.CV])
16. Knowledge-Spreader: Learning Facial Action Unit Dynamics with Extremely Limited Labels. (arXiv:2203.16678v1 [cs.CV])
17. Learning the Effect of Registration Hyperparameters with HyperMorph. (arXiv:2203.16680v1 [cs.CV])
18. Face Relighting with Geometrically Consistent Shadows. (arXiv:2203.16681v1 [cs.CV])
19. To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo. (arXiv:2203.16682v1 [cs.CV])
20. Task Adaptive Parameter Sharing for Multi-Task Learning. (arXiv:2203.16708v1 [cs.LG])
21. Exploiting Explainable Metrics for Augmented SGD. (arXiv:2203.16723v1 [cs.LG])
22. Personalized Image Aesthetics Assessment with Rich Attributes. (arXiv:2203.16754v1 [cs.CV])
23. Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models. (arXiv:2203.16755v1 [cs.CV])
24. Casual 6-DoF: free-viewpoint panorama using a handheld 360 camera. (arXiv:2203.16756v1 [cs.CV])
25. MeMOT: Multi-Object Tracking with Memory. (arXiv:2203.16761v1 [cs.CV])
26. CREATE: A Benchmark for Chinese Short Video Retrieval and Title Generation. (arXiv:2203.16763v1 [cs.CV])
27. SpatioTemporal Focus for Skeleton-based Action Recognition. (arXiv:2203.16767v1 [cs.CV])
28. ReSTR: Convolution-free Referring Image Segmentation Using Transformers. (arXiv:2203.16768v1 [cs.CV])
29. LAKe-Net: Topology-Aware Point Cloud Completion by Localizing Aligned Keypoints. (arXiv:2203.16771v1 [cs.CV])
30. Mask Atari for Deep Reinforcement Learning as POMDP Benchmarks. (arXiv:2203.16777v1 [cs.CV])
31. ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval. (arXiv:2203.16778v1 [cs.CV])
32. Weakly Supervised Patch Label Inference Networks for Efficient Pavement Distress Detection and Recognition in the Wild. (arXiv:2203.16782v1 [cs.CV])
33. Video-Text Representation Learning via Differentiable Weak Temporal Alignment. (arXiv:2203.16784v1 [cs.CV])
34. Reflection and Rotation Symmetry Detection via Equivariant Learning. (arXiv:2203.16787v1 [cs.CV])
35. Deformable Video Transformer. (arXiv:2203.16795v1 [cs.CV])
36. Ternary and Binary Quantization for Improved Classification. (arXiv:2203.16798v1 [cs.CV])
37. Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization. (arXiv:2203.16800v1 [cs.CV])
38. Rethinking Portrait Matting with Privacy Preserving. (arXiv:2203.16828v1 [cs.CV])
39. Point Scene Understanding via Disentangled Instance Mesh Reconstruction. (arXiv:2203.16832v1 [cs.CV])
40. Speaker Extraction with Co-Speech Gestures Cue. (arXiv:2203.16840v1 [eess.AS])
41. Towards Driving-Oriented Metric for Lane Detection Models. (arXiv:2203.16851v1 [cs.CV])
42. Investigating Modality Bias in Audio Visual Video Parsing. (arXiv:2203.16860v1 [cs.CV])
43. MPS-**NeRF**: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v1 [cs.CV])
44. Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds. (arXiv:2203.16895v1 [cs.CV])
45. CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow. (arXiv:2203.16896v1 [cs.CV])
46. Multi-Granularity Alignment Domain Adaptation for Object Detection. (arXiv:2203.16897v1 [cs.CV])
47. Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis. (arXiv:2203.16898v1 [cs.CV])
48. End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps. (arXiv:2203.16910v1 [cs.CV])
49. A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques. (arXiv:2203.16915v1 [cs.CV])
50. Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond. (arXiv:2203.16931v1 [cs.CV])
51. Contributions to interframe coding. (arXiv:2203.16934v1 [cs.CV])
52. Semantic Pose Verification for Outdoor Visual Localization with Self-supervised Contrastive Learning. (arXiv:2203.16945v1 [cs.CV])
53. Multimodal Fusion Transformer for Remote Sensing Image Classification. (arXiv:2203.16952v1 [cs.CV])
54. Human Instance Segmentation and Tracking via Data Association and Single-stage Detector. (arXiv:2203.16966v1 [cs.CV])
55. Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification. (arXiv:2203.16983v1 [cs.CV])
56. Measuring hand use in the home after cervical spinal cord injury using egocentric video. (arXiv:2203.16996v1 [eess.IV])
57. It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher. (arXiv:2203.17008v1 [cs.CV])
58. A Temporal Learning Approach to Inpainting Endoscopic Specularities and Its effect on Image Correspondence. (arXiv:2203.17013v1 [cs.CV])
59. Logit Normalization for Long-tail Object Detection. (arXiv:2203.17020v1 [cs.CV])
60. Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks. (arXiv:2203.17030v1 [cs.CV])
61. BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection. (arXiv:2203.17054v1 [cs.CV])
62. Cross-modal Learning of Graph Representations using Radar Point Cloud for Long-Range Gesture Recognition. (arXiv:2203.17066v1 [eess.SP])
63. CADG: A Model Based on Cross Attention for Domain Generalization. (arXiv:2203.17067v1 [cs.CV])
64. Deep Hyperspectral Unmixing using Transformer Network. (arXiv:2203.17076v1 [cs.CV])
65. AEGNN: Asynchronous Event-based Graph Neural Networks. (arXiv:2203.17149v1 [cs.CV])
66. Adaptive Mean-Residue Loss for Robust Facial Age Estimation. (arXiv:2203.17156v1 [cs.CV])
67. 3D Equivariant Graph Implicit Functions. (arXiv:2203.17178v1 [cs.CV])
68. Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow and Multi-scale Fusion. (arXiv:2203.17191v1 [cs.CV])
69. Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy. (arXiv:2203.17205v1 [cs.CV])
70. SimVQA: Exploring Simulated Environments for Visual Question Answering. (arXiv:2203.17219v1 [cs.CV])
71. Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions. (arXiv:2203.17234v1 [cs.CV])
72. ImpDet: Exploring Implicit Fields for 3D Object Detection. (arXiv:2203.17240v1 [cs.CV])
73. VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v1 [cs.CV])
74. Continuous Scene Representations for Embodied AI. (arXiv:2203.17251v1 [cs.CV])
75. A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v1 [q-bio.NC])
76. Rethinking Video Salient Object Ranking. (arXiv:2203.17257v1 [cs.CV])
77. Generating High Fidelity Data from Low-density Regions using Diffusion Models. (arXiv:2203.17260v1 [cs.CV])
78. R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])
79. Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech **Enhancement** by Re-Synthesis. (arXiv:2203.17263v1 [cs.CV])
80. TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing. (arXiv:2203.17266v1 [cs.CV])
81. A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v1 [cs.LG])
82. BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers. (arXiv:2203.17270v1 [cs.CV])
83. Do Vision-Language Pretrained Models Learn Primitive Concepts?. (arXiv:2203.17271v1 [cs.CV])
84. MyStyle: A Personalized Generative Prior. (arXiv:2203.17272v1 [cs.CV])
85. FindIt: Generalized Localization with Natural Language Queries. (arXiv:2203.17273v1 [cs.CV])
86. Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models. (arXiv:2203.17274v1 [cs.CV])
87. DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools. (arXiv:2203.17275v1 [cs.LG])
88. Bringing Old Films Back to Life. (arXiv:2203.17276v1 [cs.CV])
89. Unsupervised Part Mining for Fine-grained Image Classification. (arXiv:1902.09941v2 [cs.CV] UPDATED)
90. Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v5 [cs.SI] UPDATED)
91. EDN: Salient Object Detection via Extremely-Downsampled Network. (arXiv:2012.13093v3 [cs.CV] UPDATED)
92. Localization Distillation for Dense Object Detection. (arXiv:2102.12252v4 [cs.CV] UPDATED)
93. Cloth-Changing Person Re-identification from A Single Image with Gait Prediction and Regularization. (arXiv:2103.15537v4 [cs.CV] UPDATED)
94. DINE: Domain Adaptation from Single and Multiple Black-box Predictors. (arXiv:2104.01539v3 [cs.CV] UPDATED)
95. Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart. (arXiv:2105.14785v4 [cs.LG] UPDATED)
96. RegionViT: Regional-to-Local Attention for Vision Transformers. (arXiv:2106.02689v3 [cs.CV] UPDATED)
97. Generative Flows with Invertible Attentions. (arXiv:2106.03959v4 [cs.LG] UPDATED)
98. Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v3 [cs.CV] UPDATED)
99. DiGS : Divergence guided shape **implicit neural representation** for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)
100. Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v2 [cs.CR] UPDATED)
101. Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v3 [cs.CR] UPDATED)
102. Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v5 [cs.LG] UPDATED)
103. Stochastic Rounding for Image Interpolation and Scan Conversion. (arXiv:2110.12983v2 [cs.GR] UPDATED)
104. Deformable image registration with deep network priors: a study on longitudinal PET images. (arXiv:2111.11873v2 [eess.IV] UPDATED)
105. Transferability Metrics for Selecting Source Model Ensembles. (arXiv:2111.13011v2 [cs.CV] UPDATED)
106. The Implicit Values of A Good Hand Shake: Handheld Multi-Frame Neural Depth Refinement. (arXiv:2111.13738v2 [cs.CV] UPDATED)
107. Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning. (arXiv:2111.14213v2 [cs.LG] UPDATED)
108. ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic. (arXiv:2111.14447v2 [cs.CV] UPDATED)
109. **HDR**-**NeRF**: **High Dynamic Range** Neural Radiance Fields. (arXiv:2111.14451v3 [cs.CV] UPDATED)
110. MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image. (arXiv:2112.02753v2 [cs.CV] UPDATED)
111. Make It Move: Controllable Image-to-Video Generation with Text Descriptions. (arXiv:2112.02815v2 [cs.CV] UPDATED)
112. Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling. (arXiv:2112.04148v3 [cs.CV] UPDATED)
113. Injecting Semantic Concepts into End-to-End Image Captioning. (arXiv:2112.05230v2 [cs.CV] UPDATED)
114. Feature Erasing and Diffusion Network for Occluded Person Re-Identification. (arXiv:2112.08740v2 [cs.CV] UPDATED)
115. ScanQA: 3D Question Answering for Spatial Scene Understanding. (arXiv:2112.10482v2 [cs.CV] UPDATED)
116. BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View. (arXiv:2112.11790v2 [cs.CV] UPDATED)
117. SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v3 [cs.CV] UPDATED)
118. On the Cross-dataset Generalization in License Plate Recognition. (arXiv:2201.00267v3 [cs.CV] UPDATED)
119. Arbitrary Handwriting Image Style Transfer. (arXiv:2201.05346v4 [cs.CV] UPDATED)
120. Omnivore: A Single Model for Many Visual Modalities. (arXiv:2201.08377v2 [cs.CV] UPDATED)
121. Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint. (arXiv:2202.00886v4 [cs.RO] UPDATED)
122. D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v3 [cs.CV] UPDATED)
123. Enhancing Satellite Imagery using Deep Learning for the Sensor To Shooter Timeline. (arXiv:2203.00116v3 [cs.CV] UPDATED)
124. Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v2 [cs.CV] UPDATED)
125. Contextual Attention Network: Transformer Meets U-Net. (arXiv:2203.01932v2 [eess.IV] UPDATED)
126. Temporal Context Matters: Enhancing Single Image Prediction with Disease Progression Representations. (arXiv:2203.01933v2 [eess.IV] UPDATED)
127. OPAL: Occlusion Pattern Aware Loss for Unsupervised Light Field Disparity Estimation. (arXiv:2203.02231v3 [cs.CV] UPDATED)
128. Towards Self-Supervised Category-Level Object Pose and Size Estimation. (arXiv:2203.02884v2 [cs.CV] UPDATED)
129. A study on joint modeling and data augmentation of multi-modalities for audio-visual scene classification. (arXiv:2203.04114v2 [cs.MM] UPDATED)
130. Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment. (arXiv:2203.04121v3 [cs.CV] UPDATED)
131. SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection. (arXiv:2203.06398v3 [cs.CV] UPDATED)
132. Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs. (arXiv:2203.08516v2 [cs.CV] UPDATED)
133. Scribble-Supervised LiDAR Semantic Segmentation. (arXiv:2203.08537v2 [cs.CV] UPDATED)
134. Affective Feedback Synthesis Towards Multimodal Text and Image Data. (arXiv:2203.12692v2 [cs.MM] UPDATED)
135. Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v4 [cs.CV] UPDATED)
136. Image Compression and Actionable Intelligence With Deep Neural Networks. (arXiv:2203.13686v2 [cs.LG] UPDATED)
137. UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v2 [cs.CV] UPDATED)
138. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v2 [cs.CV] UPDATED)
139. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v3 [cs.CV] UPDATED)
140. Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v2 [cs.LG] UPDATED)
141. Proactive Image Manipulation Detection. (arXiv:2203.15880v2 [cs.CV] UPDATED)
142. Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v2 [cs.CV] UPDATED)
143. The impact of using voxel-level segmentation metrics on evaluating multifocal prostate cancer localisation. (arXiv:2203.16415v2 [eess.IV] UPDATED)
144. Fast, Accurate and Memory-Efficient Partial Permutation Synchronization. (arXiv:2203.16505v2 [cs.CV] UPDATED)
145. AdaMixer: A Fast-Converging Query-Based Object Detector. (arXiv:2203.16507v2 [cs.CV] UPDATED)
146. D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. (arXiv:2112.03028v2 [cs.RO] CROSS LISTED)
147. BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] CROSS LISTED)
## eess.IV
---
**19** new papers in eess.IV:-) 
1. COSMOS: Cross-Modality Unsupervised Domain Adaptation for 3D Medical Image Segmentation based on Target-aware Domain Translation and Iterative Self-Training. (arXiv:2203.16557v1 [eess.IV])
2. Enhancing Cancer Prediction in Challenging Screen-Detected Incident Lung Nodules Using Time-Series Deep Learning. (arXiv:2203.16606v1 [eess.IV])
3. Federated Learning for the Classification of Tumor Infiltrating Lymphocytes. (arXiv:2203.16622v1 [eess.IV])
4. Learning the Effect of Registration Hyperparameters with HyperMorph. (arXiv:2203.16680v1 [cs.CV])
5. Revisiting Document Image Dewarping by Grid Regularization. (arXiv:2203.16850v1 [eess.IV])
6. Investigating Modality Bias in Audio Visual Video Parsing. (arXiv:2203.16860v1 [cs.CV])
7. Multimodal Fusion Transformer for Remote Sensing Image Classification. (arXiv:2203.16952v1 [cs.CV])
8. Measuring hand use in the home after cervical spinal cord injury using egocentric video. (arXiv:2203.16996v1 [eess.IV])
9. Plug-and-Play Methods for Integrating Physical and Learned Models in Computational Imaging. (arXiv:2203.17061v1 [eess.IV])
10. Deep Hyperspectral Unmixing using Transformer Network. (arXiv:2203.17076v1 [cs.CV])
11. Adaptive Mean-Residue Loss for Robust Facial Age Estimation. (arXiv:2203.17156v1 [cs.CV])
12. Single-Pixel Compressive Imaging in Shift-Invariant Spaces via Exact Wavelet Frames. (arXiv:2106.00404v2 [eess.IV] UPDATED)
13. Deformable image registration with deep network priors: a study on longitudinal PET images. (arXiv:2111.11873v2 [eess.IV] UPDATED)
14. Enhancing Satellite Imagery using Deep Learning for the Sensor To Shooter Timeline. (arXiv:2203.00116v3 [cs.CV] UPDATED)
15. Contextual Attention Network: Transformer Meets U-Net. (arXiv:2203.01932v2 [eess.IV] UPDATED)
16. Temporal Context Matters: Enhancing Single Image Prediction with Disease Progression Representations. (arXiv:2203.01933v2 [eess.IV] UPDATED)
17. Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v4 [cs.CV] UPDATED)
18. Image Compression and Actionable Intelligence With Deep Neural Networks. (arXiv:2203.13686v2 [cs.LG] UPDATED)
19. The impact of using voxel-level segmentation metrics on evaluating multifocal prostate cancer localisation. (arXiv:2203.16415v2 [eess.IV] UPDATED)
## cs.LG
---
**185** new papers in cs.LG:-) 
1. Parallel framework for Dynamic Domain Decomposition of Data Assimilation problems a case study on Kalman Filter algorithm. (arXiv:2203.16535v1 [cs.LG])
2. Recent improvements of ASR models in the face of adversarial attacks. (arXiv:2203.16536v1 [cs.CR])
3. Efficient Localness Transformer for Smart Sensor-Based Energy Disaggregation. (arXiv:2203.16537v1 [cs.LG])
4. Machine Learning Approaches for Non-Intrusive Home Absence Detection Based on Appliance Electrical Use. (arXiv:2203.16538v1 [cs.LG])
5. Identification of diffracted vortex beams at different propagation distances using deep learning. (arXiv:2203.16539v1 [cs.LG])
6. Mind the gap: Challenges of deep learning approaches to Theory of Mind. (arXiv:2203.16540v1 [cs.LG])
7. Generating Scientific Articles with Machine Learning. (arXiv:2203.16569v1 [cs.LG])
8. Graph Refinement for Coreference Resolution. (arXiv:2203.16574v1 [cs.CL])
9. Calibrating constitutive models with full-field data via physics informed neural networks. (arXiv:2203.16577v1 [cs.LG])
10. Factored Adaptation for Non-Stationary Reinforcement Learning. (arXiv:2203.16582v1 [cs.LG])
11. Spatially Adaptive Online Prediction of Piecewise Regular Functions. (arXiv:2203.16587v1 [math.ST])
12. Constrained Few-shot Class-incremental Learning. (arXiv:2203.16588v1 [cs.CV])
13. A Fast and Convergent Proximal Algorithm for Regularized Nonconvex and Nonsmooth Bi-level Optimization. (arXiv:2203.16615v1 [cs.LG])
14. Federated Learning for the Classification of Tumor Infiltrating Lymphocytes. (arXiv:2203.16622v1 [eess.IV])
15. Physics-constrained Unsupervised Learning of Partial Differential Equations using Meshes. (arXiv:2203.16628v1 [cs.LG])
16. Transformer Language Models without Positional Encodings Still Learn Positional Information. (arXiv:2203.16634v1 [cs.CL])
17. Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load. (arXiv:2203.16637v1 [cs.SD])
18. FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations. (arXiv:2203.16639v1 [cs.CV])
19. Data-driven Prediction of Relevant Scenarios for Robust Optimization. (arXiv:2203.16642v1 [math.OC])
20. Generation of Speaker Representations Using Heterogeneous Training Batch Assembly. (arXiv:2203.16646v1 [cs.SD])
21. Predicting Winners of the Reality TV Dating Show $\textit{The Bachelor}$ Using Machine Learning Algorithms. (arXiv:2203.16648v1 [cs.LG])
22. Challenges in leveraging GANs for few-shot data augmentation. (arXiv:2203.16662v1 [stat.ML])
23. Flexible and Efficient Contextual Bandits with Heterogeneous Treatment Effect Oracle. (arXiv:2203.16668v1 [cs.LG])
24. System Identification via Nuclear Norm Regularization. (arXiv:2203.16673v1 [stat.ML])
25. Learning the Effect of Registration Hyperparameters with HyperMorph. (arXiv:2203.16680v1 [cs.CV])
26. To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo. (arXiv:2203.16682v1 [cs.CV])
27. Active Learning for Computationally Efficient Distribution of Binary Evolution Simulations. (arXiv:2203.16683v1 [astro-ph.SR])
28. Quasi-orthogonality and intrinsic dimensions as measures of learning and generalisation. (arXiv:2203.16687v1 [cs.LG])
29. MAE-AST: Masked Autoencoding Audio Spectrogram Transformer. (arXiv:2203.16691v1 [eess.AS])
30. Towards Differential Relational Privacy and its use in Question Answering. (arXiv:2203.16701v1 [cs.LG])
31. Monte Carlo Tree Search based Hybrid Optimization of Variational Quantum Circuits. (arXiv:2203.16707v1 [quant-ph])
32. Task Adaptive Parameter Sharing for Multi-Task Learning. (arXiv:2203.16708v1 [cs.LG])
33. An analytic theory for the dynamics of wide quantum neural networks. (arXiv:2203.16711v1 [quant-ph])
34. Exploiting Explainable Metrics for Augmented SGD. (arXiv:2203.16723v1 [cs.LG])
35. SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping. (arXiv:2203.16749v1 [eess.AS])
36. An unsupervised cluster-level based method for learning node representations of heterogeneous graphs in scientific papers. (arXiv:2203.16751v1 [cs.LG])
37. An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks. (arXiv:2203.16773v1 [eess.AS])
38. Bangla hate speech detection on social media using attention-based recurrent neural network. (arXiv:2203.16775v1 [cs.CL])
39. An Empirical Study of Language Model Integration for Transducer based Speech Recognition. (arXiv:2203.16776v1 [eess.AS])
40. Mask Atari for Deep Reinforcement Learning as POMDP Benchmarks. (arXiv:2203.16777v1 [cs.CV])
41. ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices. (arXiv:2203.16788v1 [cs.CL])
42. When Physics Meets Machine Learning: A Survey of Physics-Informed Machine Learning. (arXiv:2203.16797v1 [cs.LG])
43. Ternary and Binary Quantization for Improved Classification. (arXiv:2203.16798v1 [cs.CV])
44. Robust Meta-Reinforcement Learning with Curriculum-Based Task Sampling. (arXiv:2203.16801v1 [cs.LG])
45. Adaptive Estimation of Random Vectors with Bandit Feedback. (arXiv:2203.16810v1 [cs.LG])
46. How Does Pre-trained Wav2Vec2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications. (arXiv:2203.16822v1 [eess.AS])
47. Towards Driving-Oriented Metric for Lane Detection Models. (arXiv:2203.16851v1 [cs.CV])
48. JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech. (arXiv:2203.16852v1 [eess.AS])
49. Ransomware Detection using Process Memory. (arXiv:2203.16871v1 [cs.CR])
50. An Optimal Control Method to Compute the Most Likely Transition Path for Stochastic Dynamical Systems with Jumps. (arXiv:2203.16874v1 [math.NA])
51. Mutual information estimation for graph convolutional neural networks. (arXiv:2203.16887v1 [cs.LG])
52. A survey of neural models for the automatic analysis of conversation: Towards a better integration of the social sciences. (arXiv:2203.16891v1 [cs.CL])
53. MBORE: Multi-objective Bayesian Optimisation by Density-Ratio Estimation. (arXiv:2203.16912v1 [cs.LG])
54. Assessing the risk of re-identification arising from an attack on anonymised data. (arXiv:2203.16921v1 [cs.LG])
55. Neural Architecture Search for Speech Emotion Recognition. (arXiv:2203.16928v1 [cs.SD])
56. WavThruVec: Latent speech representation as intermediate features for neural speech synthesis. (arXiv:2203.16930v1 [cs.SD])
57. Learning from few examples with nonlinear feature maps. (arXiv:2203.16935v1 [cs.LG])
58. HiFi-VC: High Quality ASR-Based Voice Conversion. (arXiv:2203.16937v1 [cs.SD])
59. Hypergraph Convolutional Networks via Equivalency between Hypergraphs and Undirected Graphs. (arXiv:2203.16939v1 [cs.LG])
60. Direction of Arrival Estimation of Sound Sources Using Icosahedral CNNs. (arXiv:2203.16940v1 [eess.AS])
61. The ideal data compression and automatic discovery of hidden law using neural network. (arXiv:2203.16941v1 [cs.LG])
62. A data-driven approach for the closure of RANS models by the divergence of the Reynolds Stress Tensor. (arXiv:2203.16944v1 [physics.flu-dyn])
63. Multimodal Fusion Transformer for Remote Sensing Image Classification. (arXiv:2203.16952v1 [cs.CV])
64. PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v1 [cs.CL])
65. Acoustic-Net: A Novel Neural Network for Sound Localization and Quantification. (arXiv:2203.16988v1 [cs.SD])
66. Message Passing Neural Networks for Hypergraphs. (arXiv:2203.16995v1 [cs.LG])
67. SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy. (arXiv:2203.17001v1 [eess.AS])
68. Conditional Autoregressors are Interpretable Classifiers. (arXiv:2203.17002v1 [cs.LG])
69. Equivariant Diffusion for Molecule Generation in 3D. (arXiv:2203.17003v1 [cs.LG])
70. Speech **Enhancement** with Score-Based Generative Models in the Complex STFT Domain. (arXiv:2203.17004v1 [eess.AS])
71. It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher. (arXiv:2203.17008v1 [cs.CV])
72. A Temporal-oriented Broadcast ResNet for COVID-19 Detection. (arXiv:2203.17012v1 [cs.SD])
73. DeepFry: Identifying Vocal Fry Using Deep Neural Networks. (arXiv:2203.17019v1 [eess.AS])
74. CTA-RNN: Channel and Temporal-wise Attention RNN Leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition. (arXiv:2203.17023v1 [cs.SD])
75. Flat-topped Probability Density Functions for Mixture Models. (arXiv:2203.17027v1 [cs.LG])
76. Differentially Private Federated Learning via Reconfigurable Intelligent Surface. (arXiv:2203.17028v1 [eess.SP])
77. Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks. (arXiv:2203.17030v1 [cs.CV])
78. Training strategy for a lightweight countermeasure model for automatic speaker verification. (arXiv:2203.17031v1 [cs.SD])
79. Certified machine learning: A posteriori error estimation for physics-informed neural networks. (arXiv:2203.17055v1 [cs.LG])
80. Wind Farm Layout Optimisation using Set Based Multi-objective Bayesian Optimisation. (arXiv:2203.17065v1 [stat.ML])
81. Cross-modal Learning of Graph Representations using Radar Point Cloud for Long-Range Gesture Recognition. (arXiv:2203.17066v1 [eess.SP])
82. Traffic4cast at NeurIPS 2021 - Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes. (arXiv:2203.17070v1 [cs.LG])
83. Interpretation of Black Box NLP Models: A Survey. (arXiv:2203.17081v1 [cs.LG])
84. RobIn: A Robust Interpretable Deep Network for Schizophrenia Diagnosis. (arXiv:2203.17085v1 [cs.LG])
85. Quantum-Aided Meta-Learning for Bayesian Binary Neural Networks via Born Machines. (arXiv:2203.17089v1 [quant-ph])
86. Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data. (arXiv:2203.17113v1 [cs.SD])
87. Doubly-Robust Estimation for Unbiased Learning-to-Rank from Position-Biased Click Feedback. (arXiv:2203.17118v1 [cs.LG])
88. Neural Q-learning for solving elliptic PDEs. (arXiv:2203.17128v1 [math.NA])
89. Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors. (arXiv:2203.17138v1 [cs.RO])
90. Online Learning for Traffic Routing under Unknown Preferences. (arXiv:2203.17150v1 [cs.LG])
91. Predicting extreme events from data using deep machine learning: when and where. (arXiv:2203.17155v1 [cs.LG])
92. Preventing Over-Smoothing for Hypergraph Neural Networks. (arXiv:2203.17159v1 [cs.LG])
93. Recovering models of open quantum systems from data via polynomial optimization: Towards globally convergent quantum system identification. (arXiv:2203.17164v1 [quant-ph])
94. Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$. (arXiv:2203.17189v1 [cs.LG])
95. Learning from many trajectories. (arXiv:2203.17193v1 [cs.LG])
96. CatIss: An Intelligent Tool for Categorizing Issues Reports using Transformers. (arXiv:2203.17196v1 [cs.SE])
97. Adversarial Examples in Random Neural Networks with General Activations. (arXiv:2203.17209v1 [cs.LG])
98. Improved Relation Networks for End-to-End Speaker Verification and Identification. (arXiv:2203.17218v1 [eess.AS])
99. A Derivation of Nesterov's Accelerated Gradient Algorithm from Optimal Control Theory. (arXiv:2203.17226v1 [math.OC])
100. Performative Power. (arXiv:2203.17232v1 [cs.LG])
101. Bayesian optimization with known experimental and design constraints for chemistry applications. (arXiv:2203.17241v1 [math.OC])
102. Automatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities. (arXiv:2203.17242v1 [cs.SD])
103. VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v1 [cs.CV])
104. Dual Temperature Helps Contrastive Learning Without Many Negative Samples: Towards Understanding and Simplifying MoCo. (arXiv:2203.17248v1 [cs.LG])
105. Generation and Simulation of Synthetic Datasets with Copulas. (arXiv:2203.17250v1 [cs.LG])
106. Continuous Scene Representations for Embodied AI. (arXiv:2203.17251v1 [cs.CV])
107. LEAD1.0: A Large-scale Annotated Dataset for Energy Anomaly Detection in Commercial Buildings. (arXiv:2203.17256v1 [cs.LG])
108. Generating High Fidelity Data from Low-density Regions using Diffusion Models. (arXiv:2203.17260v1 [cs.CV])
109. R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])
110. Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech **Enhancement** by Re-Synthesis. (arXiv:2203.17263v1 [cs.CV])
111. A 23 MW data centre is all you need. (arXiv:2203.17265v1 [cs.LG])
112. TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing. (arXiv:2203.17266v1 [cs.CV])
113. A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v1 [cs.LG])
114. MyStyle: A Personalized Generative Prior. (arXiv:2203.17272v1 [cs.CV])
115. DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools. (arXiv:2203.17275v1 [cs.LG])
116. Graph Node-Feature Convolution for Representation Learning. (arXiv:1812.00086v2 [cs.LG] UPDATED)
117. Model Agnostic Defence against Backdoor Attacks in Machine Learning. (arXiv:1908.02203v3 [cs.LG] UPDATED)
118. Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v4 [cs.LG] UPDATED)
119. Causal Feature Selection for Algorithmic Fairness. (arXiv:2006.06053v2 [cs.LG] UPDATED)
120. A Unifying Framework for Reinforcement Learning and Planning. (arXiv:2006.15009v4 [cs.LG] UPDATED)
121. Model-based Reinforcement Learning: A Survey. (arXiv:2006.16712v4 [cs.LG] UPDATED)
122. Machine learning fairness notions: Bridging the gap with real-world applications. (arXiv:2006.16745v4 [cs.LG] UPDATED)
123. Instance Weighted Incremental Evolution Strategies for Reinforcement Learning in Dynamic Environments. (arXiv:2010.04605v2 [cs.LG] UPDATED)
124. Using IPA-Based Tacotron for Data Efficient Cross-Lingual Speaker Adaptation and Pronunciation **Enhancement**. (arXiv:2011.06392v2 [cs.SD] UPDATED)
125. Deep Reinforcement Learning for Resource Constrained Multiclass Scheduling in Wireless Networks. (arXiv:2011.13634v3 [cs.LG] UPDATED)
126. A Single-Timescale Method for Stochastic Bilevel Optimization. (arXiv:2102.04671v4 [math.OC] UPDATED)
127. A statistical framework for efficient out of distribution detection in deep neural networks. (arXiv:2102.12967v3 [cs.LG] UPDATED)
128. Continual Speaker Adaptation for Text-to-Speech Synthesis. (arXiv:2103.14512v2 [cs.CL] UPDATED)
129. DINE: Domain Adaptation from Single and Multiple Black-box Predictors. (arXiv:2104.01539v3 [cs.CV] UPDATED)
130. Muesli: Combining Improvements in Policy Optimization. (arXiv:2104.06159v2 [cs.LG] UPDATED)
131. Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart. (arXiv:2105.14785v4 [cs.LG] UPDATED)
132. OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization. (arXiv:2106.03721v3 [cs.LG] UPDATED)
133. Generative Flows with Invertible Attentions. (arXiv:2106.03959v4 [cs.LG] UPDATED)
134. DiGS : Divergence guided shape **implicit neural representation** for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)
135. Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v2 [cs.CR] UPDATED)
136. Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v2 [cs.CL] UPDATED)
137. Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization. (arXiv:2107.12438v3 [math.OC] UPDATED)
138. The paradox of the compositionality of natural language: a neural machine translation case study. (arXiv:2108.05885v2 [cs.CL] UPDATED)
139. FBDNN: Filter Banks and Deep Neural Networks for Portable and Fast Brain-Computer Interfaces. (arXiv:2109.02165v4 [eess.SP] UPDATED)
140. Rerunning OCR: A Machine Learning Approach to Quality Assessment and **Enhancement** Prediction. (arXiv:2110.01661v4 [cs.CL] UPDATED)
141. DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge Computing. (arXiv:2110.01863v2 [cs.NI] UPDATED)
142. When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?. (arXiv:2110.04184v2 [cs.LG] UPDATED)
143. Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v3 [cs.CR] UPDATED)
144. BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications. (arXiv:2110.05781v2 [eess.AS] UPDATED)
145. Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v5 [cs.LG] UPDATED)
146. Recommender Systems meet Mechanism Design. (arXiv:2110.12558v2 [cs.GT] UPDATED)
147. TUNet: A Block-online Bandwidth Extension Model based on Transformers and Self-supervised Pretraining. (arXiv:2110.13492v4 [cs.LG] UPDATED)
148. Sequence Transduction with Graph-based Supervision. (arXiv:2111.01272v2 [cs.CL] UPDATED)
149. Data-driven Set-based Estimation of Polynomial Systems with Application to SIR Epidemics. (arXiv:2111.04704v2 [eess.SY] UPDATED)
150. Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning. (arXiv:2111.14213v2 [cs.LG] UPDATED)
151. Schema matching using Gaussian mixture models with Wasserstein distance. (arXiv:2111.14244v2 [cs.LG] UPDATED)
152. GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm and Accelerator Co-Design. (arXiv:2112.11594v2 [cs.AR] UPDATED)
153. SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v3 [cs.CV] UPDATED)
154. Continual Learning for Unsupervised Anomaly Detection in Continuous Auditing of Financial Accounting Data. (arXiv:2112.13215v2 [cs.LG] UPDATED)
155. Omnivore: A Single Model for Many Visual Modalities. (arXiv:2201.08377v2 [cs.CV] UPDATED)
156. GoSafeOpt: Scalable Safe Exploration for Global Optimization of Dynamical Systems. (arXiv:2201.09562v3 [cs.LG] UPDATED)
157. Reinforcement Learning Based Query Vertex Ordering Model for Subgraph Matching. (arXiv:2201.11251v2 [cs.LG] UPDATED)
158. Stability and Generalization Capabilities of Message Passing Graph Neural Networks. (arXiv:2202.00645v3 [cs.LG] UPDATED)
159. D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v3 [cs.CV] UPDATED)
160. Weighted Programming. (arXiv:2202.07577v2 [cs.PL] UPDATED)
161. Enhancing Satellite Imagery using Deep Learning for the Sensor To Shooter Timeline. (arXiv:2203.00116v3 [cs.CV] UPDATED)
162. Contextual Attention Network: Transformer Meets U-Net. (arXiv:2203.01932v2 [eess.IV] UPDATED)
163. Learning from humans: combining imitation and deep reinforcement learning to accomplish human-level performance on a virtual foraging task. (arXiv:2203.06250v3 [cs.LG] UPDATED)
164. STICC: A multivariate spatial clustering method for repeated geographic pattern discovery with consideration of spatial contiguity. (arXiv:2203.09611v2 [cs.LG] UPDATED)
165. Speaker Embedding-aware Neural Diarization: an Efficient Framework for Overlapping Speech Diarization in Meeting Scenarios. (arXiv:2203.09767v2 [cs.SD] UPDATED)
166. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v2 [cs.IR] UPDATED)
167. Image Compression and Actionable Intelligence With Deep Neural Networks. (arXiv:2203.13686v2 [cs.LG] UPDATED)
168. Optimisation-free Classification and Density Estimation with Quantum Circuits. (arXiv:2203.14452v2 [quant-ph] UPDATED)
169. UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v2 [cs.CV] UPDATED)
170. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v2 [cs.CV] UPDATED)
171. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v3 [cs.CV] UPDATED)
172. Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v2 [cs.LG] UPDATED)
173. Radial Autoencoders for Enhanced Anomaly Detection. (arXiv:2203.15884v2 [cs.LG] UPDATED)
174. Graph Neural Networks in IoT: A Survey. (arXiv:2203.15935v2 [cs.LG] UPDATED)
175. Longitudinal Fairness with Censorship. (arXiv:2203.16024v2 [cs.LG] UPDATED)
176. Does Audio Deepfake Detection Generalize?. (arXiv:2203.16263v2 [cs.SD] UPDATED)
177. Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v2 [cs.CV] UPDATED)
178. TraHGR: Transformer for Hand Gesture Recognition via ElectroMyography. (arXiv:2203.16336v2 [eess.SP] UPDATED)
179. Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v2 [cs.CL] UPDATED)
180. Intelligent Icing Detection Model of Wind Turbine Blades Based on SCADA data. (arXiv:2101.07914v1 [cs.LG] CROSS LISTED)
181. Application of Ghost-DeblurGAN to Fiducial Marker Detection. (arXiv:2109.03379v3 [eess.IV] CROSS LISTED)
182. D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. (arXiv:2112.03028v2 [cs.RO] CROSS LISTED)
183. BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] CROSS LISTED)
184. An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese Codex. (arXiv:2203.15173v1 [cs.CL] CROSS LISTED)
185. Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v1 [eess.AS] CROSS LISTED)
## cs.AI
---
**89** new papers in cs.AI:-) 
1. Recent improvements of ASR models in the face of adversarial attacks. (arXiv:2203.16536v1 [cs.CR])
2. Efficient Localness Transformer for Smart Sensor-Based Energy Disaggregation. (arXiv:2203.16537v1 [cs.LG])
3. Machine Learning Approaches for Non-Intrusive Home Absence Detection Based on Appliance Electrical Use. (arXiv:2203.16538v1 [cs.LG])
4. Mind the gap: Challenges of deep learning approaches to Theory of Mind. (arXiv:2203.16540v1 [cs.LG])
5. Factored Adaptation for Non-Stationary Reinforcement Learning. (arXiv:2203.16582v1 [cs.LG])
6. Learning Local Displacements for Point Cloud Completion. (arXiv:2203.16600v1 [cs.CV])
7. Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v1 [cs.AI])
8. Transformer Language Models without Positional Encodings Still Learn Positional Information. (arXiv:2203.16634v1 [cs.CL])
9. Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load. (arXiv:2203.16637v1 [cs.SD])
10. FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations. (arXiv:2203.16639v1 [cs.CV])
11. Generation of Speaker Representations Using Heterogeneous Training Batch Assembly. (arXiv:2203.16646v1 [cs.SD])
12. MAE-AST: Masked Autoencoding Audio Spectrogram Transformer. (arXiv:2203.16691v1 [eess.AS])
13. Robust Disentangled Variational Speech Representation Learning for Zero-shot Voice Conversion. (arXiv:2203.16705v1 [eess.AS])
14. An analytic theory for the dynamics of wide quantum neural networks. (arXiv:2203.16711v1 [quant-ph])
15. An unsupervised cluster-level based method for learning node representations of heterogeneous graphs in scientific papers. (arXiv:2203.16751v1 [cs.LG])
16. ReSTR: Convolution-free Referring Image Segmentation Using Transformers. (arXiv:2203.16768v1 [cs.CV])
17. Learning Decoupling Features Through Orthogonality Regularization. (arXiv:2203.16772v1 [cs.SD])
18. Bangla hate speech detection on social media using attention-based recurrent neural network. (arXiv:2203.16775v1 [cs.CL])
19. Mask Atari for Deep Reinforcement Learning as POMDP Benchmarks. (arXiv:2203.16777v1 [cs.CV])
20. TrajGen: Generating Realistic and Diverse Trajectories with Reactive and Feasible Agent Behaviors for Autonomous Driving. (arXiv:2203.16792v1 [cs.RO])
21. Rethinking Portrait Matting with Privacy Preserving. (arXiv:2203.16828v1 [cs.CV])
22. Ransomware Detection using Process Memory. (arXiv:2203.16871v1 [cs.CR])
23. MPS-**NeRF**: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v1 [cs.CV])
24. A survey of neural models for the automatic analysis of conversation: Towards a better integration of the social sciences. (arXiv:2203.16891v1 [cs.CL])
25. MBORE: Multi-objective Bayesian Optimisation by Density-Ratio Estimation. (arXiv:2203.16912v1 [cs.LG])
26. Human Instance Segmentation and Tracking via Data Association and Single-stage Detector. (arXiv:2203.16966v1 [cs.CV])
27. IITD-DBAI: Multi-Stage Retrieval with Pseudo-Relevance Feedback and Query Reformulation. (arXiv:2203.17042v1 [cs.IR])
28. Cross-modal Learning of Graph Representations using Radar Point Cloud for Long-Range Gesture Recognition. (arXiv:2203.17066v1 [eess.SP])
29. Scientific and Technological Text Knowledge Extraction Method of based on Word Mixing and GRU. (arXiv:2203.17079v1 [cs.CL])
30. Interpretation of Black Box NLP Models: A Survey. (arXiv:2203.17081v1 [cs.LG])
31. A Rich Recipe Representation as Plan to Support Expressive Multi Modal Queries on Recipe Content and Preparation Process. (arXiv:2203.17109v1 [cs.AI])
32. Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors. (arXiv:2203.17138v1 [cs.RO])
33. Adaptive Mean-Residue Loss for Robust Facial Age Estimation. (arXiv:2203.17156v1 [cs.CV])
34. 3D Equivariant Graph Implicit Functions. (arXiv:2203.17178v1 [cs.CV])
35. VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v1 [cs.CV])
36. Dual Temperature Helps Contrastive Learning Without Many Negative Samples: Towards Understanding and Simplifying MoCo. (arXiv:2203.17248v1 [cs.LG])
37. Generation and Simulation of Synthetic Datasets with Copulas. (arXiv:2203.17250v1 [cs.LG])
38. Continuous Scene Representations for Embodied AI. (arXiv:2203.17251v1 [cs.CV])
39. LEAD1.0: A Large-scale Annotated Dataset for Energy Anomaly Detection in Commercial Buildings. (arXiv:2203.17256v1 [cs.LG])
40. R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])
41. A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v1 [cs.LG])
42. Do Vision-Language Pretrained Models Learn Primitive Concepts?. (arXiv:2203.17271v1 [cs.CV])
43. Modelling Value-oriented Legal Reasoning in LogiKEy. (arXiv:2006.12789v5 [cs.AI] UPDATED)
44. A Unifying Framework for Reinforcement Learning and Planning. (arXiv:2006.15009v4 [cs.LG] UPDATED)
45. Model-based Reinforcement Learning: A Survey. (arXiv:2006.16712v4 [cs.LG] UPDATED)
46. Machine learning fairness notions: Bridging the gap with real-world applications. (arXiv:2006.16745v4 [cs.LG] UPDATED)
47. Instance Weighted Incremental Evolution Strategies for Reinforcement Learning in Dynamic Environments. (arXiv:2010.04605v2 [cs.LG] UPDATED)
48. Reinforcement Learning with Dual-Observation for General Video Game Playing. (arXiv:2011.05622v4 [cs.AI] UPDATED)
49. Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v5 [cs.AI] UPDATED)
50. DINE: Domain Adaptation from Single and Multiple Black-box Predictors. (arXiv:2104.01539v3 [cs.CV] UPDATED)
51. Muesli: Combining Improvements in Policy Optimization. (arXiv:2104.06159v2 [cs.LG] UPDATED)
52. Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection. (arXiv:2106.04564v2 [cs.CL] UPDATED)
53. Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v3 [cs.CV] UPDATED)
54. DiGS : Divergence guided shape **implicit neural representation** for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)
55. Local Structure Matters Most: Perturbation Study in NLU. (arXiv:2107.13955v2 [cs.CL] UPDATED)
56. The paradox of the compositionality of natural language: a neural machine translation case study. (arXiv:2108.05885v2 [cs.CL] UPDATED)
57. FBDNN: Filter Banks and Deep Neural Networks for Portable and Fast Brain-Computer Interfaces. (arXiv:2109.02165v4 [eess.SP] UPDATED)
58. Rerunning OCR: A Machine Learning Approach to Quality Assessment and **Enhancement** Prediction. (arXiv:2110.01661v4 [cs.CL] UPDATED)
59. Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v3 [cs.CR] UPDATED)
60. Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v5 [cs.LG] UPDATED)
61. BI-RADS BERT & Using Section Segmentation to Understand Radiology Reports. (arXiv:2110.07552v2 [cs.CL] UPDATED)
62. Multi-Agent Advisor Q-Learning. (arXiv:2111.00345v4 [cs.AI] UPDATED)
63. ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic. (arXiv:2111.14447v2 [cs.CV] UPDATED)
64. Marvin: an Innovative Omni-Directional Robotic Assistant for Domestic Environments. (arXiv:2112.05597v2 [cs.RO] UPDATED)
65. Learning to acquire novel cognitive tasks with evolution, plasticity and meta-meta-learning. (arXiv:2112.08588v4 [cs.NE] UPDATED)
66. C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v2 [cs.CL] UPDATED)
67. Omnivore: A Single Model for Many Visual Modalities. (arXiv:2201.08377v2 [cs.CV] UPDATED)
68. Reinforcement Learning Based Query Vertex Ordering Model for Subgraph Matching. (arXiv:2201.11251v2 [cs.LG] UPDATED)
69. Stability and Generalization Capabilities of Message Passing Graph Neural Networks. (arXiv:2202.00645v3 [cs.LG] UPDATED)
70. Prompt-Learning for Short Text Classification. (arXiv:2202.11345v2 [cs.CL] UPDATED)
71. ParaNames: A Massively Multilingual Entity Name Corpus. (arXiv:2202.14035v2 [cs.CL] UPDATED)
72. Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v2 [cs.CV] UPDATED)
73. Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking. (arXiv:2203.03123v2 [cs.CL] UPDATED)
74. A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v4 [cs.CL] UPDATED)
75. STICC: A multivariate spatial clustering method for repeated geographic pattern discovery with consideration of spatial contiguity. (arXiv:2203.09611v2 [cs.LG] UPDATED)
76. Converse: A Tree-Based Modular Task-Oriented Dialogue System. (arXiv:2203.12187v2 [cs.CL] UPDATED)
77. Asynchronous Reinforcement Learning for Real-Time Control of Physical Robots. (arXiv:2203.12759v3 [cs.RO] UPDATED)
78. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v2 [cs.IR] UPDATED)
79. Accelerating Code Search with Deep Hashing and Code Classification. (arXiv:2203.15287v2 [cs.SE] UPDATED)
80. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v2 [cs.CV] UPDATED)
81. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v3 [cs.CV] UPDATED)
82. Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v2 [cs.LG] UPDATED)
83. Synthesis and Execution of Communicative Robotic Movements with Generative Adversarial Networks. (arXiv:2203.15640v2 [cs.RO] UPDATED)
84. Radial Autoencoders for Enhanced Anomaly Detection. (arXiv:2203.15884v2 [cs.LG] UPDATED)
85. Longitudinal Fairness with Censorship. (arXiv:2203.16024v2 [cs.LG] UPDATED)
86. Intelligent Icing Detection Model of Wind Turbine Blades Based on SCADA data. (arXiv:2101.07914v1 [cs.LG] CROSS LISTED)
87. Application of Ghost-DeblurGAN to Fiducial Marker Detection. (arXiv:2109.03379v3 [eess.IV] CROSS LISTED)
88. BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] CROSS LISTED)
89. An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese Codex. (arXiv:2203.15173v1 [cs.CL] CROSS LISTED)

