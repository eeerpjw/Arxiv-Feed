# Your interest papers
---
## cs.CV
---
### Seeing Far in the **Dark** with Patterned Flash. (arXiv:2207.12570v1 [eess.IV])
- Authors : Zhanghao Sun, Jian Wang, Yicheng Wu, Shree Nayar
- Link : [http://arxiv.org/abs/2207.12570](http://arxiv.org/abs/2207.12570)
> ABSTRACT  :  Flash illumination is widely used in imaging under **low-light** environments. However, illumination intensity falls off with propagation distance quadratically, which poses significant challenges for flash imaging at a long distance. We propose a new flash technique, named ``patterned flash'', for flash imaging at a long distance. Patterned flash concentrates optical power into a dot array. Compared with the conventional uniform flash where the signal is overwhelmed by the noise everywhere, patterned flash provides stronger signals at sparsely distributed points across the field of view to ensure the signals at those points stand out from the sensor noise. This enables post-processing to resolve important objects and details. Additionally, the patterned flash projects texture onto the scene, which can be treated as a structured light system for depth perception. Given the novel system, we develop a joint image reconstruction and depth estimation algorithm with a convolutional neural network. We build a hardware prototype and test the proposed flash technique on various scenes. The experimental results demonstrate that our patterned flash has significantly better performance at long distances in **low-light** environments.  
### Compiler-Aware Neural Architecture Search for On-Mobile **Real-time** Super-Resolution. (arXiv:2207.12577v1 [cs.CV])
- Authors : Yushu Wu, Yifan Gong, Pu Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, Yanzhi Wang
- Link : [http://arxiv.org/abs/2207.12577](http://arxiv.org/abs/2207.12577)
> ABSTRACT  :  Deep learning-based super-resolution (SR) has gained tremendous popularity in recent years because of its high image quality performance and wide application scenarios. However, prior methods typically suffer from large amounts of computations and huge power consumption, causing difficulties for real-time inference, especially on resource-limited platforms such as mobile devices. To mitigate this, we propose a compiler-aware SR neural architecture search (NAS) framework that conducts depth search and per-layer width search with adaptive SR blocks. The inference speed is directly taken into the optimization along with the SR loss to derive SR models with high image quality while satisfying the real-time inference requirement. Instead of measuring the speed on mobile devices at each iteration during the search process, a speed model incorporated with compiler optimizations is leveraged to predict the inference latency of the SR block with various width configurations for faster convergence. With the proposed framework, we achieve real-time SR inference for implementing 720p resolution with competitive SR performance (in terms of PSNR and SSIM) on GPU/DSP of mobile platforms (Samsung Galaxy S21).  
### Large-displacement 3D Object Tracking with Hybrid Non-local Optimization. (arXiv:2207.12620v1 [cs.CV])
- Authors : Xuhui Tian, Xinran Lin, Fan Zhong, Xueying Qin
- Link : [http://arxiv.org/abs/2207.12620](http://arxiv.org/abs/2207.12620)
> ABSTRACT  :  Optimization-based 3D object tracking is known to be precise and fast, but sensitive to large inter-frame displacements. In this paper we propose a fast and effective non-local 3D tracking method. Based on the observation that erroneous local minimum are mostly due to the out-of-plane rotation, we propose a hybrid approach combining non-local and local optimizations for different parameters, resulting in efficient non-local search in the 6D pose space. In addition, a precomputed robust contour-based tracking method is proposed for the pose optimization. By using long search lines with multiple candidate correspondences, it can adapt to different frame displacements without the need of coarse-to-fine search. After the pre-computation, pose updates can be conducted very fast, enabling the non-local optimization to run in **real time**. Our method outperforms all previous methods for both small and large displacements. For large displacements, the accuracy is greatly improved ($81.7\% \;\text{v.s.}\; 19.4\%$). At the same time, real-time speed ($&gt;$50fps) can be achieved with only CPU. The source code is available at \url{https://github.com/cvbubbles/nonlocal-3dtracking}.  
### $\textbf{P$^2$A}$: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos. (arXiv:2207.12730v1 [cs.CV])
- Authors : Jiang Bian, Qingzhong Wang, Haoyi Xiong, Jun Huang, Chen Liu, Xuhong Li, Jun Cheng, Jun Zhao, Feixiang Lu, Dejing Dou
- Link : [http://arxiv.org/abs/2207.12730](http://arxiv.org/abs/2207.12730)
> ABSTRACT  :  While deep learning has been widely used for video analytics, such as video classification and action detection, dense action detection with fast-moving subjects from sports videos is still challenging. In this work, we release yet another sports video dataset $\textbf{P$^2$A}$ for $\underline{P}$ing $\underline{P}$ong-$\underline{A}$ction detection, which consists of 2,721 video clips collected from the broadcasting videos of professional table tennis matches in World Table Tennis Championships and Olympiads. We work with a crew of table tennis professionals and referees to obtain fine-grained action labels (in 14 classes) for every ping-pong action that appeared in the dataset and formulate two sets of action detection problems - action localization and action recognition. We evaluate a number of commonly-seen action recognition (e.g., TSM, TSN, Video **Swin**Transformer, and Slowfast) and action localization models (e.g., BSN, BSN++, BMN, TCANet), using $\textbf{P$^2$A}$ for both problems, under various settings. These models can only achieve 48% area under the AR-AN curve for localization and 82% top-one accuracy for recognition since the ping-pong actions are dense with fast-moving subjects but broadcasting videos are with only 25 FPS. The results confirm that $\textbf{P$^2$A}$ is still a challenging task and can be used as a benchmark for action detection from videos.  
### A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation. (arXiv:2207.12770v1 [eess.IV])
- Authors : Javier Civit, Francisco Luna, Jose Maria, Rodriguez Corral, Manuel Dominguez, Arturo Morgado, Anton Civit
- Link : [http://arxiv.org/abs/2207.12770](http://arxiv.org/abs/2207.12770)
> ABSTRACT  :  Medical image segmentation can be implemented using Deep Learning methods with fast and efficient segmentation networks. Single-board computers (SBCs) are difficult to use to train deep networks due to their memory and processing limitations. Specific hardware such as Google's Edge TPU makes them suitable for **real time** predictions using complex pre-trained networks. In this work, we study the performance of two SBCs, with and without hardware acceleration for fundus image segmentation, though the conclusions of this study can be applied to the segmentation by deep neural networks of other types of medical images. To test the benefits of hardware acceleration, we use networks and datasets from a previous published work and generalize them by testing with a dataset with ultrasound thyroid images. We measure prediction times in both SBCs and compare them with a cloud based TPU system. The results show the feasibility of Machine Learning accelerated SBCs for optic disc and cup segmentation obtaining times below 25 milliseconds per image using Edge TPUs.  
### Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation. (arXiv:2207.12817v1 [cs.CV])
- Authors : Michal Balazia, kos Levente, August von, ois Br
- Link : [http://arxiv.org/abs/2207.12817](http://arxiv.org/abs/2207.12817)
> ABSTRACT  :  Body language is an eye-catching social signal and its automatic analysis can significantly advance artificial intelligence systems to understand and actively participate in social interactions. While computer vision has made impressive progress in low-level tasks like head and body pose estimation, the detection of more subtle behaviors such as gesturing, grooming, or fumbling is not well explored. In this paper we present BBSI, the first set of annotations of complex Bodily Behaviors embedded in continuous Social Interactions in a group setting. Based on previous work in psychology, we manually annotated 26 hours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15 distinct body language classes. We present comprehensive descriptive statistics on the resulting dataset as well as results of annotation quality evaluations. For automatic detection of these behaviors, we adapt the Pyramid Dilated Attention Network (PDAN), a state-of-the-art approach for human action detection. We perform experiments using four variants of spatial-temporal features as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment Networks, Temporal Shift Module and **Swin** Transformer. Results are promising and indicate a great room for improvement in this difficult task. Representing a key piece in the puzzle towards automatic understanding of social behavior, BBSI is fully available to the research community.  
### Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation. (arXiv:2207.12964v1 [cs.CV])
- Authors : Guangchen Shi, Yirui Wu, Jun Liu, Shaohua Wan, Wenhai Wang, Tong Lu
- Link : [http://arxiv.org/abs/2207.12964](http://arxiv.org/abs/2207.12964)
> ABSTRACT  :  Incremental few-shot semantic segmentation (IFSS) targets at incrementally expanding model's capacity to segment new class of images supervised by only a few samples. However, features learned on old classes could significantly drift, causing catastrophic forgetting. Moreover, few samples for pixel-level segmentation on new classes lead to notorious overfitting issues in each learning session. In this paper, we explicitly represent class-based knowledge for semantic segmentation as a category embedding and a hyper-class embedding, where the former describes exclusive semantical properties, and the latter expresses hyper-class knowledge as class-shared semantic properties. Aiming to solve IFSS problems, we present EHNet, i.e., Embedding adaptive-update and Hyper-class representation Network from two aspects. First, we propose an embedding adaptive-update strategy to avoid feature drift, which maintains old knowledge by hyper-class representation, and adaptively update category embeddings with a class-attention scheme to involve new classes learned in individual sessions. Second, to resist overfitting issues caused by few training samples, a hyper-class embedding is learned by clustering all category embeddings for initialization and aligned with category embedding of the new class for **enhancement**, where learned knowledge assists to learn new knowledge, thus alleviating performance dependence on training data scale. Significantly, these two designs provide representation capability for classes with sufficient semantics and limited biases, enabling to perform segmentation tasks requiring high semantic dependence. Experiments on PASCAL-5i and COCO datasets show that EHNet achieves new state-of-the-art performance with remarkable advantages.  
### Multimodal Object Detection via Probabilistic Ensembling. (arXiv:2104.02904v3 [cs.CV] UPDATED)
- Authors : Ting Chen, Jinghao Shi, Zelin Ye, Christoph Mertz, Deva Ramanan, Shu Kong
- Link : [http://arxiv.org/abs/2104.02904](http://arxiv.org/abs/2104.02904)
> ABSTRACT  :  Object detection with multimodal inputs can improve many safety-critical systems such as autonomous vehicles (AVs). Motivated by AVs that operate in both day and **night**, we study multimodal object detection with RGB and thermal cameras, since the latter provides much stronger object signatures under poor illumination. We explore strategies for fusing information from different modalities. Our key contribution is a probabilistic ensembling technique, ProbEn, a simple non-learned method that fuses together detections from multi-modalities. We derive ProbEn from Bayes' rule and first principles that assume conditional independence across modalities. Through probabilistic marginalization, ProbEn elegantly handles missing modalities when detectors do not fire on the same object. Importantly, ProbEn also notably improves multimodal detection even when the conditional independence assumption does not hold, e.g., fusing outputs from other fusion methods (both off-the-shelf and trained in-house). We validate ProbEn on two benchmarks containing both aligned (KAIST) and unaligned (FLIR) multimodal images, showing that ProbEn outperforms prior work by more than 13% in relative performance!  
### Stacked BNAS: Rethinking Broad Convolutional Neural Network for Neural Architecture Search. (arXiv:2111.07722v2 [cs.CV] UPDATED)
- Authors : Zixiang Ding, Yaran Chen, Nannan Li, Dongbin Zhao
- Link : [http://arxiv.org/abs/2111.07722](http://arxiv.org/abs/2111.07722)
> ABSTRACT  :  Different from other deep scalable architecture based NAS approaches, Broad Neural Architecture Search (BNAS) proposes a broad one which consists of convolution and **enhancement** blocks, dubbed Broad Convolutional Neural Network (BCNN) as search space for amazing efficiency improvement. BCNN reuses the topologies of cells in convolution block, so that BNAS can employ few cells for efficient search. Moreover, multi-scale feature fusion and knowledge embedding are proposed to improve the performance of BCNN with shallow topology. However, BNAS suffers some drawbacks: 1) insufficient representation diversity for feature fusion and **enhancement**, and 2) time consuming of knowledge embedding design by human expert.    In this paper, we propose Stacked BNAS whose search space is a developed broad scalable architecture named Stacked BCNN, with better performance than BNAS. On the one hand, Stacked BCNN treats mini-BCNN as the basic block to preserve comprehensive representation and deliver powerful feature extraction ability. On the other hand, we propose Knowledge Embedding Search (KES) to learn appropriate knowledge embeddings. Experimental results show that 1) Stacked BNAS obtains better performance than BNAS, 2) KES contributes to reduce the parameters of learned architecture with satisfactory performance, and 3) Stacked BNAS delivers state-of-the-art efficiency of 0.02 GPU days.  
### Event-guided Deblurring of Unknown **Exposure** Time Videos. (arXiv:2112.06988v3 [cs.CV] UPDATED)
- Authors : Taewoo Kim, Jeongmin Lee, Lin Wang, Jin Yoon
- Link : [http://arxiv.org/abs/2112.06988](http://arxiv.org/abs/2112.06988)
> ABSTRACT  :  Motion deblurring is a highly ill-posed problem due to the loss of motion information in the blur degradation process. Since event cameras can capture apparent motion with a high temporal resolution, several attempts have explored the potential of events for guiding deblurring. These methods generally assume that the **exposure** time is the same as the reciprocal of the video frame rate. However, this is not true in real situations, and the **exposure** time might be unknown and dynamically varies depending on the video shooting environment(e.g., illumination condition). In this paper, we address the event-guided motion deblurring assuming dynamically variable unknown **exposure** time of the frame-based camera. To this end, we first derive a new formulation for event-guided motion deblurring by considering the **exposure** and readout time in the video frame acquisition process. We then propose a novel end-to-end learning framework for event-guided motion deblurring. In particular, we design a novel **Exposure** Time-based Event Selection(ETES) module to selectively use event features by estimating the cross-modal correlation between the features from blurred frames and the events. Moreover, we propose a feature fusion module to fuse the selected features from events and blur frames effectively. We conduct extensive experiments on various datasets and demonstrate that our method achieves state-of-the-art performance.  
### Image reconstruction algorithms in radio interferometry: from handcrafted to learned regularization denoisers. (arXiv:2202.12959v2 [eess.IV] UPDATED)
- Authors : Matthieu Terris, Arwa Dabbech, Chao Tang, Yves Wiaux
- Link : [http://arxiv.org/abs/2202.12959](http://arxiv.org/abs/2202.12959)
> ABSTRACT  :  We introduce a new class of iterative image reconstruction algorithms for radio interferometry, at the interface of convex optimization and deep learning, inspired by plug-and-play methods. The approach consists in learning a prior image model by training a deep neural network (DNN) as a denoiser, and substituting it for the handcrafted proximal regularization operator of an optimization algorithm. The proposed AIRI (``AI for Regularization in radio-interferometric Imaging'') framework, for imaging complex intensity structure with diffuse and faint emission from visibility data, inherits the robustness and interpretability of optimization, and the learning power and speed of networks. Our approach relies on three steps. Firstly, we design a low dynamic range training database from optical intensity images. Secondly, we train a DNN denoiser at a noise level inferred from the signal-to-noise ratio of the data. We use training losses enhanced with a nonexpansiveness term ensuring algorithm convergence, and including on-the-fly database dynamic range **enhancement** via exponentiation. Thirdly, we plug the learned denoiser into the forward-backward optimization algorithm, resulting in a simple iterative structure alternating a denoising step with a gradient-descent data-fidelity step. We have validated AIRI against CLEAN, optimization algorithms of the SARA family, and a DNN trained to reconstruct the image directly from visibility data. Simulation results show that AIRI is competitive in imaging quality with SARA and its unconstrained forward-backward-based version uSARA, while providing significant acceleration. CLEAN remains faster but offers lower quality. The end-to-end DNN offers further acceleration, but with far lower quality than AIRI.  
### Accurate Ground-Truth Depth Image Generation via Overfit Training of Point Cloud Registration using Local Frame Sets. (arXiv:2207.07016v2 [cs.CV] UPDATED)
- Authors : Jiwan Kim, Minchang Kim, Gil Shin, Minyoung Chung
- Link : [http://arxiv.org/abs/2207.07016](http://arxiv.org/abs/2207.07016)
> ABSTRACT  :  Accurate three-dimensional perception is a fundamental task in several computer vision applications. Recently, commercial RGB-depth (RGB-D) cameras have been widely adopted as single-view depth-sensing devices owing to their efficient depth-sensing abilities. However, the depth quality of most RGB-D sensors remains insufficient owing to the inherent noise from a single-view environment. Recently, several studies have focused on the single-view depth **enhancement** of RGB-D cameras. Recent research has proposed deep-learning-based approaches that typically train networks using high-quality supervised depth datasets, which indicates that the quality of the ground-truth (GT) depth dataset is a top-most important factor for accurate system; however, such high-quality GT datasets are difficult to obtain. In this study, we developed a novel method for high-quality GT depth generation based on an RGB-D stream dataset. First, we defined consecutive depth frames in a local spatial region as a local frame set. Then, the depth frames were aligned to a certain frame in the local frame set using an unsupervised point cloud registration scheme. The registration parameters were trained based on an overfit-training scheme, which was primarily used to construct a single GT depth image for each frame set. The final GT depth dataset was constructed using several local frame sets, and each local frame set was trained independently. The primary advantage of this study is that a high-quality GT depth dataset can be constructed under various scanning environments using only the RGB-D stream dataset. Moreover, our proposed method can be used as a new benchmark GT dataset for accurate performance evaluations. We evaluated our GT dataset on previously benchmarked GT depth datasets and demonstrated that our method is superior to state-of-the-art depth **enhancement** frameworks.  
### Injecting 3D Perception of Controllable **NeRF**-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v2 [cs.CV] UPDATED)
- Authors : gi Kwak, Yuanming Li, Dongsik Yoon, Donghyeon Kim, David Han, Hanseok Ko
- Link : [http://arxiv.org/abs/2207.10257](http://arxiv.org/abs/2207.10257)
> ABSTRACT  :  Over the years, 2D GANs have achieved great successes in photorealistic portrait generation. However, they lack 3D understanding in the generation process, thus they suffer from multi-view inconsistency problem. To alleviate the issue, many 3D-aware GANs have been proposed and shown notable results, but 3D GANs struggle with editing semantic attributes. The controllability and interpretability of 3D GANs have not been much explored. In this work, we propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of discovering semantic attributes during training and controlling them in an unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN to obtain a high-fidelity 3D-controllable generator. Unlike existing latent-based methods allowing implicit pose control, the proposed 3D-controllable StyleGAN enables explicit pose control over portrait generation. This distillation allows direct compatibility between 3D control and many StyleGAN-based techniques (e.g., inversion and stylization), and also brings an advantage in terms of computational resources. Our codes are available at https://github.com/jgkwak95/SURF-GAN.  
### Face Deblurring using Dual Camera Fusion on Mobile Phones. (arXiv:2207.11617v1 [cs.CV] CROSS LISTED)
- Authors : Sheng Lai, YiChang Shih, Cheng Chu, Xiaotong Wu, Fang Tsai, Michael Krainin, Deqing Sun, Kai Liang
- Link : [http://arxiv.org/abs/2207.11617](http://arxiv.org/abs/2207.11617)
> ABSTRACT  :  Motion blur of fast-moving subjects is a longstanding problem in photography and very common on mobile phones due to limited light collection efficiency, particularly in **low-light** conditions. While we have witnessed great progress in image deblurring in recent years, most methods require significant computational power and have limitations in processing high-resolution photos with severe local motions. To this end, we develop a novel face deblurring system based on the dual camera fusion technique for mobile phones. The system detects subject motion to dynamically enable a reference camera, e.g., ultrawide angle camera commonly available on recent premium phones, and captures an auxiliary photo with faster shutter settings. While the main shot is low noise but blurry, the reference shot is sharp but noisy. We learn ML models to align and fuse these two shots and output a clear photo without motion blur. Our algorithm runs efficiently on Google Pixel 6, which takes 463 ms overhead per shot. Our experiments demonstrate the advantage and robustness of our system against alternative single-image, multi-frame, face-specific, and video deblurring algorithms as well as commercial products. To the best of our knowledge, our work is the first mobile solution for face motion deblurring that works reliably and robustly over thousands of images in diverse motion and lighting conditions.  
## eess.IV
---
### Seeing Far in the **Dark** with Patterned Flash. (arXiv:2207.12570v1 [eess.IV])
- Authors : Zhanghao Sun, Jian Wang, Yicheng Wu, Shree Nayar
- Link : [http://arxiv.org/abs/2207.12570](http://arxiv.org/abs/2207.12570)
> ABSTRACT  :  Flash illumination is widely used in imaging under **low-light** environments. However, illumination intensity falls off with propagation distance quadratically, which poses significant challenges for flash imaging at a long distance. We propose a new flash technique, named ``patterned flash'', for flash imaging at a long distance. Patterned flash concentrates optical power into a dot array. Compared with the conventional uniform flash where the signal is overwhelmed by the noise everywhere, patterned flash provides stronger signals at sparsely distributed points across the field of view to ensure the signals at those points stand out from the sensor noise. This enables post-processing to resolve important objects and details. Additionally, the patterned flash projects texture onto the scene, which can be treated as a structured light system for depth perception. Given the novel system, we develop a joint image reconstruction and depth estimation algorithm with a convolutional neural network. We build a hardware prototype and test the proposed flash technique on various scenes. The experimental results demonstrate that our patterned flash has significantly better performance at long distances in **low-light** environments.  
### Compiler-Aware Neural Architecture Search for On-Mobile **Real-time** Super-Resolution. (arXiv:2207.12577v1 [cs.CV])
- Authors : Yushu Wu, Yifan Gong, Pu Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, Yanzhi Wang
- Link : [http://arxiv.org/abs/2207.12577](http://arxiv.org/abs/2207.12577)
> ABSTRACT  :  Deep learning-based super-resolution (SR) has gained tremendous popularity in recent years because of its high image quality performance and wide application scenarios. However, prior methods typically suffer from large amounts of computations and huge power consumption, causing difficulties for real-time inference, especially on resource-limited platforms such as mobile devices. To mitigate this, we propose a compiler-aware SR neural architecture search (NAS) framework that conducts depth search and per-layer width search with adaptive SR blocks. The inference speed is directly taken into the optimization along with the SR loss to derive SR models with high image quality while satisfying the real-time inference requirement. Instead of measuring the speed on mobile devices at each iteration during the search process, a speed model incorporated with compiler optimizations is leveraged to predict the inference latency of the SR block with various width configurations for faster convergence. With the proposed framework, we achieve real-time SR inference for implementing 720p resolution with competitive SR performance (in terms of PSNR and SSIM) on GPU/DSP of mobile platforms (Samsung Galaxy S21).  
### Key frames assisted hybrid encoding for photorealistic compressive video sensing. (arXiv:2207.12627v1 [eess.IV])
- Authors : Honghao Huang, Jiajie Teng, Yu Liang, Chengyang Hu, Minghua Chen, Sigang Yang, Hongwei Chen
- Link : [http://arxiv.org/abs/2207.12627](http://arxiv.org/abs/2207.12627)
> ABSTRACT  :  Snapshot compressive imaging (SCI) encodes high-speed scene video into a snapshot measurement and then computationally makes reconstructions, allowing for efficient high-dimensional data acquisition. Numerous algorithms, ranging from regularization-based optimization and deep learning, are being investigated to improve reconstruction quality, but they are still limited by the ill-posed and information-deficient nature of the standard SCI paradigm. To overcome these drawbacks, we propose a new key frames assisted hybrid encoding paradigm for compressive video sensing, termed KH-CVS, that alternatively captures short-**exposure** key frames without coding and long-**exposure** encoded compressive frames to jointly reconstruct photorealistic video. With the use of optical flow and spatial warping, a deep convolutional neural network framework is constructed to integrate the benefits of these two types of frames. Extensive experiments on both simulations and real data from the prototype we developed verify the superiority of the proposed method.  
### A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation. (arXiv:2207.12770v1 [eess.IV])
- Authors : Javier Civit, Francisco Luna, Jose Maria, Rodriguez Corral, Manuel Dominguez, Arturo Morgado, Anton Civit
- Link : [http://arxiv.org/abs/2207.12770](http://arxiv.org/abs/2207.12770)
> ABSTRACT  :  Medical image segmentation can be implemented using Deep Learning methods with fast and efficient segmentation networks. Single-board computers (SBCs) are difficult to use to train deep networks due to their memory and processing limitations. Specific hardware such as Google's Edge TPU makes them suitable for **real time** predictions using complex pre-trained networks. In this work, we study the performance of two SBCs, with and without hardware acceleration for fundus image segmentation, though the conclusions of this study can be applied to the segmentation by deep neural networks of other types of medical images. To test the benefits of hardware acceleration, we use networks and datasets from a previous published work and generalize them by testing with a dataset with ultrasound thyroid images. We measure prediction times in both SBCs and compare them with a cloud based TPU system. The results show the feasibility of Machine Learning accelerated SBCs for optic disc and cup segmentation obtaining times below 25 milliseconds per image using Edge TPUs.  
### Image reconstruction algorithms in radio interferometry: from handcrafted to learned regularization denoisers. (arXiv:2202.12959v2 [eess.IV] UPDATED)
- Authors : Matthieu Terris, Arwa Dabbech, Chao Tang, Yves Wiaux
- Link : [http://arxiv.org/abs/2202.12959](http://arxiv.org/abs/2202.12959)
> ABSTRACT  :  We introduce a new class of iterative image reconstruction algorithms for radio interferometry, at the interface of convex optimization and deep learning, inspired by plug-and-play methods. The approach consists in learning a prior image model by training a deep neural network (DNN) as a denoiser, and substituting it for the handcrafted proximal regularization operator of an optimization algorithm. The proposed AIRI (``AI for Regularization in radio-interferometric Imaging'') framework, for imaging complex intensity structure with diffuse and faint emission from visibility data, inherits the robustness and interpretability of optimization, and the learning power and speed of networks. Our approach relies on three steps. Firstly, we design a low dynamic range training database from optical intensity images. Secondly, we train a DNN denoiser at a noise level inferred from the signal-to-noise ratio of the data. We use training losses enhanced with a nonexpansiveness term ensuring algorithm convergence, and including on-the-fly database dynamic range **enhancement** via exponentiation. Thirdly, we plug the learned denoiser into the forward-backward optimization algorithm, resulting in a simple iterative structure alternating a denoising step with a gradient-descent data-fidelity step. We have validated AIRI against CLEAN, optimization algorithms of the SARA family, and a DNN trained to reconstruct the image directly from visibility data. Simulation results show that AIRI is competitive in imaging quality with SARA and its unconstrained forward-backward-based version uSARA, while providing significant acceleration. CLEAN remains faster but offers lower quality. The end-to-end DNN offers further acceleration, but with far lower quality than AIRI.  
## cs.LG
---
### Compiler-Aware Neural Architecture Search for On-Mobile **Real-time** Super-Resolution. (arXiv:2207.12577v1 [cs.CV])
- Authors : Yushu Wu, Yifan Gong, Pu Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, Yanzhi Wang
- Link : [http://arxiv.org/abs/2207.12577](http://arxiv.org/abs/2207.12577)
> ABSTRACT  :  Deep learning-based super-resolution (SR) has gained tremendous popularity in recent years because of its high image quality performance and wide application scenarios. However, prior methods typically suffer from large amounts of computations and huge power consumption, causing difficulties for real-time inference, especially on resource-limited platforms such as mobile devices. To mitigate this, we propose a compiler-aware SR neural architecture search (NAS) framework that conducts depth search and per-layer width search with adaptive SR blocks. The inference speed is directly taken into the optimization along with the SR loss to derive SR models with high image quality while satisfying the real-time inference requirement. Instead of measuring the speed on mobile devices at each iteration during the search process, a speed model incorporated with compiler optimizations is leveraged to predict the inference latency of the SR block with various width configurations for faster convergence. With the proposed framework, we achieve real-time SR inference for implementing 720p resolution with competitive SR performance (in terms of PSNR and SSIM) on GPU/DSP of mobile platforms (Samsung Galaxy S21).  
### **Bilateral** Self-unbiased Learning from Biased Implicit Feedback. (arXiv:2207.12660v1 [cs.IR])
- Authors : woong Lee, Seongmin Park, Joonseok Lee, Jongwuk Lee
- Link : [http://arxiv.org/abs/2207.12660](http://arxiv.org/abs/2207.12660)
> ABSTRACT  :  Implicit feedback has been widely used to build commercial recommender systems. Because observed feedback represents users' click logs, there is a semantic gap between true relevance and observed feedback. More importantly, observed feedback is usually biased towards popular items, thereby overestimating the actual relevance of popular items. Although existing studies have developed unbiased learning methods using inverse propensity weighting (IPW) or causal reasoning, they solely focus on eliminating the popularity bias of items. In this paper, we propose a novel unbiased recommender learning model, namely BIlateral SElf-unbiased Recommender (BISER), to eliminate the **exposure** bias of items caused by recommender models. Specifically, BISER consists of two key components: (i) self-inverse propensity weighting (SIPW) to gradually mitigate the bias of items without incurring high computational costs; and (ii) **bilateral** unbiased learning (BU) to bridge the gap between two complementary models in model predictions, i.e., user- and item-based autoencoders, alleviating the high variance of SIPW. Extensive experiments show that BISER consistently outperforms state-of-the-art unbiased recommender models over several datasets, including Coat, Yahoo! R3, MovieLens, and CiteULike.  
### $\textbf{P$^2$A}$: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos. (arXiv:2207.12730v1 [cs.CV])
- Authors : Jiang Bian, Qingzhong Wang, Haoyi Xiong, Jun Huang, Chen Liu, Xuhong Li, Jun Cheng, Jun Zhao, Feixiang Lu, Dejing Dou
- Link : [http://arxiv.org/abs/2207.12730](http://arxiv.org/abs/2207.12730)
> ABSTRACT  :  While deep learning has been widely used for video analytics, such as video classification and action detection, dense action detection with fast-moving subjects from sports videos is still challenging. In this work, we release yet another sports video dataset $\textbf{P$^2$A}$ for $\underline{P}$ing $\underline{P}$ong-$\underline{A}$ction detection, which consists of 2,721 video clips collected from the broadcasting videos of professional table tennis matches in World Table Tennis Championships and Olympiads. We work with a crew of table tennis professionals and referees to obtain fine-grained action labels (in 14 classes) for every ping-pong action that appeared in the dataset and formulate two sets of action detection problems - action localization and action recognition. We evaluate a number of commonly-seen action recognition (e.g., TSM, TSN, Video **Swin**Transformer, and Slowfast) and action localization models (e.g., BSN, BSN++, BMN, TCANet), using $\textbf{P$^2$A}$ for both problems, under various settings. These models can only achieve 48% area under the AR-AN curve for localization and 82% top-one accuracy for recognition since the ping-pong actions are dense with fast-moving subjects but broadcasting videos are with only 25 FPS. The results confirm that $\textbf{P$^2$A}$ is still a challenging task and can be used as a benchmark for action detection from videos.  
### A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation. (arXiv:2207.12770v1 [eess.IV])
- Authors : Javier Civit, Francisco Luna, Jose Maria, Rodriguez Corral, Manuel Dominguez, Arturo Morgado, Anton Civit
- Link : [http://arxiv.org/abs/2207.12770](http://arxiv.org/abs/2207.12770)
> ABSTRACT  :  Medical image segmentation can be implemented using Deep Learning methods with fast and efficient segmentation networks. Single-board computers (SBCs) are difficult to use to train deep networks due to their memory and processing limitations. Specific hardware such as Google's Edge TPU makes them suitable for **real time** predictions using complex pre-trained networks. In this work, we study the performance of two SBCs, with and without hardware acceleration for fundus image segmentation, though the conclusions of this study can be applied to the segmentation by deep neural networks of other types of medical images. To test the benefits of hardware acceleration, we use networks and datasets from a previous published work and generalize them by testing with a dataset with ultrasound thyroid images. We measure prediction times in both SBCs and compare them with a cloud based TPU system. The results show the feasibility of Machine Learning accelerated SBCs for optic disc and cup segmentation obtaining times below 25 milliseconds per image using Edge TPUs.  
## cs.AI
---
# Paper List
---
## cs.CV
---
**128** new papers in cs.CV:-) 
1. Color Coding of Large Value Ranges Applied to Meteorological Data. (arXiv:2207.12399v1 [eess.IV])
2. 3D Shape Sequence of Human Comparison and Classification using Current and Varifolds. (arXiv:2207.12485v1 [cs.CV])
3. NeuriCam: Video Super-Resolution and Colorization Using Key Frames. (arXiv:2207.12496v1 [cs.CV])
4. Deep learning-based algorithm for assessment of knee osteoarthritis severity in radiographs matches performance of radiologists. (arXiv:2207.12521v1 [eess.IV])
5. Trainability Preserving Neural Structured Pruning. (arXiv:2207.12534v1 [cs.LG])
6. Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning. (arXiv:2207.12535v1 [cs.CR])
7. Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation. (arXiv:2207.12537v1 [cs.CV])
8. Inter-Frame Compression for Dynamic Point Cloud Geometry Coding. (arXiv:2207.12554v1 [cs.CV])
9. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v1 [cs.LG])
10. Seeing Far in the **Dark** with Patterned Flash. (arXiv:2207.12570v1 [eess.IV])
11. Translating a Visual LEGO Manual to a Machine-Executable Plan. (arXiv:2207.12572v1 [cs.CV])
12. WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v1 [cs.CL])
13. Compiler-Aware Neural Architecture Search for On-Mobile **Real-time** Super-Resolution. (arXiv:2207.12577v1 [cs.CV])
14. RenderNet: Visual Relocalization Using Virtual Viewpoints in Large-Scale Indoor Environments. (arXiv:2207.12579v1 [cs.CV])
15. TGCF: Texture guided color fusion for impressionism oil painting style rendering. (arXiv:2207.12585v1 [cs.CV])
16. Large-displacement 3D Object Tracking with Hybrid Non-local Optimization. (arXiv:2207.12620v1 [cs.CV])
17. Multi-Attention Network for Compressed Video Referring Object Segmentation. (arXiv:2207.12622v1 [cs.CV])
18. Learning Hierarchy Aware Features for Reducing Mistake Severity. (arXiv:2207.12646v1 [cs.CV])
19. Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v1 [cs.CV])
20. Efficient and Accurate Skeleton-Based Two-Person Interaction Recognition Using Inter- and Intra-body Graphs. (arXiv:2207.12648v1 [cs.CV])
21. Asymmetric Scalable Cross-modal Hashing. (arXiv:2207.12650v1 [cs.CV])
22. Can Deep Learning Assist Automatic Identification of Layered Pigments From XRF Data?. (arXiv:2207.12651v1 [cs.CV])
23. ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection. (arXiv:2207.12654v1 [cs.CV])
24. Semi-supervised 3D Object Detection with Proficient Teachers. (arXiv:2207.12655v1 [cs.CV])
25. Graph Neural Network and Spatiotemporal Transformer Attention for 3D Video Object Detection from Point Clouds. (arXiv:2207.12659v1 [cs.CV])
26. Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training. (arXiv:2207.12661v1 [cs.CV])
27. A Kendall Shape Space Approach to 3D Shape Estimation from 2D Landmarks. (arXiv:2207.12687v1 [cs.CV])
28. CENet: Toward Concise and Efficient LiDAR Semantic Segmentation for Autonomous Driving. (arXiv:2207.12691v1 [cs.CV])
29. Comparison of Deep Learning and Machine Learning Models and Frameworks for Skin Lesion Classification. (arXiv:2207.12715v1 [eess.IV])
30. MV-FCOS3D++: Multi-View Camera-Only 4D Object Detection with Pretrained Monocular Backbones. (arXiv:2207.12716v1 [cs.CV])
31. Convolutional neural networks and multi-threshold analysis for contamination detection in the apparel industry. (arXiv:2207.12720v1 [cs.CV])
32. $\textbf{P$^2$A}$: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos. (arXiv:2207.12730v1 [cs.CV])
33. Distribution Learning Based on Evolutionary Algorithm Assisted Deep Neural Networks for Imbalanced Image Classification. (arXiv:2207.12744v1 [cs.CV])
34. Criteria Comparative Learning for Real-scene Image Super-Resolution. (arXiv:2207.12767v1 [cs.CV])
35. A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation. (arXiv:2207.12770v1 [eess.IV])
36. Static and Dynamic Concepts for Self-supervised Video Representation Learning. (arXiv:2207.12795v1 [cs.CV])
37. Class-Aware Universum Inspired Re-Balance Learning for Long-Tailed Recognition. (arXiv:2207.12808v1 [cs.CV])
38. Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation. (arXiv:2207.12817v1 [cs.CV])
39. S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning. (arXiv:2207.12819v1 [cs.CV])
40. Compositional Human-Scene Interaction Synthesis with Semantic Control. (arXiv:2207.12824v1 [cs.CV])
41. Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning. (arXiv:2207.12833v1 [cs.CV])
42. KinePose: A temporally optimized inverse kinematics technique for 6DOF human pose estimation with biomechanical constraints. (arXiv:2207.12841v1 [cs.CV])
43. Unsupervised Domain Adaptation for Video Transformers in Action Recognition. (arXiv:2207.12842v1 [cs.CV])
44. Bessel Equivariant Networks for Inversion of Transmission Effects in Multi-Mode Optical Fibres. (arXiv:2207.12849v1 [physics.optics])
45. Towards Smart City Security: Violence and Weaponized Violence Detection using DCNN. (arXiv:2207.12850v1 [cs.CV])
46. Visually explaining 3D-CNN predictions for video classification with an adaptive occlusion sensitivity analysis. (arXiv:2207.12859v1 [cs.CV])
47. FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair. (arXiv:2207.12863v1 [cs.CV])
48. Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition. (arXiv:2207.12866v1 [eess.AS])
49. Generalized Probabilistic U-Net for medical image segementation. (arXiv:2207.12872v1 [cs.CV])
50. Detection of road traffic crashes based on collision estimation. (arXiv:2207.12886v1 [cs.CV])
51. LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection. (arXiv:2207.12888v1 [cs.CV])
52. Cross-Modality Image Registration using a Training-Time Privileged Third Modality. (arXiv:2207.12901v1 [cs.CV])
53. AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction. (arXiv:2207.12909v1 [cs.CV])
54. A Guide to Image and Video based Small Object Detection using Deep Learning : Case Study of Maritime Surveillance. (arXiv:2207.12926v1 [cs.CV])
55. A Reliable Online Method for Joint Estimation of Focal Length and Camera Rotation. (arXiv:2207.12934v1 [cs.CV])
56. Semantic Segmentation for Autonomous Driving: Model Evaluation, Dataset Generation, Perspective Comparison, and Real-Time Capability. (arXiv:2207.12939v1 [cs.CV])
57. Learning Generalizable Latent Representations for Novel Degradations in Super Resolution. (arXiv:2207.12941v1 [cs.CV])
58. AMF: Adaptable Weighting Fusion with Multiple Fine-tuning for Image Classification. (arXiv:2207.12944v1 [cs.CV])
59. Contextual Text Block Detection towards Scene Text Understanding. (arXiv:2207.12955v1 [cs.CV])
60. Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation. (arXiv:2207.12964v1 [cs.CV])
61. Nondestructive Quality Control in Powder Metallurgy using Hyperspectral Imaging. (arXiv:2207.12966v1 [cs.CV])
62. TransFiner: A Full-Scale Refinement Approach for Multiple Object Tracking. (arXiv:2207.12967v1 [cs.CV])
63. Tracking Every Thing in the Wild. (arXiv:2207.12978v1 [cs.CV])
64. Efficient One Pass Self-distillation with Zipf's Label Smoothing. (arXiv:2207.12980v1 [cs.CV])
65. Explaining Deep Neural Networks for Point Clouds using Gradient-based Visualisations. (arXiv:2207.12984v1 [cs.CV])
66. Monocular 3D Object Detection with Depth from Motion. (arXiv:2207.12988v1 [cs.CV])
67. V$^2$L: Leveraging Vision and Vision-language Models into Large-scale Product Retrieval. (arXiv:2207.12994v1 [cs.CV])
68. Robust and Efficient Segmentation of Cross-domain Medical Images. (arXiv:2207.12995v1 [cs.CV])
69. Topological Optimized Convolutional Visual Recurrent Network for Brain Tumor Segmentation and Classification. (arXiv:2207.13021v1 [eess.IV])
70. Pseudo-Pair based Self-Similarity Learning for Unsupervised Person Re-identification. (arXiv:2207.13035v1 [cs.CV])
71. Learning Resolution-Adaptive Representations for Cross-Resolution Person Re-Identification. (arXiv:2207.13037v1 [cs.CV])
72. Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models. (arXiv:2207.13038v1 [cs.CV])
73. Efficient High-Resolution Deep Learning: A Survey. (arXiv:2207.13050v1 [cs.CV])
74. NewsStories: Illustrating articles with visual summaries. (arXiv:2207.13061v1 [cs.CV])
75. Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis. (arXiv:2207.13064v1 [cs.CV])
76. DeFakePro: Decentralized DeepFake Attacks Detection using ENF Authentication. (arXiv:2207.13070v1 [cs.CR])
77. DETRs with Hybrid Matching. (arXiv:2207.13080v1 [cs.CV])
78. Task Agnostic and Post-hoc Unseen Distribution Detection. (arXiv:2207.13083v1 [cs.LG])
79. Group DETR: Fast Training Convergence with Decoupled One-to-Many Label Assignment. (arXiv:2207.13085v1 [cs.CV])
80. What you get is not always what you see: pitfalls in solar array assessment using overhead imagery. (arXiv:1902.10895v2 [cs.CV] UPDATED)
81. Sparse Signal Models for Data Augmentation in Deep Learning ATR. (arXiv:2012.09284v2 [cs.CV] UPDATED)
82. Multimodal Object Detection via Probabilistic Ensembling. (arXiv:2104.02904v3 [cs.CV] UPDATED)
83. Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v3 [cs.LG] UPDATED)
84. Matching Visual Features to Hierarchical Semantic Topics for Image Paragraph Captioning. (arXiv:2105.04143v2 [cs.CV] UPDATED)
85. Heterogeneous Contrastive Learning. (arXiv:2105.09401v3 [cs.LG] UPDATED)
86. Pose Refinement with Joint Optimization of Visual Points and Lines. (arXiv:2110.03940v2 [cs.CV] UPDATED)
87. Making Corgis Important for Honeycomb Classification: Adversarial Attacks on Concept-based Explainability Tools. (arXiv:2110.07120v2 [cs.LG] UPDATED)
88. Guiding Visual Question Generation. (arXiv:2110.08226v3 [cs.LG] UPDATED)
89. Iterative Teaching by Label Synthesis. (arXiv:2110.14432v3 [cs.LG] UPDATED)
90. Sliced Recursive Transformer. (arXiv:2111.05297v3 [cs.CV] UPDATED)
91. Stacked BNAS: Rethinking Broad Convolutional Neural Network for Neural Architecture Search. (arXiv:2111.07722v2 [cs.CV] UPDATED)
92. TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems. (arXiv:2111.09999v2 [cs.CV] UPDATED)
93. Self-slimmed Vision Transformer. (arXiv:2111.12624v2 [cs.CV] UPDATED)
94. Adaptive Token Sampling For Efficient Vision Transformers. (arXiv:2111.15667v3 [cs.CV] UPDATED)
95. Event-guided Deblurring of Unknown **Exposure** Time Videos. (arXiv:2112.06988v3 [cs.CV] UPDATED)
96. TAFIM: Targeted Adversarial Attacks against Facial Image Manipulations. (arXiv:2112.09151v2 [cs.CV] UPDATED)
97. Contrastive Learning of Semantic and Visual Representations for Text Tracking. (arXiv:2112.14976v2 [cs.CV] UPDATED)
98. Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning. (arXiv:2201.03529v2 [cs.LG] UPDATED)
99. PT4AL: Using Self-Supervised Pretext Tasks for Active Learning. (arXiv:2201.07459v3 [cs.CV] UPDATED)
100. Image reconstruction algorithms in radio interferometry: from handcrafted to learned regularization denoisers. (arXiv:2202.12959v2 [eess.IV] UPDATED)
101. Exploring Wilderness Characteristics Using Explainable Machine Learning in Satellite Imagery. (arXiv:2203.00379v3 [cs.CV] UPDATED)
102. Language Matters: A Weakly Supervised Vision-Language Pre-training Approach for Scene Text Detection and Spotting. (arXiv:2203.03911v2 [cs.CV] UPDATED)
103. ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer. (arXiv:2203.03952v5 [cs.CV] UPDATED)
104. SynthStrip: Skull-Stripping for Any Brain Image. (arXiv:2203.09974v2 [eess.IV] UPDATED)
105. PACS: A Dataset for Physical Audiovisual CommonSense Reasoning. (arXiv:2203.11130v2 [cs.LG] UPDATED)
106. Self-supervised Semantic Segmentation Grounded in Visual Concepts. (arXiv:2203.13868v2 [cs.CV] UPDATED)
107. Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification. (arXiv:2203.16983v3 [cs.CV] UPDATED)
108. Structured Graph Variational Autoencoders for Indoor Furniture layout Generation. (arXiv:2204.04867v3 [cs.CV] UPDATED)
109. MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v2 [cs.CV] UPDATED)
110. Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images. (arXiv:2204.12454v4 [cs.CV] UPDATED)
111. HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation. (arXiv:2204.13132v2 [cs.CV] UPDATED)
112. Video Extrapolation in Space and Time. (arXiv:2205.02084v3 [cs.CV] UPDATED)
113. HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance. (arXiv:2205.05677v4 [cs.CV] UPDATED)
114. Unsupervised Image Representation Learning with Deep Latent Particles. (arXiv:2205.15821v2 [cs.CV] UPDATED)
115. Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation Using Bounding Boxes. (arXiv:2206.01203v2 [cs.CV] UPDATED)
116. Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v3 [cs.CV] UPDATED)
117. Robustness Implies Generalization via Data-Dependent Generalization Bounds. (arXiv:2206.13497v3 [cs.LG] UPDATED)
118. Autonomous Intraluminal Navigation of a Soft Robot using Deep-Learning-based Visual Servoing. (arXiv:2207.00401v2 [cs.RO] UPDATED)
119. Accurate Ground-Truth Depth Image Generation via Overfit Training of Point Cloud Registration using Local Frame Sets. (arXiv:2207.07016v2 [cs.CV] UPDATED)
120. Injecting 3D Perception of Controllable **NeRF**-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v2 [cs.CV] UPDATED)
121. Spatial-temporal Analysis for Automated Concrete Workability Estimation. (arXiv:2207.11635v2 [cs.CV] UPDATED)
122. Sub-Aperture Feature Adaptation in Single Image Super-resolution Model for Light Field Imaging. (arXiv:2207.11894v2 [cs.CV] UPDATED)
123. Optimal Boxes: Boosting End-to-End Scene Text Recognition by Adjusting Annotated Bounding Boxes via Reinforcement Learning. (arXiv:2207.11934v2 [cs.CV] UPDATED)
124. RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation. (arXiv:2207.11984v2 [cs.CV] UPDATED)
125. 3D Siamese Transformer Network for Single Object Tracking on Point Clouds. (arXiv:2207.11995v2 [cs.CV] UPDATED)
126. Riemannian Geometry Approach for Minimizing Distortion and its Applications. (arXiv:2207.12038v2 [cs.CV] UPDATED)
127. Domain Decorrelation with Potential Energy Ranking. (arXiv:2207.12194v2 [cs.CV] UPDATED)
128. Face Deblurring using Dual Camera Fusion on Mobile Phones. (arXiv:2207.11617v1 [cs.CV] CROSS LISTED)
## eess.IV
---
**21** new papers in eess.IV:-) 
1. Color Coding of Large Value Ranges Applied to Meteorological Data. (arXiv:2207.12399v1 [eess.IV])
2. Deep learning-based algorithm for assessment of knee osteoarthritis severity in radiographs matches performance of radiologists. (arXiv:2207.12521v1 [eess.IV])
3. Inter-Frame Compression for Dynamic Point Cloud Geometry Coding. (arXiv:2207.12554v1 [cs.CV])
4. Seeing Far in the **Dark** with Patterned Flash. (arXiv:2207.12570v1 [eess.IV])
5. Compiler-Aware Neural Architecture Search for On-Mobile **Real-time** Super-Resolution. (arXiv:2207.12577v1 [cs.CV])
6. Key frames assisted hybrid encoding for photorealistic compressive video sensing. (arXiv:2207.12627v1 [eess.IV])
7. Can Deep Learning Assist Automatic Identification of Layered Pigments From XRF Data?. (arXiv:2207.12651v1 [cs.CV])
8. Comparison of Deep Learning and Machine Learning Models and Frameworks for Skin Lesion Classification. (arXiv:2207.12715v1 [eess.IV])
9. A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation. (arXiv:2207.12770v1 [eess.IV])
10. Bessel Equivariant Networks for Inversion of Transmission Effects in Multi-Mode Optical Fibres. (arXiv:2207.12849v1 [physics.optics])
11. Cross-Modality Image Registration using a Training-Time Privileged Third Modality. (arXiv:2207.12901v1 [cs.CV])
12. Learning Generalizable Latent Representations for Novel Degradations in Super Resolution. (arXiv:2207.12941v1 [cs.CV])
13. Learning Series-Parallel Lookup Tables for Efficient Image Super-Resolution. (arXiv:2207.12987v1 [eess.IV])
14. Topological Optimized Convolutional Visual Recurrent Network for Brain Tumor Segmentation and Classification. (arXiv:2207.13021v1 [eess.IV])
15. Plug-and-Play Compressed Sensing: Theoretical Guarantees on Exact and Robust Recovery. (arXiv:2207.13031v1 [eess.IV])
16. DeFakePro: Decentralized DeepFake Attacks Detection using ENF Authentication. (arXiv:2207.13070v1 [cs.CR])
17. Sparse Signal Models for Data Augmentation in Deep Learning ATR. (arXiv:2012.09284v2 [cs.CV] UPDATED)
18. Image reconstruction algorithms in radio interferometry: from handcrafted to learned regularization denoisers. (arXiv:2202.12959v2 [eess.IV] UPDATED)
19. SynthStrip: Skull-Stripping for Any Brain Image. (arXiv:2203.09974v2 [eess.IV] UPDATED)
20. Sub-Aperture Feature Adaptation in Single Image Super-resolution Model for Light Field Imaging. (arXiv:2207.11894v2 [cs.CV] UPDATED)
21. Robust Phase Retrieval with Green Noise Binary Masks. (arXiv:2012.09410v3 [eess.IV] CROSS LISTED)
## cs.LG
---
**132** new papers in cs.LG:-) 
1. Versatile Weight Attack via Flipping Limited Bits. (arXiv:2207.12405v1 [cs.CR])
2. Automated discovery of interpretable gravitational-wave population models. (arXiv:2207.12409v1 [astro-ph.IM])
3. Provably Efficient Fictitious Play Policy Optimization for Zero-Sum Markov Games with Structured Transitions. (arXiv:2207.12463v1 [cs.LG])
4. Machine Learning to Predict the Antimicrobial Activity of Cold Atmospheric Plasma-Activated Liquids. (arXiv:2207.12478v1 [cs.LG])
5. Estimating and Controlling for Fairness via Sensitive Attribute Predictors. (arXiv:2207.12497v1 [cs.LG])
6. Benchmark time series data sets for PyTorch -- the torchtime package. (arXiv:2207.12503v1 [cs.LG])
7. On the benefits of non-linear weight updates. (arXiv:2207.12505v1 [cs.LG])
8. Optimizing Empty Container Repositioning and Fleet Deployment via Configurable Semi-POMDPs. (arXiv:2207.12509v1 [cs.LG])
9. Approximate Low-Rank Decomposition for Real Symmetric Tensors. (arXiv:2207.12529v1 [math.NA])
10. Cooperative Actor-Critic via TD Error Aggregation. (arXiv:2207.12533v1 [eess.SY])
11. Trainability Preserving Neural Structured Pruning. (arXiv:2207.12534v1 [cs.LG])
12. Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning. (arXiv:2207.12535v1 [cs.CR])
13. Bayesian tensor factorization for predicting clinical outcomes using integrated human genetics evidence. (arXiv:2207.12538v1 [cs.LG])
14. $p$-DkNN: Out-of-Distribution Detection Through Statistical Testing of Deep Representations. (arXiv:2207.12545v1 [cs.LG])
15. The Bearable Lightness of Big Data: Towards Massive Public Datasets in Scientific Machine Learning. (arXiv:2207.12546v1 [cs.LG])
16. An Empirical Deep Dive into Deep Learning's Driving Dynamics. (arXiv:2207.12547v1 [cs.LG])
17. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v1 [cs.LG])
18. AMLB: an AutoML Benchmark. (arXiv:2207.12560v1 [cs.LG])
19. Compiler-Aware Neural Architecture Search for On-Mobile **Real-time** Super-Resolution. (arXiv:2207.12577v1 [cs.CV])
20. A Retrospective on ICSE 2022. (arXiv:2207.12578v1 [cs.SE])
21. Classifier-Free Diffusion Guidance. (arXiv:2207.12598v1 [cs.LG])
22. A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics. (arXiv:2207.12599v1 [cs.LG])
23. Learning Protein Representations via Complete 3D Graph Networks. (arXiv:2207.12600v1 [cs.LG])
24. Differentially Private Estimation via Statistical Depth. (arXiv:2207.12602v1 [stat.ML])
25. Physics Embedded Machine Learning for Electromagnetic Data Imaging. (arXiv:2207.12607v1 [physics.comp-ph])
26. Exploring the Design of Adaptation Protocols for Improved Generalization and Machine Learning Safety. (arXiv:2207.12615v1 [cs.LG])
27. A Learning and Control Perspective for Microfinance. (arXiv:2207.12631v1 [q-fin.GN])
28. Variance estimation in graphs with the fused lasso. (arXiv:2207.12638v1 [math.ST])
29. Learning Bipedal Walking On Planned Footsteps For Humanoid Robots. (arXiv:2207.12644v1 [cs.RO])
30. Efficient and Accurate Skeleton-Based Two-Person Interaction Recognition Using Inter- and Intra-body Graphs. (arXiv:2207.12648v1 [cs.CV])
31. Can Deep Learning Assist Automatic Identification of Layered Pigments From XRF Data?. (arXiv:2207.12651v1 [cs.CV])
32. **Bilateral** Self-unbiased Learning from Biased Implicit Feedback. (arXiv:2207.12660v1 [cs.IR])
33. Time Majority Voting, a PC-based EEG Classifier for Non-expert Users. (arXiv:2207.12662v1 [cs.LG])
34. A Data Driven Method for Multi-step Prediction of Ship Roll Motion in High Sea States. (arXiv:2207.12673v1 [cs.LG])
35. Analyzing Sharpness along GD Trajectory: Progressive Sharpening and Edge of Stability. (arXiv:2207.12678v1 [cs.LG])
36. Active Learning of Ordinal Embeddings: A User Study on Football Data. (arXiv:2207.12710v1 [cs.LG])
37. $\textbf{P$^2$A}$: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos. (arXiv:2207.12730v1 [cs.CV])
38. ScoreCAM GNN: une explication optimale des r\'eseaux profonds sur graphes. (arXiv:2207.12748v1 [cs.LG])
39. Thermodynamics of learning physical phenomena. (arXiv:2207.12749v1 [cs.LG])
40. A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation. (arXiv:2207.12770v1 [eess.IV])
41. Quiver neural networks. (arXiv:2207.12773v1 [cs.LG])
42. Reconciling Security and Communication Efficiency in Federated Learning. (arXiv:2207.12779v1 [cs.LG])
43. An Explainable Decision Support System for Predictive Process Analytics. (arXiv:2207.12782v1 [cs.LG])
44. Static and Dynamic Concepts for Self-supervised Video Representation Learning. (arXiv:2207.12795v1 [cs.CV])
45. PIXEL: Physics-Informed Cell Representations for Fast and Accurate PDE Solvers. (arXiv:2207.12800v1 [cs.LG])
46. Neural Design for Genetic Perturbation Experiments. (arXiv:2207.12805v1 [q-bio.QM])
47. S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning. (arXiv:2207.12819v1 [cs.CV])
48. Lifelong DP: Consistently Bounded Differential Privacy in Lifelong Machine Learning. (arXiv:2207.12831v1 [cs.LG])
49. Partial-Monotone Adaptive Submodular Maximization. (arXiv:2207.12840v1 [cs.LG])
50. Extreme compression of sentence-transformer ranker models: faster inference, longer battery life, and less storage on edge devices. (arXiv:2207.12852v1 [cs.LG])
51. Variational multiscale reinforcement learning for discovering reduced order closure models of nonlinear spatiotemporal transport systems. (arXiv:2207.12854v1 [cs.LG])
52. Efficient Learning of Accurate Surrogates for Simulations of Complex Systems. (arXiv:2207.12855v1 [cs.LG])
53. Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition. (arXiv:2207.12866v1 [eess.AS])
54. Generalized Probabilistic U-Net for medical image segementation. (arXiv:2207.12872v1 [cs.CV])
55. Repeated Environment Inference for Invariant Learning. (arXiv:2207.12876v1 [cs.LG])
56. Representing Random Utility Choice Models with Neural Networks. (arXiv:2207.12877v1 [cs.LG])
57. Enhancing Collaborative Filtering Recommender with Prompt-Based Sentiment Analysis. (arXiv:2207.12883v1 [cs.IR])
58. CFLIT: Coexisting Federated Learning and Information Transfer. (arXiv:2207.12884v1 [cs.IT])
59. Learning-Augmented Maximum Flow. (arXiv:2207.12911v1 [cs.DS])
60. A Guide to Image and Video based Small Object Detection using Deep Learning : Case Study of Maritime Surveillance. (arXiv:2207.12926v1 [cs.CV])
61. Demystifying Graph Convolution with a Simple Concatenation. (arXiv:2207.12931v1 [cs.LG])
62. Hyperdimensional Computing vs. Neural Networks: Comparing Architecture and Learning Process. (arXiv:2207.12932v1 [cs.NE])
63. From Interpretable Filters to Predictions of Convolutional Neural Networks with Explainable Artificial Intelligence. (arXiv:2207.12958v1 [cs.LG])
64. Efficient Algorithms for Sparse Moment Problems without Separation. (arXiv:2207.13008v1 [cs.LG])
65. Modeling the Social Influence of COVID-19 via Personalized Propagation with Deep Learning. (arXiv:2207.13016v1 [cs.SI])
66. Is Attention Interpretation? A Quantitative Assessment On Sets. (arXiv:2207.13018v1 [cs.LG])
67. Improved and Interpretable Defense to Transferred Adversarial Examples by Jacobian Norm with Selective Input Gradient Regularization. (arXiv:2207.13036v1 [cs.LG])
68. Domain Adaptation under Open Set Label Shift. (arXiv:2207.13048v1 [cs.LG])
69. Coronavirus disease situation analysis and prediction using machine learning: a study on Bangladeshi population. (arXiv:2207.13056v1 [cs.LG])
70. Finding Deep-Learning Compilation Bugs with NNSmith. (arXiv:2207.13066v1 [cs.LG])
71. DeFakePro: Decentralized DeepFake Attacks Detection using ENF Authentication. (arXiv:2207.13070v1 [cs.CR])
72. Future-Dependent Value-Based Off-Policy Evaluation in POMDPs. (arXiv:2207.13081v1 [cs.LG])
73. Offline Reinforcement Learning at Multiple Frequencies. (arXiv:2207.13082v1 [cs.LG])
74. Task Agnostic and Post-hoc Unseen Distribution Detection. (arXiv:2207.13083v1 [cs.LG])
75. Sharp Concentration Results for Heavy-Tailed Distributions. (arXiv:2003.13819v3 [math.PR] UPDATED)
76. Dualize, Split, Randomize: Toward Fast Nonsmooth Optimization Algorithms. (arXiv:2004.02635v4 [math.OC] UPDATED)
77. Buffer Pool Aware Query Scheduling via Deep Reinforcement Learning. (arXiv:2007.10568v2 [cs.DB] UPDATED)
78. Collaborative Three-Tier Architecture Non-contact Respiratory Rate Monitoring using Target Tracking and False Peaks Eliminating Algorithms. (arXiv:2011.08482v4 [cs.RO] UPDATED)
79. Sparse Signal Models for Data Augmentation in Deep Learning ATR. (arXiv:2012.09284v2 [cs.CV] UPDATED)
80. Evolving Reinforcement Learning Algorithms. (arXiv:2101.03958v5 [cs.LG] UPDATED)
81. Modeling Financial Products and their Supply Chains. (arXiv:2102.02329v2 [cs.LG] UPDATED)
82. Alleviation of Temperature Variation Induced Accuracy Deg-radation in Ferroelectric FinFET Based Neural Network. (arXiv:2103.03111v4 [cs.LG] UPDATED)
83. Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v3 [cs.LG] UPDATED)
84. Heterogeneous Contrastive Learning. (arXiv:2105.09401v3 [cs.LG] UPDATED)
85. Federated Learning with Positive and Unlabeled Data. (arXiv:2106.10904v2 [cs.LG] UPDATED)
86. Solution of Physics-based Bayesian Inverse Problems with Deep Generative Priors. (arXiv:2107.02926v2 [stat.ML] UPDATED)
87. Few-Shot Domain Adaptation For End-to-End Communication. (arXiv:2108.00874v2 [cs.LG] UPDATED)
88. safe-control-gym: a Unified Benchmark Suite for Safe Learning-based Control and Reinforcement Learning in Robotics. (arXiv:2109.06325v4 [cs.RO] UPDATED)
89. Quantifying Inequality in Underreported Medical Conditions. (arXiv:2110.04133v2 [cs.CY] UPDATED)
90. Discriminative Multimodal Learning via Conditional Priors in Generative Models. (arXiv:2110.04616v2 [cs.LG] UPDATED)
91. Making Corgis Important for Honeycomb Classification: Adversarial Attacks on Concept-based Explainability Tools. (arXiv:2110.07120v2 [cs.LG] UPDATED)
92. Guiding Visual Question Generation. (arXiv:2110.08226v3 [cs.LG] UPDATED)
93. Iterative Teaching by Label Synthesis. (arXiv:2110.14432v3 [cs.LG] UPDATED)
94. Sliced Recursive Transformer. (arXiv:2111.05297v3 [cs.CV] UPDATED)
95. Modeling Irregular Time Series with Continuous Recurrent Units. (arXiv:2111.11344v3 [cs.LG] UPDATED)
96. Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning. (arXiv:2201.03529v2 [cs.LG] UPDATED)
97. An Adaptive Deep Clustering Pipeline to Inform Text Labeling at Scale. (arXiv:2202.01211v2 [cs.CL] UPDATED)
98. Verification-Aided Deep Ensemble Selection. (arXiv:2202.03898v2 [cs.LG] UPDATED)
99. Rethinking Pareto Frontier for Performance Evaluation of Deep Neural Networks. (arXiv:2202.09275v4 [cs.LG] UPDATED)
100. Cooperative Behavior Planning for Automated Driving using Graph Neural Networks. (arXiv:2202.11376v2 [cs.RO] UPDATED)
101. BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task. (arXiv:2202.13174v3 [cs.CL] UPDATED)
102. The Optimal Noise in Noise-Contrastive Learning Is Not What You Think. (arXiv:2203.01110v2 [stat.ML] UPDATED)
103. KamNet: An Integrated Spatiotemporal Deep Neural Network for Rare Event Search in KamLAND-Zen. (arXiv:2203.01870v5 [physics.ins-det] UPDATED)
104. Variational Inference with Locally Enhanced Bounds for Hierarchical Models. (arXiv:2203.04432v2 [cs.LG] UPDATED)
105. Kan Extensions in Data Science and Machine Learning. (arXiv:2203.09018v2 [cs.LG] UPDATED)
106. PACS: A Dataset for Physical Audiovisual CommonSense Reasoning. (arXiv:2203.11130v2 [cs.LG] UPDATED)
107. Probing Speech Emotion Recognition Transformers for Linguistic Knowledge. (arXiv:2204.00400v2 [cs.CL] UPDATED)
108. Federated Learning for Energy-limited Wireless Networks: A Partial Model Aggregation Approach. (arXiv:2204.09746v2 [cs.LG] UPDATED)
109. Engineering flexible machine learning systems by traversing functionally invariant paths in weight space. (arXiv:2205.00334v3 [cs.LG] UPDATED)
110. SKILL-IL: Disentangling Skill and Knowledge in Multitask Imitation Learning. (arXiv:2205.03130v2 [cs.LG] UPDATED)
111. Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v3 [cs.AI] UPDATED)
112. Multi-agent Databases via Independent Learning. (arXiv:2205.14323v2 [cs.DB] UPDATED)
113. A Model of One-Shot Generalization. (arXiv:2205.14553v2 [cs.LG] UPDATED)
114. Unsupervised Image Representation Learning with Deep Latent Particles. (arXiv:2205.15821v2 [cs.CV] UPDATED)
115. Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v3 [cs.CV] UPDATED)
116. Robustness Implies Generalization via Data-Dependent Generalization Bounds. (arXiv:2206.13497v3 [cs.LG] UPDATED)
117. Autonomous Intraluminal Navigation of a Soft Robot using Deep-Learning-based Visual Servoing. (arXiv:2207.00401v2 [cs.RO] UPDATED)
118. Folding over Neural Networks. (arXiv:2207.01090v2 [cs.PL] UPDATED)
119. DLME: Deep Local-flatness Manifold Embedding. (arXiv:2207.03160v2 [cs.LG] UPDATED)
120. HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python. (arXiv:2207.03517v2 [stat.ML] UPDATED)
121. LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. (arXiv:2207.04429v2 [cs.RO] UPDATED)
122. IMG-NILM: A Deep learning NILM approach using energy heatmaps. (arXiv:2207.05463v2 [cs.LG] UPDATED)
123. Exploring Adversarial Examples and Adversarial Robustness of Convolutional Neural Networks by Mutual Information. (arXiv:2207.05756v2 [cs.LG] UPDATED)
124. Masked Autoencoders that Listen. (arXiv:2207.06405v2 [cs.SD] UPDATED)
125. Implicit Regularization with Polynomial Growth in Deep Tensor Factorization. (arXiv:2207.08942v2 [cs.LG] UPDATED)
126. A Deep Learning Framework for Wind Turbine Repair Action Prediction Using Alarm Sequences and Long Short Term Memory Algorithms. (arXiv:2207.09457v2 [cs.LG] UPDATED)
127. Learning Physics from the Machine: An Interpretable Boosted Decision Tree Analysis for the Majorana Demonstrator. (arXiv:2207.10710v2 [physics.data-an] UPDATED)
128. Federated Semi-Supervised Domain Adaptation via Knowledge Transfer. (arXiv:2207.10727v2 [cs.LG] UPDATED)
129. OCTAL: Graph Representation Learning for LTL Model Checking. (arXiv:2207.11649v2 [cs.PL] UPDATED)
130. Generative Subgraph Contrast for Self-Supervised Graph Representation Learning. (arXiv:2207.11996v2 [cs.LG] UPDATED)
131. A Confident Deep Learning loss function for one-step Conformal Prediction approximation. (arXiv:2207.12377v2 [cs.LG] UPDATED)
132. Revisiting Parameter Reuse to Overcome Catastrophic Forgetting in Neural Networks. (arXiv:2207.11005v1 [cs.LG] CROSS LISTED)
## cs.AI
---
**58** new papers in cs.AI:-) 
1. Overwatch: Learning Patterns in Code Edit Sequences. (arXiv:2207.12456v1 [cs.PL])
2. Provably Efficient Fictitious Play Policy Optimization for Zero-Sum Markov Games with Structured Transitions. (arXiv:2207.12463v1 [cs.LG])
3. Optimizing Empty Container Repositioning and Fleet Deployment via Configurable Semi-POMDPs. (arXiv:2207.12509v1 [cs.LG])
4. Trainability Preserving Neural Structured Pruning. (arXiv:2207.12534v1 [cs.LG])
5. Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v1 [cs.LG])
6. Translating a Visual LEGO Manual to a Machine-Executable Plan. (arXiv:2207.12572v1 [cs.CV])
7. WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v1 [cs.CL])
8. How should I compute my candidates? A taxonomy and classification of diagnosis computation algorithms. (arXiv:2207.12583v1 [cs.AI])
9. Classifier-Free Diffusion Guidance. (arXiv:2207.12598v1 [cs.LG])
10. A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics. (arXiv:2207.12599v1 [cs.LG])
11. Bundle MCR: Towards Conversational Bundle Recommendation. (arXiv:2207.12628v1 [cs.IR])
12. Learning Bipedal Walking On Planned Footsteps For Humanoid Robots. (arXiv:2207.12644v1 [cs.RO])
13. Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v1 [cs.CV])
14. Single MCMC Chain Parallelisation on Decision Trees. (arXiv:2207.12688v1 [cs.AI])
15. Advanced Conditional Variational Autoencoders (A-CVAE): Towards interpreting open-domain conversation generation via disentangling latent feature representation. (arXiv:2207.12696v1 [cs.CL])
16. XInsight: eXplainable Data Analysis Through The Lens of Causality. (arXiv:2207.12718v1 [cs.DB])
17. Distribution Learning Based on Evolutionary Algorithm Assisted Deep Neural Networks for Imbalanced Image Classification. (arXiv:2207.12744v1 [cs.CV])
18. Using Abstraction for Interpretable Robot Programs in Stochastic Domains. (arXiv:2207.12763v1 [cs.AI])
19. Clustering Object-Centric Event Logs. (arXiv:2207.12764v1 [cs.AI])
20. Reconciling Security and Communication Efficiency in Federated Learning. (arXiv:2207.12779v1 [cs.LG])
21. Neural Design for Genetic Perturbation Experiments. (arXiv:2207.12805v1 [q-bio.QM])
22. Lifelong DP: Consistently Bounded Differential Privacy in Lifelong Machine Learning. (arXiv:2207.12831v1 [cs.LG])
23. Towards Smart City Security: Violence and Weaponized Violence Detection using DCNN. (arXiv:2207.12850v1 [cs.CV])
24. Transition1x -- a Dataset for Building Generalizable Reactive Machine Learning Potentials. (arXiv:2207.12858v1 [physics.chem-ph])
25. LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection. (arXiv:2207.12888v1 [cs.CV])
26. Semantic Segmentation for Autonomous Driving: Model Evaluation, Dataset Generation, Perspective Comparison, and Real-Time Capability. (arXiv:2207.12939v1 [cs.CV])
27. Improved and Interpretable Defense to Transferred Adversarial Examples by Jacobian Norm with Selective Input Gradient Regularization. (arXiv:2207.13036v1 [cs.LG])
28. Coronavirus disease situation analysis and prediction using machine learning: a study on Bangladeshi population. (arXiv:2207.13056v1 [cs.LG])
29. NewsStories: Illustrating articles with visual summaries. (arXiv:2207.13061v1 [cs.CV])
30. Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis. (arXiv:2207.13064v1 [cs.CV])
31. Offline Reinforcement Learning at Multiple Frequencies. (arXiv:2207.13082v1 [cs.LG])
32. Task Agnostic and Post-hoc Unseen Distribution Detection. (arXiv:2207.13083v1 [cs.LG])
33. Advanced Semantics for Commonsense Knowledge Extraction. (arXiv:2011.00905v3 [cs.AI] UPDATED)
34. Evolving Reinforcement Learning Algorithms. (arXiv:2101.03958v5 [cs.LG] UPDATED)
35. DiffSRL: Learning Dynamical State Representation for Deformable Object Manipulation with Differentiable Simulator. (arXiv:2110.12352v2 [cs.RO] UPDATED)
36. Iterative Teaching by Label Synthesis. (arXiv:2110.14432v3 [cs.LG] UPDATED)
37. Sliced Recursive Transformer. (arXiv:2111.05297v3 [cs.CV] UPDATED)
38. Pay More Attention to History: A Context Modelling Strategy for Conversational Text-to-SQL. (arXiv:2112.08735v2 [cs.CL] UPDATED)
39. Contrastive Learning of Semantic and Visual Representations for Text Tracking. (arXiv:2112.14976v2 [cs.CV] UPDATED)
40. BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task. (arXiv:2202.13174v3 [cs.CL] UPDATED)
41. PACS: A Dataset for Physical Audiovisual CommonSense Reasoning. (arXiv:2203.11130v2 [cs.LG] UPDATED)
42. Self-supervised Semantic Segmentation Grounded in Visual Concepts. (arXiv:2203.13868v2 [cs.CV] UPDATED)
43. MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v2 [cs.CV] UPDATED)
44. Engineering flexible machine learning systems by traversing functionally invariant paths in weight space. (arXiv:2205.00334v3 [cs.LG] UPDATED)
45. Video Extrapolation in Space and Time. (arXiv:2205.02084v3 [cs.CV] UPDATED)
46. Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v3 [cs.AI] UPDATED)
47. Unsupervised Image Representation Learning with Deep Latent Particles. (arXiv:2205.15821v2 [cs.CV] UPDATED)
48. Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v3 [cs.CV] UPDATED)
49. Robustness Implies Generalization via Data-Dependent Generalization Bounds. (arXiv:2206.13497v3 [cs.LG] UPDATED)
50. Autonomous Intraluminal Navigation of a Soft Robot using Deep-Learning-based Visual Servoing. (arXiv:2207.00401v2 [cs.RO] UPDATED)
51. HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python. (arXiv:2207.03517v2 [stat.ML] UPDATED)
52. LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. (arXiv:2207.04429v2 [cs.RO] UPDATED)
53. Exploring Adversarial Examples and Adversarial Robustness of Convolutional Neural Networks by Mutual Information. (arXiv:2207.05756v2 [cs.LG] UPDATED)
54. Masked Autoencoders that Listen. (arXiv:2207.06405v2 [cs.SD] UPDATED)
55. Implicit Regularization with Polynomial Growth in Deep Tensor Factorization. (arXiv:2207.08942v2 [cs.LG] UPDATED)
56. A Deep Learning Framework for Wind Turbine Repair Action Prediction Using Alarm Sequences and Long Short Term Memory Algorithms. (arXiv:2207.09457v2 [cs.LG] UPDATED)
57. Generative Subgraph Contrast for Self-Supervised Graph Representation Learning. (arXiv:2207.11996v2 [cs.LG] UPDATED)
58. Revisiting Parameter Reuse to Overcome Catastrophic Forgetting in Neural Networks. (arXiv:2207.11005v1 [cs.LG] CROSS LISTED)

