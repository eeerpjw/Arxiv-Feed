# Your interest papers
---
## cs.CV
---
### Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata. (arXiv:2206.01813v1 [cs.CV])
- Authors : Seonghyeon Nam, Abhijith Punnappurath
- Link : [http://arxiv.org/abs/2206.01813](http://arxiv.org/abs/2206.01813)
> ABSTRACT  :  Most camera images are rendered and saved in the standard RGB (sRGB) format by the camera's hardware. Due to the in-camera photo-finishing routines, nonlinear sRGB images are undesirable for computer vision tasks that assume a direct relationship between pixel values and scene radiance. For such applications, linear raw-RGB sensor images are preferred. Saving images in their raw-RGB format is still uncommon due to the large storage requirement and lack of support by many imaging applications. Several "raw reconstruction" methods have been proposed that utilize specialized metadata sampled from the raw-RGB image at capture time and embedded in the sRGB image. This metadata is used to parameterize a mapping function to de-render the sRGB image back to its original raw-RGB format when needed. Existing raw reconstruction methods rely on simple sampling strategies and global mapping to perform the de-rendering. This paper shows how to improve the de-rendering results by jointly learning sampling and reconstruction. Our experiments show that our learned sampling can adapt to the image content to produce better raw reconstructions than existing methods. We also describe an online fine-tuning strategy for the reconstruction network to improve results further.  
### Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image. (arXiv:2206.01856v1 [cs.CV])
- Authors : Khang Ta, Abhishek Aich, Akash Gupta
- Link : [http://arxiv.org/abs/2206.01856](http://arxiv.org/abs/2206.01856)
> ABSTRACT  :  Image **enhancement** approaches often assume that the noise is signal independent, and approximate the degradation model as zero-mean additive Gaussian noise. However, this assumption does not hold for biomedical imaging systems where sensor-based sources of noise are proportional to signal strengths, and the noise is better represented as a Poisson process. In this work, we explore a sparsity and dictionary learning-based approach and present a novel self-supervised learning method for single-image denoising where the noise is approximated as a Poisson process, requiring no clean ground-truth data. Specifically, we approximate traditional iterative optimization algorithms for image denoising with a recurrent neural network which enforces sparsity with respect to the weights of the network. Since the sparse representations are based on the underlying image, it is able to suppress the spurious components (noise) in the image patches, thereby introducing implicit regularization for denoising task through the network structure. Experiments on two bio-imaging datasets demonstrate that our method outperforms the state-of-the-art approaches in terms of PSNR and SSIM. Our qualitative results demonstrate that, in addition to higher performance on standard quantitative metrics, we are able to recover much more subtle details than other compared approaches.  
### Face Recognition Accuracy Across Demographics: Shining a Light Into the Problem. (arXiv:2206.01881v1 [cs.CV])
- Authors : Haiyu Wu, tor Albiero
- Link : [http://arxiv.org/abs/2206.01881](http://arxiv.org/abs/2206.01881)
> ABSTRACT  :  This is the first work that we are aware of to explore how the level of brightness of the skin region in a pair of face images impacts face recognition accuracy. Image pairs with both images having mean face skin brightness in an upper-middle range of brightness are found to have the highest matching accuracy across demographics and matchers. Image pairs with both images having mean face skin brightness that is too **dark** or too light are found to have an increased false match rate (FMR). Image pairs with strongly different face skin brightness are found to have decreased FMR and increased false non-match rate (FNMR). Using a brightness information metric that captures the variation in brightness in the face skin region, the variation in matching accuracy is shown to correlate with the level of information available in the face skin region. For operational scenarios where image acquisition is controlled, we propose acquiring images with lighting adjusted to yield face skin brightness in a narrow range.  
### Implicit Neural Representation for Mesh-Free Inverse Obstacle Scattering. (arXiv:2206.02027v1 [cs.CV])
- Authors : Tin Vla, Hieu Nguyen, Ivan Dokmani
- Link : [http://arxiv.org/abs/2206.02027](http://arxiv.org/abs/2206.02027)
> ABSTRACT  :  Implicit representation of shapes as level sets of multilayer perceptrons has recently flourished in different shape analysis, compression, and reconstruction tasks. In this paper, we introduce an **implicit neural representation**-based framework for solving the inverse obstacle scattering problem in a mesh-free fashion. We efficiently express the obstacle shape as the zero-level set of a signed distance function which is implicitly determined by a small number of network parameters. To solve the direct scattering problem, we implement the implicit boundary integral method. It uses projections of the grid points in the tubular neighborhood onto the boundary to compute the PDE solution instead of a grid-size-dependent extraction method of surface points such as Marching Cubes. The implicit representation conveniently handles the shape perturbation in the optimization process. To update the shape, we use PyTorch's automatic differentiation to backpropagate the loss function w.r.t. the network parameters, allowing us to avoid complex and error-prone manual derivation of the shape derivative. The proposed framework makes the inverse scattering problem more tractable with fewer parameters to optimize in comparison to the memory-inefficient grid-based approaches and outputs high-quality reconstruction results.  
### PIDNet: A **Real-time** Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v1 [cs.CV])
- Authors : Jiacong Xu, Zixiang Xiong
- Link : [http://arxiv.org/abs/2206.02066](http://arxiv.org/abs/2206.02066)
> ABSTRACT  :  Two-branch network architecture has shown its efficiency and effectiveness for real-time semantic segmentation tasks. However, direct fusion of low-level details and high-level semantics will lead to a phenomenon that the detailed features are easily overwhelmed by surrounding contextual information, namely overshoot in this paper, which limits the improvement of the accuracy of existed two-branch models. In this paper, we bridge a connection between Convolutional Neural Network (CNN) and Proportional-Integral-Derivative (PID) controller and reveal that the two-branch network is nothing but a Proportional-Integral (PI) controller, which inherently suffers from the similar overshoot issue. To alleviate this issue, we propose a novel three-branch network architecture: PIDNet, which possesses three branches to parse the detailed, context and boundary information (derivative of semantics), respectively, and employs boundary attention to guide the fusion of detailed and context branches in final stage. The family of PIDNets achieve the best trade-off between inference speed and accuracy and their test accuracy surpasses all the existed models with similar inference speed on Cityscapes, CamVid and COCO-Stuff datasets. Especially, PIDNet-S achieves 78.6% mIOU with inference speed of 93.2 FPS on Cityscapes test set and 81.6% mIOU with speed of 153.7 FPS on CamVid test set.  
### All One Needs to Know about Priors for Deep Image **Restoration** and **Enhancement**: A Survey. (arXiv:2206.02070v1 [cs.CV])
- Authors : Yunfan Lu, Yiqi Lin, Hao Wu, Yunhao Luo, Xu Zheng, Lin Wang
- Link : [http://arxiv.org/abs/2206.02070](http://arxiv.org/abs/2206.02070)
> ABSTRACT  :  Image **restoration** and **enhancement** is a process of improving the image quality by removing degradations, such as noise, blur, and resolution degradation. Deep learning (DL) has recently been applied to image **restoration** and **enhancement**. Due to its ill-posed property, plenty of works have explored priors to facilitate training deep neural networks (DNNs). However, the importance of priors has not been systematically studied and analyzed by far in the research community. Therefore, this paper serves as the first study that provides a comprehensive overview of recent advancements of priors for deep image **restoration** and **enhancement**. Our work covers five primary contents: (1) A theoretical analysis of priors for deep image **restoration** and **enhancement**; (2) A hierarchical and structural taxonomy of priors commonly used in the DL-based methods; (3) An insightful discussion on each prior regarding its principle, potential, and applications; (4) A summary of crucial problems by highlighting the potential future directions to spark more research in the community; (5) An open-source repository that provides a taxonomy of all mentioned works and code links.  
### LDRNet: Enabling **Real-time** Document Localization on Mobile Devices. (arXiv:2206.02136v1 [cs.CV])
- Authors : Han Wu, Holland Qian, Huaming Wu
- Link : [http://arxiv.org/abs/2206.02136](http://arxiv.org/abs/2206.02136)
> ABSTRACT  :  While Identity Document Verification (IDV) technology on mobile devices becomes ubiquitous in modern business operations, the risk of identity theft and fraud is increasing. The identity document holder is normally required to participate in an online video interview to circumvent impostors. However, the current IDV process depends on an additional human workforce to support online step-by-step guidance which is inefficient and expensive. The performance of existing AI-based approaches cannot meet the real-time and lightweight demands of mobile devices. In this paper, we address those challenges by designing an edge intelligence-assisted approach for real-time IDV. Aiming at improving the responsiveness of the IDV process, we propose a new document localization model for mobile devices, LDRNet, to Localize the identity Document in **Real-time**. On the basis of a lightweight backbone network, we build three prediction branches for LDRNet, the corner points prediction, the line borders prediction and the document classification. We design novel supplementary targets, the equal-division points, and use a new loss function named Line Loss, to improve the speed and accuracy of our approach. In addition to the IDV process, LDRNet is an efficient and reliable document localization alternative for all kinds of mobile applications. As a matter of proof, we compare the performance of LDRNet with other popular approaches on localizing general document datasets. The experimental results show that LDRNet runs at a speed up to 790 FPS which is 47x faster, while still achieving comparable Jaccard Index(JI) in single-model and single-scale tests.  
### Recurrent Video **Restoration** Transformer with Guided Deformable Attention. (arXiv:2206.02146v1 [cs.CV])
- Authors : Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2206.02146](http://arxiv.org/abs/2206.02146)
> ABSTRACT  :  Video **restoration** aims at restoring multiple high-quality frames from multiple low-quality frames. Existing video **restoration** methods generally fall into two extreme cases, i.e., they either restore all frames in parallel or restore the video frame by frame in a recurrent way, which would result in different merits and drawbacks. Typically, the former has the advantage of temporal information fusion. However, it suffers from large model size and intensive memory consumption; the latter has a relatively small model size as it shares parameters across frames; however, it lacks long-range dependency modeling ability and parallelizability. In this paper, we attempt to integrate the advantages of the two cases by proposing a recurrent video **restoration** transformer, namely RVRT. RVRT processes local neighboring frames in parallel within a globally recurrent framework which can achieve a good trade-off between model size, effectiveness, and efficiency. Specifically, RVRT divides the video into multiple clips and uses the previously inferred clip feature to estimate the subsequent clip feature. Within each clip, different frame features are jointly updated with implicit feature aggregation. Across different clips, the guided deformable attention is designed for clip-to-clip alignment, which predicts multiple relevant locations from the whole inferred clip and aggregates their features by the attention mechanism. Extensive experiments on video super-resolution, deblurring, and denoising show that the proposed RVRT achieves state-of-the-art performance on benchmark datasets with balanced model size, testing memory and runtime.  
### Semi-Supervised Learning for Mars Imagery Classification and Segmentation. (arXiv:2206.02180v1 [cs.CV])
- Authors : Wenjing Wang, Lilang Lin, Zejia Fan, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2206.02180](http://arxiv.org/abs/2206.02180)
> ABSTRACT  :  With the progress of Mars exploration, numerous Mars image data are collected and need to be analyzed. However, due to the imbalance and distortion of Martian data, the performance of existing computer vision models is unsatisfactory. In this paper, we introduce a semi-supervised framework for machine vision on Mars and try to resolve two specific tasks: classification and segmentation. Contrastive learning is a powerful representation learning technique. However, there is too much information overlap between Martian data samples, leading to a contradiction between contrastive learning and Martian data. Our key idea is to reconcile this contradiction with the help of annotations and further take advantage of unlabeled data to improve performance. For classification, we propose to ignore inner-class pairs on labeled data as well as neglect negative pairs on unlabeled data, forming supervised inter-class contrastive learning and unsupervised similarity learning. For segmentation, we extend supervised inter-class contrastive learning into an element-wise mode and use online pseudo labels for supervision on unlabeled areas. Experimental results show that our learning strategies can improve the classification and segmentation models by a large margin and outperform state-of-the-art approaches.  
### FOF: Learning Fourier Occupancy Field for Monocular **Real-time** Human Reconstruction. (arXiv:2206.02194v1 [cs.CV])
- Authors : Qiao Feng, Yebin Liu, Kun Lai, Jingyu Yang, Kun Li
- Link : [http://arxiv.org/abs/2206.02194](http://arxiv.org/abs/2206.02194)
> ABSTRACT  :  The advent of deep learning has led to significant progress in monocular human reconstruction. However, existing representations, such as parametric models, voxel grids, meshes and **implicit neural representation**s, have difficulties achieving high-quality results and real-time speed at the same time. In this paper, we propose Fourier Occupancy Field (FOF), a novel powerful, efficient and flexible 3D representation, for monocular real-time and accurate human reconstruction. The FOF represents a 3D object with a 2D field orthogonal to the view direction where at each 2D position the occupancy field of the object along the view direction is compactly represented with the first few terms of Fourier series, which retains the topology and neighborhood relation in the 2D domain. A FOF can be stored as a multi-channel image, which is compatible with 2D convolutional neural networks and can bridge the gap between 3D geometries and 2D images. The FOF is very flexible and extensible, e.g., parametric models can be easily integrated into a FOF as a prior to generate more robust results. Based on FOF, we design the first 30+FPS high-fidelity real-time monocular human reconstruction framework. We demonstrate the potential of FOF on both public dataset and real captured data. The code will be released for research purposes.  
### Two Decades of Bengali Handwritten Digit Recognition: A Survey. (arXiv:2206.02234v1 [cs.CV])
- Authors : Ashikur Rahman, Bakhtiar Hasan, Sabbir Ahmed, Tasnim Ahmed, Hamjajul Ashmafee, Mohammad Ridwan, Hasanul Kabir
- Link : [http://arxiv.org/abs/2206.02234](http://arxiv.org/abs/2206.02234)
> ABSTRACT  :  Handwritten Digit Recognition (**HDR**) is one of the most challenging tasks in the domain of Optical Character Recognition (OCR). Irrespective of language, there are some inherent challenges of **HDR**, which mostly arise due to the variations in writing styles across individuals, writing medium and environment, inability to maintain the same strokes while writing any digit repeatedly, etc. In addition to that, the structural complexities of the digits of a particular language may lead to ambiguous scenarios of **HDR**. Over the years, researchers have developed numerous offline and online **HDR** pipelines, where different image processing techniques are combined with traditional Machine Learning (ML)-based and/or Deep Learning (DL)-based architectures. Although evidence of extensive review studies on **HDR** exists in the literature for languages, such as: English, Arabic, Indian, Farsi, Chinese, etc., few surveys on Bengali **HDR** (B**HDR**) can be found, which lack a comprehensive analysis of the challenges, the underlying recognition process, and possible future directions. In this paper, the characteristics and inherent ambiguities of Bengali handwritten digits along with a comprehensive insight of two decades of the state-of-the-art datasets and approaches towards offline B**HDR** have been analyzed. Furthermore, several real-life application-specific studies, which involve B**HDR**, have also been discussed in detail. This paper will also serve as a compendium for researchers interested in the science behind offline B**HDR**, instigating the exploration of newer avenues of relevant research that may further lead to better offline recognition of Bengali handwritten digits in different application areas.  
### HIFI-Net: A Novel Network for **Enhancement** to Underwater Images. (arXiv:2206.02295v1 [cs.CV])
- Authors : Jiajia Zhou, Junbin Zhuang, Yan Zheng, Di Wu
- Link : [http://arxiv.org/abs/2206.02295](http://arxiv.org/abs/2206.02295)
> ABSTRACT  :  A novel network for **enhancement** to underwater images is proposed in this paper. It contains a Reinforcement Fusion Module for Haar wavelet images (RFM-Haar) based on Reinforcement Fusion Unit (RFU), which is used to fuse an original image and some important information within it. Fusion is achieved for better **enhancement**. As this network make "Haar Images into Fusion Images", it is called HIFI-Net. The experimental results show the proposed HIFI-Net performs best among many state-of-the-art methods on three datasets at three normal metrics and a new metric.  
### How to Train Your Dragon: Tamed Warping Network for Semantic Video Segmentation. (arXiv:2005.01344v3 [cs.CV] UPDATED)
- Authors : Junyi Feng, Songyuan Li, Yifeng Chen, Fuxian Huang, Jiabao Cui, Xi Li
- Link : [http://arxiv.org/abs/2005.01344](http://arxiv.org/abs/2005.01344)
> ABSTRACT  :  **Real-time** semantic segmentation on high-resolution videos is challenging due to the strict requirements of speed. Recent approaches have utilized the inter-frame continuity to reduce redundant computation by warping the feature maps across adjacent frames, greatly speeding up the inference phase. However, their accuracy drops significantly owing to the imprecise motion estimation and error accumulation. In this paper, we propose to introduce a simple and effective correction stage right after the warping stage to form a framework named Tamed Warping Network (TWNet), aiming to improve the accuracy and robustness of warping-based models. The experimental results on the Cityscapes dataset show that with the correction, the accuracy (mIoU) significantly increases from 67.3% to 71.6%, and the speed edges down from 65.5 FPS to 61.8 FPS. For non-rigid categories such as "human" and "object", the improvements of IoU are even higher than 18 percentage points.  
### Multimodal Object Detection via Probabilistic Ensembling. (arXiv:2104.02904v2 [cs.CV] UPDATED)
- Authors : Ting Chen, Jinghao Shi, Zelin Ye, Christoph Mertz, Shu Kong, Deva Ramanan
- Link : [http://arxiv.org/abs/2104.02904](http://arxiv.org/abs/2104.02904)
> ABSTRACT  :  Object detection with multimodal inputs can improve many safety-critical systems such as autonomous vehicles (AVs). Motivated by AVs that operate in both day and **night**, we study multimodal object detection with RGB and thermal cameras, since the latter provides much stronger object signatures under poor illumination. We explore strategies for fusing information from different modalities. Our key contribution is a probabilistic ensembling technique, ProbEn, a simple non-learned method that fuses together detections from multi-modalities. We derive ProbEn from Bayes' rule and first principles that assume conditional independence across modalities. Through probabilistic marginalization, ProbEn elegantly handles missing modalities when detectors do not fire on the same object. Importantly, ProbEn also notably improves multimodal detection even when the conditional independence assumption does not hold, e.g., fusing outputs from other fusion methods (both off-the-shelf and trained in-house). We validate ProbEn on two benchmarks containing both aligned (KAIST) and unaligned (FLIR) multimodal images, showing that ProbEn outperforms prior work by more than 13% in relative performance!  
### Very Lightweight Photo Retouching Network with Conditional Sequential Modulation. (arXiv:2104.06279v2 [cs.CV] UPDATED)
- Authors : Yihao Liu, Jingwen He, Xiangyu Chen, Zhengwen Zhang, Hengyuan Zhao, Chao Dong, Yu Qiao
- Link : [http://arxiv.org/abs/2104.06279](http://arxiv.org/abs/2104.06279)
> ABSTRACT  :  Photo retouching aims at improving the aesthetic visual quality of images that suffer from photographic defects, especially for poor contrast, over/under **exposure**, and inharmonious saturation. In practice, photo retouching can be accomplished by a series of image processing operations. As most commonly-used retouching operations are pixel-independent, i.e., the manipulation on one pixel is uncorrelated with its neighboring pixels, we can take advantage of this property and design a specialized algorithm for efficient global photo retouching. We analyze these global operations and find that they can be mathematically formulated by a Multi-Layer Perceptron (MLP). Based on this observation, we propose an extremely lightweight framework -- Conditional Sequential Retouching Network (CSRNet). Benefiting from the utilization of $1\times1$ convolution, CSRNet only contains less than 37K trainable parameters, which are orders of magnitude smaller than existing learning-based methods. Experiments show that our method achieves state-of-the-art performance on the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. In addition to achieve global photo retouching, the proposed framework can be easily extended to learn local **enhancement** effects. The extended model, namely CSRNet-L, also achieves competitive results in various local **enhancement** tasks. Codes are available at https://github.com/lyh-18/CSRNet.  
### LLVIP: A Visible-infrared Paired Dataset for **Low-light** Vision. (arXiv:2108.10831v3 [cs.CV] UPDATED)
- Authors : Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, Shengjie Liu, Wenli Zhou
- Link : [http://arxiv.org/abs/2108.10831](http://arxiv.org/abs/2108.10831)
> ABSTRACT  :  It is very challenging for various visual tasks such as image fusion, pedestrian detection and image-to-image translation in **low light** conditions due to the loss of effective target areas. In this case, infrared and visible images can be used together to provide both rich detail information and effective target areas. In this paper, we present LLVIP, a visible-infrared paired dataset for **low-light** vision. This dataset contains 30976 images, or 15488 pairs, most of which were taken at very **dark** scenes, and all of the images are strictly aligned in time and space. Pedestrians in the dataset are labeled. We compare the dataset with other visible-infrared datasets and evaluate the performance of some popular visual algorithms including image fusion, pedestrian detection and image-to-image translation on the dataset. The experimental results demonstrate the complementary effect of fusion on image information, and find the deficiency of existing algorithms of the three visual tasks in very **low-light** conditions. We believe the LLVIP dataset will contribute to the community of computer vision by promoting image fusion, pedestrian detection and image-to-image translation in very **low-light** applications. The dataset is being released in https://bupt-ai-cz.github.io/LLVIP. Raw data is also provided for further research such as image registration.  
### K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways. (arXiv:2110.11048v2 [cs.CV] UPDATED)
- Authors : Donghee Paek, Hyun Kong, Kevin Tirta
- Link : [http://arxiv.org/abs/2110.11048](http://arxiv.org/abs/2110.11048)
> ABSTRACT  :  Lane detection is a critical function for autonomous driving. With the recent development of deep learning and the publication of camera lane datasets and benchmarks, camera lane detection networks (CLDNs) have been remarkably developed. Unfortunately, CLDNs rely on camera images which are often distorted near the vanishing line and prone to poor lighting condition. This is in contrast with Lidar lane detection networks (LLDNs), which can directly extract the lane lines on the bird's eye view (BEV) for motion planning and operate robustly under various lighting conditions. However, LLDNs have not been actively studied, mostly due to the absence of large public lidar lane datasets. In this paper, we introduce KAIST-Lane (K-Lane), the world's first and the largest public urban road and highway lane dataset for Lidar. K-Lane has more than 15K frames and contains annotations of up to six lanes under various road and traffic conditions, e.g., occluded roads of multiple occlusion levels, roads at day and **night** times, merging (converging and diverging) and curved lanes. We also provide baseline networks we term Lidar lane detection networks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the spatial characteristics of lane lines on the point cloud, which are sparse, thin, and stretched along the entire ground plane of the point cloud. From experimental results, LLDN-GFC achieves the state-of-the-art performance with an F1- score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong performance under various lighting conditions, which is unlike CLDNs, and also robust even in the case of severe occlusions, unlike LLDNs using the conventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, and complete development kits including evaluation, visualization and annotation tools are available at https://github.com/kaist-avelab/k-lane.  
### CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation. (arXiv:2204.09914v3 [cs.CV] UPDATED)
- Authors : Xiaoyan Li, Gang Zhang, Hongyu Pan, Zhenhua Wang
- Link : [http://arxiv.org/abs/2204.09914](http://arxiv.org/abs/2204.09914)
> ABSTRACT  :  LiDAR semantic segmentation essential for advanced autonomous driving is required to be accurate, fast, and easy-deployed on mobile platforms. Previous point-based or sparse voxel-based methods are far away from real-time applications since time-consuming neighbor searching or sparse 3D convolution are employed. Recent 2D projection-based methods, including range view and multi-view fusion, can run in **real time**, but suffer from lower accuracy due to information loss during the 2D projection. Besides, to improve the performance, previous methods usually adopt test time augmentation (TTA), which further slows down the inference process. To achieve a better speed-accuracy trade-off, we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both effectiveness and efficiency mainly by the following two techniques: 1) the novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D projected grid for efficiency, while summarizes both 2D and 3D features on 3D point for minimal information loss; 2) the proposed transformation consistency loss narrows the gap between the single-time model inference and TTA. The experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the CPGNet without ensemble models or TTA is comparable with the state-of-the-art RPVNet, while it runs 4.7 times faster.  
### End-to-End Rubbing **Restoration** Using Generative Adversarial Networks. (arXiv:2205.03743v2 [cs.CV] UPDATED)
- Authors : Gongbo Sun, Zijie Zheng, Ming Zhang
- Link : [http://arxiv.org/abs/2205.03743](http://arxiv.org/abs/2205.03743)
> ABSTRACT  :  Rubbing **restoration**s are significant for preserving world cultural history. In this paper, we propose the RubbingGAN model for restoring incomplete rubbing characters. Specifically, we collect characters from the Zhang Menglong Bei and build up the first rubbing **restoration** dataset. We design the first generative adversarial network for rubbing **restoration**. Based on the dataset we collect, we apply the RubbingGAN to learn the Zhang Menglong Bei font style and restore the characters. The results of experiments show that RubbingGAN can repair both slightly and severely incomplete rubbing characters fast and effectively.  
### DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes. (arXiv:2205.15723v2 [cs.CV] UPDATED)
- Authors : Wei Liu, Pei Cao, Weijia Mao, Wenqiao Zhang, David Junhao, Jussi Keppo, Ying Shan, Xiaohu Qie, Mike Zheng
- Link : [http://arxiv.org/abs/2205.15723](http://arxiv.org/abs/2205.15723)
> ABSTRACT  :  Modeling dynamic scenes is important for many applications such as virtual reality and telepresence. Despite achieving unprecedented fidelity for novel view synthesis in dynamic scenes, existing methods based on Neural Radiance Fields (**NeRF**) suffer from slow convergence (i.e., model training time measured in days). In this paper, we present DeVRF, a novel representation to accelerate learning dynamic radiance fields. The core of DeVRF is to model both the 3D canonical space and 4D deformation field of a dynamic, non-rigid scene with explicit and discrete voxel-based representations. However, it is quite challenging to train such a representation which has a large number of model parameters, often resulting in overfitting issues. To overcome this challenge, we devise a novel static-to-dynamic learning paradigm together with a new data capture setup that is convenient to deploy in practice. This paradigm unlocks efficient learning of deformable radiance fields via utilizing the 3D volumetric canonical space learnt from multi-view static images to ease the learning of 4D voxel deformation field with only few-view dynamic sequences. To further improve the efficiency of our DeVRF and its synthesized novel view's quality, we conduct thorough explorations and identify a set of strategies. We evaluate DeVRF on both synthetic and real-world dynamic scenes with different types of deformation. Experiments demonstrate that DeVRF achieves two orders of magnitude speedup (100x faster) with on-par high-fidelity results compared to the previous state-of-the-art approaches. The code and dataset will be released in https://github.com/showlab/DeVRF.  
## eess.IV
---
### Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata. (arXiv:2206.01813v1 [cs.CV])
- Authors : Seonghyeon Nam, Abhijith Punnappurath
- Link : [http://arxiv.org/abs/2206.01813](http://arxiv.org/abs/2206.01813)
> ABSTRACT  :  Most camera images are rendered and saved in the standard RGB (sRGB) format by the camera's hardware. Due to the in-camera photo-finishing routines, nonlinear sRGB images are undesirable for computer vision tasks that assume a direct relationship between pixel values and scene radiance. For such applications, linear raw-RGB sensor images are preferred. Saving images in their raw-RGB format is still uncommon due to the large storage requirement and lack of support by many imaging applications. Several "raw reconstruction" methods have been proposed that utilize specialized metadata sampled from the raw-RGB image at capture time and embedded in the sRGB image. This metadata is used to parameterize a mapping function to de-render the sRGB image back to its original raw-RGB format when needed. Existing raw reconstruction methods rely on simple sampling strategies and global mapping to perform the de-rendering. This paper shows how to improve the de-rendering results by jointly learning sampling and reconstruction. Our experiments show that our learned sampling can adapt to the image content to produce better raw reconstructions than existing methods. We also describe an online fine-tuning strategy for the reconstruction network to improve results further.  
### Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image. (arXiv:2206.01856v1 [cs.CV])
- Authors : Khang Ta, Abhishek Aich, Akash Gupta
- Link : [http://arxiv.org/abs/2206.01856](http://arxiv.org/abs/2206.01856)
> ABSTRACT  :  Image **enhancement** approaches often assume that the noise is signal independent, and approximate the degradation model as zero-mean additive Gaussian noise. However, this assumption does not hold for biomedical imaging systems where sensor-based sources of noise are proportional to signal strengths, and the noise is better represented as a Poisson process. In this work, we explore a sparsity and dictionary learning-based approach and present a novel self-supervised learning method for single-image denoising where the noise is approximated as a Poisson process, requiring no clean ground-truth data. Specifically, we approximate traditional iterative optimization algorithms for image denoising with a recurrent neural network which enforces sparsity with respect to the weights of the network. Since the sparse representations are based on the underlying image, it is able to suppress the spurious components (noise) in the image patches, thereby introducing implicit regularization for denoising task through the network structure. Experiments on two bio-imaging datasets demonstrate that our method outperforms the state-of-the-art approaches in terms of PSNR and SSIM. Our qualitative results demonstrate that, in addition to higher performance on standard quantitative metrics, we are able to recover much more subtle details than other compared approaches.  
### Recurrent Video **Restoration** Transformer with Guided Deformable Attention. (arXiv:2206.02146v1 [cs.CV])
- Authors : Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, Luc Van
- Link : [http://arxiv.org/abs/2206.02146](http://arxiv.org/abs/2206.02146)
> ABSTRACT  :  Video **restoration** aims at restoring multiple high-quality frames from multiple low-quality frames. Existing video **restoration** methods generally fall into two extreme cases, i.e., they either restore all frames in parallel or restore the video frame by frame in a recurrent way, which would result in different merits and drawbacks. Typically, the former has the advantage of temporal information fusion. However, it suffers from large model size and intensive memory consumption; the latter has a relatively small model size as it shares parameters across frames; however, it lacks long-range dependency modeling ability and parallelizability. In this paper, we attempt to integrate the advantages of the two cases by proposing a recurrent video **restoration** transformer, namely RVRT. RVRT processes local neighboring frames in parallel within a globally recurrent framework which can achieve a good trade-off between model size, effectiveness, and efficiency. Specifically, RVRT divides the video into multiple clips and uses the previously inferred clip feature to estimate the subsequent clip feature. Within each clip, different frame features are jointly updated with implicit feature aggregation. Across different clips, the guided deformable attention is designed for clip-to-clip alignment, which predicts multiple relevant locations from the whole inferred clip and aggregates their features by the attention mechanism. Extensive experiments on video super-resolution, deblurring, and denoising show that the proposed RVRT achieves state-of-the-art performance on benchmark datasets with balanced model size, testing memory and runtime.  
### Single pixel imaging at high pixel resolutions. (arXiv:2206.02510v1 [physics.optics])
- Authors : Anna Pastuszczak, Piotr Wr
- Link : [http://arxiv.org/abs/2206.02510](http://arxiv.org/abs/2206.02510)
> ABSTRACT  :  The usually reported pixel resolution of single pixel imaging (SPI) varies between $32 \times 32$ and $256 \times 256$ pixels falling far below imaging standards with classical methods. Low resolution results from the trade-off between the acceptable compression ratio, the limited DMD modulation frequency, and reasonable reconstruction time, and has not improved significantly during the decade of intensive research on SPI. In this paper we show that image measurement at the full resolution of the DMD, which lasts only a fraction of a second, is possible for sparse images or in a situation when the field of view is limited but is a priori unknown. We propose the sampling and reconstruction strategies that enable us to reconstruct sparse images at the resolution of $1024 \times 768$ within the time of $0.3~$s. Non-sparse images are reconstructed with less details. The compression ratio is on the order of $0.4 \%$ which corresponds to an acquisition frequency of $7~$Hz. Sampling is differential, binary, and non-adaptive, and includes information on multiple partitioning of the image which later allows us to determine the actual field of view. Reconstruction is based on the differential Fourier domain regularized inversion (D-FDRI). The proposed SPI framework is an alternative to both adaptive SPI, which is challenging to implement in **real time**, and to classical compressive sensing image recovery methods, which are very slow at high resolutions.  
### Real-World Image Super-Resolution by Exclusionary Dual-Learning. (arXiv:2206.02609v1 [cs.CV])
- Authors : Hao Li, Jinghui Qin, Zhijing Yang, Pengxu Wei, Jinshan Pan, Liang Lin, Yukai Shi
- Link : [http://arxiv.org/abs/2206.02609](http://arxiv.org/abs/2206.02609)
> ABSTRACT  :  Real-world image super-resolution is a practical image **restoration** problem that aims to obtain high-quality images from in-the-wild input, has recently received considerable attention with regard to its tremendous application potentials. Although deep learning-based methods have achieved promising **restoration** quality on real-world image super-resolution datasets, they ignore the relationship between L1- and perceptual- minimization and roughly adopt auxiliary large-scale datasets for pre-training. In this paper, we discuss the image types within a corrupted image and the property of perceptual- and Euclidean- based evaluation protocols. Then we propose a method, Real-World image Super-Resolution by Exclusionary Dual-Learning (RWSR-EDL) to address the feature diversity in perceptual- and L1- based cooperative learning. Moreover, a noise-guidance data collection strategy is developed to address the training time consumption in multiple datasets optimization. When an auxiliary dataset is incorporated, RWSR-EDL achieves promising results and repulses any training time increment by adopting the noise-guidance data collection strategy. Extensive experiments show that RWSR-EDL achieves competitive performance over state-of-the-art methods on four in-the-wild image super-resolution datasets.  
### Day-to-**Night** Image Synthesis for Training **Night**time Neural ISPs. (arXiv:2206.02715v1 [cs.CV])
- Authors : Abhijith Punnappurath, Abdullah Abuolaim, Abdelrahman Abdelhamed, Alex Levinshtein
- Link : [http://arxiv.org/abs/2206.02715](http://arxiv.org/abs/2206.02715)
> ABSTRACT  :  Many flagship smartphone cameras now use a dedicated neural image signal processor (ISP) to render noisy raw sensor images to the final processed output. Training **night**mode ISP networks relies on large-scale datasets of image pairs with: (1) a noisy raw image captured with a short **exposure** and a high ISO gain; and (2) a ground truth low-noise raw image captured with a long **exposure** and low ISO that has been rendered through the ISP. Capturing such image pairs is tedious and time-consuming, requiring careful setup to ensure alignment between the image pairs. In addition, ground truth images are often prone to motion blur due to the long **exposure**. To address this problem, we propose a method that synthesizes **night**time images from daytime images. Daytime images are easy to capture, exhibit low-noise (even on smartphone cameras) and rarely suffer from motion blur. We outline a processing framework to convert daytime raw images to have the appearance of realistic **night**time raw images with different levels of noise. Our procedure allows us to easily produce aligned noisy and clean **night**time image pairs. We show the effectiveness of our synthesis framework by training neural ISPs for **night**mode rendering. Furthermore, we demonstrate that using our synthetic **night**time images together with small amounts of real data (e.g., 5% to 10%) yields performance almost on par with training exclusively on real **night**time images. Our dataset and code are available at https://github.com/SamsungLabs/day-to-**night**.  
### Compound Multi-branch Feature Fusion for Real Image **Restoration**. (arXiv:2206.02748v1 [eess.IV])
- Authors : Mao Fan, Jung Liu, Hsien Liu
- Link : [http://arxiv.org/abs/2206.02748](http://arxiv.org/abs/2206.02748)
> ABSTRACT  :  Image **restoration** is a challenging and ill-posed problem which also has been a long-standing issue. However, most of learning based **restoration** methods are proposed to target one degradation type which means they are lack of generalization. In this paper, we proposed a multi-branch **restoration** model inspired from the Human Visual System (i.e., Retinal Ganglion Cells) which can achieve multiple **restoration** tasks in a general framework. The experiments show that the proposed multi-branch architecture, called CMFNet, has competitive performance results on four datasets, including image dehazing, deraindrop, and deblurring, which are very common applications for autonomous cars. The source code and pretrained models of three **restoration** tasks are available at https://github.com/FanChiMao/CMFNet.  
## cs.LG
---
### Very Lightweight Photo Retouching Network with Conditional Sequential Modulation. (arXiv:2104.06279v2 [cs.CV] UPDATED)
- Authors : Yihao Liu, Jingwen He, Xiangyu Chen, Zhengwen Zhang, Hengyuan Zhao, Chao Dong, Yu Qiao
- Link : [http://arxiv.org/abs/2104.06279](http://arxiv.org/abs/2104.06279)
> ABSTRACT  :  Photo retouching aims at improving the aesthetic visual quality of images that suffer from photographic defects, especially for poor contrast, over/under **exposure**, and inharmonious saturation. In practice, photo retouching can be accomplished by a series of image processing operations. As most commonly-used retouching operations are pixel-independent, i.e., the manipulation on one pixel is uncorrelated with its neighboring pixels, we can take advantage of this property and design a specialized algorithm for efficient global photo retouching. We analyze these global operations and find that they can be mathematically formulated by a Multi-Layer Perceptron (MLP). Based on this observation, we propose an extremely lightweight framework -- Conditional Sequential Retouching Network (CSRNet). Benefiting from the utilization of $1\times1$ convolution, CSRNet only contains less than 37K trainable parameters, which are orders of magnitude smaller than existing learning-based methods. Experiments show that our method achieves state-of-the-art performance on the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. In addition to achieve global photo retouching, the proposed framework can be easily extended to learn local **enhancement** effects. The extended model, namely CSRNet-L, also achieves competitive results in various local **enhancement** tasks. Codes are available at https://github.com/lyh-18/CSRNet.  
### From data to functa: Your data point is a function and you can treat it like one. (arXiv:2201.12204v2 [cs.LG] UPDATED)
- Authors : Emilien Dupont, Hyunjik Kim, Ali Eslami, Danilo Rezende, Dan Rosenbaum
- Link : [http://arxiv.org/abs/2201.12204](http://arxiv.org/abs/2201.12204)
> ABSTRACT  :  It is common practice in deep learning to represent a measurement of the world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying signal represented by these measurements is often continuous, e.g. the scene depicted in an image. A powerful continuous alternative is then to represent these measurements using an **implicit neural representation**, a neural function trained to output the appropriate measurement value for any input spatial location. In this paper, we take this idea to its next level: what would it take to perform deep learning on these functions instead, treating them as data? In this context we refer to the data as functa, and propose a framework for deep learning on functa. This view presents a number of challenges around efficient conversion from data to functa, compact representation of functa, and effectively solving downstream tasks on functa. We outline a recipe to overcome these challenges and apply it to a wide range of data modalities including images, 3D shapes, neural radiance fields (**NeRF**) and data on manifolds. We demonstrate that this approach has various compelling properties across data modalities, in particular on the canonical tasks of generative modeling, data imputation, novel view synthesis and classification.  
### CANShield: Signal-based Intrusion Detection for Controller Area Networks. (arXiv:2205.01306v2 [cs.CR] UPDATED)
- Authors : Md Hasan, Yang Xiao, Pablo Moriano, Wenjing Lou, Thomas Hou
- Link : [http://arxiv.org/abs/2205.01306](http://arxiv.org/abs/2205.01306)
> ABSTRACT  :  Modern vehicles rely on a fleet of electronic control units (ECUs) connected through controller area network (CAN) buses for critical vehicular control. However, with the expansion of advanced connectivity features in automobiles and the elevated risks of internal system **exposure**, the CAN bus is increasingly prone to intrusions and injection attacks. The ordinary injection attacks disrupt the typical timing properties of the CAN data stream, and the rule-based intrusion detection systems (IDS) can easily detect them. However, advanced attackers can inject false data to the time series sensory data (signal), while looking innocuous by the pattern/frequency of the CAN messages. Such attacks can bypass the rule-based IDS or any anomaly-based IDS built on binary payload data. To make the vehicles robust against such intelligent attacks, we propose CANShield, a signal-based intrusion detection framework for the CAN bus. CANShield consists of three modules: a data preprocessing module that handles the high-dimensional CAN data stream at the signal level and makes them suitable for a deep learning model; a data analyzer module consisting of multiple deep autoencoder (AE) networks, each analyzing the time-series data from a different temporal perspective; and finally an attack detection module that uses an ensemble method to make the final decision. Evaluation results on two high-fidelity signal-based CAN attack datasets show the high accuracy and responsiveness of CANShield in detecting wide-range of advanced intrusion attacks.  
## cs.AI
---
### PIDNet: A **Real-time** Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v1 [cs.CV])
- Authors : Jiacong Xu, Zixiang Xiong
- Link : [http://arxiv.org/abs/2206.02066](http://arxiv.org/abs/2206.02066)
> ABSTRACT  :  Two-branch network architecture has shown its efficiency and effectiveness for real-time semantic segmentation tasks. However, direct fusion of low-level details and high-level semantics will lead to a phenomenon that the detailed features are easily overwhelmed by surrounding contextual information, namely overshoot in this paper, which limits the improvement of the accuracy of existed two-branch models. In this paper, we bridge a connection between Convolutional Neural Network (CNN) and Proportional-Integral-Derivative (PID) controller and reveal that the two-branch network is nothing but a Proportional-Integral (PI) controller, which inherently suffers from the similar overshoot issue. To alleviate this issue, we propose a novel three-branch network architecture: PIDNet, which possesses three branches to parse the detailed, context and boundary information (derivative of semantics), respectively, and employs boundary attention to guide the fusion of detailed and context branches in final stage. The family of PIDNets achieve the best trade-off between inference speed and accuracy and their test accuracy surpasses all the existed models with similar inference speed on Cityscapes, CamVid and COCO-Stuff datasets. Especially, PIDNet-S achieves 78.6% mIOU with inference speed of 93.2 FPS on Cityscapes test set and 81.6% mIOU with speed of 153.7 FPS on CamVid test set.  
### All One Needs to Know about Priors for Deep Image **Restoration** and **Enhancement**: A Survey. (arXiv:2206.02070v1 [cs.CV])
- Authors : Yunfan Lu, Yiqi Lin, Hao Wu, Yunhao Luo, Xu Zheng, Lin Wang
- Link : [http://arxiv.org/abs/2206.02070](http://arxiv.org/abs/2206.02070)
> ABSTRACT  :  Image **restoration** and **enhancement** is a process of improving the image quality by removing degradations, such as noise, blur, and resolution degradation. Deep learning (DL) has recently been applied to image **restoration** and **enhancement**. Due to its ill-posed property, plenty of works have explored priors to facilitate training deep neural networks (DNNs). However, the importance of priors has not been systematically studied and analyzed by far in the research community. Therefore, this paper serves as the first study that provides a comprehensive overview of recent advancements of priors for deep image **restoration** and **enhancement**. Our work covers five primary contents: (1) A theoretical analysis of priors for deep image **restoration** and **enhancement**; (2) A hierarchical and structural taxonomy of priors commonly used in the DL-based methods; (3) An insightful discussion on each prior regarding its principle, potential, and applications; (4) A summary of crucial problems by highlighting the potential future directions to spark more research in the community; (5) An open-source repository that provides a taxonomy of all mentioned works and code links.  
### HIFI-Net: A Novel Network for **Enhancement** to Underwater Images. (arXiv:2206.02295v1 [cs.CV])
- Authors : Jiajia Zhou, Junbin Zhuang, Yan Zheng, Di Wu
- Link : [http://arxiv.org/abs/2206.02295](http://arxiv.org/abs/2206.02295)
> ABSTRACT  :  A novel network for **enhancement** to underwater images is proposed in this paper. It contains a Reinforcement Fusion Module for Haar wavelet images (RFM-Haar) based on Reinforcement Fusion Unit (RFU), which is used to fuse an original image and some important information within it. Fusion is achieved for better **enhancement**. As this network make "Haar Images into Fusion Images", it is called HIFI-Net. The experimental results show the proposed HIFI-Net performs best among many state-of-the-art methods on three datasets at three normal metrics and a new metric.  
### LLVIP: A Visible-infrared Paired Dataset for **Low-light** Vision. (arXiv:2108.10831v3 [cs.CV] UPDATED)
- Authors : Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, Shengjie Liu, Wenli Zhou
- Link : [http://arxiv.org/abs/2108.10831](http://arxiv.org/abs/2108.10831)
> ABSTRACT  :  It is very challenging for various visual tasks such as image fusion, pedestrian detection and image-to-image translation in **low light** conditions due to the loss of effective target areas. In this case, infrared and visible images can be used together to provide both rich detail information and effective target areas. In this paper, we present LLVIP, a visible-infrared paired dataset for **low-light** vision. This dataset contains 30976 images, or 15488 pairs, most of which were taken at very **dark** scenes, and all of the images are strictly aligned in time and space. Pedestrians in the dataset are labeled. We compare the dataset with other visible-infrared datasets and evaluate the performance of some popular visual algorithms including image fusion, pedestrian detection and image-to-image translation on the dataset. The experimental results demonstrate the complementary effect of fusion on image information, and find the deficiency of existing algorithms of the three visual tasks in very **low-light** conditions. We believe the LLVIP dataset will contribute to the community of computer vision by promoting image fusion, pedestrian detection and image-to-image translation in very **low-light** applications. The dataset is being released in https://bupt-ai-cz.github.io/LLVIP. Raw data is also provided for further research such as image registration.  
### K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways. (arXiv:2110.11048v2 [cs.CV] UPDATED)
- Authors : Donghee Paek, Hyun Kong, Kevin Tirta
- Link : [http://arxiv.org/abs/2110.11048](http://arxiv.org/abs/2110.11048)
> ABSTRACT  :  Lane detection is a critical function for autonomous driving. With the recent development of deep learning and the publication of camera lane datasets and benchmarks, camera lane detection networks (CLDNs) have been remarkably developed. Unfortunately, CLDNs rely on camera images which are often distorted near the vanishing line and prone to poor lighting condition. This is in contrast with Lidar lane detection networks (LLDNs), which can directly extract the lane lines on the bird's eye view (BEV) for motion planning and operate robustly under various lighting conditions. However, LLDNs have not been actively studied, mostly due to the absence of large public lidar lane datasets. In this paper, we introduce KAIST-Lane (K-Lane), the world's first and the largest public urban road and highway lane dataset for Lidar. K-Lane has more than 15K frames and contains annotations of up to six lanes under various road and traffic conditions, e.g., occluded roads of multiple occlusion levels, roads at day and **night** times, merging (converging and diverging) and curved lanes. We also provide baseline networks we term Lidar lane detection networks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the spatial characteristics of lane lines on the point cloud, which are sparse, thin, and stretched along the entire ground plane of the point cloud. From experimental results, LLDN-GFC achieves the state-of-the-art performance with an F1- score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong performance under various lighting conditions, which is unlike CLDNs, and also robust even in the case of severe occlusions, unlike LLDNs using the conventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, and complete development kits including evaluation, visualization and annotation tools are available at https://github.com/kaist-avelab/k-lane.  
### End-to-End Rubbing **Restoration** Using Generative Adversarial Networks. (arXiv:2205.03743v2 [cs.CV] UPDATED)
- Authors : Gongbo Sun, Zijie Zheng, Ming Zhang
- Link : [http://arxiv.org/abs/2205.03743](http://arxiv.org/abs/2205.03743)
> ABSTRACT  :  Rubbing **restoration**s are significant for preserving world cultural history. In this paper, we propose the RubbingGAN model for restoring incomplete rubbing characters. Specifically, we collect characters from the Zhang Menglong Bei and build up the first rubbing **restoration** dataset. We design the first generative adversarial network for rubbing **restoration**. Based on the dataset we collect, we apply the RubbingGAN to learn the Zhang Menglong Bei font style and restore the characters. The results of experiments show that RubbingGAN can repair both slightly and severely incomplete rubbing characters fast and effectively.  
# Paper List
---
## cs.CV
---
**162** new papers in cs.CV:-) 
1. A review of machine learning approaches, challenges and prospects for computational tumor pathology. (arXiv:2206.01728v1 [eess.IV])
2. Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs. (arXiv:2206.01731v1 [eess.IV])
3. Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline. (arXiv:2206.01733v1 [cs.CV])
4. Using UAS Imagery and Computer Vision to Support Site-Specific Weed Control in Corn. (arXiv:2206.01734v1 [cs.CV])
5. Examining the behaviour of state-of-the-art convolutional neural networks for brain tumor detection with and without transfer learning. (arXiv:2206.01735v1 [eess.IV])
6. Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (arXiv:2206.01736v1 [eess.IV])
7. MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation. (arXiv:2206.01737v1 [eess.IV])
8. RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding. (arXiv:2206.01738v1 [eess.IV])
9. Mutual- and Self- Prototype Alignment for Semi-supervised Medical Image Segmentation. (arXiv:2206.01739v1 [eess.IV])
10. Denoising Fast X-Ray Fluorescence Raster Scans of Paintings. (arXiv:2206.01740v1 [eess.IV])
11. Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation. (arXiv:2206.01741v1 [eess.IV])
12. Learning Probabilistic Structural Representation for Biomedical Image Segmentation. (arXiv:2206.01742v1 [eess.IV])
13. Orthogonal Transform based Generative Adversarial Network for Image Dehazing. (arXiv:2206.01743v1 [eess.IV])
14. Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial Intelligence Techniques. (arXiv:2206.01745v1 [eess.IV])
15. Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach. (arXiv:2206.01746v1 [eess.IV])
16. Radar Guided Dynamic Visual Attention for Resource-Efficient RGB Object Detection. (arXiv:2206.01772v1 [cs.CV])
17. Monkeypox Image Data collection. (arXiv:2206.01774v1 [eess.IV])
18. Real-Time Super-Resolution for Real-World Images on Mobile Devices. (arXiv:2206.01777v1 [cs.CV])
19. R2U++: A Multiscale Recurrent Residual U-Net with Dense Skip Connections for Medical Image Segmentation. (arXiv:2206.01793v1 [eess.IV])
20. Additive MIL: Intrinsic Interpretability for Pathology. (arXiv:2206.01794v1 [cs.CV])
21. Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata. (arXiv:2206.01813v1 [cs.CV])
22. EAANet: Efficient Attention Augmented Convolutional Networks. (arXiv:2206.01821v1 [cs.CV])
23. The Gamma Generalized Normal Distribution: A Descriptor of SAR Imagery. (arXiv:2206.01826v1 [stat.ME])
24. Drawing out of Distribution with Neuro-Symbolic Generative Models. (arXiv:2206.01829v1 [cs.LG])
25. Spatial Feature Mapping for 6DoF Object Pose Estimation. (arXiv:2206.01831v1 [cs.CV])
26. Coffee Roast Intelligence. (arXiv:2206.01841v1 [cs.CV])
27. Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning. (arXiv:2206.01843v1 [cs.CV])
28. Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image. (arXiv:2206.01856v1 [cs.CV])
29. Image Data collection and implementation of deep learning-based model in detecting Monkeypox disease using modified VGG16. (arXiv:2206.01862v1 [eess.IV])
30. Recurrent Image Registration using Mutual Attention based Network. (arXiv:2206.01863v1 [cs.CV])
31. SPGNet: Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional Space. (arXiv:2206.01867v1 [cs.CV])
32. Face Recognition Accuracy Across Demographics: Shining a Light Into the Problem. (arXiv:2206.01881v1 [cs.CV])
33. A Superimposed Divide-and-Conquer Image Recognition Method for SEM Images of Nanoparticles on The Surface of Monocrystalline silicon with High Aggregation Degree. (arXiv:2206.01884v1 [cs.CV])
34. Modeling of Textures to Predict Immune Cell Status and Survival of Brain Tumour Patients. (arXiv:2206.01897v1 [eess.IV])
35. Saliency Attack: Towards Imperceptible Black-box Adversarial Attack. (arXiv:2206.01898v1 [cs.LG])
36. Deep Radiomic Analysis for Predicting Coronavirus Disease 2019 in Computerized Tomography and X-ray Images. (arXiv:2206.01903v1 [eess.IV])
37. Video-based Human-Object Interaction Detection from Tubelet Tokens. (arXiv:2206.01908v1 [cs.CV])
38. The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition. (arXiv:2206.01910v1 [cs.CV])
39. Nerfels: Renderable Neural Codes for Improved Camera Pose Estimation. (arXiv:2206.01916v1 [cs.CV])
40. From Pixels to Objects: Cubic Visual Attention for Visual Question Answering. (arXiv:2206.01923v1 [cs.CV])
41. Occlusion-Resistant Instance Segmentation of Piglets in Farrowing Pens Using Center Clustering Network. (arXiv:2206.01942v1 [cs.CV])
42. C$^3$Fusion: Consistent Contrastive Colon Fusion, Towards Deep SLAM in Colonoscopy. (arXiv:2206.01961v1 [cs.CV])
43. Rethinking the Openness of CLIP. (arXiv:2206.01986v1 [cs.CV])
44. Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation. (arXiv:2206.01988v1 [cs.CV])
45. CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v1 [cs.CV])
46. MSR: Making Self-supervised learning Robust to Aggressive Augmentations. (arXiv:2206.01999v1 [cs.CV])
47. CVNets: High Performance Library for Computer Vision. (arXiv:2206.02002v1 [cs.CV])
48. APES: Articulated Part Extraction from Sprite Sheets. (arXiv:2206.02015v1 [cs.CV])
49. Implicit Neural Representation for Mesh-Free Inverse Obstacle Scattering. (arXiv:2206.02027v1 [cs.CV])
50. Guided Deep Metric Learning. (arXiv:2206.02029v1 [cs.CV])
51. Learning Speaker-specific Lip-to-Speech Generation. (arXiv:2206.02050v1 [cs.CV])
52. PIDNet: A **Real-time** Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v1 [cs.CV])
53. All One Needs to Know about Priors for Deep Image **Restoration** and **Enhancement**: A Survey. (arXiv:2206.02070v1 [cs.CV])
54. Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval. (arXiv:2206.02082v1 [cs.CV])
55. Towards the Creation of a Nutrition and Food Group Based Image Database. (arXiv:2206.02086v1 [cs.CV])
56. Accurate Scoliosis Vertebral Landmark Localization on X-ray Images via Shape-constrained Multi-stage Cascaded CNNs. (arXiv:2206.02087v1 [cs.CV])
57. Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation. (arXiv:2206.02099v1 [cs.CV])
58. AUTM Flow: Atomic Unrestricted Time Machine for Monotonic Normalizing Flows. (arXiv:2206.02102v1 [cs.LG])
59. ContraCLIP: Interpretable GAN generation driven by pairs of contrasting sentences. (arXiv:2206.02104v1 [cs.CV])
60. Computer Vision-based Characterization of Large-scale Jet Flames using a Synthetic Infrared Image Generation Approach. (arXiv:2206.02110v1 [cs.CV])
61. Cannot See the Forest for the Trees: Aggregating Multiple Viewpoints to Better Classify Objects in Videos. (arXiv:2206.02116v1 [cs.CV])
62. ShapePU: A New PU Learning Framework Regularized by Global Consistency for Scribble Supervised Cardiac Segmentation. (arXiv:2206.02118v1 [cs.CV])
63. MPANet: Multi-Patch Attention For Infrared Small Target object Detection. (arXiv:2206.02120v1 [cs.CV])
64. Federated Adversarial Training with Transformers. (arXiv:2206.02131v1 [cs.LG])
65. LDRNet: Enabling **Real-time** Document Localization on Mobile Devices. (arXiv:2206.02136v1 [cs.CV])
66. Recurrent Video **Restoration** Transformer with Guided Deformable Attention. (arXiv:2206.02146v1 [cs.CV])
67. HPGNN: Using Hierarchical Graph Neural Networks for Outdoor Point Cloud Processing. (arXiv:2206.02153v1 [cs.CV])
68. Vanilla Feature Distillation for Improving the Accuracy-Robustness Trade-Off in Adversarial Training. (arXiv:2206.02158v1 [cs.CV])
69. MotionCNN: A Strong Baseline for Motion Prediction in Autonomous Driving. (arXiv:2206.02163v1 [cs.CV])
70. Semi-Supervised Learning for Mars Imagery Classification and Segmentation. (arXiv:2206.02180v1 [cs.CV])
71. Functional Ensemble Distillation. (arXiv:2206.02183v1 [cs.LG])
72. M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation. (arXiv:2206.02187v1 [cs.CV])
73. FOF: Learning Fourier Occupancy Field for Monocular **Real-time** Human Reconstruction. (arXiv:2206.02194v1 [cs.CV])
74. GridShift: A Faster Mode-seeking Algorithm for Image Segmentation and Object Tracking. (arXiv:2206.02200v1 [cs.CV])
75. 3D Convolutional with Attention for Action Recognition. (arXiv:2206.02203v1 [cs.CV])
76. U(1) Symmetry-breaking Observed in Generic CNN Bottleneck Layers. (arXiv:2206.02220v1 [cs.CV])
77. Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography. (arXiv:2206.02225v1 [eess.IV])
78. Two Decades of Bengali Handwritten Digit Recognition: A Survey. (arXiv:2206.02234v1 [cs.CV])
79. Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey. (arXiv:2206.02257v1 [cs.CV])
80. SealID: Saimaa ringed seal re-identification dataset. (arXiv:2206.02260v1 [cs.CV])
81. Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning. (arXiv:2206.02261v1 [cs.CV])
82. Estimating Building Energy Efficiency From Street View Imagery, Aerial Imagery, and Land Surface Temperature Data. (arXiv:2206.02270v1 [cs.CV])
83. Autoregressive Model for Multi-Pass SAR Change Detection Based on Image Stacks. (arXiv:2206.02278v1 [eess.IV])
84. E^2VTS: Energy-Efficient Video Text Spotting from Unmanned Aerial Vehicles. (arXiv:2206.02281v1 [cs.CV])
85. Tagged-MRI2Audio with Attention Guided Heterogeneous Translator. (arXiv:2206.02284v1 [cs.SD])
86. AugLoss: A Learning Methodology for Real-World Dataset Corruption. (arXiv:2206.02286v1 [cs.LG])
87. ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training. (arXiv:2206.02288v1 [cs.CV])
88. HIFI-Net: A Novel Network for **Enhancement** to Underwater Images. (arXiv:2206.02295v1 [cs.CV])
89. Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v1 [cs.CV])
90. Evaluation-oriented Knowledge Distillation for Deep Face Recognition. (arXiv:2206.02325v1 [cs.CV])
91. JigsawHSI: a network for Hyperspectral Image classification. (arXiv:2206.02327v1 [cs.CV])
92. MASNet:Improve Performance of Siamese Networks with Mutual-attention for Remote Sensing Change Detection Tasks. (arXiv:2206.02331v1 [cs.CV])
93. OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression. (arXiv:2206.02338v1 [cs.CV])
94. WHU-Stereo: A Challenging Benchmark for Stereo Matching of High-Resolution Satellite Images. (arXiv:2206.02342v1 [cs.CV])
95. Contrastive Graph Multimodal Model for Text Classification in Videos. (arXiv:2206.02343v1 [cs.CV])
96. Anomaly Detection with Test Time Augmentation and Consistency Evaluation. (arXiv:2206.02345v1 [cs.CV])
97. Structured Binary Neural Networks for Image Recognition. (arXiv:1909.09934v4 [cs.CV] UPDATED)
98. How to Train Your Dragon: Tamed Warping Network for Semantic Video Segmentation. (arXiv:2005.01344v3 [cs.CV] UPDATED)
99. SAGE: Sequential Attribute Generator for Analyzing Glioblastomas using Limited Dataset. (arXiv:2005.07225v2 [eess.IV] UPDATED)
100. Using Unlabeled Data for Increasing Low-Shot Classification Accuracy of Relevant and Open-Set Irrelevant Images. (arXiv:2010.00721v2 [cs.CV] UPDATED)
101. How Far Can We Get with Neural Networks Straight from JPEG?. (arXiv:2012.14426v2 [cs.CV] UPDATED)
102. Multi-point dimensionality reduction to improve projection layout reliability. (arXiv:2101.06224v4 [cs.CV] UPDATED)
103. Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision. (arXiv:2103.12209v3 [cs.CV] UPDATED)
104. Instance-level Image Retrieval using Reranking Transformers. (arXiv:2103.12236v3 [cs.CV] UPDATED)
105. MLAN: Multi-Level Adversarial Network for Domain Adaptive Semantic Segmentation. (arXiv:2103.12991v2 [cs.CV] UPDATED)
106. Multimodal Object Detection via Probabilistic Ensembling. (arXiv:2104.02904v2 [cs.CV] UPDATED)
107. The art of defense: letting networks fool the attacker. (arXiv:2104.02963v3 [cs.CV] UPDATED)
108. Very Lightweight Photo Retouching Network with Conditional Sequential Modulation. (arXiv:2104.06279v2 [cs.CV] UPDATED)
109. Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v6 [cs.CV] UPDATED)
110. Redundant representations help generalization in wide neural networks. (arXiv:2106.03485v3 [stat.ML] UPDATED)
111. Spectral Unsupervised Domain Adaptation for Visual Recognition. (arXiv:2106.06112v3 [cs.CV] UPDATED)
112. Learning of feature points without additional supervision improves reinforcement learning from images. (arXiv:2106.07995v3 [cs.LG] UPDATED)
113. Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v4 [cs.LG] UPDATED)
114. Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v6 [cs.CV] UPDATED)
115. Towards A Fairer Landmark Recognition Dataset. (arXiv:2108.08874v2 [cs.CV] UPDATED)
116. LLVIP: A Visible-infrared Paired Dataset for **Low-light** Vision. (arXiv:2108.10831v3 [cs.CV] UPDATED)
117. Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v6 [cs.CV] UPDATED)
118. Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v3 [cs.CV] UPDATED)
119. Multi-Class Cell Detection Using Spatial Context Representation. (arXiv:2110.04886v2 [cs.CV] UPDATED)
120. K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways. (arXiv:2110.11048v2 [cs.CV] UPDATED)
121. Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention. (arXiv:2110.12372v3 [eess.IV] UPDATED)
122. A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v2 [cs.CV] UPDATED)
123. CodEx: A Modular Framework for Joint Temporal De-blurring and Tomographic Reconstruction. (arXiv:2111.06069v2 [eess.IV] UPDATED)
124. Self-supervised Re-renderable Facial Albedo Reconstruction from Single Image. (arXiv:2111.08282v2 [cs.CV] UPDATED)
125. Exploring dual-attention mechanism with multi-scale feature extraction scheme for skin lesion segmentation. (arXiv:2111.08708v2 [eess.IV] UPDATED)
126. Few-Shot Object Detection via Association and DIscrimination. (arXiv:2111.11656v2 [cs.CV] UPDATED)
127. Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling. (arXiv:2111.14819v2 [cs.CV] UPDATED)
128. Dual-Flow Transformation Network for Deformable Image Registration with Region Consistency Constraint. (arXiv:2112.02249v2 [cs.CV] UPDATED)
129. Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models. (arXiv:2112.03860v2 [cs.CV] UPDATED)
130. Shaping Visual Representations with Attributes for Few-Shot Recognition. (arXiv:2112.06398v3 [cs.CV] UPDATED)
131. Structure-Aware Image Segmentation with Homotopy Warping. (arXiv:2112.07812v2 [cs.CV] UPDATED)
132. MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection. (arXiv:2112.08935v3 [cs.CV] UPDATED)
133. Unpaired Referring Expression Grounding via Bidirectional Cross-Modal Matching. (arXiv:2201.06686v2 [cs.CV] UPDATED)
134. Deep Kernelized Dense Geometric Matching. (arXiv:2202.00667v2 [cs.CV] UPDATED)
135. Student Dangerous Behavior Detection in School. (arXiv:2202.09550v2 [cs.CV] UPDATED)
136. Transformers in Medical Image Analysis: A Review. (arXiv:2202.12165v2 [cs.CV] UPDATED)
137. Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey. (arXiv:2202.13589v2 [cs.CV] UPDATED)
138. NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation. (arXiv:2203.01502v2 [cs.CV] UPDATED)
139. Depth-Independent Depth Completion via Least Square Estimation. (arXiv:2203.03317v2 [cs.CV] UPDATED)
140. Contrastive Learning for Cross-Domain Open World Recognition. (arXiv:2203.09257v2 [cs.CV] UPDATED)
141. Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v3 [cs.CV] UPDATED)
142. Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v3 [cs.CV] UPDATED)
143. LiDAR Snowfall Simulation for Robust 3D Object Detection. (arXiv:2203.15118v2 [cs.CV] UPDATED)
144. Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v2 [cs.AI] UPDATED)
145. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v2 [cs.CV] UPDATED)
146. E^2TAD: An Energy-Efficient Tracking-based Action Detector. (arXiv:2204.04416v3 [cs.CV] UPDATED)
147. CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation. (arXiv:2204.09914v3 [cs.CV] UPDATED)
148. Visual Attention Emerges from Recurrent Sparse Reconstruction. (arXiv:2204.10962v2 [cs.CV] UPDATED)
149. Data-Efficient Backdoor Attacks. (arXiv:2204.12281v2 [cs.CV] UPDATED)
150. Attention-based Knowledge Distillation in Multi-attention Tasks: The Impact of a DCT-driven Loss. (arXiv:2205.01997v2 [cs.CV] UPDATED)
151. End-to-End Rubbing **Restoration** Using Generative Adversarial Networks. (arXiv:2205.03743v2 [cs.CV] UPDATED)
152. Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v2 [cs.CV] UPDATED)
153. Domain Invariant Masked Autoencoders for Self-supervised Learning from Multi-domains. (arXiv:2205.04771v2 [cs.CV] UPDATED)
154. Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v3 [cs.CV] UPDATED)
155. An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v2 [cs.LG] UPDATED)
156. Strengthening Skeletal Action Recognizers via Leveraging Temporal Patterns. (arXiv:2205.14405v2 [cs.CV] UPDATED)
157. Guided Diffusion Model for Adversarial Purification. (arXiv:2205.14969v2 [cs.CV] UPDATED)
158. DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes. (arXiv:2205.15723v2 [cs.CV] UPDATED)
159. Labeling Where Adapting Fails: Cross-Domain Semantic Segmentation with Point Supervision via Active Selection. (arXiv:2206.00181v2 [cs.CV] UPDATED)
160. Anomaly detection in surveillance videos using transformer based attention model. (arXiv:2206.01524v2 [cs.CV] UPDATED)
161. OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v2 [cs.LG] UPDATED)
162. Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v4 [eess.IV] CROSS LISTED)
## eess.IV
---
**50** new papers in eess.IV:-) 
1. A review of machine learning approaches, challenges and prospects for computational tumor pathology. (arXiv:2206.01728v1 [eess.IV])
2. Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs. (arXiv:2206.01731v1 [eess.IV])
3. Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline. (arXiv:2206.01733v1 [cs.CV])
4. Using UAS Imagery and Computer Vision to Support Site-Specific Weed Control in Corn. (arXiv:2206.01734v1 [cs.CV])
5. Examining the behaviour of state-of-the-art convolutional neural networks for brain tumor detection with and without transfer learning. (arXiv:2206.01735v1 [eess.IV])
6. Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (arXiv:2206.01736v1 [eess.IV])
7. MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation. (arXiv:2206.01737v1 [eess.IV])
8. RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding. (arXiv:2206.01738v1 [eess.IV])
9. Mutual- and Self- Prototype Alignment for Semi-supervised Medical Image Segmentation. (arXiv:2206.01739v1 [eess.IV])
10. Denoising Fast X-Ray Fluorescence Raster Scans of Paintings. (arXiv:2206.01740v1 [eess.IV])
11. Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation. (arXiv:2206.01741v1 [eess.IV])
12. Learning Probabilistic Structural Representation for Biomedical Image Segmentation. (arXiv:2206.01742v1 [eess.IV])
13. Orthogonal Transform based Generative Adversarial Network for Image Dehazing. (arXiv:2206.01743v1 [eess.IV])
14. Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial Intelligence Techniques. (arXiv:2206.01745v1 [eess.IV])
15. Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach. (arXiv:2206.01746v1 [eess.IV])
16. Monkeypox Image Data collection. (arXiv:2206.01774v1 [eess.IV])
17. Real-Time Super-Resolution for Real-World Images on Mobile Devices. (arXiv:2206.01777v1 [cs.CV])
18. R2U++: A Multiscale Recurrent Residual U-Net with Dense Skip Connections for Medical Image Segmentation. (arXiv:2206.01793v1 [eess.IV])
19. Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata. (arXiv:2206.01813v1 [cs.CV])
20. The Gamma Generalized Normal Distribution: A Descriptor of SAR Imagery. (arXiv:2206.01826v1 [stat.ME])
21. Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image. (arXiv:2206.01856v1 [cs.CV])
22. Image Data collection and implementation of deep learning-based model in detecting Monkeypox disease using modified VGG16. (arXiv:2206.01862v1 [eess.IV])
23. Modeling of Textures to Predict Immune Cell Status and Survival of Brain Tumour Patients. (arXiv:2206.01897v1 [eess.IV])
24. Deep Radiomic Analysis for Predicting Coronavirus Disease 2019 in Computerized Tomography and X-ray Images. (arXiv:2206.01903v1 [eess.IV])
25. Computer Vision-based Characterization of Large-scale Jet Flames using a Synthetic Infrared Image Generation Approach. (arXiv:2206.02110v1 [cs.CV])
26. Recurrent Video **Restoration** Transformer with Guided Deformable Attention. (arXiv:2206.02146v1 [cs.CV])
27. Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography. (arXiv:2206.02225v1 [eess.IV])
28. Autoregressive Model for Multi-Pass SAR Change Detection Based on Image Stacks. (arXiv:2206.02278v1 [eess.IV])
29. Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v1 [cs.CV])
30. Robust Image Protection Countering Cropping Manipulation. (arXiv:2206.02405v1 [cs.CV])
31. Is More Data All You Need? A Causal Exploration. (arXiv:2206.02409v1 [cs.AI])
32. mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation. (arXiv:2206.02425v1 [eess.IV])
33. Universal Photometric Stereo Network using Global Lighting Contexts. (arXiv:2206.02452v1 [cs.CV])
34. Single pixel imaging at high pixel resolutions. (arXiv:2206.02510v1 [physics.optics])
35. Scan4CFU: Low-cost, open-source bacterial colony tracking over large areas and extended incubation times. (arXiv:2206.02534v1 [q-bio.QM])
36. Machine Learning for Detection of 3D Features using sparse X-ray data. (arXiv:2206.02564v1 [cs.CV])
37. Coding of volumetric content with MIV using VVC subpictures. (arXiv:2206.02588v1 [eess.IV])
38. Real-World Image Super-Resolution by Exclusionary Dual-Learning. (arXiv:2206.02609v1 [cs.CV])
39. Day-to-**Night** Image Synthesis for Training **Night**time Neural ISPs. (arXiv:2206.02715v1 [cs.CV])
40. Compound Multi-branch Feature Fusion for Real Image **Restoration**. (arXiv:2206.02748v1 [eess.IV])
41. SAGE: Sequential Attribute Generator for Analyzing Glioblastomas using Limited Dataset. (arXiv:2005.07225v2 [eess.IV] UPDATED)
42. Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v4 [eess.IV] UPDATED)
43. E1D3 U-Net for Brain Tumor Segmentation: Submission to the RSNA-ASNR-MICCAI BraTS 2021 Challenge. (arXiv:2110.02519v2 [eess.IV] UPDATED)
44. Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention. (arXiv:2110.12372v3 [eess.IV] UPDATED)
45. CodEx: A Modular Framework for Joint Temporal De-blurring and Tomographic Reconstruction. (arXiv:2111.06069v2 [eess.IV] UPDATED)
46. Exploring dual-attention mechanism with multi-scale feature extraction scheme for skin lesion segmentation. (arXiv:2111.08708v2 [eess.IV] UPDATED)
47. Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v3 [cs.CV] UPDATED)
48. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v2 [cs.CV] UPDATED)
49. Physics-assisted Generative Adversarial Network for X-Ray Tomography. (arXiv:2204.03703v2 [eess.IV] UPDATED)
50. Dynamic Tomography Reconstruction by Projection-Domain Separable Modeling. (arXiv:2204.09935v2 [eess.IV] UPDATED)
## cs.LG
---
**240** new papers in cs.LG:-) 
1. A review of machine learning approaches, challenges and prospects for computational tumor pathology. (arXiv:2206.01728v1 [eess.IV])
2. Torsional Diffusion for Molecular Conformer Generation. (arXiv:2206.01729v1 [physics.chem-ph])
3. Nonsmooth automatic differentiation: a cheap gradient principle and other complexity results. (arXiv:2206.01730v1 [math.NA])
4. Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs. (arXiv:2206.01731v1 [eess.IV])
5. Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline. (arXiv:2206.01733v1 [cs.CV])
6. Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (arXiv:2206.01736v1 [eess.IV])
7. Orthogonal Transform based Generative Adversarial Network for Image Dehazing. (arXiv:2206.01743v1 [eess.IV])
8. Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach. (arXiv:2206.01746v1 [eess.IV])
9. Federated Deep Learning Meets Autonomous Vehicle Perception: Design and Verification. (arXiv:2206.01748v1 [cs.RO])
10. Uncertainty Estimation in Machine Learning. (arXiv:2206.01749v1 [cs.LG])
11. Optimal Competitive-Ratio Control. (arXiv:2206.01782v1 [math.OC])
12. R2U++: A Multiscale Recurrent Residual U-Net with Dense Skip Connections for Medical Image Segmentation. (arXiv:2206.01793v1 [eess.IV])
13. Additive MIL: Intrinsic Interpretability for Pathology. (arXiv:2206.01794v1 [cs.CV])
14. Robust Topological Inference in the Presence of Outliers. (arXiv:2206.01795v1 [math.ST])
15. Do-Operation Guided Causal Representation Learning with Reduced Supervision Strength. (arXiv:2206.01802v1 [cs.LG])
16. Learning Fine Scale Dynamics from Coarse Observations via Inner Recurrence. (arXiv:2206.01807v1 [cs.LG])
17. Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks. (arXiv:2206.01812v1 [cs.LG])
18. Contrastive learning unifies $t$-SNE and UMAP. (arXiv:2206.01816v1 [cs.LG])
19. QAGCN: A Graph Convolutional Network-based Multi-Relation Question Answering System. (arXiv:2206.01818v1 [cs.AI])
20. A Robust Backpropagation-Free Framework for Images. (arXiv:2206.01820v1 [cs.NE])
21. Debiased Machine Learning without Sample-Splitting for Stable Estimators. (arXiv:2206.01825v1 [econ.EM])
22. Drawing out of Distribution with Neuro-Symbolic Generative Models. (arXiv:2206.01829v1 [cs.LG])
23. Dimension Independent Generalization of DP-SGD for Overparameterized Smooth Convex Optimization. (arXiv:2206.01836v1 [cs.LG])
24. Differentially Private Model Compression. (arXiv:2206.01838v1 [cs.LG])
25. Coffee Roast Intelligence. (arXiv:2206.01841v1 [cs.CV])
26. Out-of-Distribution Detection using BiGAN and MDL. (arXiv:2206.01851v1 [cs.LG])
27. Extreme Compression for Pre-trained Transformers Made Simple and Efficient. (arXiv:2206.01859v1 [cs.CL])
28. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. (arXiv:2206.01861v1 [cs.CL])
29. Model-Informed Generative Adversarial Network (MI-GAN) for Learning Optimal Power Flow. (arXiv:2206.01864v1 [cs.LG])
30. Estimating the Effect of Team Hitting Strategies Using Counterfactual Virtual Simulation in Baseball. (arXiv:2206.01871v1 [cs.AI])
31. An Unpooling Layer for Graph Generation. (arXiv:2206.01874v1 [cs.LG])
32. Learning in Congestion Games with Bandit Feedback. (arXiv:2206.01880v1 [cs.GT])
33. Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning. (arXiv:2206.01888v1 [cs.LG])
34. Initial Study into Application of Feature Density and Linguistically-backed Embedding to Improve Machine Learning-based Cyberbullying Detection. (arXiv:2206.01889v1 [cs.CL])
35. Adaptive Tree Backup Algorithms for Temporal-Difference Reinforcement Learning. (arXiv:2206.01896v1 [cs.LG])
36. Saliency Attack: Towards Imperceptible Black-box Adversarial Attack. (arXiv:2206.01898v1 [cs.LG])
37. Evaluation of creating scoring opportunities for teammates in soccer via trajectory prediction. (arXiv:2206.01899v1 [cs.AI])
38. Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v1 [cs.AI])
39. Soft Adversarial Training Can Retain Natural Accuracy. (arXiv:2206.01904v1 [cs.LG])
40. Hybrid Architectures for Distributed Machine Learning in Heterogeneous Wireless Networks. (arXiv:2206.01906v1 [cs.LG])
41. Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation. (arXiv:2206.01909v1 [cs.LG])
42. Neural Lyapunov Control of Unknown Nonlinear Systems with Stability Guarantees. (arXiv:2206.01913v1 [eess.SY])
43. Classification at the Accuracy Limit -- Facing the Problem of Data Ambiguity. (arXiv:2206.01922v1 [cs.LG])
44. Variational Monte Carlo Approach to Partial Differential Equations with Neural Networks. (arXiv:2206.01927v1 [math.NA])
45. Investigating Brain Connectivity with Graph Neural Networks and GNNExplainer. (arXiv:2206.01930v1 [q-bio.NC])
46. Stochastic Multiple Target Sampling Gradient Descent. (arXiv:2206.01934v1 [cs.LG])
47. Learning Generative Factors of Neuroimaging Data with Variational auto-encoders. (arXiv:2206.01939v1 [cs.LG])
48. Robust Meta-learning with Sampling Noise and Label Noise via Eigen-Reptile. (arXiv:2206.01944v1 [cs.LG])
49. Exploring the Potential of Feature Density in Estimating Machine Learning Classifier Performance with Application to Cyberbullying Detection. (arXiv:2206.01949v1 [cs.CL])
50. Comparing Performance of Different Linguistically-Backed Word Embeddings for Cyberbullying Detection. (arXiv:2206.01950v1 [cs.CL])
51. C$^3$Fusion: Consistent Contrastive Colon Fusion, Towards Deep SLAM in Colonoscopy. (arXiv:2206.01961v1 [cs.CV])
52. Formal Specifications from Natural Language. (arXiv:2206.01962v1 [cs.SE])
53. Modelling and Mining of Patient Pathways: A Scoping Review. (arXiv:2206.01980v1 [cs.CY])
54. Geodesic Properties of a Generalized Wasserstein Embedding for Time Series Analysis. (arXiv:2206.01984v1 [cs.LG])
55. Rethinking the Openness of CLIP. (arXiv:2206.01986v1 [cs.CV])
56. Combinatorial Causal Bandits. (arXiv:2206.01995v1 [cs.LG])
57. MSR: Making Self-supervised learning Robust to Aggressive Augmentations. (arXiv:2206.01999v1 [cs.CV])
58. Hybrid Value Estimation for Off-policy Evaluation and Offline Reinforcement Learning. (arXiv:2206.02000v1 [cs.LG])
59. CVNets: High Performance Library for Computer Vision. (arXiv:2206.02002v1 [cs.CV])
60. Combinatorial optimization for low bit-width neural networks. (arXiv:2206.02006v1 [cs.LG])
61. Causal Discovery in Heterogeneous Environments Under the Sparse Mechanism Shift Hypothesis. (arXiv:2206.02013v1 [cs.LG])
62. Is $L^2$ Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Network?. (arXiv:2206.02016v1 [cs.LG])
63. Between Rate-Distortion Theory & Value Equivalence in Model-Based Reinforcement Learning. (arXiv:2206.02025v1 [cs.LG])
64. Guided Deep Metric Learning. (arXiv:2206.02029v1 [cs.CV])
65. A Neural Network Approach for Homogenization of Multiscale Problems. (arXiv:2206.02032v1 [cs.LG])
66. A Control Theoretic Framework for Adaptive Gradient Optimizers in Machine Learning. (arXiv:2206.02034v1 [cs.LG])
67. Interpolating Between Softmax Policy Gradient and Neural Replicator Dynamics with Capped Implicit Exploration. (arXiv:2206.02036v1 [cs.LG])
68. Beyond Value: CHECKLIST for Testing Inferences in Planning-Based RL. (arXiv:2206.02039v1 [cs.AI])
69. MetaNOR: A Meta-Learnt Nonlocal Operator Regression Approach for Metamaterial Modeling. (arXiv:2206.02040v1 [cond-mat.mtrl-sci])
70. First-Order Algorithms for Min-Max Optimization in Geodesic Metric Spaces. (arXiv:2206.02041v1 [math.OC])
71. Developing hierarchical anticipations via neural network-based event segmentation. (arXiv:2206.02042v1 [cs.LG])
72. UAV-Aided Multi-Community Federated Learning. (arXiv:2206.02043v1 [cs.IT])
73. On the Generalization Power of the Overfitted Three-Layer Neural Tangent Kernel Model. (arXiv:2206.02047v1 [cs.LG])
74. Interpretable Models Capable of Handling Systematic Missingness in Imbalanced Classes and Heterogeneous Datasets. (arXiv:2206.02056v1 [cs.LG])
75. When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction. (arXiv:2206.02058v1 [stat.ML])
76. Your Neighbors Are Communicating: Towards Powerful and Scalable Graph Neural Networks. (arXiv:2206.02059v1 [cs.LG])
77. Active Bayesian Causal Inference. (arXiv:2206.02063v1 [cs.LG])
78. Learning Robust Representations Of Generative Models Using Set-Based Artificial Fingerprints. (arXiv:2206.02067v1 [cs.LG])
79. Deciding What to Model: Value-Equivalent Sampling for Reinforcement Learning. (arXiv:2206.02072v1 [cs.LG])
80. Straggler-Resilient Personalized Federated Learning. (arXiv:2206.02078v1 [cs.LG])
81. Inference for Interpretable Machine Learning: Fast, Model-Agnostic Confidence Intervals for Feature Importance. (arXiv:2206.02088v1 [stat.ML])
82. Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequence Optimization. (arXiv:2206.02092v1 [cs.LG])
83. Using Connectome Features to Constrain Echo State Networks. (arXiv:2206.02094v1 [cs.LG])
84. ARC -- Actor Residual Critic for Adversarial Imitation Learning. (arXiv:2206.02095v1 [cs.LG])
85. PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding. (arXiv:2206.02096v1 [cs.LG])
86. Search Space Adaptation for Differentiable Neural Architecture Search in Image Classification. (arXiv:2206.02098v1 [cs.LG])
87. AUTM Flow: Atomic Unrestricted Time Machine for Monotonic Normalizing Flows. (arXiv:2206.02102v1 [cs.LG])
88. Interpretable Mixture of Experts for Structured Data. (arXiv:2206.02107v1 [cs.LG])
89. Learning Dynamics and Generalization in Reinforcement Learning. (arXiv:2206.02126v1 [cs.LG])
90. DeeprETA: An ETA Post-processing System at Scale. (arXiv:2206.02127v1 [cs.LG])
91. Federated Adversarial Training with Transformers. (arXiv:2206.02131v1 [cs.LG])
92. Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks. (arXiv:2206.02139v1 [cs.LG])
93. Which models are innately best at uncertainty estimation?. (arXiv:2206.02152v1 [cs.LG])
94. HPGNN: Using Hierarchical Graph Neural Networks for Outdoor Point Cloud Processing. (arXiv:2206.02153v1 [cs.CV])
95. Perspectives of Non-Expert Users on Cyber Security and Privacy: An Analysis of Online Discussions on Twitter. (arXiv:2206.02156v1 [cs.CR])
96. Never mind the metrics -- what about the uncertainty? Visualising confusion matrix metric distributions. (arXiv:2206.02157v1 [cs.LG])
97. Vanilla Feature Distillation for Improving the Accuracy-Robustness Trade-Off in Adversarial Training. (arXiv:2206.02158v1 [cs.CV])
98. Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach. (arXiv:2206.02164v1 [cs.LG])
99. A Survey on Deep Learning based Channel Estimation in Doubly Dispersive Environments. (arXiv:2206.02165v1 [cs.IT])
100. Factored Conditional Filtering: Tracking States and Estimating Parameters in High-Dimensional Spaces. (arXiv:2206.02178v1 [cs.AI])
101. Functional Ensemble Distillation. (arXiv:2206.02183v1 [cs.LG])
102. Machine learning applications for electricity market agent-based models: A systematic literature review. (arXiv:2206.02196v1 [cs.MA])
103. GridShift: A Faster Mode-seeking Algorithm for Image Segmentation and Object Tracking. (arXiv:2206.02200v1 [cs.CV])
104. Performance Comparison of Simple Transformer and Res-CNN-BiLSTM for Cyberbullying Classification. (arXiv:2206.02206v1 [cs.CL])
105. Variable-rate hierarchical CPC leads to acoustic unit discovery in speech. (arXiv:2206.02211v1 [cs.SD])
106. Statistical Deep Learning for Spatial and Spatio-Temporal Data. (arXiv:2206.02218v1 [stat.ML])
107. U(1) Symmetry-breaking Observed in Generic CNN Bottleneck Layers. (arXiv:2206.02220v1 [cs.CV])
108. Models of human preference for learning reward functions. (arXiv:2206.02231v1 [cs.LG])
109. Enforcing Group Fairness in Algorithmic Decision Making: Utility Maximization Under Sufficiency. (arXiv:2206.02237v1 [cs.CY])
110. OntoMerger: An Ontology Integration Library for Deduplicating and Connecting Knowledge Graph Nodes. (arXiv:2206.02238v1 [cs.AI])
111. Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models. (arXiv:2206.02246v1 [cs.SD])
112. Augmenting Netflix Search with In-Session Adapted Recommendations. (arXiv:2206.02254v1 [cs.IR])
113. Use-Case-Grounded Simulations for Explanation Evaluation. (arXiv:2206.02256v1 [cs.HC])
114. Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning. (arXiv:2206.02261v1 [cs.CV])
115. Diffusion-GAN: Training GANs with Diffusion. (arXiv:2206.02262v1 [cs.LG])
116. Information Threshold, Bayesian Inference and Decision-Making. (arXiv:2206.02266v1 [stat.ML])
117. Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling. (arXiv:2206.02275v1 [cs.LG])
118. AugLoss: A Learning Methodology for Real-World Dataset Corruption. (arXiv:2206.02286v1 [cs.LG])
119. Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v1 [cs.CV])
120. Asymptotic Instance-Optimal Algorithms for Interactive Decision Making. (arXiv:2206.02326v1 [cs.LG])
121. JigsawHSI: a network for Hyperspectral Image classification. (arXiv:2206.02327v1 [cs.CV])
122. Hashing Learning with Hyper-Class Representation. (arXiv:2206.02334v1 [cs.LG])
123. Complex Locomotion Skill Learning via Differentiable Physics. (arXiv:2206.02341v1 [cs.AI])
124. Decentralized, Communication- and Coordination-free Learning in Structured Matching Markets. (arXiv:2206.02344v1 [cs.AI])
125. On Low-rank Trace Regression under General Sampling Distribution. (arXiv:1904.08576v3 [cs.LG] UPDATED)
126. Gravity-Inspired Graph Autoencoders for Directed Link Prediction. (arXiv:1905.09570v5 [cs.LG] UPDATED)
127. RFN: A Random-Feature Based Newton Method for Empirical Risk Minimization in Reproducing Kernel Hilbert Spaces. (arXiv:2002.04753v4 [cs.LG] UPDATED)
128. Robust Persistence Diagrams using Reproducing Kernels. (arXiv:2006.10012v2 [math.ST] UPDATED)
129. Using Unlabeled Data for Increasing Low-Shot Classification Accuracy of Relevant and Open-Set Irrelevant Images. (arXiv:2010.00721v2 [cs.CV] UPDATED)
130. Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations. (arXiv:2011.07633v2 [cs.CR] UPDATED)
131. Nudge: Accelerating Overdue Pull Requests Towards Completion. (arXiv:2011.12468v4 [cs.SE] UPDATED)
132. Accelerating Training of Batch Normalization: A Manifold Perspective. (arXiv:2101.02916v3 [cs.LG] UPDATED)
133. On $L^q$ Convergence of the Hamiltonian Monte Carlo. (arXiv:2101.08688v2 [math.CA] UPDATED)
134. Adaptive Pairwise Weights for Temporal Credit Assignment. (arXiv:2102.04999v2 [cs.LG] UPDATED)
135. A New Look and Convergence Rate of Federated Multi-Task Learning with Laplacian Regularization. (arXiv:2102.07148v4 [cs.LG] UPDATED)
136. Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning. (arXiv:2102.08581v2 [cs.LG] UPDATED)
137. Monotonic Alpha-divergence Minimisation for Variational Inference. (arXiv:2103.05684v3 [stat.CO] UPDATED)
138. Towards Optimal Algorithms for Multi-Player Bandits without Collision Sensing Information. (arXiv:2103.13059v2 [stat.ML] UPDATED)
139. The art of defense: letting networks fool the attacker. (arXiv:2104.02963v3 [cs.CV] UPDATED)
140. Very Lightweight Photo Retouching Network with Conditional Sequential Modulation. (arXiv:2104.06279v2 [cs.CV] UPDATED)
141. Understanding the Eluder Dimension. (arXiv:2104.06970v2 [cs.LG] UPDATED)
142. SMLSOM: The shrinking maximum likelihood self-organizing map. (arXiv:2104.13971v2 [cs.LG] UPDATED)
143. Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v2 [cs.IR] UPDATED)
144. Characterizing the SLOPE Trade-off: A Variational Perspective and the Donoho-Tanner Limit. (arXiv:2105.13302v2 [math.ST] UPDATED)
145. Learning from Counterfactual Links for Link Prediction. (arXiv:2106.02172v2 [cs.LG] UPDATED)
146. Redundant representations help generalization in wide neural networks. (arXiv:2106.03485v3 [stat.ML] UPDATED)
147. Learning of feature points without additional supervision improves reinforcement learning from images. (arXiv:2106.07995v3 [cs.LG] UPDATED)
148. Multi-Domain Active Learning: Literature Review and Comparative Study. (arXiv:2106.13516v5 [cs.LG] UPDATED)
149. Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v4 [cs.LG] UPDATED)
150. Accounting for shared covariates in semi-parametric Bayesian additive regression trees. (arXiv:2108.07636v6 [stat.ML] UPDATED)
151. DA-MUSIC: Data-Driven DoA Estimation via Deep Augmented MUSIC Algorithm. (arXiv:2109.10581v4 [eess.SP] UPDATED)
152. Introducing Symmetries to Black Box Meta Reinforcement Learning. (arXiv:2109.10781v2 [cs.LG] UPDATED)
153. 3D Infomax improves GNNs for Molecular Property Prediction. (arXiv:2110.04126v4 [cs.LG] UPDATED)
154. Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters. (arXiv:2110.04156v3 [cs.LG] UPDATED)
155. KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. (arXiv:2110.04330v2 [cs.CL] UPDATED)
156. Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v3 [cs.CV] UPDATED)
157. Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs. (arXiv:2110.05038v3 [cs.LG] UPDATED)
158. On the Convergence Theory for Hessian-Free Bilevel Algorithms. (arXiv:2110.07004v3 [cs.LG] UPDATED)
159. Adapt to Adaptation: Learning Personalization for Cross-Silo Federated Learning. (arXiv:2110.08394v3 [cs.LG] UPDATED)
160. A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v2 [cs.CV] UPDATED)
161. ECG synthesis with Neural ODE and GAN models. (arXiv:2111.00314v2 [cs.LG] UPDATED)
162. Double Control Variates for Gradient Estimation in Discrete Latent Variable Models. (arXiv:2111.05300v3 [stat.ML] UPDATED)
163. Distribution-Invariant Differential Privacy. (arXiv:2111.05791v2 [cs.CR] UPDATED)
164. Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling. (arXiv:2111.14819v2 [cs.CV] UPDATED)
165. Evaluation of Machine Learning Techniques for Forecast Uncertainty Quantification. (arXiv:2111.14844v5 [cs.LG] UPDATED)
166. Efficient Symptom Inquiring and Diagnosis via Adaptive Alignment of Reinforcement Learning and Classification. (arXiv:2112.00733v2 [cs.LG] UPDATED)
167. Causal Distillation for Language Models. (arXiv:2112.02505v2 [cs.CL] UPDATED)
168. Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models. (arXiv:2112.03860v2 [cs.CV] UPDATED)
169. Logarithmic Unbiased Quantization: Simple 4-bit Training in Deep Learning. (arXiv:2112.10769v3 [cs.LG] UPDATED)
170. Distributed Random Reshuffling over Networks. (arXiv:2112.15287v3 [math.OC] UPDATED)
171. A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs. (arXiv:2201.00402v2 [math.OC] UPDATED)
172. ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling. (arXiv:2201.01337v3 [cs.CL] UPDATED)
173. Wind Park Power Prediction: Attention-Based Graph Networks and Deep Learning to Capture Wake Losses. (arXiv:2201.03229v2 [cs.LG] UPDATED)
174. DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation. (arXiv:2201.04038v2 [cs.LG] UPDATED)
175. Physics-informed neural networks for modeling rate- and temperature-dependent plasticity. (arXiv:2201.08363v2 [cond-mat.mtrl-sci] UPDATED)
176. Learning Resource Allocation Policies from Observational Data with an Application to Homeless Services Delivery. (arXiv:2201.10053v2 [cs.LG] UPDATED)
177. Convergence of Invariant Graph Networks. (arXiv:2201.10129v2 [cs.LG] UPDATED)
178. Neural Approximation of Extended Persistent Homology on Graphs. (arXiv:2201.12032v2 [cs.LG] UPDATED)
179. From data to functa: Your data point is a function and you can treat it like one. (arXiv:2201.12204v2 [cs.LG] UPDATED)
180. Training Thinner and Deeper Neural Networks: Jumpstart Regularization. (arXiv:2201.12795v2 [cs.LG] UPDATED)
181. Single Time-scale Actor-critic Method to Solve the Linear Quadratic Regulator with Convergence Guarantees. (arXiv:2202.00048v2 [math.OC] UPDATED)
182. Deep Kernelized Dense Geometric Matching. (arXiv:2202.00667v2 [cs.CV] UPDATED)
183. Deep invariant networks with differentiable augmentation layers. (arXiv:2202.02142v4 [cs.LG] UPDATED)
184. COIL: Constrained Optimization in Learned Latent Space: Learning Representations for Valid Solutions. (arXiv:2202.02163v4 [cs.NE] UPDATED)
185. Conditional Gradients for the Approximately Vanishing Ideal. (arXiv:2202.03349v11 [cs.LG] UPDATED)
186. Simplified Graph Convolution with Heterophily. (arXiv:2202.04139v2 [cs.LG] UPDATED)
187. EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction. (arXiv:2202.05146v4 [q-bio.BM] UPDATED)
188. Understanding Rare Spurious Correlations in Neural Networks. (arXiv:2202.05189v2 [cs.LG] UPDATED)
189. Sample-Efficient Reinforcement Learning with loglog(T) Switching Cost. (arXiv:2202.06385v2 [cs.LG] UPDATED)
190. Active Uncertainty Reduction for Human-Robot Interaction: An Implicit Dual Control Approach. (arXiv:2202.07720v2 [cs.RO] UPDATED)
191. Gradient Estimation with Discrete Stein Operators. (arXiv:2202.09497v3 [stat.ML] UPDATED)
192. Bayes-Optimal Classifiers under Group Fairness. (arXiv:2202.09724v4 [stat.ML] UPDATED)
193. Global-Local Regularization Via Distributional Robustness. (arXiv:2203.00553v2 [cs.LG] UPDATED)
194. Koopman Methods for Estimation of Animal Motions over Unknown Submanifolds. (arXiv:2203.05646v2 [stat.ML] UPDATED)
195. On Connecting Deep Trigonometric Networks with Deep Gaussian Processes: Covariance, Expressivity, and Neural Tangent Kernel. (arXiv:2203.07411v2 [cs.LG] UPDATED)
196. Graph Neural Network Sensitivity Under Probabilistic Error Model. (arXiv:2203.07831v2 [stat.ML] UPDATED)
197. On the Generalization Mystery in Deep Learning. (arXiv:2203.10036v3 [cs.LG] UPDATED)
198. Optimal Fine-Grained N:M sparsity for Activations and Neural Gradients. (arXiv:2203.10991v2 [cs.LG] UPDATED)
199. Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v3 [cs.CV] UPDATED)
200. Text Transformations in Contrastive Self-Supervised Learning: A Review. (arXiv:2203.12000v2 [cs.CL] UPDATED)
201. Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v3 [cs.CV] UPDATED)
202. Sparse Federated Learning with Hierarchical Personalized Models. (arXiv:2203.13517v2 [cs.LG] UPDATED)
203. LiDAR Snowfall Simulation for Robust 3D Object Detection. (arXiv:2203.15118v2 [cs.CV] UPDATED)
204. Guaranteed Bounds for Posterior Inference in Universal Probabilistic Programming. (arXiv:2204.02948v2 [cs.PL] UPDATED)
205. Physics-assisted Generative Adversarial Network for X-Ray Tomography. (arXiv:2204.03703v2 [eess.IV] UPDATED)
206. Structure-aware Protein Self-supervised Learning. (arXiv:2204.04213v2 [cs.LG] UPDATED)
207. Augmentation-Free Graph Contrastive Learning with Performance Guarantee. (arXiv:2204.04874v2 [cs.LG] UPDATED)
208. From Spoken Thoughts to Automated Driving Commentary: Predicting and Explaining Intelligent Vehicles' Actions. (arXiv:2204.09109v2 [cs.AI] UPDATED)
209. Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations. (arXiv:2204.09781v3 [cs.DL] UPDATED)
210. Scalable Sensitivity and Uncertainty Analysis for Causal-Effect Estimates of Continuous-Valued Interventions. (arXiv:2204.10022v3 [cs.LG] UPDATED)
211. Visual Attention Emerges from Recurrent Sparse Reconstruction. (arXiv:2204.10962v2 [cs.CV] UPDATED)
212. Data-Efficient Backdoor Attacks. (arXiv:2204.12281v2 [cs.CV] UPDATED)
213. Supervised Contrastive CSI Representation Learning for Massive MIMO Positioning. (arXiv:2204.12796v2 [cs.IT] UPDATED)
214. Learning Effective SDEs from Brownian Dynamics Simulations of Colloidal Particles. (arXiv:2205.00286v2 [math.DS] UPDATED)
215. CANShield: Signal-based Intrusion Detection for Controller Area Networks. (arXiv:2205.01306v2 [cs.CR] UPDATED)
216. Nonstationary Bandit Learning via Predictive Sampling. (arXiv:2205.01970v3 [cs.LG] UPDATED)
217. Fair Bayes-Optimal Classifiers Under Predictive Parity. (arXiv:2205.07182v2 [stat.ML] UPDATED)
218. ClusterEA: Scalable Entity Alignment with Stochastic Training and Normalized Mini-batch Similarities. (arXiv:2205.10312v2 [cs.DB] UPDATED)
219. A Review of Safe Reinforcement Learning: Methods, Theory and Applications. (arXiv:2205.10330v3 [cs.AI] UPDATED)
220. Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v3 [cs.CV] UPDATED)
221. Exploring the stimulative effect on following drivers in a consecutive lane-change using microscopic vehicle trajectory data. (arXiv:2205.11252v2 [cs.LG] UPDATED)
222. An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v2 [cs.LG] UPDATED)
223. Uniform Generalization Bound on Time and Inverse Temperature for Gradient Descent Algorithm and its Application to Analysis of Simulated Annealing. (arXiv:2205.12959v2 [cs.LG] UPDATED)
224. Optimizing Objective Functions from Trained ReLU Neural Networks via Sampling. (arXiv:2205.14189v2 [math.OC] UPDATED)
225. Provably Auditing Ordinary Least Squares in Low Dimensions. (arXiv:2205.14284v2 [stat.ML] UPDATED)
226. Automatic Expert Selection for Multi-Scenario and Multi-Task Search. (arXiv:2205.14321v2 [cs.LG] UPDATED)
227. Approximation of Functionals by Neural Network without Curse of Dimensionality. (arXiv:2205.14421v2 [math.NA] UPDATED)
228. Machine Learning for Microcontroller-Class Hardware -- A Review. (arXiv:2205.14550v2 [cs.LG] UPDATED)
229. Contributions to Representation Learning with Graph Autoencoders and Applications to Music Recommendation. (arXiv:2205.14651v2 [cs.LG] UPDATED)
230. Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student Settings and its Superiority to Kernel Methods. (arXiv:2205.14818v2 [stat.ML] UPDATED)
231. Connecting adversarial attacks and optimal transport for domain adaptation. (arXiv:2205.15424v2 [cs.LG] UPDATED)
232. k-Means Maximum Entropy Exploration. (arXiv:2205.15623v2 [cs.LG] UPDATED)
233. The CLRS Algorithmic Reasoning Benchmark. (arXiv:2205.15659v2 [cs.LG] UPDATED)
234. Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints. (arXiv:2205.15953v2 [cs.LG] UPDATED)
235. Feature Selection for Discovering Distributional Treatment Effect Modifiers. (arXiv:2206.00516v2 [cs.LG] UPDATED)
236. SolarGAN: Synthetic Annual Solar Irradiance Time Series on Urban Building Facades via Deep Generative Networks. (arXiv:2206.00747v2 [cs.LG] UPDATED)
237. Hybrid Models for Mixed Variables in Bayesian Optimization. (arXiv:2206.01409v2 [cs.LG] UPDATED)
238. Non-Intrusive Reduced Models based on Operator Inference for Chaotic Systems. (arXiv:2206.01604v2 [cs.LG] UPDATED)
239. OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v2 [cs.LG] UPDATED)
240. Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection. (arXiv:2206.00962v1 [cs.CL] CROSS LISTED)
## cs.AI
---
**126** new papers in cs.AI:-) 
1. A review of machine learning approaches, challenges and prospects for computational tumor pathology. (arXiv:2206.01728v1 [eess.IV])
2. Nonsmooth automatic differentiation: a cheap gradient principle and other complexity results. (arXiv:2206.01730v1 [math.NA])
3. Using UAS Imagery and Computer Vision to Support Site-Specific Weed Control in Corn. (arXiv:2206.01734v1 [cs.CV])
4. Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (arXiv:2206.01736v1 [eess.IV])
5. Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial Intelligence Techniques. (arXiv:2206.01745v1 [eess.IV])
6. Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach. (arXiv:2206.01746v1 [eess.IV])
7. Uncertainty Estimation in Machine Learning. (arXiv:2206.01749v1 [cs.LG])
8. [Re] Badder Seeds: Reproducing the Evaluation of Lexical Methods for Bias Measurement. (arXiv:2206.01767v1 [cs.CL])
9. Radar Guided Dynamic Visual Attention for Resource-Efficient RGB Object Detection. (arXiv:2206.01772v1 [cs.CV])
10. Do-Operation Guided Causal Representation Learning with Reduced Supervision Strength. (arXiv:2206.01802v1 [cs.LG])
11. Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks. (arXiv:2206.01812v1 [cs.LG])
12. Option Discovery for Autonomous Generation of Symbolic Knowledge. (arXiv:2206.01815v1 [cs.AI])
13. QAGCN: A Graph Convolutional Network-based Multi-Relation Question Answering System. (arXiv:2206.01818v1 [cs.AI])
14. HDDL 2.1: Towards Defining an HTN Formalism with Time. (arXiv:2206.01822v1 [cs.AI])
15. Drawing out of Distribution with Neuro-Symbolic Generative Models. (arXiv:2206.01829v1 [cs.LG])
16. Coffee Roast Intelligence. (arXiv:2206.01841v1 [cs.CV])
17. Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning. (arXiv:2206.01843v1 [cs.CV])
18. Design and Implementation of an Heuristic-Enhanced Branch-and-Bound Solver for MILP. (arXiv:2206.01857v1 [cs.AI])
19. Estimating the Effect of Team Hitting Strategies Using Counterfactual Virtual Simulation in Baseball. (arXiv:2206.01871v1 [cs.AI])
20. A Superimposed Divide-and-Conquer Image Recognition Method for SEM Images of Nanoparticles on The Surface of Monocrystalline silicon with High Aggregation Degree. (arXiv:2206.01884v1 [cs.CV])
21. Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning. (arXiv:2206.01888v1 [cs.LG])
22. Initial Study into Application of Feature Density and Linguistically-backed Embedding to Improve Machine Learning-based Cyberbullying Detection. (arXiv:2206.01889v1 [cs.CL])
23. Modeling of Textures to Predict Immune Cell Status and Survival of Brain Tumour Patients. (arXiv:2206.01897v1 [eess.IV])
24. Evaluation of creating scoring opportunities for teammates in soccer via trajectory prediction. (arXiv:2206.01899v1 [cs.AI])
25. Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v1 [cs.AI])
26. Soft Adversarial Training Can Retain Natural Accuracy. (arXiv:2206.01904v1 [cs.LG])
27. The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition. (arXiv:2206.01910v1 [cs.CV])
28. Discovering Ancestral Instrumental Variables for Causal Inference from Observational Data. (arXiv:2206.01931v1 [cs.AI])
29. Stochastic Multiple Target Sampling Gradient Descent. (arXiv:2206.01934v1 [cs.LG])
30. Robust Meta-learning with Sampling Noise and Label Noise via Eigen-Reptile. (arXiv:2206.01944v1 [cs.LG])
31. Exploring the Potential of Feature Density in Estimating Machine Learning Classifier Performance with Application to Cyberbullying Detection. (arXiv:2206.01949v1 [cs.CL])
32. Comparing Performance of Different Linguistically-Backed Word Embeddings for Cyberbullying Detection. (arXiv:2206.01950v1 [cs.CL])
33. MPE inference using an Incremental Build-Infer-Approximate Paradigm. (arXiv:2206.01954v1 [cs.AI])
34. Instance-wise Prompt Tuning for Pretrained Language Models. (arXiv:2206.01958v1 [cs.CL])
35. C$^3$Fusion: Consistent Contrastive Colon Fusion, Towards Deep SLAM in Colonoscopy. (arXiv:2206.01961v1 [cs.CV])
36. MACC: Cross-Layer Multi-Agent Congestion Control with Deep Reinforcement Learning. (arXiv:2206.01972v1 [cs.NI])
37. Modelling and Mining of Patient Pathways: A Scoping Review. (arXiv:2206.01980v1 [cs.CY])
38. Evaluation of Xilinx Deep Learning Processing Unit under Neutron Irradiation. (arXiv:2206.01981v1 [physics.ins-det])
39. CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v1 [cs.CV])
40. Causal Discovery in Heterogeneous Environments Under the Sparse Mechanism Shift Hypothesis. (arXiv:2206.02013v1 [cs.LG])
41. Symmetry as a Representation of Intuitive Geometry?. (arXiv:2206.02019v1 [cs.AI])
42. Guided Deep Metric Learning. (arXiv:2206.02029v1 [cs.CV])
43. Interpolating Between Softmax Policy Gradient and Neural Replicator Dynamics with Capped Implicit Exploration. (arXiv:2206.02036v1 [cs.LG])
44. Beyond Value: CHECKLIST for Testing Inferences in Planning-Based RL. (arXiv:2206.02039v1 [cs.AI])
45. Developing hierarchical anticipations via neural network-based event segmentation. (arXiv:2206.02042v1 [cs.LG])
46. Fast and Accurate Error Simulation for CNNs against Soft Errors. (arXiv:2206.02051v1 [cs.AR])
47. Interpretable Models Capable of Handling Systematic Missingness in Imbalanced Classes and Heterogeneous Datasets. (arXiv:2206.02056v1 [cs.LG])
48. Your Neighbors Are Communicating: Towards Powerful and Scalable Graph Neural Networks. (arXiv:2206.02059v1 [cs.LG])
49. Active Bayesian Causal Inference. (arXiv:2206.02063v1 [cs.LG])
50. PIDNet: A **Real-time** Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v1 [cs.CV])
51. Learning Robust Representations Of Generative Models Using Set-Based Artificial Fingerprints. (arXiv:2206.02067v1 [cs.LG])
52. All One Needs to Know about Priors for Deep Image **Restoration** and **Enhancement**: A Survey. (arXiv:2206.02070v1 [cs.CV])
53. LAE: Language-Aware Encoder for Monolingual and Multilingual ASR. (arXiv:2206.02093v1 [cs.CL])
54. AUTM Flow: Atomic Unrestricted Time Machine for Monotonic Normalizing Flows. (arXiv:2206.02102v1 [cs.LG])
55. Product safety idioms: a method for building causal Bayesian networks for product safety and risk assessment. (arXiv:2206.02144v1 [cs.AI])
56. Sentiment Analysis of Online Travel Reviews Based on Capsule Network and Sentiment Lexicon. (arXiv:2206.02160v1 [cs.CL])
57. Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach. (arXiv:2206.02164v1 [cs.LG])
58. Formally Verified Solution Methods for Infinite-Horizon Markov Decision Processes. (arXiv:2206.02169v1 [cs.AI])
59. Factored Conditional Filtering: Tracking States and Estimating Parameters in High-Dimensional Spaces. (arXiv:2206.02178v1 [cs.AI])
60. Variable-rate hierarchical CPC leads to acoustic unit discovery in speech. (arXiv:2206.02211v1 [cs.SD])
61. Sequential Counterfactual Decision-Making Under Confounded Reward. (arXiv:2206.02216v1 [cs.AI])
62. Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography. (arXiv:2206.02225v1 [eess.IV])
63. Finetuning a Kalaallisut-English machine translation system using web-crawled data. (arXiv:2206.02230v1 [cs.CL])
64. Models of human preference for learning reward functions. (arXiv:2206.02231v1 [cs.LG])
65. OntoMerger: An Ontology Integration Library for Deduplicating and Connecting Knowledge Graph Nodes. (arXiv:2206.02238v1 [cs.AI])
66. Conceptual Design of the Memory System of the Robot Cognitive Architecture ArmarX. (arXiv:2206.02241v1 [cs.AI])
67. Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models. (arXiv:2206.02246v1 [cs.SD])
68. Augmenting Netflix Search with In-Session Adapted Recommendations. (arXiv:2206.02254v1 [cs.IR])
69. Use-Case-Grounded Simulations for Explanation Evaluation. (arXiv:2206.02256v1 [cs.HC])
70. Estimating Building Energy Efficiency From Street View Imagery, Aerial Imagery, and Land Surface Temperature Data. (arXiv:2206.02270v1 [cs.CV])
71. A knowledge graph representation learning approach to predict novel kinase-substrate interactions. (arXiv:2206.02290v1 [q-bio.QM])
72. HIFI-Net: A Novel Network for **Enhancement** to Underwater Images. (arXiv:2206.02295v1 [cs.CV])
73. Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v1 [cs.CV])
74. MASNet:Improve Performance of Siamese Networks with Mutual-attention for Remote Sensing Change Detection Tasks. (arXiv:2206.02331v1 [cs.CV])
75. Effects of Augmented-Reality-Based Assisting Interfaces on Drivers' Object-wise Situational Awareness in Highly Autonomous Vehicles. (arXiv:2206.02332v1 [cs.HC])
76. On the Advance of Making Language Models Better Reasoners. (arXiv:2206.02336v1 [cs.CL])
77. Complex Locomotion Skill Learning via Differentiable Physics. (arXiv:2206.02341v1 [cs.AI])
78. WHU-Stereo: A Challenging Benchmark for Stereo Matching of High-Resolution Satellite Images. (arXiv:2206.02342v1 [cs.CV])
79. Decentralized, Communication- and Coordination-free Learning in Structured Matching Markets. (arXiv:2206.02344v1 [cs.AI])
80. Anomaly Detection with Test Time Augmentation and Consistency Evaluation. (arXiv:2206.02345v1 [cs.CV])
81. RealAnt: An Open-Source Low-Cost Quadruped for Education and Research in Real-World Reinforcement Learning. (arXiv:2011.03085v2 [cs.RO] UPDATED)
82. RADAR-X: An Interactive Mixed Initiative Planning Interface Pairing Contrastive Explanations and Revised Plan Suggestions. (arXiv:2011.09644v2 [cs.AI] UPDATED)
83. Nudge: Accelerating Overdue Pull Requests Towards Completion. (arXiv:2011.12468v4 [cs.SE] UPDATED)
84. Adaptive Pairwise Weights for Temporal Credit Assignment. (arXiv:2102.04999v2 [cs.LG] UPDATED)
85. Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning. (arXiv:2102.08581v2 [cs.LG] UPDATED)
86. Multi-Domain Active Learning: Literature Review and Comparative Study. (arXiv:2106.13516v5 [cs.LG] UPDATED)
87. Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v4 [cs.LG] UPDATED)
88. LLVIP: A Visible-infrared Paired Dataset for **Low-light** Vision. (arXiv:2108.10831v3 [cs.CV] UPDATED)
89. PCNN: A physics-constrained neural network for multiphase flows. (arXiv:2109.08965v2 [physics.flu-dyn] UPDATED)
90. Introducing Symmetries to Black Box Meta Reinforcement Learning. (arXiv:2109.10781v2 [cs.LG] UPDATED)
91. 3D Infomax improves GNNs for Molecular Property Prediction. (arXiv:2110.04126v4 [cs.LG] UPDATED)
92. Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters. (arXiv:2110.04156v3 [cs.LG] UPDATED)
93. Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs. (arXiv:2110.05038v3 [cs.LG] UPDATED)
94. K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways. (arXiv:2110.11048v2 [cs.CV] UPDATED)
95. Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling. (arXiv:2111.14819v2 [cs.CV] UPDATED)
96. Evaluation of Machine Learning Techniques for Forecast Uncertainty Quantification. (arXiv:2111.14844v5 [cs.LG] UPDATED)
97. Efficient Symptom Inquiring and Diagnosis via Adaptive Alignment of Reinforcement Learning and Classification. (arXiv:2112.00733v2 [cs.LG] UPDATED)
98. MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection. (arXiv:2112.08935v3 [cs.CV] UPDATED)
99. A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs. (arXiv:2201.00402v2 [math.OC] UPDATED)
100. ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling. (arXiv:2201.01337v3 [cs.CL] UPDATED)
101. Wind Park Power Prediction: Attention-Based Graph Networks and Deep Learning to Capture Wake Losses. (arXiv:2201.03229v2 [cs.LG] UPDATED)
102. DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation. (arXiv:2201.04038v2 [cs.LG] UPDATED)
103. Deep invariant networks with differentiable augmentation layers. (arXiv:2202.02142v4 [cs.LG] UPDATED)
104. Automatic Identification of Self-Admitted Technical Debt from Four Different Sources. (arXiv:2202.02387v3 [cs.SE] UPDATED)
105. Sample-Efficient Reinforcement Learning with loglog(T) Switching Cost. (arXiv:2202.06385v2 [cs.LG] UPDATED)
106. Student Dangerous Behavior Detection in School. (arXiv:2202.09550v2 [cs.CV] UPDATED)
107. The Quest for a Common Model of the Intelligent Decision Maker. (arXiv:2202.13252v3 [cs.AI] UPDATED)
108. Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey. (arXiv:2202.13589v2 [cs.CV] UPDATED)
109. Global-Local Regularization Via Distributional Robustness. (arXiv:2203.00553v2 [cs.LG] UPDATED)
110. Optimal Fine-Grained N:M sparsity for Activations and Neural Gradients. (arXiv:2203.10991v2 [cs.LG] UPDATED)
111. Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v3 [cs.CV] UPDATED)
112. Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v2 [cs.AI] UPDATED)
113. Structure-aware Protein Self-supervised Learning. (arXiv:2204.04213v2 [cs.LG] UPDATED)
114. From Spoken Thoughts to Automated Driving Commentary: Predicting and Explaining Intelligent Vehicles' Actions. (arXiv:2204.09109v2 [cs.AI] UPDATED)
115. Data-Efficient Backdoor Attacks. (arXiv:2204.12281v2 [cs.CV] UPDATED)
116. Understanding User Perceptions, Collaborative Experience and User Engagement in Different Human-AI Interaction Designs for Co-Creative Systems. (arXiv:2204.13217v2 [cs.HC] UPDATED)
117. End-to-End Rubbing **Restoration** Using Generative Adversarial Networks. (arXiv:2205.03743v2 [cs.CV] UPDATED)
118. Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v2 [cs.CV] UPDATED)
119. ClusterEA: Scalable Entity Alignment with Stochastic Training and Normalized Mini-batch Similarities. (arXiv:2205.10312v2 [cs.DB] UPDATED)
120. A Review of Safe Reinforcement Learning: Methods, Theory and Applications. (arXiv:2205.10330v3 [cs.AI] UPDATED)
121. An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v2 [cs.LG] UPDATED)
122. Leveraging Causal Inference for Explainable Automatic Program Repair. (arXiv:2205.13342v2 [cs.SE] UPDATED)
123. Machine Learning-Based User Scheduling in Integrated Satellite-HAPS-Ground Networks. (arXiv:2205.13958v3 [cs.AI] UPDATED)
124. Guided Diffusion Model for Adversarial Purification. (arXiv:2205.14969v2 [cs.CV] UPDATED)
125. OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v2 [cs.LG] UPDATED)
126. Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection. (arXiv:2206.00962v1 [cs.CL] CROSS LISTED)

