# Your interest papers
---
## cs.CV
---
### PARSE challenge 2022: Pulmonary Arteries Segmentation using **Swin** U-Net Transformer(**Swin** UNETR) and U-Net. (arXiv:2208.09636v1 [eess.IV])
- Authors : Akansh Maurya, Kunal Dashrath, Rohan Padhy, Kalluri Ramakrishna, Ganapathy Krishnamurthi
- Link : [http://arxiv.org/abs/2208.09636](http://arxiv.org/abs/2208.09636)
> ABSTRACT  :  In this work, we present our proposed method to segment the pulmonary arteries from the CT scans using **Swin** UNETR and U-Net-based deep neural network architecture. Six models, three models based on **Swin** UNETR, and three models based on 3D U-net with residual units were ensemble using a weighted average to make the final segmentation masks. Our team achieved a multi-level dice score of 84.36 percent through this method. The code of our work is available on the following link: https://github.com/akansh12/parse2022. This work is part of the MICCAI PARSE 2022 challenge.  
### SnowFormer: Scale-aware Transformer via Context Interaction for Single Image Desnowing. (arXiv:2208.09703v1 [cs.CV])
- Authors : Sixiang Chen, Tian Ye, Yun Liu, Erkang Chen, Jun Shi, Jingchun Zhou
- Link : [http://arxiv.org/abs/2208.09703](http://arxiv.org/abs/2208.09703)
> ABSTRACT  :  Single image desnowing is a common yet challenging task. The complex snow degradations and diverse degradation scales demand strong representation ability. In order for the desnowing network to see various snow degradations and model the context interaction of local details and global information, we propose a powerful architecture dubbed as SnowFormer. First, it performs Scale-aware Feature Aggregation in the encoder to capture rich snow information of various degradations. Second, in order to tackle with large-scale degradation, it uses a novel Context Interaction Transformer Block in the decoder, which conducts context interaction of local details and global information from previous scale-aware feature aggregation in global context interaction. And the introduction of local context interaction improves recovery of scene details. Third, we devise a Heterogeneous Feature Projection Head which progressively fuse features from both the encoder and decoder and project the refined feature into the clean image. Extensive experiments demonstrate that the proposed SnowFormer achieves significant improvements over other SOTA methods. Compared with SOTA single image desnowing method HDCW-Net, it boosts the PSNR metric by 9.2dB on the CSD testset. Moreover, it also achieves a 5.13dB increase in PSNR compared with general image **restoration** architecture NAFNet, which verifies the strong representation ability of our SnowFormer for snow removal task. The code is released in \url{https://github.com/Ephemeral182/SnowFormer}.  
### HST: Hierarchical **Swin** Transformer for Compressed Image Super-resolution. (arXiv:2208.09885v1 [cs.CV])
- Authors : Bingchen Li, Xin Li, Yiting Lu, Sen Liu, Ruoyu Feng, Zhibo Chen
- Link : [http://arxiv.org/abs/2208.09885](http://arxiv.org/abs/2208.09885)
> ABSTRACT  :  Compressed Image Super-resolution has achieved great attention in recent years, where images are degraded with compression artifacts and low-resolution artifacts. Since the complex hybrid distortions, it is hard to restore the distorted image with the simple cooperation of super-resolution and compression artifacts removing. In this paper, we take a step forward to propose the Hierarchical **Swin** Transformer (HST) network to restore the low-resolution compressed image, which jointly captures the hierarchical feature representations and enhances each-scale representation with **Swin** transformer, respectively. Moreover, we find that the pretraining with Super-resolution (SR) task is vital in compressed image super-resolution. To explore the effects of different SR pretraining, we take the commonly-used SR tasks (e.g., bicubic and different real super-resolution simulations) as our pretraining tasks, and reveal that SR plays an irreplaceable role in the compressed image super-resolution. With the cooperation of HST and pre-training, our HST achieves the fifth place in AIM 2022 challenge on the low-quality compressed image super-resolution track, with the PSNR of 23.51dB. Extensive experiments and ablation studies have validated the effectiveness of our proposed methods.  
### FairDisCo: Fairer AI in Dermatology via Disentanglement Contrastive Learning. (arXiv:2208.10013v1 [cs.CV])
- Authors : Siyi Du, Ben Hers, Nourhan Bayasi, Ghassan Hamarneh, Rafeef Garbi
- Link : [http://arxiv.org/abs/2208.10013](http://arxiv.org/abs/2208.10013)
> ABSTRACT  :  Deep learning models have achieved great success in automating skin lesion diagnosis. However, the ethnic disparity in these models' predictions, where lesions on **dark**er skin types are usually underrepresented and have lower diagnosis accuracy, receives little attention. In this paper, we propose FairDisCo, a disentanglement deep learning framework with contrastive learning that utilizes an additional network branch to remove sensitive attributes, i.e. skin-type information from representations for fairness and another contrastive branch to enhance feature extraction. We compare FairDisCo to three fairness methods, namely, resampling, reweighting, and attribute-aware, on two newly released skin lesion datasets with different skin types: Fitzpatrick17k and Diverse Dermatology Images (DDI). We adapt two fairness-based metrics DPM and EOM for our multiple classes and sensitive attributes task, highlighting the skin-type bias in skin lesion classification. Extensive experimental evaluation demonstrates the effectiveness of FairDisCo, with fairer and superior performance on skin lesion classification tasks.  
### Rethinking Knowledge Distillation via Cross-Entropy. (arXiv:2208.10139v1 [cs.CV])
- Authors : Zhendong Yang, Zhe Li, Yuan Gong, Tianke Zhang, Shanshan Lao, Chun Yuan, Yu Li
- Link : [http://arxiv.org/abs/2208.10139](http://arxiv.org/abs/2208.10139)
> ABSTRACT  :  Knowledge Distillation (KD) has developed extensively and boosted various tasks. The classical KD method adds the KD loss to the original cross-entropy (CE) loss. We try to decompose the KD loss to explore its relation with the CE loss. Surprisingly, we find it can be regarded as a combination of the CE loss and an extra loss which has the identical form as the CE loss. However, we notice the extra loss forces the student's relative probability to learn the teacher's absolute probability. Moreover, the sum of the two probabilities is different, making it hard to optimize. To address this issue, we revise the formulation and propose a distributed loss. In addition, we utilize teachers' target output as the soft target, proposing the soft loss. Combining the soft loss and the distributed loss, we propose a new KD loss (NKD). Furthermore, we smooth students' target output to treat it as the soft target for training without teachers and propose a teacher-free new KD loss (tf-NKD). Our method achieves state-of-the-art performance on CIFAR-100 and ImageNet. For example, with ResNet-34 as the teacher, we boost the ImageNet Top-1 accuracy of ResNet18 from 69.90% to 71.96%. In training without teachers, MobileNet, ResNet-18 and **Swin**Transformer-Tiny achieve 70.04%, 70.76%, and 81.48%, which are 0.83%, 0.86%, and 0.30% higher than the baseline, respectively. The code is available at https://github.com/yzd-v/cls_KD.  
### Seeing Objects in **dark** with Continual Contrastive Learning. (arXiv:2112.02891v3 [cs.CV] UPDATED)
- Authors : Ujjal Kr
- Link : [http://arxiv.org/abs/2112.02891](http://arxiv.org/abs/2112.02891)
> ABSTRACT  :  Object Detection, a fundamental computer vision problem, has paramount importance in smart camera systems. However, a truly reliable camera system could be achieved if and only if the underlying object detection component is robust enough across varying imaging conditions (or domains), for instance, different times of the day, adverse weather conditions, etc. In an effort to achieving a reliable camera system, in this paper, we make an attempt to train such a robust detector. Unfortunately, to build a well-performing detector across varying imaging conditions, one would require labeled training images (often in large numbers) from a plethora of corner cases. As manually obtaining such a large labeled dataset may be infeasible, we suggest using synthetic images, to mimic different training image domains. We propose a novel, contrastive learning method to align the latent representations of a pair of real and synthetic images, to make the detector robust to the different domains. However, we found that merely contrasting the embeddings may lead to catastrophic forgetting of the information essential for object detection. Hence, we employ a continual learning based penalty, to alleviate the issue of forgetting, while contrasting the representations. We showcase that our proposed method outperforms a wide range of alternatives to address the extremely challenging, yet under-studied scenario of object detection at **night**-time.  
### Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment. (arXiv:2203.14557v3 [eess.IV] UPDATED)
- Authors : Junyong You, Zheng Zhang
- Link : [http://arxiv.org/abs/2203.14557](http://arxiv.org/abs/2203.14557)
> ABSTRACT  :  Visual (image, video) quality assessments can be modelled by visual features in different domains, e.g., spatial, frequency, and temporal domains. Perceptual mechanisms in the human visual system (HVS) play a crucial role in generation of quality perception. This paper proposes a general framework for no-reference visual quality assessment using efficient windowed transformer architectures. A lightweight module for multi-stage channel attention is integrated into **Swin** (shifted window) Transformer. Such module can represent appropriate perceptual mechanisms in image quality assessment (IQA) to build an accurate IQA model. Meanwhile, representative features for image quality perception in the spatial and frequency domains can also be derived from the IQA model, which are then fed into another windowed transformer architecture for video quality assessment (VQA). The VQA model efficiently reuses attention information across local windows to tackle the issue of expensive time and memory complexities of original transformer. Experimental results on both large-scale IQA and VQA databases demonstrate that the proposed quality assessment models outperform other state-of-the-art models by large margins. The complete source code will be published on Github.  
### DBN-Mix: Training Dual Branch Network Using **Bilateral** Mixup Augmentation for Long-Tailed Visual Recognition. (arXiv:2207.02173v2 [cs.CV] UPDATED)
- Authors : Jae Soon, In Young, Jun Won
- Link : [http://arxiv.org/abs/2207.02173](http://arxiv.org/abs/2207.02173)
> ABSTRACT  :  There is growing interest in the challenging visual perception task of learning from long-tailed class distributions. The extreme class imbalance in the training dataset biases the model to prefer recognizing majority class data over minority class data. Furthermore, the lack of diversity in minority class samples makes it difficult to find a good representation. In this paper, we propose an effective data augmentation method, referred to as **bilateral** mixup augmentation, which can improve the performance of long-tailed visual recognition. The **bilateral** mixup augmentation combines two samples generated by a uniform sampler and a re-balanced sampler and augments the training dataset to enhance the representation learning for minority classes. We also reduce the classifier bias using class-wise temperature scaling, which scales the logits differently per class in the training phase. We apply both ideas to the dual-branch network (DBN) framework, presenting a new model, named dual-branch network with **bilateral** mixup (DBN-Mix). Experiments on popular long-tailed visual recognition datasets show that DBN-Mix improves performance significantly over baseline and that the proposed method achieves state-of-the-art performance in some categories of benchmarks.  
### UPST-**NeRF**: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene. (arXiv:2208.07059v2 [cs.CV] UPDATED)
- Authors : Yaosen Chen, Qi Yuan, Zhiqiang Li, Yuegen Liu, Wei Wang, Chaoping Xie, Xuming Wen, Qien Yu
- Link : [http://arxiv.org/abs/2208.07059](http://arxiv.org/abs/2208.07059)
> ABSTRACT  :  3D scenes photorealistic stylization aims to generate photorealistic images from arbitrary novel views according to a given style image while ensuring consistency when rendering from different viewpoints. Some existing stylization methods with neural radiance fields can effectively predict stylized scenes by combining the features of the style image with multi-view images to train 3D scenes. However, these methods generate novel view images that contain objectionable artifacts. Besides, they cannot achieve universal photorealistic stylization for a 3D scene. Therefore, a styling image must retrain a 3D scene representation network based on a neural radiation field. We propose a novel 3D scene photorealistic style transfer framework to address these issues. It can realize photorealistic 3D scene style transfer with a 2D style image. We first pre-trained a 2D photorealistic style transfer network, which can meet the photorealistic style transfer between any given content image and style image. Then, we use voxel features to optimize a 3D scene and get the geometric representation of the scene. Finally, we jointly optimize a hyper network to realize the scene photorealistic style transfer of arbitrary style images. In the transfer stage, we use a pre-trained 2D photorealistic network to constrain the photorealistic style of different views and different style images in the 3D scene. The experimental results show that our method not only realizes the 3D photorealistic style transfer of arbitrary style images but also outperforms the existing methods in terms of visual quality and consistency. Project page:https://semchan.github.io/UPST_**NeRF**.  
## eess.IV
---
### PARSE challenge 2022: Pulmonary Arteries Segmentation using **Swin** U-Net Transformer(**Swin** UNETR) and U-Net. (arXiv:2208.09636v1 [eess.IV])
- Authors : Akansh Maurya, Kunal Dashrath, Rohan Padhy, Kalluri Ramakrishna, Ganapathy Krishnamurthi
- Link : [http://arxiv.org/abs/2208.09636](http://arxiv.org/abs/2208.09636)
> ABSTRACT  :  In this work, we present our proposed method to segment the pulmonary arteries from the CT scans using **Swin** UNETR and U-Net-based deep neural network architecture. Six models, three models based on **Swin** UNETR, and three models based on 3D U-net with residual units were ensemble using a weighted average to make the final segmentation masks. Our team achieved a multi-level dice score of 84.36 percent through this method. The code of our work is available on the following link: https://github.com/akansh12/parse2022. This work is part of the MICCAI PARSE 2022 challenge.  
### HST: Hierarchical **Swin** Transformer for Compressed Image Super-resolution. (arXiv:2208.09885v1 [cs.CV])
- Authors : Bingchen Li, Xin Li, Yiting Lu, Sen Liu, Ruoyu Feng, Zhibo Chen
- Link : [http://arxiv.org/abs/2208.09885](http://arxiv.org/abs/2208.09885)
> ABSTRACT  :  Compressed Image Super-resolution has achieved great attention in recent years, where images are degraded with compression artifacts and low-resolution artifacts. Since the complex hybrid distortions, it is hard to restore the distorted image with the simple cooperation of super-resolution and compression artifacts removing. In this paper, we take a step forward to propose the Hierarchical **Swin** Transformer (HST) network to restore the low-resolution compressed image, which jointly captures the hierarchical feature representations and enhances each-scale representation with **Swin** transformer, respectively. Moreover, we find that the pretraining with Super-resolution (SR) task is vital in compressed image super-resolution. To explore the effects of different SR pretraining, we take the commonly-used SR tasks (e.g., bicubic and different real super-resolution simulations) as our pretraining tasks, and reveal that SR plays an irreplaceable role in the compressed image super-resolution. With the cooperation of HST and pre-training, our HST achieves the fifth place in AIM 2022 challenge on the low-quality compressed image super-resolution track, with the PSNR of 23.51dB. Extensive experiments and ablation studies have validated the effectiveness of our proposed methods.  
### What Does a One-Bit Quanta Image Sensor Offer?. (arXiv:2208.10350v1 [eess.IV])
- Authors : 
- Link : [http://arxiv.org/abs/2208.10350](http://arxiv.org/abs/2208.10350)
> ABSTRACT  :  The one-bit quanta image sensor (QIS) is a photon-counting device that captures image intensities using binary bits. Assuming that the analog voltage generated at the floating diffusion of the photodiode follows a Poisson-Gaussian distribution, the sensor produces either a ``1'' if the voltage is above a certain threshold or ``0'' if it is below the threshold. The concept of this binary sensor has been proposed for more than a decade, and physical devices have been built to realize the concept. However, what benefits does a one-bit QIS offer compared to a conventional multi-bit CMOS image sensor? Besides the known empirical results, are there theoretical proofs to support these findings?    The goal of this paper is to provide new theoretical support from a signal processing perspective. In particular, it is theoretically found that the sensor can offer three benefits: (1) **Low-light**: One-bit QIS performs better at **low-light** because it has a low read noise, and its one-bit quantization can produce an error-free measurement. However, this requires the **exposure** time to be appropriately configured. (2) Frame rate: One-bit sensors can operate at a much higher speed because a response is generated as soon as a photon is detected. However, in the presence of read noise, there exists an optimal frame rate beyond which the performance will degrade. A Closed-form expression of the optimal frame rate is derived. (3) Dynamic range: One-bit QIS offers a higher dynamic range. The benefit is brought by two complementary characteristics of the sensor: nonlinearity and **exposure** bracketing. The decoupling of the two factors is theoretically proved, and closed-form expressions are derived.  
### Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment. (arXiv:2203.14557v3 [eess.IV] UPDATED)
- Authors : Junyong You, Zheng Zhang
- Link : [http://arxiv.org/abs/2203.14557](http://arxiv.org/abs/2203.14557)
> ABSTRACT  :  Visual (image, video) quality assessments can be modelled by visual features in different domains, e.g., spatial, frequency, and temporal domains. Perceptual mechanisms in the human visual system (HVS) play a crucial role in generation of quality perception. This paper proposes a general framework for no-reference visual quality assessment using efficient windowed transformer architectures. A lightweight module for multi-stage channel attention is integrated into **Swin** (shifted window) Transformer. Such module can represent appropriate perceptual mechanisms in image quality assessment (IQA) to build an accurate IQA model. Meanwhile, representative features for image quality perception in the spatial and frequency domains can also be derived from the IQA model, which are then fed into another windowed transformer architecture for video quality assessment (VQA). The VQA model efficiently reuses attention information across local windows to tackle the issue of expensive time and memory complexities of original transformer. Experimental results on both large-scale IQA and VQA databases demonstrate that the proposed quality assessment models outperform other state-of-the-art models by large margins. The complete source code will be published on Github.  
## cs.LG
---
### Stop&Hop: Early Classification of Irregular Time Series. (arXiv:2208.09795v1 [cs.LG])
- Authors : Thomas Hartvigsen, Walter Gerych, Jidapa Thadajarassiri, Xiangnan Kong, Elke Rundensteiner
- Link : [http://arxiv.org/abs/2208.09795](http://arxiv.org/abs/2208.09795)
> ABSTRACT  :  Early classification algorithms help users react faster to their machine learning model's predictions. Early warning systems in hospitals, for example, let clinicians improve their patients' outcomes by accurately predicting infections. While early classification systems are advancing rapidly, a major gap remains: existing systems do not consider irregular time series, which have uneven and often-long gaps between their observations. Such series are notoriously pervasive in impactful domains like healthcare. We bridge this gap and study early classification of irregular time series, a new setting for early classifiers that opens doors to more real-world problems. Our solution, Stop&amp;Hop, uses a continuous-time recurrent network to model ongoing irregular time series in **real time**, while an irregularity-aware halting policy, trained with reinforcement learning, predicts when to stop and classify the streaming series. By taking real-valued step sizes, the halting policy flexibly decides exactly when to stop ongoing series in **real time**. This way, Stop&amp;Hop seamlessly integrates information contained in the timing of observations, a new and vital source for early classification in this setting, with the time series values to provide early classifications for irregular time series. Using four synthetic and three real-world datasets, we demonstrate that Stop&amp;Hop consistently makes earlier and more-accurate predictions than state-of-the-art alternatives adapted to this new problem. Our code is publicly available at https://github.com/thartvigsen/StopAndHop.  
### Heterogeneous Graph Masked Autoencoders. (arXiv:2208.09957v1 [cs.LG])
- Authors : Yijun Tian, Kaiwen Dong, Chunhui Zhang, Chuxu Zhang
- Link : [http://arxiv.org/abs/2208.09957](http://arxiv.org/abs/2208.09957)
> ABSTRACT  :  Generative self-supervised learning (SSL), especially masked autoencoders, has become one of the most exciting learning paradigms and has shown great potential in handling graph data. However, real-world graphs are always heterogeneous, which poses three critical challenges that existing methods ignore: 1) how to capture complex graph structure? 2) how to incorporate various node attributes? and 3) how to encode different node positions? In light of this, we study the problem of generative SSL on heterogeneous graphs and propose HGMAE, a novel heterogeneous graph masked autoencoder model to address these challenges. HGMAE captures comprehensive graph information via two innovative masking techniques and three unique training strategies. In particular, we first develop metapath masking and adaptive attribute masking with dynamic mask rate to enable effective and stable learning on heterogeneous graphs. We then design several training strategies including metapath-based edge reconstruction to adopt complex structural information, target attribute **restoration** to incorporate various node attributes, and positional feature prediction to encode node positional information. Extensive experiments demonstrate that HGMAE outperforms both contrastive and generative state-of-the-art baselines on several tasks across multiple datasets.  
### Seeing Objects in **dark** with Continual Contrastive Learning. (arXiv:2112.02891v3 [cs.CV] UPDATED)
- Authors : Ujjal Kr
- Link : [http://arxiv.org/abs/2112.02891](http://arxiv.org/abs/2112.02891)
> ABSTRACT  :  Object Detection, a fundamental computer vision problem, has paramount importance in smart camera systems. However, a truly reliable camera system could be achieved if and only if the underlying object detection component is robust enough across varying imaging conditions (or domains), for instance, different times of the day, adverse weather conditions, etc. In an effort to achieving a reliable camera system, in this paper, we make an attempt to train such a robust detector. Unfortunately, to build a well-performing detector across varying imaging conditions, one would require labeled training images (often in large numbers) from a plethora of corner cases. As manually obtaining such a large labeled dataset may be infeasible, we suggest using synthetic images, to mimic different training image domains. We propose a novel, contrastive learning method to align the latent representations of a pair of real and synthetic images, to make the detector robust to the different domains. However, we found that merely contrasting the embeddings may lead to catastrophic forgetting of the information essential for object detection. Hence, we employ a continual learning based penalty, to alleviate the issue of forgetting, while contrasting the representations. We showcase that our proposed method outperforms a wide range of alternatives to address the extremely challenging, yet under-studied scenario of object detection at **night**-time.  
### Conditional Born machine for Monte Carlo event generation. (arXiv:2205.07674v2 [quant-ph] UPDATED)
- Authors : Oriel Kiss, Michele Grossi, Enrique Kajomovitz, Sofia Vallecorsa
- Link : [http://arxiv.org/abs/2205.07674](http://arxiv.org/abs/2205.07674)
> ABSTRACT  :  Generative modeling is a promising task for near-term quantum devices, which can use the stochastic nature of quantum measurements as a random source. So called Born machines are purely quantum models and promise to generate probability distributions in a quantum way, inaccessible to classical computers. This paper presents an application of Born machines to Monte Carlo simulations and extends their reach to multivariate and conditional distributions. Models are run on (noisy) simulators and IBM Quantum superconducting quantum hardware.    More specifically, Born machines are used to generate muonic force carrier (MFC) events resulting from scattering processes between muons and the detector material in high-energy physics colliders experiments. MFCs are bosons appearing in beyond-the-standard-model theoretical frameworks, which are candidates for **dark** matter. Empirical evidence suggests that Born machines can reproduce the marginal distributions and correlations of data sets from Monte Carlo simulations.  
### DBN-Mix: Training Dual Branch Network Using **Bilateral** Mixup Augmentation for Long-Tailed Visual Recognition. (arXiv:2207.02173v2 [cs.CV] UPDATED)
- Authors : Jae Soon, In Young, Jun Won
- Link : [http://arxiv.org/abs/2207.02173](http://arxiv.org/abs/2207.02173)
> ABSTRACT  :  There is growing interest in the challenging visual perception task of learning from long-tailed class distributions. The extreme class imbalance in the training dataset biases the model to prefer recognizing majority class data over minority class data. Furthermore, the lack of diversity in minority class samples makes it difficult to find a good representation. In this paper, we propose an effective data augmentation method, referred to as **bilateral** mixup augmentation, which can improve the performance of long-tailed visual recognition. The **bilateral** mixup augmentation combines two samples generated by a uniform sampler and a re-balanced sampler and augments the training dataset to enhance the representation learning for minority classes. We also reduce the classifier bias using class-wise temperature scaling, which scales the logits differently per class in the training phase. We apply both ideas to the dual-branch network (DBN) framework, presenting a new model, named dual-branch network with **bilateral** mixup (DBN-Mix). Experiments on popular long-tailed visual recognition datasets show that DBN-Mix improves performance significantly over baseline and that the proposed method achieves state-of-the-art performance in some categories of benchmarks.  
## cs.AI
---
### Topical: Learning Repository Embeddings from Source Code using Attention. (arXiv:2208.09495v1 [cs.SE])
- Authors : Agathe Lherondelle, Yash Satsangi, Fran Silavong, Shaltiel Eloul, Sean Moran
- Link : [http://arxiv.org/abs/2208.09495](http://arxiv.org/abs/2208.09495)
> ABSTRACT  :  Machine learning on source code (MLOnCode) promises to transform how software is delivered. By mining the context and relationship between software artefacts, MLOnCode augments the software developers capabilities with code auto-generation, code recommendation, code auto-tagging and other data-driven **enhancement**s. For many of these tasks a script level representation of code is sufficient, however, in many cases a repository level representation that takes into account various dependencies and repository structure is imperative, for example, auto-tagging repositories with topics or auto-documentation of repository code etc. Existing methods for computing repository level representations suffer from (a) reliance on natural language documentation of code (for example, README files) (b) naive aggregation of method/script-level representation, for example, by concatenation or averaging. This paper introduces Topical a deep neural network to generate repository level embeddings of publicly available GitHub code repositories directly from source code. Topical incorporates an attention mechanism that projects the source code, the full dependency graph and the script level textual information into a dense repository-level representation. To compute the repository-level representations, Topical is trained to predict the topics associated with a repository, on a dataset of publicly available GitHub repositories that were crawled along with their ground truth topic tags. Our experiments show that the embeddings computed by Topical are able to outperform multiple baselines, including baselines that naively combine the method-level representations through averaging or concatenation at the task of repository auto-tagging.  
### Seeing Objects in **dark** with Continual Contrastive Learning. (arXiv:2112.02891v3 [cs.CV] UPDATED)
- Authors : Ujjal Kr
- Link : [http://arxiv.org/abs/2112.02891](http://arxiv.org/abs/2112.02891)
> ABSTRACT  :  Object Detection, a fundamental computer vision problem, has paramount importance in smart camera systems. However, a truly reliable camera system could be achieved if and only if the underlying object detection component is robust enough across varying imaging conditions (or domains), for instance, different times of the day, adverse weather conditions, etc. In an effort to achieving a reliable camera system, in this paper, we make an attempt to train such a robust detector. Unfortunately, to build a well-performing detector across varying imaging conditions, one would require labeled training images (often in large numbers) from a plethora of corner cases. As manually obtaining such a large labeled dataset may be infeasible, we suggest using synthetic images, to mimic different training image domains. We propose a novel, contrastive learning method to align the latent representations of a pair of real and synthetic images, to make the detector robust to the different domains. However, we found that merely contrasting the embeddings may lead to catastrophic forgetting of the information essential for object detection. Hence, we employ a continual learning based penalty, to alleviate the issue of forgetting, while contrasting the representations. We showcase that our proposed method outperforms a wide range of alternatives to address the extremely challenging, yet under-studied scenario of object detection at **night**-time.  
# Paper List
---
## cs.CV
---
**152** new papers in cs.CV:-) 
1. Blind Image Deblurring with Unknown Kernel Size and Substantial Noise. (arXiv:2208.09483v1 [eess.IV])
2. Explainable Biometrics in the Age of Deep Learning. (arXiv:2208.09500v1 [cs.CV])
3. Exploring the Limits of Synthetic Creation of Solar EUV Images via Image-to-Image Translation. (arXiv:2208.09512v1 [astro-ph.SR])
4. Accelerating Vision Transformer Training via a Patch Sampling Schedule. (arXiv:2208.09520v1 [cs.CV])
5. A Dual Modality Approach For (Zero-Shot) Multi-Label Classification. (arXiv:2208.09562v1 [cs.CV])
6. Multiple Instance Neuroimage Transformer. (arXiv:2208.09567v1 [cs.CV])
7. Contrastive Domain Adaptation for Early Misinformation Detection: A Case Study on COVID-19. (arXiv:2208.09578v1 [cs.CV])
8. Learning in Audio-visual Context: A Review, Analysis, and New Perspective. (arXiv:2208.09579v1 [cs.CV])
9. Review on Action Recognition for Accident Detection in Smart City Transportation Systems. (arXiv:2208.09588v1 [cs.CV])
10. Transforming the Interactive Segmentation for Medical Imaging. (arXiv:2208.09592v1 [cs.CV])
11. Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks. (arXiv:2208.09596v1 [cs.CV])
12. Analyzing Adversarial Robustness of Vision Transformers against Spatial and Spectral Attacks. (arXiv:2208.09602v1 [cs.CV])
13. MemoNav: Selecting Informative Memories for Visual Navigation. (arXiv:2208.09610v1 [cs.CV])
14. Persuasion Strategies in Advertisements: Dataset, Modeling, and Baselines. (arXiv:2208.09626v1 [cs.CL])
15. PARSE challenge 2022: Pulmonary Arteries Segmentation using **Swin** U-Net Transformer(**Swin** UNETR) and U-Net. (arXiv:2208.09636v1 [eess.IV])
16. A Visual Analytics Framework for Composing a Hierarchical Classification for Medieval Illuminations. (arXiv:2208.09657v1 [cs.CV])
17. Offline Handwritten Mathematical Recognition using Adversarial Learning and Transformers. (arXiv:2208.09662v1 [cs.CV])
18. Modeling, Quantifying, and Predicting Subjectivity of Image Aesthetics. (arXiv:2208.09666v1 [cs.CV])
19. Generalised Co-Salient Object Detection. (arXiv:2208.09668v1 [cs.CV])
20. Net2Brain: A Toolbox to compare artificial vision models with human brain responses. (arXiv:2208.09677v1 [cs.CV])
21. Finding Emotions in Faces: A Meta-Classifier. (arXiv:2208.09678v1 [cs.CV])
22. YOLOV: Making Still Image Object Detectors Great at Video Object Detection. (arXiv:2208.09686v1 [cs.CV])
23. Learning Sub-Pixel Disparity Distribution for Light Field Depth Estimation. (arXiv:2208.09688v1 [cs.CV])
24. Effectiveness of Function Matching in Driving Scene Recognition. (arXiv:2208.09694v1 [cs.CV])
25. Fuse and Attend: Generalized Embedding Learning for Art and Sketches. (arXiv:2208.09698v1 [cs.CV])
26. SnowFormer: Scale-aware Transformer via Context Interaction for Single Image Desnowing. (arXiv:2208.09703v1 [cs.CV])
27. DenseShift: Towards Accurate and Transferable Low-Bit Shift Network. (arXiv:2208.09708v1 [cs.CV])
28. Learning Primitive-aware Discriminative Representations for FSL. (arXiv:2208.09717v1 [cs.CV])
29. A Multi-Head Model for Continual Learning via Out-of-Distribution Replay. (arXiv:2208.09734v1 [cs.LG])
30. Artifact-Based Domain Generalization of Skin Lesion Models. (arXiv:2208.09756v1 [cs.CV])
31. JVLDLoc: a Joint Optimization of Visual-LiDAR Constraints and Direction Priors for Localization in Driving Scenario. (arXiv:2208.09777v1 [cs.CV])
32. RGBD1K: A Large-scale Dataset and Benchmark for RGB-D Object Tracking. (arXiv:2208.09787v1 [cs.CV])
33. FaceOff: A Video-to-Video Face Swapping System. (arXiv:2208.09788v1 [cs.CV])
34. Towards MOOCs for Lip Reading: Using Synthetic Talking Heads to Train Humans in Lipreading at Scale. (arXiv:2208.09796v1 [cs.CV])
35. Forensic Dental Age Estimation Using Modified Deep Learning Neural Network. (arXiv:2208.09799v1 [eess.IV])
36. PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition. (arXiv:2208.09801v1 [cs.CV])
37. LWA-HAND: Lightweight Attention Hand for Interacting Hand Reconstruction. (arXiv:2208.09815v1 [cs.CV])
38. Depth-Assisted ResiDualGAN for Cross-Domain Aerial Images Semantic Segmentation. (arXiv:2208.09823v1 [cs.CV])
39. CenDerNet: Center and Curvature Representations for Render-and-Compare 6D Pose Estimation. (arXiv:2208.09829v1 [cs.CV])
40. qDWI-Morph: Motion-compensated quantitative Diffusion-Weighted MRI analysis for fetal lung maturity assessment. (arXiv:2208.09836v1 [cs.CV])
41. CODER: Coupled Diversity-Sensitive Momentum Contrastive Learning for Image-Text Retrieval. (arXiv:2208.09843v1 [cs.CV])
42. CycleTrans: Learning Neutral yet Discriminative Features for Visible-Infrared Person Re-Identification. (arXiv:2208.09844v1 [cs.CV])
43. Multi-task Learning for Monocular Depth and Defocus Estimations with Real Images. (arXiv:2208.09848v1 [cs.CV])
44. Semantic-enhanced Image Clustering. (arXiv:2208.09849v1 [cs.CV])
45. Objects Can Move: 3D Change Detection by Geometric Transformation Constistency. (arXiv:2208.09870v1 [cs.CV])
46. DPTNet: A Dual-Path Transformer Architecture for Scene Text Detection. (arXiv:2208.09878v1 [cs.CV])
47. Masked Video Modeling with Correlation-aware Contrastive Learning for Breast Cancer Diagnosis in Ultrasound. (arXiv:2208.09881v1 [cs.CV])
48. DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination. (arXiv:2208.09884v1 [cs.LG])
49. HST: Hierarchical **Swin** Transformer for Compressed Image Super-resolution. (arXiv:2208.09885v1 [cs.CV])
50. SIM2E: Benchmarking the Group Equivariant Capability of Correspondence Matching Algorithms. (arXiv:2208.09896v1 [cs.CV])
51. Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation. (arXiv:2208.09910v1 [cs.CV])
52. A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective. (arXiv:2208.09913v1 [cs.LG])
53. A Web Application for Experimenting and Validating Remote Measurement of Vital Signs. (arXiv:2208.09916v1 [cs.AI])
54. A semi-supervised Teacher-Student framework for surgical tool detection and localization. (arXiv:2208.09926v1 [cs.CV])
55. Improving GANs for Long-Tailed Data through Group Spectral Regularization. (arXiv:2208.09932v1 [cs.CV])
56. Equalization and Brightness Mapping Modes of Color-to-Gray Projection Operators. (arXiv:2208.09950v1 [cs.CV])
57. PLMCL: Partial-Label Momentum Curriculum Learning for Multi-Label Image Classification. (arXiv:2208.09999v1 [cs.CV])
58. TransNet: Category-Level Transparent Object Pose Estimation. (arXiv:2208.10002v1 [cs.CV])
59. A diverse large-scale building dataset and a novel plug-and-play domain generalization method for building extraction. (arXiv:2208.10004v1 [cs.CV])
60. FairDisCo: Fairer AI in Dermatology via Disentanglement Contrastive Learning. (arXiv:2208.10013v1 [cs.CV])
61. GCISG: Guided Causal Invariant Learning for Improved Syn-to-real Generalization. (arXiv:2208.10024v1 [cs.CV])
62. A Simple Baseline for Multi-Camera 3D Object Detection. (arXiv:2208.10035v1 [cs.CV])
63. Towards Calibrated Hyper-Sphere Representation via Distribution Overlap Coefficient for Long-tailed Learning. (arXiv:2208.10043v1 [cs.CV])
64. Multilayer deep feature extraction for visual texture recognition. (arXiv:2208.10044v1 [cs.CV])
65. Reference-Limited Compositional Zero-Shot Learning. (arXiv:2208.10046v1 [cs.CV])
66. Minkowski Tracker: A Sparse Spatio-Temporal R-CNN for Joint Object Detection and Tracking. (arXiv:2208.10056v1 [cs.CV])
67. Identifying Auxiliary or Adversarial Tasks Using Necessary Condition Analysis for Adversarial Multi-task Video Understanding. (arXiv:2208.10077v1 [cs.CV])
68. Lirot.ai: A Novel Platform for Crowd-Sourcing Retinal Image Segmentations. (arXiv:2208.10100v1 [cs.CV])
69. Revising Image-Text Retrieval via Multi-Modal Entailment. (arXiv:2208.10126v1 [cs.CV])
70. SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization. (arXiv:2208.10128v1 [cs.CV])
71. Rethinking Knowledge Distillation via Cross-Entropy. (arXiv:2208.10139v1 [cs.CV])
72. STS: Surround-view Temporal Stereo for Multi-view 3D Detection. (arXiv:2208.10145v1 [cs.CV])
73. Meta-Causal Feature Learning for Out-of-Distribution Generalization. (arXiv:2208.10156v1 [cs.CV])
74. Prompt-Matched Semantic Segmentation. (arXiv:2208.10159v1 [cs.CV])
75. Multi-Granularity Distillation Scheme Towards Lightweight Semi-Supervised Semantic Segmentation. (arXiv:2208.10169v1 [cs.CV])
76. Noise-Adaptive Intelligent Programmable Meta-Imager. (arXiv:2208.10171v1 [eess.SP])
77. TaCo: Textual Attribute Recognition via Contrastive Learning. (arXiv:2208.10180v1 [cs.CV])
78. Aesthetics Driven Autonomous Time-Lapse Photography Generation by Virtual and Real Robots. (arXiv:2208.10181v1 [cs.CV])
79. Learning Low Bending and Low Distortion Manifold Embeddings: Theory and Applications. (arXiv:2208.10193v1 [math.NA])
80. PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling. (arXiv:2208.10211v1 [cs.CV])
81. Dynamic Adaptive Threshold based Learning for Noisy Annotations Robust Facial Expression Recognition. (arXiv:2208.10221v1 [cs.CV])
82. An anomaly detection approach for backdoored neural networks: face recognition as a case study. (arXiv:2208.10231v1 [cs.CV])
83. Learning Branched Fusion and Orthogonal Projection for Face-Voice Association. (arXiv:2208.10238v1 [cs.CV])
84. To show or not to show: Redacting sensitive text from videos of electronic displays. (arXiv:2208.10270v1 [cs.CV])
85. Automatic Signboard Detection and Localization in Densely Populated Developing Cities. (arXiv:2003.01936v4 [cs.CV] UPDATED)
86. Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v9 [cs.CV] UPDATED)
87. Actor and Action Modular Network for Text-based Video Segmentation. (arXiv:2011.00786v2 [cs.CV] UPDATED)
88. Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v4 [cs.CL] UPDATED)
89. TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types. (arXiv:2102.02115v2 [eess.IV] UPDATED)
90. STS-GAN: Can We Synthesize Solid Texture with High Fidelity from Arbitrary Exemplars?. (arXiv:2102.03973v6 [cs.CV] UPDATED)
91. Preventing Oversmoothing in VAE via Generalized Variance Parameterization. (arXiv:2102.08663v2 [cs.LG] UPDATED)
92. NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification. (arXiv:2103.04053v6 [cs.CV] UPDATED)
93. An Evaluation of Self-Supervised Pre-Training for Skin-Lesion Analysis. (arXiv:2106.09229v3 [cs.CV] UPDATED)
94. Visual Probing: Cognitive Framework for Explaining Self-Supervised Image Representations. (arXiv:2106.11054v3 [cs.CV] UPDATED)
95. OSLO: On-the-Sphere Learning for Omnidirectional images and its application to 360-degree image compression. (arXiv:2107.09179v2 [eess.IV] UPDATED)
96. Exploring Separable Attention for Multi-Contrast MR Image Super-Resolution. (arXiv:2109.01664v2 [eess.IV] UPDATED)
97. HybridSDF: Combining Deep Implicit Shapes and Geometric Primitives for 3D Shape Representation and Manipulation. (arXiv:2109.10767v3 [cs.CV] UPDATED)
98. Benchmarking the Robustness of Spatial-Temporal Models Against Corruptions. (arXiv:2110.06513v2 [cs.CV] UPDATED)
99. Explainable multiple abnormality classification of chest CT volumes. (arXiv:2111.12215v3 [eess.IV] UPDATED)
100. CLIP Meets Video Captioning: Concept-Aware Representation Learning Does Matter. (arXiv:2111.15162v2 [cs.CV] UPDATED)
101. Seeing Objects in **dark** with Continual Contrastive Learning. (arXiv:2112.02891v3 [cs.CV] UPDATED)
102. Colloquium: Advances in automation of quantum dot devices control. (arXiv:2112.09362v2 [quant-ph] UPDATED)
103. PointCaps: Raw Point Cloud Processing using Capsule Networks with Euclidean Distance Routing. (arXiv:2112.11258v2 [cs.CV] UPDATED)
104. On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles. (arXiv:2201.05057v3 [cs.CV] UPDATED)
105. Density reconstruction from schlieren images through Bayesian nonparametric models. (arXiv:2201.05233v3 [physics.flu-dyn] UPDATED)
106. Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution. (arXiv:2201.10747v2 [eess.IV] UPDATED)
107. Filtering In Neural Implicit Functions. (arXiv:2201.13013v4 [cs.CV] UPDATED)
108. STC: Spatio-Temporal Contrastive Learning for Video Instance Segmentation. (arXiv:2202.03747v2 [cs.CV] UPDATED)
109. Feature-level augmentation to improve robustness of deep neural networks to affine transformations. (arXiv:2202.05152v4 [cs.CV] UPDATED)
110. A Generic Self-Supervised Framework of Learning Invariant Discriminative Features. (arXiv:2202.06914v2 [cs.LG] UPDATED)
111. RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering. (arXiv:2203.03949v4 [cs.CV] UPDATED)
112. Non-generative Generalized Zero-shot Learning via Task-correlated Disentanglement and Controllable Samples Synthesis. (arXiv:2203.05335v4 [cs.CV] UPDATED)
113. AugShuffleNet: Communicate More, Compute Less. (arXiv:2203.06589v2 [cs.CV] UPDATED)
114. End-to-End Video Text Spotting with Transformer. (arXiv:2203.10539v3 [cs.CV] UPDATED)
115. K-space and Image Domain Collaborative Energy based Model for Parallel MRI Reconstruction. (arXiv:2203.10776v3 [eess.IV] UPDATED)
116. ImageNet Challenging Classification with the Raspberry Pi: An Incremental Local Stochastic Gradient Descent Algorithm. (arXiv:2203.11853v3 [cs.CV] UPDATED)
117. StructToken : Rethinking Semantic Segmentation with Structural Prior. (arXiv:2203.12612v4 [cs.CV] UPDATED)
118. Open-set Recognition via Augmentation-based Similarity Learning. (arXiv:2203.13238v3 [cs.CV] UPDATED)
119. Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment. (arXiv:2203.14557v3 [eess.IV] UPDATED)
120. Unified Implicit Neural Stylization. (arXiv:2204.01943v3 [cs.CV] UPDATED)
121. Unsupervised Prompt Learning for Vision-Language Models. (arXiv:2204.03649v2 [cs.CV] UPDATED)
122. Deconstructed Generation-Based Zero-Shot Model. (arXiv:2204.11280v2 [cs.CV] UPDATED)
123. GIT: A Generative Image-to-text Transformer for Vision and Language. (arXiv:2205.14100v4 [cs.CV] UPDATED)
124. Rethinking the Augmentation Module in Contrastive Learning: Learning Hierarchical Augmentation Invariance with Expanded Views. (arXiv:2206.00227v2 [cs.CV] UPDATED)
125. DE-Net: Dynamic Text-guided Image Editing Adversarial Networks. (arXiv:2206.01160v2 [cs.CV] UPDATED)
126. Learning Speaker-specific Lip-to-Speech Generation. (arXiv:2206.02050v2 [cs.CV] UPDATED)
127. Gender Artifacts in Visual Datasets. (arXiv:2206.09191v2 [cs.CV] UPDATED)
128. GAN2X: Non-Lambertian Inverse Rendering of Image GANs. (arXiv:2206.09244v3 [cs.CV] UPDATED)
129. Towards Global-Scale Crowd+AI Techniques to Map and Assess Sidewalks for People with Disabilities. (arXiv:2206.13677v2 [cs.CV] UPDATED)
130. e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce. (arXiv:2207.00208v2 [cs.LG] UPDATED)
131. DBN-Mix: Training Dual Branch Network Using **Bilateral** Mixup Augmentation for Long-Tailed Visual Recognition. (arXiv:2207.02173v2 [cs.CV] UPDATED)
132. Classification of Bark Beetle-Induced Forest Tree Mortality using Deep Learning. (arXiv:2207.07241v2 [cs.CV] UPDATED)
133. SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video Anomaly Detection. (arXiv:2207.08003v2 [cs.CV] UPDATED)
134. BYEL : Bootstrap Your Emotion Latent. (arXiv:2207.10003v2 [cs.LG] UPDATED)
135. Trainability Preserving Neural Structured Pruning. (arXiv:2207.12534v2 [cs.LG] UPDATED)
136. Retrieval-Augmented Transformer for Image Captioning. (arXiv:2207.13162v2 [cs.CV] UPDATED)
137. Class-Difficulty Based Methods for Long-Tailed Visual Recognition. (arXiv:2207.14499v2 [cs.CV] UPDATED)
138. Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks. (arXiv:2207.14682v2 [cs.SD] UPDATED)
139. KD-SCFNet: Towards More Accurate and Efficient Salient Object Detection via Knowledge Distillation. (arXiv:2208.02178v2 [cs.CV] UPDATED)
140. Convolutional Ensembling based Few-Shot Defect Detection Technique. (arXiv:2208.03288v2 [cs.CV] UPDATED)
141. HaloAE: An HaloNet based Local Transformer Auto-Encoder for Anomaly Detection and Localization. (arXiv:2208.03486v2 [cs.CV] UPDATED)
142. ExpansionNet v2: Block Static Expansion in fast end to end training for Image Captioning. (arXiv:2208.06551v3 [cs.CV] UPDATED)
143. Faster Attention Is What You Need: A Fast Self-Attention Neural Network Backbone Architecture for the Edge via Double-Condensing Attention Condensers. (arXiv:2208.06980v2 [cs.CV] UPDATED)
144. UPST-**NeRF**: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene. (arXiv:2208.07059v2 [cs.CV] UPDATED)
145. Context-aware Mixture-of-Experts for Unbiased Scene Graph Generation. (arXiv:2208.07109v2 [cs.CV] UPDATED)
146. Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation. (arXiv:2208.08315v3 [eess.IV] UPDATED)
147. Class-Aware Visual Prompt Tuning for Vision-Language Pre-Trained Model. (arXiv:2208.08340v2 [cs.CV] UPDATED)
148. TSCom-Net: Coarse-to-Fine 3D Textured Shape Completion Network. (arXiv:2208.08768v2 [cs.CV] UPDATED)
149. Towards Label-efficient Automatic Diagnosis and Analysis: A Comprehensive Survey of Advanced Deep Learning-based Weakly-supervised, Semi-supervised and Self-supervised Techniques in Histopathological Image Analysis. (arXiv:2208.08789v2 [cs.CV] UPDATED)
150. COPE: End-to-end trainable Constant Runtime Object Pose Estimation. (arXiv:2208.08807v2 [cs.CV] UPDATED)
151. Wildfire Forecasting with Satellite Images and Deep Generative Model. (arXiv:2208.09411v2 [cs.CV] UPDATED)
152. M2HF: Multi-level Multi-modal Hybrid Fusion for Text-Video Retrieval. (arXiv:2208.07664v1 [cs.MM] CROSS LISTED)
## eess.IV
---
**28** new papers in eess.IV:-) 
1. Blind Image Deblurring with Unknown Kernel Size and Substantial Noise. (arXiv:2208.09483v1 [eess.IV])
2. PARSE challenge 2022: Pulmonary Arteries Segmentation using **Swin** U-Net Transformer(**Swin** UNETR) and U-Net. (arXiv:2208.09636v1 [eess.IV])
3. Learning Sub-Pixel Disparity Distribution for Light Field Depth Estimation. (arXiv:2208.09688v1 [cs.CV])
4. Forensic Dental Age Estimation Using Modified Deep Learning Neural Network. (arXiv:2208.09799v1 [eess.IV])
5. Hilti-Oxford Dataset: A Millimetre-Accurate Benchmark for Simultaneous Localization and Mapping. (arXiv:2208.09825v1 [cs.RO])
6. Multi-task Learning for Monocular Depth and Defocus Estimations with Real Images. (arXiv:2208.09848v1 [cs.CV])
7. HST: Hierarchical **Swin** Transformer for Compressed Image Super-resolution. (arXiv:2208.09885v1 [cs.CV])
8. Improving GANs for Long-Tailed Data through Group Spectral Regularization. (arXiv:2208.09932v1 [cs.CV])
9. Equalization and Brightness Mapping Modes of Color-to-Gray Projection Operators. (arXiv:2208.09950v1 [cs.CV])
10. Deep 3D Vessel Segmentation based on Cross Transformer Network. (arXiv:2208.10148v1 [eess.IV])
11. Contributions \`a l'asservissement visuel et \`a l'imagerie en m\'edecine. (arXiv:2208.10284v1 [cs.RO])
12. Optimising Chest X-Rays for Image Analysis by Identifying and Removing Confounding Factors. (arXiv:2208.10320v1 [eess.IV])
13. What Does a One-Bit Quanta Image Sensor Offer?. (arXiv:2208.10350v1 [eess.IV])
14. TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types. (arXiv:2102.02115v2 [eess.IV] UPDATED)
15. STS-GAN: Can We Synthesize Solid Texture with High Fidelity from Arbitrary Exemplars?. (arXiv:2102.03973v6 [cs.CV] UPDATED)
16. OSLO: On-the-Sphere Learning for Omnidirectional images and its application to 360-degree image compression. (arXiv:2107.09179v2 [eess.IV] UPDATED)
17. Exploring Separable Attention for Multi-Contrast MR Image Super-Resolution. (arXiv:2109.01664v2 [eess.IV] UPDATED)
18. Explainable multiple abnormality classification of chest CT volumes. (arXiv:2111.12215v3 [eess.IV] UPDATED)
19. Ensemble learning priors unfolding for scalable Snapshot Compressive Sensing. (arXiv:2201.10419v2 [eess.IV] UPDATED)
20. Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution. (arXiv:2201.10747v2 [eess.IV] UPDATED)
21. K-space and Image Domain Collaborative Energy based Model for Parallel MRI Reconstruction. (arXiv:2203.10776v3 [eess.IV] UPDATED)
22. Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment. (arXiv:2203.14557v3 [eess.IV] UPDATED)
23. Fast algorithms for nonlinear and constrained phase retrieval in near-field X-ray holography based on Tikhonov regularization. (arXiv:2205.01099v2 [eess.IV] UPDATED)
24. PatchNR: Learning from Small Data by Patch Normalizing Flow Regularization. (arXiv:2205.12021v2 [cs.LG] UPDATED)
25. Convex Quantization Preserves Logconcavity. (arXiv:2206.05598v2 [eess.SP] UPDATED)
26. Stability of Image-Reconstruction Algorithms. (arXiv:2206.07128v2 [math.OC] UPDATED)
27. Faster Attention Is What You Need: A Fast Self-Attention Neural Network Backbone Architecture for the Edge via Double-Condensing Attention Condensers. (arXiv:2208.06980v2 [cs.CV] UPDATED)
28. Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation. (arXiv:2208.08315v3 [eess.IV] UPDATED)
## cs.LG
---
**208** new papers in cs.LG:-) 
1. Graph neural networks for materials science and chemistry. (arXiv:2208.09481v1 [physics.chem-ph])
2. Blind Image Deblurring with Unknown Kernel Size and Substantial Noise. (arXiv:2208.09483v1 [eess.IV])
3. In Silico Prediction of Blood-Brain Barrier Permeability of Chemical Compounds through Molecular Feature Modeling. (arXiv:2208.09484v1 [q-bio.QM])
4. Exploring the Limits of Synthetic Creation of Solar EUV Images via Image-to-Image Translation. (arXiv:2208.09512v1 [astro-ph.SR])
5. Spectral Decomposition Representation for Reinforcement Learning. (arXiv:2208.09515v1 [cs.LG])
6. Exploring Popularity Bias in Music Recommendation Models and Commercial Steaming Services. (arXiv:2208.09517v1 [cs.IR])
7. Recurrent Neural Network-based Anti-jamming Framework for Defense Against Multiple Jamming Policies. (arXiv:2208.09518v1 [cs.LG])
8. Intersection of Parallels as an Early Stopping Criterion. (arXiv:2208.09529v1 [cs.LG])
9. Predicting Exotic Hadron Masses with Data Augmentation Using Multilayer Perceptron. (arXiv:2208.09538v1 [hep-ph])
10. Meta Learning for High-dimensional Ising Model Selection Using $\ell_1$-regularized Logistic Regression. (arXiv:2208.09539v1 [cs.LG])
11. Sudakov-Fernique post-AMP, and a new proof of the local convexity of the TAP free energy. (arXiv:2208.09550v1 [math.PR])
12. Game-Theoretic Algorithms for Conditional Moment Matching. (arXiv:2208.09551v1 [cs.GT])
13. Neural network facilitated ab initio derivation of linear formula: A case study on formulating the relationship between DNA motifs and gene expression. (arXiv:2208.09559v1 [q-bio.QM])
14. A Dual Modality Approach For (Zero-Shot) Multi-Label Classification. (arXiv:2208.09562v1 [cs.CV])
15. Multiple Instance Neuroimage Transformer. (arXiv:2208.09567v1 [cs.CV])
16. Calculus on MDPs: Potential Shaping as a Gradient. (arXiv:2208.09570v1 [cs.LG])
17. Study of Novel Sparse Array Design Based on the Maximum Inter-Element Spacing Criterion. (arXiv:2208.09574v1 [cs.LG])
18. Data-Driven Causal Effect Estimation Based on Graphical Causal Modelling: A Survey. (arXiv:2208.09590v1 [cs.AI])
19. TopoDiff: A Performance and Constraint-Guided Diffusion Model for Topology Optimization. (arXiv:2208.09591v1 [cs.LG])
20. Transferable Cross-Tokamak Disruption Prediction with Deep Hybrid Neural Network Feature Extractor. (arXiv:2208.09594v1 [physics.plasm-ph])
21. The Saddle-Point Accountant for Differential Privacy. (arXiv:2208.09595v1 [cs.CR])
22. Looking For A Match: Self-supervised Clustering For Automatic Doubt Matching In e-learning Platforms. (arXiv:2208.09600v1 [cs.LG])
23. Weighted Maximum Entropy Inverse Reinforcement Learning. (arXiv:2208.09611v1 [cs.LG])
24. An ensemble meta-estimator to predict source code testability. (arXiv:2208.09614v1 [cs.SE])
25. A Novel Hybrid Sampling Framework for Imbalanced Learning. (arXiv:2208.09619v1 [cs.LG])
26. Learning to predict test effectiveness. (arXiv:2208.09623v1 [cs.SE])
27. Are You Comfortable Now: Deep Learning the Temporal Variation in Thermal Comfort in Winters. (arXiv:2208.09628v1 [cs.LG])
28. Adam Can Converge Without Any Modification on Update Rules. (arXiv:2208.09632v1 [cs.LG])
29. The computational complexity of some explainable clustering problems. (arXiv:2208.09643v1 [cs.LG])
30. Few-Shot Learning of Accurate Folding Landscape for Protein Structure Prediction. (arXiv:2208.09652v1 [cs.LG])
31. A Domain Generalization Approach for Out-Of-Distribution 12-lead ECG Classification with Convolutional Neural Networks. (arXiv:2208.09656v1 [cs.LG])
32. A biologically-inspired evaluation of molecular generative machine learning. (arXiv:2208.09658v1 [cs.LG])
33. Trigger-free Event Detection via Derangement Reading Comprehension. (arXiv:2208.09659v1 [cs.CL])
34. From Time Series to Networks in R with the ts2net Package. (arXiv:2208.09660v1 [cs.SI])
35. Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles. (arXiv:2208.09665v1 [cs.HC])
36. Machine learning based surrogate models for microchannel heat sink optimization. (arXiv:2208.09683v1 [physics.flu-dyn])
37. Effectiveness of Function Matching in Driving Scene Recognition. (arXiv:2208.09694v1 [cs.CV])
38. Fuse and Attend: Generalized Embedding Learning for Art and Sketches. (arXiv:2208.09698v1 [cs.CV])
39. DenseShift: Towards Accurate and Transferable Low-Bit Shift Network. (arXiv:2208.09708v1 [cs.CV])
40. Adversarial contamination of networks in the setting of vertex nomination: a new trimming method. (arXiv:2208.09710v1 [stat.ML])
41. Improving Multilayer-Perceptron(MLP)-based Network Anomaly Detection with Birch Clustering on CICIDS-2017 Dataset. (arXiv:2208.09711v1 [cs.CR])
42. Matrix Completion with Cross-Concentrated Sampling: Bridging Uniform Sampling and CUR Sampling. (arXiv:2208.09723v1 [cs.LG])
43. On Robustness in Nonconvex Optimization with Application to Defense Planning. (arXiv:2208.09725v1 [math.OC])
44. A Multi-Head Model for Continual Learning via Out-of-Distribution Replay. (arXiv:2208.09734v1 [cs.LG])
45. C$^{2}$IMUFS: Complementary and Consensus Learning-based Incomplete Multi-view Unsupervised Feature Selection. (arXiv:2208.09736v1 [cs.LG])
46. Near-Optimal $\Phi$-Regret Learning in Extensive-Form Games. (arXiv:2208.09747v1 [cs.GT])
47. MLExchange -- A web-based platform enabling exchangeable machine learning workflows. (arXiv:2208.09751v1 [cs.LG])
48. FLIS: Clustered Federated Learning via Inference Similarity for Non-IID Data Distribution. (arXiv:2208.09754v1 [cs.LG])
49. Robust Node Classification on Graphs: Jointly from Bayesian Label Transition and Topology-based Label Propagation. (arXiv:2208.09779v1 [cs.LG])
50. Stop&Hop: Early Classification of Irregular Time Series. (arXiv:2208.09795v1 [cs.LG])
51. PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition. (arXiv:2208.09801v1 [cs.CV])
52. Critical Bach Size Minimizes Stochastic First-Order Oracle Complexity of Deep Learning Optimizer using Hyperparameters Close to One. (arXiv:2208.09814v1 [cs.LG])
53. Robust Tests in Online Decision-Making. (arXiv:2208.09819v1 [stat.ML])
54. Representation Learning with Graph Neural Networks for Speech Emotion Recognition. (arXiv:2208.09830v1 [cs.SD])
55. Combating Noisy-Labeled and Imbalanced Data by Two Stage Bi-Dimensional Sample Selection. (arXiv:2208.09833v1 [cs.LG])
56. Comparison-based Conversational Recommender System with Relative Bandit Feedback. (arXiv:2208.09837v1 [cs.IR])
57. Semantic-enhanced Image Clustering. (arXiv:2208.09849v1 [cs.CV])
58. Last-Iterate Convergence with Full- and Noisy-Information Feedback in Two-Player Zero-Sum Games. (arXiv:2208.09855v1 [cs.GT])
59. Emergence of hierarchical modes from deep learning. (arXiv:2208.09859v1 [cs.LG])
60. Twin Papers: A Simple Framework of Causal Inference for Citations via Coupling. (arXiv:2208.09862v1 [cs.DL])
61. Provably Tightest Linear Approximation for Robustness Verification of Sigmoid-like Neural Networks. (arXiv:2208.09872v1 [cs.LG])
62. DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination. (arXiv:2208.09884v1 [cs.LG])
63. G2{\Phi}net: Relating Genotype and Biomechanical Phenotype of Tissues with Deep Learning. (arXiv:2208.09889v1 [q-bio.TO])
64. Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v1 [cs.LG])
65. Multiple Descent in the Multiple Random Feature Model. (arXiv:2208.09897v1 [math.ST])
66. Provable Adaptivity in Adam. (arXiv:2208.09900v1 [cs.LG])
67. MentorGNN: Deriving Curriculum for Pre-Training GNNs. (arXiv:2208.09905v1 [cs.LG])
68. A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective. (arXiv:2208.09913v1 [cs.LG])
69. Alexa, Predict My Flight Delay. (arXiv:2208.09921v1 [cs.LG])
70. A semi-supervised Teacher-Student framework for surgical tool detection and localization. (arXiv:2208.09926v1 [cs.CV])
71. ProPaLL: Probabilistic Partial Label Learning. (arXiv:2208.09931v1 [cs.LG])
72. Improving GANs for Long-Tailed Data through Group Spectral Regularization. (arXiv:2208.09932v1 [cs.CV])
73. AA-Forecast: Anomaly-Aware Forecast for Extreme Events. (arXiv:2208.09933v1 [stat.ML])
74. Instability and Local Minima in GAN Training with Kernel Discriminators. (arXiv:2208.09938v1 [cs.LG])
75. MolGraph: a Python package for the implementation of small molecular graphs and graph neural networks with TensorFlow and Keras. (arXiv:2208.09944v1 [cs.LG])
76. Do-AIQ: A Design-of-Experiment Approach to Quality Evaluation of AI Mislabel Detection Algorithm. (arXiv:2208.09953v1 [stat.ML])
77. Energy-aware Scheduling of Virtualized Base Stations in O-RAN with Online Learning. (arXiv:2208.09956v1 [cs.NI])
78. Heterogeneous Graph Masked Autoencoders. (arXiv:2208.09957v1 [cs.LG])
79. Performance, Opaqueness, Consequences, and Assumptions: Simple questions for responsible planning of machine learning solutions. (arXiv:2208.09966v1 [cs.LG])
80. Inferring Sensitive Attributes from Model Explanations. (arXiv:2208.09967v1 [cs.CR])
81. Transfer Ranking in Finance: Applications to Cross-Sectional Momentum with Data Scarcity. (arXiv:2208.09968v1 [q-fin.TR])
82. Bayesian Complementary Kernelized Learning for Multidimensional Spatiotemporal Data. (arXiv:2208.09978v1 [stat.ML])
83. Collaboration between parallel connected neural networks -- A possible criterion for distinguishing artificial neural networks from natural organs. (arXiv:2208.09983v1 [cs.LG])
84. NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs. (arXiv:2208.10010v1 [cs.LG])
85. Simple and Optimal Stochastic Gradient Methods for Nonsmooth Nonconvex Optimization. (arXiv:2208.10025v1 [cs.LG])
86. Learning Invariant Representations under General Interventions on the Response. (arXiv:2208.10027v1 [stat.ME])
87. Evaluating and Crafting Datasets Effective for Deep Learning With Data Maps. (arXiv:2208.10033v1 [cs.LG])
88. Robust Bayesian Nonnegative Matrix Factorization with Implicit Regularizers. (arXiv:2208.10053v1 [cs.LG])
89. MetaRF: Differentiable Random Forest for Reaction Yield Prediction with a Few Trails. (arXiv:2208.10083v1 [cs.LG])
90. Socially Fair Center-based and Linear Subspace Clustering. (arXiv:2208.10095v1 [cs.LG])
91. Hierarchical Capsule Prediction Network for Marketing Campaigns Effect. (arXiv:2208.10113v1 [stat.ML])
92. SoK: Machine Learning with Confidential Computing. (arXiv:2208.10134v1 [cs.CR])
93. BRIEF but Powerful: Byzantine-Robust and Privacy-Preserving Federated Learning via Model Segmentation and Secure clustering. (arXiv:2208.10161v1 [cs.CR])
94. Learning Low Bending and Low Distortion Manifold Embeddings: Theory and Applications. (arXiv:2208.10193v1 [math.NA])
95. LTE4G: Long-Tail Experts for Graph Neural Networks. (arXiv:2208.10205v1 [cs.LG])
96. Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (arXiv:2208.10224v1 [cs.CR])
97. One Model, Any CSP: Graph Neural Networks as Fast Global Search Heuristics for Constraint Satisfaction. (arXiv:2208.10227v1 [cs.AI])
98. Survey of NLP in Pharmacology: Methodology, Tasks, Resources, Knowledge, and Tools. (arXiv:2208.10228v1 [cs.CL])
99. Predicting the protein-ligand affinity from molecular dynamics trajectories. (arXiv:2208.10230v1 [q-bio.BM])
100. An anomaly detection approach for backdoored neural networks: face recognition as a case study. (arXiv:2208.10231v1 [cs.CV])
101. When BERT Fails -- The Limits of EHR Classification. (arXiv:2208.10245v1 [cs.CL])
102. SDBERT: SparseDistilBERT, a faster and smaller BERT model. (arXiv:2208.10246v1 [cs.CL])
103. Generalized Attention Mechanism and Relative Position for Transformer. (arXiv:2208.10247v1 [cs.CL])
104. Composing RNNs and FSTs for Small Data: Recovering Missing Characters in Old Hawaiian Text. (arXiv:2208.10248v1 [cs.CL])
105. Multi-Task Learning for Depression Detection in Dialogs. (arXiv:2208.10250v1 [cs.CL])
106. An Exploratory Study of Tweets about the SARS-CoV-2 Omicron Variant: Insights from Sentiment Analysis, Language Interpretation, Source Tracking, Type Classification, and Embedded URL Detection. (arXiv:2208.10252v1 [cs.CL])
107. On the non-efficient PAC learnability of acyclic conjunctive queries. (arXiv:2208.10255v1 [cs.DB])
108. Meta-Learning Online Control for Linear Dynamical Systems. (arXiv:2208.10259v1 [cs.LG])
109. Using Large Language Models to Simulate Multiple Humans. (arXiv:2208.10264v1 [cs.CL])
110. A semantic web approach to uplift decentralized household energy data. (arXiv:2208.10265v1 [cs.AI])
111. To show or not to show: Redacting sensitive text from videos of electronic displays. (arXiv:2208.10270v1 [cs.CV])
112. Long-Short History of Gradients is All You Need: Detecting Malicious and Unreliable Clients in Federated Learning. (arXiv:2208.10273v1 [cs.CR])
113. Practical Vertical Federated Learning with Unsupervised Representation Learning. (arXiv:2208.10278v1 [cs.CR])
114. Defensive Distillation based Adversarial Attacks Mitigation Method for Channel Estimation using Deep Learning Models in Next-Generation Wireless Networks. (arXiv:2208.10279v1 [cs.CR])
115. A Twitter-Driven Deep Learning Mechanism for the Determination of Vehicle Hijacking Spots in Cities. (arXiv:2208.10280v1 [cs.CL])
116. Deterministic Graph-Walking Program Mining. (arXiv:2208.10290v1 [cs.LG])
117. GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification. (arXiv:1905.11475v3 [cs.LG] UPDATED)
118. Estimating Smooth GLM in Non-interactive Local Differential Privacy Model with Public Unlabeled Data. (arXiv:1910.00482v4 [cs.LG] UPDATED)
119. Confident Learning: Estimating Uncertainty in Dataset Labels. (arXiv:1911.00068v6 [stat.ML] UPDATED)
120. FlexiBO: A Decoupled Cost-Aware Multi-Objective Optimization Approach for Deep Neural Networks. (arXiv:2001.06588v3 [cs.LG] UPDATED)
121. Leveraging Cross Feedback of User and Item Embeddings with Attention for Variational Autoencoder based Collaborative Filtering. (arXiv:2002.09145v3 [cs.LG] UPDATED)
122. Multivariate Boosted Trees and Applications to Forecasting and Control. (arXiv:2003.03835v2 [cs.LG] UPDATED)
123. Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v9 [cs.CV] UPDATED)
124. Model-Free Non-Stationary RL: Near-Optimal Regret and Applications in Multi-Agent RL and Inventory Control. (arXiv:2010.03161v4 [cs.LG] UPDATED)
125. Optimal Client Sampling for Federated Learning. (arXiv:2010.13723v3 [cs.LG] UPDATED)
126. Quadratic Metric Elicitation for Fairness and Beyond. (arXiv:2011.01516v3 [stat.ML] UPDATED)
127. Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. (arXiv:2012.11654v5 [stat.ML] UPDATED)
128. STS-GAN: Can We Synthesize Solid Texture with High Fidelity from Arbitrary Exemplars?. (arXiv:2102.03973v6 [cs.CV] UPDATED)
129. Preventing Oversmoothing in VAE via Generalized Variance Parameterization. (arXiv:2102.08663v2 [cs.LG] UPDATED)
130. Graph-Embedded Subspace Support Vector Data Description. (arXiv:2104.14370v2 [cs.LG] UPDATED)
131. A structured proof of Kolmogorov's Superposition Theorem. (arXiv:2105.00408v2 [math.FA] UPDATED)
132. An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural Network. (arXiv:2105.11675v3 [cs.LG] UPDATED)
133. Goal Misgeneralization in Deep Reinforcement Learning. (arXiv:2105.14111v5 [cs.LG] UPDATED)
134. On the Theory of Reinforcement Learning with Once-per-Episode Feedback. (arXiv:2105.14363v3 [cs.LG] UPDATED)
135. Robust Graph Meta-learning for Weakly-supervised Few-shot Node Classification. (arXiv:2106.06873v2 [cs.LG] UPDATED)
136. Visual Probing: Cognitive Framework for Explaining Self-Supervised Image Representations. (arXiv:2106.11054v3 [cs.CV] UPDATED)
137. Distributed Saddle-Point Problems Under Similarity. (arXiv:2107.10706v3 [math.OC] UPDATED)
138. Complexity of Inexact Proximal Point Algorithm for minimizing convex functions with Holderian Growth. (arXiv:2108.04482v5 [cs.LG] UPDATED)
139. Instance-wise or Class-wise? A Tale of Neighbor Shapley for Concept-based Explanation. (arXiv:2109.01369v5 [cs.LG] UPDATED)
140. A Simple Unified Framework for Anomaly Detection in Deep Reinforcement Learning. (arXiv:2109.09889v2 [cs.LG] UPDATED)
141. Attentive Walk-Aggregating Graph Neural Networks. (arXiv:2110.02667v2 [cs.LG] UPDATED)
142. Local and Global Context-Based Pairwise Models for Sentence Ordering. (arXiv:2110.04291v2 [cs.CL] UPDATED)
143. Adjacency constraint for efficient hierarchical reinforcement learning. (arXiv:2111.00213v4 [cs.LG] UPDATED)
144. Explainable multiple abnormality classification of chest CT volumes. (arXiv:2111.12215v3 [eess.IV] UPDATED)
145. Seeing Objects in **dark** with Continual Contrastive Learning. (arXiv:2112.02891v3 [cs.CV] UPDATED)
146. Colloquium: Advances in automation of quantum dot devices control. (arXiv:2112.09362v2 [quant-ph] UPDATED)
147. Automatic Meta-Path Discovery for Effective Graph-Based Recommendation. (arXiv:2112.12845v4 [cs.IR] UPDATED)
148. Stochastic Weight Averaging Revisited. (arXiv:2201.00519v3 [cs.LG] UPDATED)
149. DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture Fleeting Intraday Trading Opportunities. (arXiv:2201.09058v3 [q-fin.TR] UPDATED)
150. Exploring Differential Geometry in Neural Implicits. (arXiv:2201.09263v4 [cs.GR] UPDATED)
151. Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution. (arXiv:2201.10747v2 [eess.IV] UPDATED)
152. Multi-Agent Reinforcement Learning for Network Load Balancing in Data Center. (arXiv:2201.11727v4 [cs.DC] UPDATED)
153. Calibration of P-values for calibration and for deviation of a subpopulation from the full population. (arXiv:2202.00100v5 [stat.ME] UPDATED)
154. Learning entanglement breakdown as a phase transition by confusion. (arXiv:2202.00348v2 [quant-ph] UPDATED)
155. Do Differentiable Simulators Give Better Policy Gradients?. (arXiv:2202.00817v2 [cs.LG] UPDATED)
156. Feature-level augmentation to improve robustness of deep neural networks to affine transformations. (arXiv:2202.05152v4 [cs.CV] UPDATED)
157. Deep Learning based Coverage and Rate Manifold Estimation in Cellular Networks. (arXiv:2202.06390v2 [cs.NI] UPDATED)
158. A Generic Self-Supervised Framework of Learning Invariant Discriminative Features. (arXiv:2202.06914v2 [cs.LG] UPDATED)
159. Microgrid Optimal Energy Scheduling Considering Neural Network based Battery Degradation. (arXiv:2202.12416v3 [eess.SP] UPDATED)
160. CandidateDrug4Cancer: An Open Molecular Graph Learning Benchmark on Drug Discovery for Cancer. (arXiv:2203.00836v2 [cs.LG] UPDATED)
161. Are discrete units necessary for Spoken Language Modeling?. (arXiv:2203.05936v2 [cs.CL] UPDATED)
162. Bit-Metric Decoding Rate in Multi-User MIMO Systems: Theory. (arXiv:2203.06271v3 [cs.IT] UPDATED)
163. AugShuffleNet: Communicate More, Compute Less. (arXiv:2203.06589v2 [cs.CV] UPDATED)
164. ASE: Anomaly Scoring Based Ensemble Learning for Imbalanced Datasets. (arXiv:2203.10769v3 [cs.LG] UPDATED)
165. ImageNet Challenging Classification with the Raspberry Pi: An Incremental Local Stochastic Gradient Descent Algorithm. (arXiv:2203.11853v3 [cs.CV] UPDATED)
166. Enhancing Zero-Shot Many to Many Voice Conversion with Self-Attention VAE. (arXiv:2203.16037v2 [cs.SD] UPDATED)
167. Learning Downstream Task by Selectively Capturing Complementary Knowledge from Multiple Self-supervisedly Learning Pretexts. (arXiv:2204.05248v2 [cs.LG] UPDATED)
168. A Personalized Dialogue Generator with Implicit User Persona Detection. (arXiv:2204.07372v2 [cs.CL] UPDATED)
169. Merging of neural networks. (arXiv:2204.09973v2 [cs.LG] UPDATED)
170. Deconstructed Generation-Based Zero-Shot Model. (arXiv:2204.11280v2 [cs.CV] UPDATED)
171. Hierarchical Bayesian Modelling for Knowledge Transfer Across Engineering Fleets via Multitask Learning. (arXiv:2204.12404v2 [stat.ML] UPDATED)
172. Transferring Chemical and Energetic Knowledge Between Molecular Systems with Machine Learning. (arXiv:2205.03339v2 [physics.chem-ph] UPDATED)
173. Universal Caching. (arXiv:2205.04860v2 [cs.IT] UPDATED)
174. Conditional Born machine for Monte Carlo event generation. (arXiv:2205.07674v2 [quant-ph] UPDATED)
175. A Boosting Algorithm for Positive-Unlabeled Learning. (arXiv:2205.09485v3 [cs.LG] UPDATED)
176. PatchNR: Learning from Small Data by Patch Normalizing Flow Regularization. (arXiv:2205.12021v2 [cs.LG] UPDATED)
177. Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v2 [cs.LG] UPDATED)
178. Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power. (arXiv:2205.13863v2 [cs.LG] UPDATED)
179. VC Theoretical Explanation of Double Descent. (arXiv:2205.15549v2 [stat.ML] UPDATED)
180. Use-Case-Grounded Simulations for Explanation Evaluation. (arXiv:2206.02256v2 [cs.HC] UPDATED)
181. Regret Analysis of Certainty Equivalence Policies in Continuous-Time Linear-Quadratic Systems. (arXiv:2206.04434v2 [cs.LG] UPDATED)
182. Discovery and density estimation of latent confounders in Bayesian networks with evidence lower bound. (arXiv:2206.05490v4 [cs.LG] UPDATED)
183. Stability of Image-Reconstruction Algorithms. (arXiv:2206.07128v2 [math.OC] UPDATED)
184. Simultaneously Learning Stochastic and Adversarial Bandits with General Graph Feedback. (arXiv:2206.07908v2 [cs.LG] UPDATED)
185. DeePKS+ABACUS as a Bridge between Expensive Quantum Mechanical Models and Machine Learning Potentials. (arXiv:2206.10093v2 [physics.chem-ph] UPDATED)
186. Non-Determinism and the Lawlessness of Machine Learning Code. (arXiv:2206.11834v2 [cs.CY] UPDATED)
187. e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce. (arXiv:2207.00208v2 [cs.LG] UPDATED)
188. DBN-Mix: Training Dual Branch Network Using **Bilateral** Mixup Augmentation for Long-Tailed Visual Recognition. (arXiv:2207.02173v2 [cs.CV] UPDATED)
189. Uncertainty-Aware Mixed-Variable Machine Learning for Materials Design. (arXiv:2207.04994v2 [stat.ML] UPDATED)
190. A semi-supervised methodology for fishing activity detection using the geometry behind the trajectory of multiple vessels. (arXiv:2207.05514v2 [cs.LG] UPDATED)
191. Improving Task-free Continual Learning by Distributionally Robust Memory Evolution. (arXiv:2207.07256v2 [cs.LG] UPDATED)
192. SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video Anomaly Detection. (arXiv:2207.08003v2 [cs.CV] UPDATED)
193. On stabilizing reinforcement learning without Lyapunov functions. (arXiv:2207.08730v5 [eess.SY] UPDATED)
194. BYEL : Bootstrap Your Emotion Latent. (arXiv:2207.10003v2 [cs.LG] UPDATED)
195. Trainability Preserving Neural Structured Pruning. (arXiv:2207.12534v2 [cs.LG] UPDATED)
196. Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v4 [cs.CL] UPDATED)
197. DL-DRL: A double-layer deep reinforcement learning approach for large-scale task scheduling of multi-UAV. (arXiv:2208.02447v2 [cs.LG] UPDATED)
198. Isoform Function Prediction Using Deep Neural Network. (arXiv:2208.03325v2 [q-bio.GN] UPDATED)
199. A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining. (arXiv:2208.03646v2 [cs.LG] UPDATED)
200. An Empirical Exploration of Cross-domain Alignment between Language and Electroencephalogram. (arXiv:2208.06348v2 [q-bio.NC] UPDATED)
201. Towards Spatio-Temporal Cross-Platform Graph Embedding Fusion for Urban Traffic Flow Prediction. (arXiv:2208.06947v2 [cs.LG] UPDATED)
202. FedMR: Fedreated Learning via Model Recombination. (arXiv:2208.07677v2 [cs.LG] UPDATED)
203. Role of Data Augmentation in Unsupervised Anomaly Detection. (arXiv:2208.07734v2 [cs.LG] UPDATED)
204. DPA-1: Pretraining of Attention-based Deep Potential Model for Molecular Simulation. (arXiv:2208.08236v2 [physics.chem-ph] UPDATED)
205. Network inference via process motifs for lagged correlation in linear stochastic processes. (arXiv:2208.08871v2 [stat.ML] UPDATED)
206. Quality issues in Machine Learning Software Systems. (arXiv:2208.08982v2 [cs.SE] UPDATED)
207. Dance Style Transfer with Cross-modal Transformer. (arXiv:2208.09406v2 [cs.LG] UPDATED)
208. Stock Performance Evaluation for Portfolio Design from Different Sectors of the Indian Stock Market. (arXiv:2208.07166v1 [q-fin.PM] CROSS LISTED)
## cs.AI
---
**108** new papers in cs.AI:-) 
1. Topical: Learning Repository Embeddings from Source Code using Attention. (arXiv:2208.09495v1 [cs.SE])
2. Globus Automation Services: Research process automation across the space-time continuum. (arXiv:2208.09513v1 [cs.DC])
3. [Re] Differentiable Spatial Planning using Transformers. (arXiv:2208.09536v1 [cs.RO])
4. Evaluating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks. (arXiv:2208.09554v1 [cs.AI])
5. Personalized Decision Making -- A Conceptual Introduction. (arXiv:2208.09558v1 [cs.AI])
6. A Dual Modality Approach For (Zero-Shot) Multi-Label Classification. (arXiv:2208.09562v1 [cs.CV])
7. Probabilities of Causation with Nonbinary Treatment and Effect. (arXiv:2208.09568v1 [cs.AI])
8. Unit Selection with Nonbinary Treatment and Effect. (arXiv:2208.09569v1 [cs.AI])
9. Contrastive Domain Adaptation for Early Misinformation Detection: A Case Study on COVID-19. (arXiv:2208.09578v1 [cs.CV])
10. Learning in Audio-visual Context: A Review, Analysis, and New Perspective. (arXiv:2208.09579v1 [cs.CV])
11. Review on Action Recognition for Accident Detection in Smart City Transportation Systems. (arXiv:2208.09588v1 [cs.CV])
12. Data-Driven Causal Effect Estimation Based on Graphical Causal Modelling: A Survey. (arXiv:2208.09590v1 [cs.AI])
13. Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection. (arXiv:2208.09601v1 [cs.CL])
14. MemoNav: Selecting Informative Memories for Visual Navigation. (arXiv:2208.09610v1 [cs.CV])
15. Fully Automated End-to-End Fake Audio Detection. (arXiv:2208.09618v1 [cs.SD])
16. A Novel Hybrid Sampling Framework for Imbalanced Learning. (arXiv:2208.09619v1 [cs.LG])
17. Representing Knowledge by Spans: A Knowledge-Enhanced Model for Information Extraction. (arXiv:2208.09625v1 [cs.CL])
18. Are You Comfortable Now: Deep Learning the Temporal Variation in Thermal Comfort in Winters. (arXiv:2208.09628v1 [cs.LG])
19. An Initial Investigation for Detecting Vocoder Fingerprints of Fake Audio. (arXiv:2208.09646v1 [cs.SD])
20. Few-Shot Learning of Accurate Folding Landscape for Protein Structure Prediction. (arXiv:2208.09652v1 [cs.LG])
21. A Domain Generalization Approach for Out-Of-Distribution 12-lead ECG Classification with Convolutional Neural Networks. (arXiv:2208.09656v1 [cs.LG])
22. A biologically-inspired evaluation of molecular generative machine learning. (arXiv:2208.09658v1 [cs.LG])
23. Trigger-free Event Detection via Derangement Reading Comprehension. (arXiv:2208.09659v1 [cs.CL])
24. Net2Brain: A Toolbox to compare artificial vision models with human brain responses. (arXiv:2208.09677v1 [cs.CV])
25. Fuse and Attend: Generalized Embedding Learning for Art and Sketches. (arXiv:2208.09698v1 [cs.CV])
26. DenseShift: Towards Accurate and Transferable Low-Bit Shift Network. (arXiv:2208.09708v1 [cs.CV])
27. SemEval-2022 Task 8: Multi-lingual News Article Similarity. (arXiv:2208.09715v1 [cs.CL])
28. Cognitive Modeling of Semantic Fluency Using Transformers. (arXiv:2208.09719v1 [cs.CL])
29. MLExchange -- A web-based platform enabling exchangeable machine learning workflows. (arXiv:2208.09751v1 [cs.LG])
30. Artifact-Based Domain Generalization of Skin Lesion Models. (arXiv:2208.09756v1 [cs.CV])
31. Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization. (arXiv:2208.09770v1 [cs.CL])
32. FastCPH: Efficient Survival Analysis for Neural Networks. (arXiv:2208.09793v1 [stat.ML])
33. I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning. (arXiv:2208.09828v1 [cs.CL])
34. Tyche: A library for probabilistic reasoning and belief modelling in Python. (arXiv:2208.09838v1 [cs.AI])
35. DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination. (arXiv:2208.09884v1 [cs.LG])
36. Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v1 [cs.LG])
37. SIM2E: Benchmarking the Group Equivariant Capability of Correspondence Matching Algorithms. (arXiv:2208.09896v1 [cs.CV])
38. A Web Application for Experimenting and Validating Remote Measurement of Vital Signs. (arXiv:2208.09916v1 [cs.AI])
39. ProPaLL: Probabilistic Partial Label Learning. (arXiv:2208.09931v1 [cs.LG])
40. Bipartite Matchings with Group Fairness and Individual Fairness Constraints. (arXiv:2208.09951v1 [cs.AI])
41. Performance, Opaqueness, Consequences, and Assumptions: Simple questions for responsible planning of machine learning solutions. (arXiv:2208.09966v1 [cs.LG])
42. Development of a CAV-based Intersection Control System and Corridor Level Impact Assessment. (arXiv:2208.09973v1 [cs.AI])
43. Antecedent Predictions Are Dominant for Tree-Based Code Generation. (arXiv:2208.09998v1 [cs.SE])
44. Selection Collider Bias in Large Language Models. (arXiv:2208.10063v1 [cs.CL])
45. Identifying Auxiliary or Adversarial Tasks Using Necessary Condition Analysis for Adversarial Multi-task Video Understanding. (arXiv:2208.10077v1 [cs.CV])
46. Incorporating Domain Knowledge through Task Augmentation for Front-End JavaScript Code Generation. (arXiv:2208.10091v1 [cs.SE])
47. Revising Image-Text Retrieval via Multi-Modal Entailment. (arXiv:2208.10126v1 [cs.CV])
48. BRIEF but Powerful: Byzantine-Robust and Privacy-Preserving Federated Learning via Model Segmentation and Secure clustering. (arXiv:2208.10161v1 [cs.CR])
49. KEEP: An Industrial Pre-Training Framework for Online Recommendation via Knowledge Extraction and Plugging. (arXiv:2208.10174v1 [cs.IR])
50. Dynamic Adaptive Threshold based Learning for Noisy Annotations Robust Facial Expression Recognition. (arXiv:2208.10221v1 [cs.CV])
51. From Easy to Hard: A Dual Curriculum Learning Framework for Context-Aware Document Ranking. (arXiv:2208.10226v1 [cs.IR])
52. One Model, Any CSP: Graph Neural Networks as Fast Global Search Heuristics for Constraint Satisfaction. (arXiv:2208.10227v1 [cs.AI])
53. Unit Testing for Concepts in Neural Networks. (arXiv:2208.10244v1 [cs.CL])
54. Prediction of User Request and Complaint in Spoken Customer-Agent Conversations. (arXiv:2208.10249v1 [cs.CL])
55. Multi-Task Learning for Depression Detection in Dialogs. (arXiv:2208.10250v1 [cs.CL])
56. Rethinking Textual Adversarial Defense for Pre-trained Language Models. (arXiv:2208.10251v1 [cs.CL])
57. On the non-efficient PAC learnability of acyclic conjunctive queries. (arXiv:2208.10255v1 [cs.DB])
58. Information-Theoretic Equivalence of Entropic Multi-Marginal Optimal Transport: a Theory for Multi-Agent Communication. (arXiv:2208.10256v1 [cs.IT])
59. Using Large Language Models to Simulate Multiple Humans. (arXiv:2208.10264v1 [cs.CL])
60. A semantic web approach to uplift decentralized household energy data. (arXiv:2208.10265v1 [cs.AI])
61. An Entropy-based Measure of Intelligence Degree of System Structures. (arXiv:2208.10266v1 [nlin.AO])
62. To show or not to show: Redacting sensitive text from videos of electronic displays. (arXiv:2208.10270v1 [cs.CV])
63. Model-Free Non-Stationary RL: Near-Optimal Regret and Applications in Multi-Agent RL and Inventory Control. (arXiv:2010.03161v4 [cs.LG] UPDATED)
64. A novel weighted approach for time series forecasting based on visibility graph. (arXiv:2103.13870v6 [cs.SI] UPDATED)
65. Goal Misgeneralization in Deep Reinforcement Learning. (arXiv:2105.14111v5 [cs.LG] UPDATED)
66. On the Theory of Reinforcement Learning with Once-per-Episode Feedback. (arXiv:2105.14363v3 [cs.LG] UPDATED)
67. Visual Probing: Cognitive Framework for Explaining Self-Supervised Image Representations. (arXiv:2106.11054v3 [cs.CV] UPDATED)
68. A Simple Unified Framework for Anomaly Detection in Deep Reinforcement Learning. (arXiv:2109.09889v2 [cs.LG] UPDATED)
69. HybridSDF: Combining Deep Implicit Shapes and Geometric Primitives for 3D Shape Representation and Manipulation. (arXiv:2109.10767v3 [cs.CV] UPDATED)
70. Hierarchical information matters: Text classification via tree based graph neural network. (arXiv:2110.02047v2 [cs.CL] UPDATED)
71. Local and Global Context-Based Pairwise Models for Sentence Ordering. (arXiv:2110.04291v2 [cs.CL] UPDATED)
72. Callee: Recovering Call Graphs for Binaries with Transfer and Contrastive Learning. (arXiv:2111.01415v3 [cs.SE] UPDATED)
73. Machine Learning Models Disclosure from Trusted Research Environments (TRE), Challenges and Opportunities. (arXiv:2111.05628v2 [cs.CR] UPDATED)
74. AlphaGarden: Learning to Autonomously Tend a Polyculture Garden. (arXiv:2111.06014v2 [cs.RO] UPDATED)
75. Perform Like an Engine: A Closed-Loop Neural-Symbolic Learning Framework for Knowledge Graph Inference. (arXiv:2112.01040v2 [cs.AI] UPDATED)
76. Seeing Objects in **dark** with Continual Contrastive Learning. (arXiv:2112.02891v3 [cs.CV] UPDATED)
77. DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture Fleeting Intraday Trading Opportunities. (arXiv:2201.09058v3 [q-fin.TR] UPDATED)
78. Do Differentiable Simulators Give Better Policy Gradients?. (arXiv:2202.00817v2 [cs.LG] UPDATED)
79. On Linking Level Segments. (arXiv:2203.05057v2 [cs.AI] UPDATED)
80. End-to-End Video Text Spotting with Transformer. (arXiv:2203.10539v3 [cs.CV] UPDATED)
81. ASE: Anomaly Scoring Based Ensemble Learning for Imbalanced Datasets. (arXiv:2203.10769v3 [cs.LG] UPDATED)
82. Open-set Recognition via Augmentation-based Similarity Learning. (arXiv:2203.13238v3 [cs.CV] UPDATED)
83. Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning. (arXiv:2204.01437v2 [cs.AI] UPDATED)
84. A Multi-Transformation Evolutionary Framework for Influence Maximization in Social Networks. (arXiv:2204.03297v2 [cs.NE] UPDATED)
85. EfficientFi: Towards Large-Scale Lightweight WiFi Sensing via CSI Compression. (arXiv:2204.04138v2 [cs.NI] UPDATED)
86. A Boosting Algorithm for Positive-Unlabeled Learning. (arXiv:2205.09485v3 [cs.LG] UPDATED)
87. Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v2 [cs.LG] UPDATED)
88. Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power. (arXiv:2205.13863v2 [cs.LG] UPDATED)
89. Use-Case-Grounded Simulations for Explanation Evaluation. (arXiv:2206.02256v2 [cs.HC] UPDATED)
90. A Graph-Enhanced Click Model for Web Search. (arXiv:2206.08621v2 [cs.IR] UPDATED)
91. Emotion Analysis using Multi-Layered Networks for Graphical Representation of Tweets. (arXiv:2207.00907v2 [cs.AI] UPDATED)
92. Learning to Rank with Small Set of Ground Truth Data. (arXiv:2207.01188v2 [cs.IR] UPDATED)
93. On stabilizing reinforcement learning without Lyapunov functions. (arXiv:2207.08730v5 [eess.SY] UPDATED)
94. BYEL : Bootstrap Your Emotion Latent. (arXiv:2207.10003v2 [cs.LG] UPDATED)
95. Trainability Preserving Neural Structured Pruning. (arXiv:2207.12534v2 [cs.LG] UPDATED)
96. Retrieval-Augmented Transformer for Image Captioning. (arXiv:2207.13162v2 [cs.CV] UPDATED)
97. Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v4 [cs.CL] UPDATED)
98. Class-Difficulty Based Methods for Long-Tailed Visual Recognition. (arXiv:2207.14499v2 [cs.CV] UPDATED)
99. Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks. (arXiv:2207.14682v2 [cs.SD] UPDATED)
100. Evolutionary bagging for ensemble learning. (arXiv:2208.02400v2 [cs.NE] UPDATED)
101. Convolutional Ensembling based Few-Shot Defect Detection Technique. (arXiv:2208.03288v2 [cs.CV] UPDATED)
102. Isoform Function Prediction Using Deep Neural Network. (arXiv:2208.03325v2 [q-bio.GN] UPDATED)
103. HaloAE: An HaloNet based Local Transformer Auto-Encoder for Anomaly Detection and Localization. (arXiv:2208.03486v2 [cs.CV] UPDATED)
104. CORNET: A neurosymbolic approach to learning conditional table formatting rules by example. (arXiv:2208.06032v3 [cs.AI] UPDATED)
105. An Empirical Exploration of Cross-domain Alignment between Language and Electroencephalogram. (arXiv:2208.06348v2 [q-bio.NC] UPDATED)
106. Who Finds the Short Proof? An Exploration of Variants of Boolos' Curious Inference using Higher-order Automated Theorem Provers. (arXiv:2208.06879v2 [math.LO] UPDATED)
107. Role of Data Augmentation in Unsupervised Anomaly Detection. (arXiv:2208.07734v2 [cs.LG] UPDATED)
108. UnCommonSense: Informative Negative Knowledge about Everyday Concepts. (arXiv:2208.09292v2 [cs.AI] UPDATED)

