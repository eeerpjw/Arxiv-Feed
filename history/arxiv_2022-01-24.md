# Your interest papers
---
## cs.CV
---
### Steerable Pyramid Transform Enables Robust Left Ventricle Quantification. (arXiv:2201.08388v1 [eess.IV])
- Authors : Xiangyang Zhu, **Kede Ma**, Wufeng Xue
- Link : [http://arxiv.org/abs/2201.08388](http://arxiv.org/abs/2201.08388)
> ABSTRACT  :  Although multifarious variants of convolutional neural networks (CNNs) have proved successful in cardiac index quantification, they seem vulnerable to mild input perturbations, e.g., spatial transformations, image distortions, and adversarial attacks. Such brittleness erodes our trust in CNN-based automated diagnosis of various cardiovascular diseases. In this work, we describe a simple and effective method to learn robust CNNs for left ventricle (LV) quantification, including cavity and myocardium areas, directional dimensions, and regional wall thicknesses. The key to the success of our approach is the use of the biologically-inspired steerable pyramid transform (SPT) as fixed front-end processing, which brings three computational advantages to LV quantification. First, the basis functions of SPT match the anatomical structure of the LV as well as the geometric characteristics of the estimated indices. Second, SPT enables sharing a CNN across different orientations as a form of parameter regularization, and explicitly captures the scale variations of the LV in a natural way. Third, the residual highpass subband can be conveniently discarded to further encourage robust feature learning. A concise and effective metric, named Robustness Ratio, is proposed to evaluate the robustness under various input perturbations. Extensive experiments on 145 cardiac sequences show that our SPT-augmented method performs favorably against state-of-the-art algorithms in terms of prediction accuracy, but is significantly more robust under input perturbations.  
### Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v5 [cs.LG] UPDATED)
- Authors : Jeff Clune, Nick Cheney
- Link : [http://arxiv.org/abs/2108.12056](http://arxiv.org/abs/2108.12056)
> ABSTRACT  :  Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning, and retaining, new information without repeated **exposure** to it. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using task-specific modules with constrained circumstances of application. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is accomplished by regulating the activity of weights in a convolutional neural network on the basis of inputs using top-down modulation generated by a second feed-forward neural network. We find that our method learns continually under domain transfer with sparse bursts of activity in weights that are recycled across tasks, rather than by maintaining task-specific modules. Sparse synaptic bursting is found to balance enhanced and diminished activity in a way that facilitates adaptation to new inputs without corrupting previously acquired functions. This behavior emerges during a prior meta-learning phase in which regulated synapses are selectively disinhibited, or grown, from an initial state of uniform suppression.  
### TerViT: An Efficient Ternary Vision Transformer. (arXiv:2201.08050v2 [cs.CV] UPDATED)
- Authors : Sheng Xu, Yanjing Li, Teli Ma, Bohan Zeng, Baochang Zhang, Peng Gao, Jinhu Lv
- Link : [http://arxiv.org/abs/2201.08050](http://arxiv.org/abs/2201.08050)
> ABSTRACT  :  Vision transformers (ViTs) have demonstrated great potential in various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. In this paper, we introduce a ternary vision transformer (TerViT) to ternarize the weights in ViTs, which are challenged by the large loss surface gap between real-valued and ternary parameters. To address the issue, we introduce a progressive training scheme by first training 8-bit transformers and then TerViT, and achieve a better optimization than conventional methods. Furthermore, we introduce channel-wise ternarization, by partitioning each matrix to different channels, each of which is with an unique distribution and ternarization interval. We apply our methods to popular DeiT and **Swin** backbones, and extensive results show that we can achieve competitive performance. For example, TerViT can quantize **Swin**-S to 13.1MB model size while achieving above 79% Top-1 accuracy on ImageNet dataset.  
## eess.IV
---
### Steerable Pyramid Transform Enables Robust Left Ventricle Quantification. (arXiv:2201.08388v1 [eess.IV])
- Authors : Xiangyang Zhu, **Kede Ma**, Wufeng Xue
- Link : [http://arxiv.org/abs/2201.08388](http://arxiv.org/abs/2201.08388)
> ABSTRACT  :  Although multifarious variants of convolutional neural networks (CNNs) have proved successful in cardiac index quantification, they seem vulnerable to mild input perturbations, e.g., spatial transformations, image distortions, and adversarial attacks. Such brittleness erodes our trust in CNN-based automated diagnosis of various cardiovascular diseases. In this work, we describe a simple and effective method to learn robust CNNs for left ventricle (LV) quantification, including cavity and myocardium areas, directional dimensions, and regional wall thicknesses. The key to the success of our approach is the use of the biologically-inspired steerable pyramid transform (SPT) as fixed front-end processing, which brings three computational advantages to LV quantification. First, the basis functions of SPT match the anatomical structure of the LV as well as the geometric characteristics of the estimated indices. Second, SPT enables sharing a CNN across different orientations as a form of parameter regularization, and explicitly captures the scale variations of the LV in a natural way. Third, the residual highpass subband can be conveniently discarded to further encourage robust feature learning. A concise and effective metric, named Robustness Ratio, is proposed to evaluate the robustness under various input perturbations. Extensive experiments on 145 cardiac sequences show that our SPT-augmented method performs favorably against state-of-the-art algorithms in terms of prediction accuracy, but is significantly more robust under input perturbations.  
## cs.LG
---
### Evaluating Generalization in Classical and Quantum Generative Models. (arXiv:2201.08770v1 [cs.LG])
- Authors : Kaitlin Gili, Marta Mauri, Alejandro Perdomo
- Link : [http://arxiv.org/abs/2201.08770](http://arxiv.org/abs/2201.08770)
> ABSTRACT  :  Defining and accurately measuring generalization in generative models remains an ongoing challenge and a topic of active research within the machine learning community. This is in contrast to discriminative models, where there is a clear definition of generalization, i.e., the model's classification accuracy when faced with unseen data. In this work, we construct a simple and unambiguous approach to evaluate the generalization capabilities of generative models. Using the sample-based generalization metrics proposed here, any generative model, from state-of-the-art classical generative models such as GANs to quantum models such as Quantum Circuit Born Machines, can be evaluated on the same ground on a concrete well-defined framework. In contrast to other sample-based metrics for probing generalization, we leverage constrained optimization problems (e.g., cardinality constrained problems) and use these discrete datasets to define specific metrics capable of unambiguously measuring the quality of the samples and the model's generalization capabilities for generating data beyond the training set but still within the valid solution space. Additionally, our metrics can diagnose trainability issues such as mode collapse and overfitting, as we illustrate when comparing GANs to quantum-inspired models built out of tensor networks. Our simulation results show that our quantum-inspired models have up to a $68 \times$ **enhancement** in generating unseen unique and valid samples compared to GANs, and a ratio of 61:2 for generating samples with better quality than those observed in the training set. We foresee these metrics as valuable tools for rigorously defining practical quantum advantage in the domain of generative modeling.  
### Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v5 [cs.LG] UPDATED)
- Authors : Jeff Clune, Nick Cheney
- Link : [http://arxiv.org/abs/2108.12056](http://arxiv.org/abs/2108.12056)
> ABSTRACT  :  Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning, and retaining, new information without repeated **exposure** to it. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using task-specific modules with constrained circumstances of application. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is accomplished by regulating the activity of weights in a convolutional neural network on the basis of inputs using top-down modulation generated by a second feed-forward neural network. We find that our method learns continually under domain transfer with sparse bursts of activity in weights that are recycled across tasks, rather than by maintaining task-specific modules. Sparse synaptic bursting is found to balance enhanced and diminished activity in a way that facilitates adaptation to new inputs without corrupting previously acquired functions. This behavior emerges during a prior meta-learning phase in which regulated synapses are selectively disinhibited, or grown, from an initial state of uniform suppression.  
### TerViT: An Efficient Ternary Vision Transformer. (arXiv:2201.08050v2 [cs.CV] UPDATED)
- Authors : Sheng Xu, Yanjing Li, Teli Ma, Bohan Zeng, Baochang Zhang, Peng Gao, Jinhu Lv
- Link : [http://arxiv.org/abs/2201.08050](http://arxiv.org/abs/2201.08050)
> ABSTRACT  :  Vision transformers (ViTs) have demonstrated great potential in various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. In this paper, we introduce a ternary vision transformer (TerViT) to ternarize the weights in ViTs, which are challenged by the large loss surface gap between real-valued and ternary parameters. To address the issue, we introduce a progressive training scheme by first training 8-bit transformers and then TerViT, and achieve a better optimization than conventional methods. Furthermore, we introduce channel-wise ternarization, by partitioning each matrix to different channels, each of which is with an unique distribution and ternarization interval. We apply our methods to popular DeiT and **Swin** backbones, and extensive results show that we can achieve competitive performance. For example, TerViT can quantize **Swin**-S to 13.1MB model size while achieving above 79% Top-1 accuracy on ImageNet dataset.  
## cs.AI
---
### Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v5 [cs.LG] UPDATED)
- Authors : Jeff Clune, Nick Cheney
- Link : [http://arxiv.org/abs/2108.12056](http://arxiv.org/abs/2108.12056)
> ABSTRACT  :  Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning, and retaining, new information without repeated **exposure** to it. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using task-specific modules with constrained circumstances of application. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is accomplished by regulating the activity of weights in a convolutional neural network on the basis of inputs using top-down modulation generated by a second feed-forward neural network. We find that our method learns continually under domain transfer with sparse bursts of activity in weights that are recycled across tasks, rather than by maintaining task-specific modules. Sparse synaptic bursting is found to balance enhanced and diminished activity in a way that facilitates adaptation to new inputs without corrupting previously acquired functions. This behavior emerges during a prior meta-learning phase in which regulated synapses are selectively disinhibited, or grown, from an initial state of uniform suppression.  
# Paper List
---
## cs.CV
---
**60** new papers in cs.CV:-) 
1. Improving Specificity in Mammography Using Cross-correlation between Wavelet and Fourier Transform. (arXiv:2201.08385v1 [eess.IV])
2. Steerable Pyramid Transform Enables Robust Left Ventricle Quantification. (arXiv:2201.08388v1 [eess.IV])
3. SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v1 [eess.IV])
4. FaceOcc: A Diverse, High-quality Face Occlusion Dataset for Human Face Extraction. (arXiv:2201.08425v1 [cs.CV])
5. A Visual Analytics Approach to Building Logistic Regression Models and its Application to Health Records. (arXiv:2201.08429v1 [cs.LG])
6. An Empirical Investigation of Model-to-Model Distribution Shifts in Trained Convolutional Filters. (arXiv:2201.08465v1 [cs.CV])
7. Vertical Federated Edge Learning with Distributed Integrated Sensing and Communication. (arXiv:2201.08512v1 [eess.SP])
8. What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review. (arXiv:2201.08550v1 [cs.CV])
9. Classroom Slide Narration System. (arXiv:2201.08574v1 [cs.CV])
10. SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v1 [eess.IV])
11. Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization. (arXiv:2201.08613v1 [cs.CV])
12. Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World. (arXiv:2201.08619v1 [cs.CV])
13. VIPriors 2: Visual Inductive Priors for Data-Efficient Deep Learning Challenges. (arXiv:2201.08625v1 [cs.CV])
14. Multi-view Monocular Depth and Uncertainty Prediction with Deep SfM in Dynamic Environments. (arXiv:2201.08633v1 [cs.CV])
15. Conceptor Learning for Class Activation Mapping. (arXiv:2201.08636v1 [cs.CV])
16. Enhancing Pseudo Label Quality for Semi-SupervisedDomain-Generalized Medical Image Segmentation. (arXiv:2201.08657v1 [cs.CV])
17. Fast Differentiable Matrix Square Root. (arXiv:2201.08663v1 [cs.CV])
18. Dynamic Deep Convolutional Candlestick Learner. (arXiv:2201.08669v1 [cs.CV])
19. Exploring Fusion Strategies for Accurate RGBT Visual Object Tracking. (arXiv:2201.08673v1 [cs.CV])
20. Distance-Ratio-Based Formulation for Metric Learning. (arXiv:2201.08676v1 [cs.LG])
21. A Comprehensive Study of Vision Transformers on Dense Prediction Tasks. (arXiv:2201.08683v1 [cs.CV])
22. SparseAlign: A Super-Resolution Algorithm for Automatic Marker Localization and Deformation Estimation in Cryo-Electron Tomography. (arXiv:2201.08706v1 [eess.IV])
23. Improving Across-Dataset Brain Tissue Segmentation Using Transformer. (arXiv:2201.08741v1 [eess.IV])
24. ERS: a novel comprehensive endoscopy image dataset for machine learning, compliant with the MST 3.0 specification. (arXiv:2201.08746v1 [cs.CV])
25. Object Detection in Aerial Images: What Improves the Accuracy?. (arXiv:2201.08763v1 [cs.CV])
26. Contrastive and Selective Hidden Embeddings for Medical Image Segmentation. (arXiv:2201.08779v1 [cs.CV])
27. AiTLAS: Artificial Intelligence Toolbox for Earth Observation. (arXiv:2201.08789v1 [cs.CV])
28. Realtime 3D Object Detection for Headsets. (arXiv:2201.08812v1 [cs.CV])
29. Active Predictive Coding Networks: A Neural Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies. (arXiv:2201.08813v1 [cs.CV])
30. Learning from One and Only One Shot. (arXiv:2201.08815v1 [cs.CV])
31. Skyline variations allow estimating distance to trees on landscape photos using semantic segmentation. (arXiv:2201.08816v1 [cs.CV])
32. Reliable Detection of Doppelg\"angers based on Deep Face Representations. (arXiv:2201.08831v1 [cs.CV])
33. Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v1 [cs.CV])
34. Efficient Globally Optimal 2D-to-3D Deformable Shape Matching. (arXiv:1601.06070v3 [cs.CV] UPDATED)
35. Analysis and algorithms for $\ell_p$-based semi-supervised learning on graphs. (arXiv:1901.05031v3 [math.NA] UPDATED)
36. From Pixel to Patch: Synthesize Context-aware Features for Zero-shot Semantic Segmentation. (arXiv:2009.12232v4 [cs.CV] UPDATED)
37. Contrastive Learning with Stronger Augmentations. (arXiv:2104.07713v2 [cs.CV] UPDATED)
38. On Aliased Resizing and Surprising Subtleties in GAN Evaluation. (arXiv:2104.11222v3 [cs.CV] UPDATED)
39. CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v4 [eess.IV] UPDATED)
40. End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v3 [physics.data-an] UPDATED)
41. You Never Cluster Alone. (arXiv:2106.01908v3 [cs.CV] UPDATED)
42. ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v3 [cs.CV] UPDATED)
43. Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v5 [cs.LG] UPDATED)
44. A Review of Computer Vision Technologies for Fish Tracking. (arXiv:2110.02551v3 [cs.CV] UPDATED)
45. THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling. (arXiv:2110.06607v3 [cs.CV] UPDATED)
46. MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining. (arXiv:2110.08009v3 [cs.LG] UPDATED)
47. PP-ShiTu: A Practical Lightweight Image Recognition System. (arXiv:2111.00775v2 [cs.CV] UPDATED)
48. BEVT: BERT Pretraining of Video Transformers. (arXiv:2112.01529v2 [cs.CV] UPDATED)
49. Learning Body-Aware 3D Shape Generative Models. (arXiv:2112.07022v3 [cs.GR] UPDATED)
50. Self-Supervised Approach to Addressing Zero-Shot Learning Problem. (arXiv:2201.01391v2 [cs.CV] UPDATED)
51. Learning with Less Labels in Digital Pathology via Scribble Supervision from Natural Images. (arXiv:2201.02627v2 [eess.IV] UPDATED)
52. In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v2 [cs.LG] UPDATED)
53. Transformers in Action: Weakly Supervised Action Segmentation. (arXiv:2201.05675v2 [cs.CV] UPDATED)
54. Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification. (arXiv:2201.05809v2 [cs.LG] UPDATED)
55. Two-Stage is Enough: A Concise Deep Unfolding Reconstruction Network for Flexible Video Compressive Sensing. (arXiv:2201.05810v2 [eess.IV] UPDATED)
56. YOLO -- You only look 10647 times. (arXiv:2201.06159v2 [cs.CV] UPDATED)
57. Self-supervised Video Representation Learning with Cascade Positive Retrieval. (arXiv:2201.07989v2 [cs.CV] UPDATED)
58. TerViT: An Efficient Ternary Vision Transformer. (arXiv:2201.08050v2 [cs.CV] UPDATED)
59. DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis. (arXiv:2201.08295v2 [cs.CV] UPDATED)
60. Stitch it in Time: GAN-Based Facial Editing of Real Videos. (arXiv:2201.08361v2 [cs.CV] UPDATED)
## eess.IV
---
**15** new papers in eess.IV:-) 
1. Improving Specificity in Mammography Using Cross-correlation between Wavelet and Fourier Transform. (arXiv:2201.08385v1 [eess.IV])
2. Steerable Pyramid Transform Enables Robust Left Ventricle Quantification. (arXiv:2201.08388v1 [eess.IV])
3. SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v1 [eess.IV])
4. alpha-Deep Probabilistic Inference (alpha-DPI): efficient uncertainty quantification from exoplanet astrometry to black hole feature extraction. (arXiv:2201.08506v1 [astro-ph.IM])
5. SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v1 [eess.IV])
6. Quantitative phase and refractive index imaging of 3D objects via optical transfer function reshaping. (arXiv:2201.08594v1 [physics.optics])
7. The Security of Deep Learning Defences for Medical Imaging. (arXiv:2201.08661v1 [cs.CR])
8. SparseAlign: A Super-Resolution Algorithm for Automatic Marker Localization and Deformation Estimation in Cryo-Electron Tomography. (arXiv:2201.08706v1 [eess.IV])
9. Improving Across-Dataset Brain Tissue Segmentation Using Transformer. (arXiv:2201.08741v1 [eess.IV])
10. Skyline variations allow estimating distance to trees on landscape photos using semantic segmentation. (arXiv:2201.08816v1 [cs.CV])
11. Estimating Solar and Wind Power Production using Computer Vision Deep Learning Techniques on Weather Maps. (arXiv:2103.08727v2 [eess.IV] UPDATED)
12. A QoE Model in Point Cloud Video Streaming. (arXiv:2111.02985v4 [cs.MM] UPDATED)
13. Learning with Less Labels in Digital Pathology via Scribble Supervision from Natural Images. (arXiv:2201.02627v2 [eess.IV] UPDATED)
14. Two-Stage is Enough: A Concise Deep Unfolding Reconstruction Network for Flexible Video Compressive Sensing. (arXiv:2201.05810v2 [eess.IV] UPDATED)
15. CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v4 [eess.IV] CROSS LISTED)
## cs.LG
---
**131** new papers in cs.LG:-) 
1. Unicorn: Reasoning about Configurable System Performance through the lens of Causality. (arXiv:2201.08413v1 [cs.LG])
2. Scalable Sampling for Nonsymmetric Determinantal Point Processes. (arXiv:2201.08417v1 [cs.LG])
3. A Visual Analytics Approach to Building Logistic Regression Models and its Application to Health Records. (arXiv:2201.08429v1 [cs.LG])
4. Reproducibility in Learning. (arXiv:2201.08430v1 [cs.LG])
5. DROPO: Sim-to-Real Transfer with Offline Domain Randomization. (arXiv:2201.08434v1 [cs.RO])
6. VUDENC: Vulnerability Detection with Deep Learning on a Natural Codebase for Python. (arXiv:2201.08441v1 [cs.CR])
7. Neural Network Quantization with AI Model Efficiency Toolkit (AIMET). (arXiv:2201.08442v1 [cs.LG])
8. A Prescriptive Dirichlet Power Allocation Policy with Deep Reinforcement Learning. (arXiv:2201.08445v1 [cs.LG])
9. Kinit Classification in Ethiopian Chants, Azmaris and Modern Music: A New Dataset and CNN Benchmark. (arXiv:2201.08448v1 [cs.SD])
10. Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency. (arXiv:2201.08451v1 [cs.CL])
11. Hybrid Graph Models for Logic Optimization via Spatio-Temporal Information. (arXiv:2201.08455v1 [cs.LG])
12. Federated Learning with Heterogeneous Architectures using Graph HyperNetworks. (arXiv:2201.08459v1 [cs.LG])
13. An Empirical Investigation of Model-to-Model Distribution Shifts in Trained Convolutional Filters. (arXiv:2201.08465v1 [cs.CV])
14. Android Malware Detection using Feature Ranking of Permissions. (arXiv:2201.08468v1 [cs.CR])
15. RoboMal: Malware Detection for Robot Network Systems. (arXiv:2201.08470v1 [cs.RO])
16. Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios. (arXiv:2201.08474v1 [cs.CR])
17. GenGNN: A Generic FPGA Framework for Graph Neural Network Acceleration. (arXiv:2201.08475v1 [cs.LG])
18. DDPG-Driven Deep-Unfolding with Adaptive Depth for Channel Estimation with Sparse Bayesian Learning. (arXiv:2201.08477v1 [eess.SP])
19. Deep Attention-Based Supernovae Classification of Multi-Band Light-Curves. (arXiv:2201.08482v1 [astro-ph.IM])
20. Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming. (arXiv:2201.08484v1 [cs.MA])
21. TOFU: Towards Obfuscated Federated Updates by Encoding Weight Updates into Gradients from Proxy Data. (arXiv:2201.08494v1 [cs.LG])
22. Deep reinforcement learning under signal temporal logic constraints using Lagrangian relaxation. (arXiv:2201.08504v1 [stat.ML])
23. alpha-Deep Probabilistic Inference (alpha-DPI): efficient uncertainty quantification from exoplanet astrometry to black hole feature extraction. (arXiv:2201.08506v1 [astro-ph.IM])
24. High-Dimensional Inference over Networks: Linear Convergence and Statistical Guarantees. (arXiv:2201.08507v1 [cs.LG])
25. How does unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis. (arXiv:2201.08514v1 [cs.LG])
26. LRSVRG-IMC: An SVRG-Based Algorithm for LowRank Inductive Matrix Completion. (arXiv:2201.08516v1 [cs.LG])
27. Optimal variance-reduced stochastic approximation in Banach spaces. (arXiv:2201.08518v1 [math.ST])
28. Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning. (arXiv:2201.08520v1 [cs.LG])
29. To SMOTE, or not to SMOTE?. (arXiv:2201.08528v1 [cs.LG])
30. Spatiotemporal Analysis Using Riemannian Composition of Diffusion Operators. (arXiv:2201.08530v1 [stat.ML])
31. Instance-Dependent Confidence and Early Stopping for Reinforcement Learning. (arXiv:2201.08536v1 [stat.ML])
32. AutoDistill: an End-to-End Framework to Explore and Distill Hardware-Efficient Language Models. (arXiv:2201.08539v1 [cs.LG])
33. Deep Learning-Accelerated 3D Carbon Storage Reservoir Pressure Forecasting Based on Data Assimilation Using Surface Displacement from InSAR. (arXiv:2201.08543v1 [stat.ML])
34. Fair Node Representation Learning via Adaptive Data Augmentation. (arXiv:2201.08549v1 [cs.LG])
35. Enhancing Hyperbolic Graph Embeddings via Contrastive Learning. (arXiv:2201.08554v1 [cs.LG])
36. Identifying Adversarial Attacks on Text Classifiers. (arXiv:2201.08555v1 [cs.CL])
37. Robust Unsupervised Graph Representation Learning via Mutual Information Maximization. (arXiv:2201.08557v1 [cs.LG])
38. Individual Treatment Effect Estimation Through Controlled Neural Network Training in Two Stages. (arXiv:2201.08559v1 [cs.LG])
39. Hold On and Swipe: A Touch-Movement Based Continuous Authentication Schema based on Machine Learning. (arXiv:2201.08564v1 [cs.CR])
40. Human Activity Recognition models using Limited Consumer Device Sensors and Machine Learning. (arXiv:2201.08565v1 [cs.LG])
41. Deep Q-learning: a robust control approach. (arXiv:2201.08610v1 [cs.LG])
42. Learning deterministic hydrodynamic equations from stochastic active particle dynamics. (arXiv:2201.08623v1 [cond-mat.soft])
43. Training Hybrid Classical-Quantum Classifiers via Stochastic Variational Optimization. (arXiv:2201.08629v1 [quant-ph])
44. A phase transition for finding needles in nonlinear haystacks with LASSO artificial neural networks. (arXiv:2201.08652v1 [stat.ML])
45. Unity Smoothing for Handling Inconsistent Evidence in Bayesian Networks and Unity Propagation for Faster Inference. (arXiv:2201.08659v1 [cs.LG])
46. On the adaptation of recurrent neural networks for system identification. (arXiv:2201.08660v1 [cs.LG])
47. The Security of Deep Learning Defences for Medical Imaging. (arXiv:2201.08661v1 [cs.CR])
48. Fast Differentiable Matrix Square Root. (arXiv:2201.08663v1 [cs.CV])
49. Clipped DeepControl: deep neural network two-dimensional pulse design with an amplitude constraint layer. (arXiv:2201.08668v1 [physics.med-ph])
50. Random Noise vs State-of-the-Art Probabilistic Forecasting Methods : A Case Study on CRPS-Sum Discrimination Ability. (arXiv:2201.08671v1 [cs.LG])
51. Distance-Ratio-Based Formulation for Metric Learning. (arXiv:2201.08676v1 [cs.LG])
52. Physical Activity Recognition by Utilising Smartphone Sensor Signals. (arXiv:2201.08688v1 [cs.HC])
53. A deep learning energy method for hyperelasticity and viscoelasticity. (arXiv:2201.08690v1 [cs.LG])
54. Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation. (arXiv:2201.08702v1 [cs.CL])
55. Adaptive Data Analysis with Correlated Observations. (arXiv:2201.08704v1 [cs.LG])
56. Improved Random Features for Dot Product Kernels. (arXiv:2201.08712v1 [stat.ML])
57. Equivalent Distance Geometry Error for Molecular Conformation Comparison. (arXiv:2201.08714v1 [q-bio.BM])
58. Less is Less: When Are Snippets Insufficient for Human vs Machine Relevance Estimation?. (arXiv:2201.08721v1 [cs.IR])
59. Sequential Item Recommendation in the MOBA Game Dota 2. (arXiv:2201.08724v1 [cs.LG])
60. Low-Interception Waveform: To Prevent the Recognition of Spectrum Waveform Modulation via Adversarial Examples. (arXiv:2201.08731v1 [cs.LG])
61. Meta Learning MDPs with Linear Transition Models. (arXiv:2201.08732v1 [cs.LG])
62. Privacy Policies Across the Ages: Content and Readability of Privacy Policies 1996--2021. (arXiv:2201.08739v1 [cs.CR])
63. Impacts of Students Academic Performance Trajectories on Final Academic Success. (arXiv:2201.08744v1 [cs.CY])
64. Evaluating Generalization in Classical and Quantum Generative Models. (arXiv:2201.08770v1 [cs.LG])
65. Real-Time Seizure Detection using EEG: A Comprehensive Comparison of Recent Approaches under a Realistic Setting. (arXiv:2201.08780v1 [cs.LG])
66. FedComm: Federated Learning as a Medium for Covert Communication. (arXiv:2201.08786v1 [cs.CR])
67. Deconfounding to Explanation Evaluation in Graph Neural Networks. (arXiv:2201.08802v1 [cs.LG])
68. GAP-Gen: Guided Automatic Python Code Generation. (arXiv:2201.08810v1 [cs.PL])
69. Active Predictive Coding Networks: A Neural Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies. (arXiv:2201.08813v1 [cs.CV])
70. Representing Long-Range Context for Graph Neural Networks with Global Attention. (arXiv:2201.08821v1 [cs.LG])
71. APack: Off-Chip, Lossless Data Compression for Efficient Deep Learning Inference. (arXiv:2201.08830v1 [cs.AR])
72. Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search. (arXiv:2201.08832v1 [cs.LG])
73. Marginal Effects for Non-Linear Prediction Functions. (arXiv:2201.08837v1 [cs.LG])
74. Analysis and algorithms for $\ell_p$-based semi-supervised learning on graphs. (arXiv:1901.05031v3 [math.NA] UPDATED)
75. Cost-sensitive Boosting Pruning Trees for depression detection on Twitter. (arXiv:1906.00398v3 [cs.LG] UPDATED)
76. Preventing Value Function Collapse in Ensemble {Q}-Learning by Maximizing Representation Diversity. (arXiv:2006.13823v3 [cs.LG] UPDATED)
77. Survey on Causal-based Machine Learning Fairness Notions. (arXiv:2010.09553v5 [cs.LG] UPDATED)
78. Deep Gravity: enhancing mobility flows generation with deep neural networks and geographic information. (arXiv:2012.00489v5 [cs.LG] UPDATED)
79. Graph Neural Networks: Taxonomy, Advances and Trends. (arXiv:2012.08752v3 [cs.LG] UPDATED)
80. Learning elliptic partial differential equations with randomized linear algebra. (arXiv:2102.00491v2 [math.NA] UPDATED)
81. Individual dynamic prediction of clinical endpoint from large dimensional longitudinal biomarker history: a landmark approach. (arXiv:2102.01466v2 [stat.ML] UPDATED)
82. Symbolic Behaviour in Artificial Intelligence. (arXiv:2102.03406v2 [cs.AI] UPDATED)
83. On the convergence of group-sparse autoencoders. (arXiv:2102.07003v2 [cs.LG] UPDATED)
84. CARMI: A Cache-Aware Learned Index with a Cost-based Construction Algorithm. (arXiv:2103.00858v3 [cs.DB] UPDATED)
85. Contrastive Learning with Stronger Augmentations. (arXiv:2104.07713v2 [cs.CV] UPDATED)
86. On Aliased Resizing and Surprising Subtleties in GAN Evaluation. (arXiv:2104.11222v3 [cs.CV] UPDATED)
87. CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v4 [eess.IV] UPDATED)
88. End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v3 [physics.data-an] UPDATED)
89. A generalization of the randomized singular value decomposition. (arXiv:2105.13052v3 [math.NA] UPDATED)
90. Non-negative matrix factorization algorithms greatly improve topic model fits. (arXiv:2105.13440v2 [stat.ML] UPDATED)
91. DISSECT: Disentangled Simultaneous Explanations via Concept Traversals. (arXiv:2105.15164v2 [cs.LG] UPDATED)
92. You Never Cluster Alone. (arXiv:2106.01908v3 [cs.CV] UPDATED)
93. ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v3 [cs.CV] UPDATED)
94. GemNet: Universal Directional Graph Neural Networks for Molecules. (arXiv:2106.08903v8 [physics.comp-ph] UPDATED)
95. Particle Cloud Generation with Message Passing Generative Adversarial Networks. (arXiv:2106.11535v3 [cs.LG] UPDATED)
96. Robust and Fully-Dynamic Coreset for Continuous-and-Bounded Learning (With Outliers) Problems. (arXiv:2107.00068v2 [cs.LG] UPDATED)
97. Constrained Policy Gradient Method for Safe and Fast Reinforcement Learning: a Neural Tangent Kernel Based Approach. (arXiv:2107.09139v2 [cs.LG] UPDATED)
98. Czech News Dataset for Semantic Textual Similarity. (arXiv:2108.08708v3 [cs.CL] UPDATED)
99. A fuzzy-rough uncertainty measure to discover bias encoded explicitly or implicitly in features of structured pattern classification datasets. (arXiv:2108.09098v2 [cs.LG] UPDATED)
100. Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training. (arXiv:2108.09373v3 [cs.DC] UPDATED)
101. Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v5 [cs.LG] UPDATED)
102. A survey on datasets for fairness-aware machine learning. (arXiv:2110.00530v3 [cs.LG] UPDATED)
103. Nash Convergence of Mean-Based Learning Algorithms in First Price Auctions. (arXiv:2110.03906v2 [cs.GT] UPDATED)
104. Satisficing Paths and Independent Multi-Agent Reinforcement Learning in Stochastic Games. (arXiv:2110.04638v2 [cs.GT] UPDATED)
105. MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining. (arXiv:2110.08009v3 [cs.LG] UPDATED)
106. Lipschitz Bandits with Batched Feedback. (arXiv:2110.09722v3 [cs.LG] UPDATED)
107. User Multi-Interest Modeling for Behavioral Cognition. (arXiv:2110.11337v3 [cs.LG] UPDATED)
108. Multi-Agent Reinforcement Learning for Active Voltage Control on Power Distribution Networks. (arXiv:2110.14300v5 [cs.LG] UPDATED)
109. Directional Message Passing on Molecular Graphs via Synthetic Coordinates. (arXiv:2111.04718v3 [cs.LG] UPDATED)
110. Gradients are Not All You Need. (arXiv:2111.05803v2 [cs.LG] UPDATED)
111. Dictionary-based Low-Rank Approximations and the Mixed Sparse Coding problem. (arXiv:2111.12399v2 [cs.LG] UPDATED)
112. Evaluation of Machine Learning Techniques for Forecast Uncertainty Quantification. (arXiv:2111.14844v4 [cs.LG] UPDATED)
113. BEVT: BERT Pretraining of Video Transformers. (arXiv:2112.01529v2 [cs.CV] UPDATED)
114. Pairwise Learning for Neural Link Prediction. (arXiv:2112.02936v6 [cs.LG] UPDATED)
115. Learning Body-Aware 3D Shape Generative Models. (arXiv:2112.07022v3 [cs.GR] UPDATED)
116. Automated Deep Learning: Neural Architecture Search Is Not the End. (arXiv:2112.09245v2 [cs.LG] UPDATED)
117. Self-Supervised Approach to Addressing Zero-Shot Learning Problem. (arXiv:2201.01391v2 [cs.CV] UPDATED)
118. Learning with Less Labels in Digital Pathology via Scribble Supervision from Natural Images. (arXiv:2201.02627v2 [eess.IV] UPDATED)
119. In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v2 [cs.LG] UPDATED)
120. Optimal Fixed-Budget Best Arm Identification using the Augmented Inverse Probability Weighting Estimator in Two-Armed Gaussian Bandits with Unknown Variances. (arXiv:2201.04469v3 [stat.ML] UPDATED)
121. Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next. (arXiv:2201.05624v2 [cs.LG] UPDATED)
122. Transformers in Action: Weakly Supervised Action Segmentation. (arXiv:2201.05675v2 [cs.CV] UPDATED)
123. Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification. (arXiv:2201.05809v2 [cs.LG] UPDATED)
124. UDC: Unified DNAS for Compressible TinyML Models. (arXiv:2201.05842v2 [cs.LG] UPDATED)
125. Learning grammar with a divide-and-concur neural network. (arXiv:2201.07341v2 [cs.CL] UPDATED)
126. TerViT: An Efficient Ternary Vision Transformer. (arXiv:2201.08050v2 [cs.CV] UPDATED)
127. Safe Deep RL in 3D Environments using Human Feedback. (arXiv:2201.08102v2 [cs.LG] UPDATED)
128. Symplectic Momentum Neural Networks -- Using Discrete Variational Mechanics as a prior in Deep Learning. (arXiv:2201.08281v2 [cs.LG] UPDATED)
129. Stitch it in Time: GAN-Based Facial Editing of Real Videos. (arXiv:2201.08361v2 [cs.CV] UPDATED)
130. A Non-Expert's Introduction to Data Ethics for Mathematicians. (arXiv:2201.07794v1 [math.HO] CROSS LISTED)
131. Learning Estimates At The Edge Using Intermittent And Aged Measurement Updates. (arXiv:2201.08020v1 [cs.LG] CROSS LISTED)
## cs.AI
---
**51** new papers in cs.AI:-) 
1. Improving Specificity in Mammography Using Cross-correlation between Wavelet and Fourier Transform. (arXiv:2201.08385v1 [eess.IV])
2. Unicorn: Reasoning about Configurable System Performance through the lens of Causality. (arXiv:2201.08413v1 [cs.LG])
3. SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v1 [eess.IV])
4. Neural Network Quantization with AI Model Efficiency Toolkit (AIMET). (arXiv:2201.08442v1 [cs.LG])
5. A Prescriptive Dirichlet Power Allocation Policy with Deep Reinforcement Learning. (arXiv:2201.08445v1 [cs.LG])
6. Automatic Item Generation of Figural Analogy Problems: A Review and Outlook. (arXiv:2201.08450v1 [cs.AI])
7. Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency. (arXiv:2201.08451v1 [cs.CL])
8. Federated Learning with Heterogeneous Architectures using Graph HyperNetworks. (arXiv:2201.08459v1 [cs.LG])
9. An Empirical Investigation of Model-to-Model Distribution Shifts in Trained Convolutional Filters. (arXiv:2201.08465v1 [cs.CV])
10. Classic Graph Structural Features Outperform Factorization-Based Graph Embedding Methods on Community Labeling. (arXiv:2201.08481v1 [cs.SI])
11. Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming. (arXiv:2201.08484v1 [cs.MA])
12. Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning. (arXiv:2201.08520v1 [cs.LG])
13. Can Machines Generate Personalized Music? A Hybrid Favorite-aware Method for User Preference Music Transfer. (arXiv:2201.08526v1 [cs.SD])
14. Fair Node Representation Learning via Adaptive Data Augmentation. (arXiv:2201.08549v1 [cs.LG])
15. Classroom Slide Narration System. (arXiv:2201.08574v1 [cs.CV])
16. Trustworthy Knowledge Graph Completion Based on Multi-sourced Noisy Data. (arXiv:2201.08580v1 [cs.IR])
17. Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World. (arXiv:2201.08619v1 [cs.CV])
18. VIPriors 2: Visual Inductive Priors for Data-Efficient Deep Learning Challenges. (arXiv:2201.08625v1 [cs.CV])
19. Conceptor Learning for Class Activation Mapping. (arXiv:2201.08636v1 [cs.CV])
20. Enhancing Pseudo Label Quality for Semi-SupervisedDomain-Generalized Medical Image Segmentation. (arXiv:2201.08657v1 [cs.CV])
21. On the adaptation of recurrent neural networks for system identification. (arXiv:2201.08660v1 [cs.LG])
22. Random Noise vs State-of-the-Art Probabilistic Forecasting Methods : A Case Study on CRPS-Sum Discrimination Ability. (arXiv:2201.08671v1 [cs.LG])
23. Gender Bias in Text: Labeled Datasets and Lexicons. (arXiv:2201.08675v1 [cs.CL])
24. Scales and Hedges in a Logic with Analogous Semantics. (arXiv:2201.08677v1 [cs.AI])
25. Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation. (arXiv:2201.08702v1 [cs.CL])
26. Less is Less: When Are Snippets Insufficient for Human vs Machine Relevance Estimation?. (arXiv:2201.08721v1 [cs.IR])
27. Meta Learning MDPs with Linear Transition Models. (arXiv:2201.08732v1 [cs.LG])
28. Towards Building Economic Models of Conversational Search. (arXiv:2201.08742v1 [cs.IR])
29. Inferring Brain Dynamics via Multimodal Joint Graph Representation EEG-fMRI. (arXiv:2201.08747v1 [q-bio.NC])
30. Object Detection in Aerial Images: What Improves the Accuracy?. (arXiv:2201.08763v1 [cs.CV])
31. Under-Approximating Expected Total Rewards in POMDPs. (arXiv:2201.08772v1 [cs.AI])
32. Active Predictive Coding Networks: A Neural Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies. (arXiv:2201.08813v1 [cs.CV])
33. Learning from One and Only One Shot. (arXiv:2201.08815v1 [cs.CV])
34. Preventing Value Function Collapse in Ensemble {Q}-Learning by Maximizing Representation Diversity. (arXiv:2006.13823v3 [cs.LG] UPDATED)
35. Symbolic Behaviour in Artificial Intelligence. (arXiv:2102.03406v2 [cs.AI] UPDATED)
36. Contrastive Learning with Stronger Augmentations. (arXiv:2104.07713v2 [cs.CV] UPDATED)
37. DISSECT: Disentangled Simultaneous Explanations via Concept Traversals. (arXiv:2105.15164v2 [cs.LG] UPDATED)
38. Multi-Objective Path-Based D* Lite. (arXiv:2108.00710v3 [cs.RO] UPDATED)
39. Knowledge Graph Reasoning with Relational Digraph. (arXiv:2108.06040v2 [cs.AI] UPDATED)
40. Czech News Dataset for Semantic Textual Similarity. (arXiv:2108.08708v3 [cs.CL] UPDATED)
41. Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v5 [cs.LG] UPDATED)
42. Nash Convergence of Mean-Based Learning Algorithms in First Price Auctions. (arXiv:2110.03906v2 [cs.GT] UPDATED)
43. Evaluation of Machine Learning Techniques for Forecast Uncertainty Quantification. (arXiv:2111.14844v4 [cs.LG] UPDATED)
44. Pairwise Learning for Neural Link Prediction. (arXiv:2112.02936v6 [cs.LG] UPDATED)
45. English-to-Chinese Transliteration with Phonetic Back-transliteration. (arXiv:2112.10321v2 [cs.CL] UPDATED)
46. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. (arXiv:2112.11446v2 [cs.CL] UPDATED)
47. In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v2 [cs.LG] UPDATED)
48. Towards a Reference Software Architecture for Human-AI Teaming in Smart Manufacturing. (arXiv:2201.04876v5 [cs.SE] UPDATED)
49. Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next. (arXiv:2201.05624v2 [cs.LG] UPDATED)
50. Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification. (arXiv:2201.05809v2 [cs.LG] UPDATED)
51. HEAM: High-Efficiency Approximate Multiplier Optimization for Deep Neural Networks. (arXiv:2201.08022v2 [cs.AR] UPDATED)
