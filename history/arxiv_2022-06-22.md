# Your interest papers
---
## cs.CV
---
### Augmented Imagefication: A Data-driven Fault Detection Method for Aircraft Air Data Sensors. (arXiv:2206.09055v1 [cs.CV])
- Authors : Hang Zhao, Jinyi Ma, Zhongzhi Li, Yiqun Dong, Jianliang Ai
- Link : [http://arxiv.org/abs/2206.09055](http://arxiv.org/abs/2206.09055)
> ABSTRACT  :  In this paper, a novel data-driven approach named Augmented Imagefication for Fault detection (FD) of aircraft air data sensors (ADS) is proposed. Exemplifying the FD problem of aircraft air data sensors, an online FD scheme on edge device based on deep neural network (DNN) is developed. First, the aircraft inertial reference unit measurements is adopted as equivalent inputs, which is scalable to different aircraft/flight cases. Data associated with 6 different aircraft/flight conditions are collected to provide diversity (scalability) in the training/testing database. Then Augmented Imagefication is proposed for the DNN-based prediction of flying conditions. The raw data are reshaped as a grayscale image for convolutional operation, and the necessity of augmentation is analyzed and pointed out. Different kinds of augmented method, i.e. Flip, Repeat, Tile and their combinations are discussed, the result shows that the All Repeat operation in both axes of image matrix leads to the best performance of DNN. The interpretability of DNN is studied based on Grad-CAM, which provide a better understanding and further solidifies the robustness of DNN. Next the DNN model, VGG-16 with augmented imagefication data is optimized for mobile hardware deployment. After pruning of DNN, a lightweight model (98.79% smaller than original VGG-16) with high accuracy (slightly up by 0.27%) and fast speed (time delay is reduced by 87.54%) is obtained. And the hyperparameters optimization of DNN based on TPE is implemented and the best combination of hyperparameters is determined (learning rate 0.001, iterative epochs 600, and batch size 100 yields the highest accuracy at 0.987). Finally, a online FD deployment based on edge device, Jetson Nano, is developed and the **real time** monitoring of aircraft is achieved. We believe that this method is instructive for addressing the FD problems in other similar fields.  
### **Swin**BERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v4 [cs.CV] UPDATED)
- Authors : Kevin Lin, Linjie Li, Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, Lijuan Wang
- Link : [http://arxiv.org/abs/2111.13196](http://arxiv.org/abs/2111.13196)
> ABSTRACT  :  The canonical approach to video captioning dictates a caption generation model to learn from offline-extracted dense video features. These feature extractors usually operate on video frames sampled at a fixed frame rate and are often trained on image/video understanding tasks, without adaption to video captioning data. In this work, we present **Swin**BERT, an end-to-end transformer-based model for video captioning, which takes video frame patches directly as inputs, and outputs a natural language description. Instead of leveraging multiple 2D/3D feature extractors, our method adopts a video transformer to encode spatial-temporal representations that can adapt to variable lengths of video input without dedicated design for different frame rates. Based on this model architecture, we show that video captioning can benefit significantly from more densely sampled video frames as opposed to previous successes with sparsely sampled video frames for video-and-language understanding tasks (e.g., video question answering). Moreover, to avoid the inherent redundancy in consecutive video frames, we propose adaptively learning a sparse attention mask and optimizing it for task-specific performance improvement through better long-range video sequence modeling. Through extensive experiments on 5 video captioning datasets, we show that **Swin**BERT achieves across-the-board performance improvements over previous methods, often by a large margin. The learned sparse attention masks in addition push the limit to new state of the arts, and can be transferred between different video lengths and between different datasets. Code is available at https://github.com/microsoft/**Swin**BERT  
### Proximal Denoiser for Convergent Plug-and-Play Optimization with Nonconvex Regularization. (arXiv:2201.13256v4 [math.OC] UPDATED)
- Authors : Samuel Hurault, Arthur Leclaire, Nicolas Papadakis
- Link : [http://arxiv.org/abs/2201.13256](http://arxiv.org/abs/2201.13256)
> ABSTRACT  :  Plug-and-Play (PnP) methods solve ill-posed inverse problems through iterative proximal algorithms by replacing a proximal operator by a denoising operation. When applied with deep neural network denoisers, these methods have shown state-of-the-art visual performance for image **restoration** problems. However, their theoretical convergence analysis is still incomplete. Most of the existing convergence results consider nonexpansive denoisers, which is non-realistic, or limit their analysis to strongly convex data-fidelity terms in the inverse problem to solve. Recently, it was proposed to train the denoiser as a gradient descent step on a functional parameterized by a deep neural network. Using such a denoiser guarantees the convergence of the PnP version of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this paper, we show that this gradient denoiser can actually correspond to the proximal operator of another scalar function. Given this new result, we exploit the convergence theory of proximal algorithms in the nonconvex setting to obtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM (Alternating Direction Method of Multipliers). When built on top of a smooth gradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target stationary points of an explicit functional. These convergence results are confirmed with numerical experiments on deblurring, super-resolution and inpainting.  
### On the Generalization of BasicVSR++ to Video Deblurring and Denoising. (arXiv:2204.05308v2 [cs.CV] UPDATED)
- Authors : Shangchen Zhou, Xiangyu Xu, Chen Change
- Link : [http://arxiv.org/abs/2204.05308](http://arxiv.org/abs/2204.05308)
> ABSTRACT  :  The exploitation of long-term information has been a long-standing problem in video **restoration**. The recent BasicVSR and BasicVSR++ have shown remarkable performance in video super-resolution through long-term propagation and effective alignment. Their success has led to a question of whether they can be transferred to different video **restoration** tasks. In this work, we extend BasicVSR++ to a generic framework for video **restoration** tasks. In tasks where inputs and outputs possess identical spatial size, the input resolution is reduced by strided convolutions to maintain efficiency. With only minimal changes from BasicVSR++, the proposed framework achieves compelling performance with great efficiency in various video **restoration** tasks including video deblurring and denoising. Notably, BasicVSR++ achieves comparable performance to Transformer-based approaches with up to 79% of parameter reduction and 44x speedup. The promising results demonstrate the importance of propagation and alignment in video **restoration** tasks beyond just video super-resolution. Code and models are available at https://github.com/ckkelvinchan/BasicVSR_PlusPlus.  
### A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal. (arXiv:2206.05520v2 [cs.CV] UPDATED)
- Authors : Renwei Yang, YiKe Liu, Bing Zeng
- Link : [http://arxiv.org/abs/2206.05520](http://arxiv.org/abs/2206.05520)
> ABSTRACT  :  There are several previous methods based on neural network can have great performance in denoising salt and pepper noise. However, those methods are based on a hypothesis that the value of salt and pepper noise is exactly 0 and 255. It is not true in the real world. The result of those methods deviate sharply when the value is different from 0 and 255. To overcome this weakness, our method aims at designing a convolutional neural network to detect the noise pixels in a wider range of value and then a filter is used to modify pixel value to 0, which is beneficial for further filtering. Additionally, another convolutional neural network is used to conduct the denoising and **restoration** work.  
### Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal. (arXiv:2206.06803v2 [cs.CV] UPDATED)
- Authors : Yuan Feng, Yaojun Hu, Pengfei Fang, Yanhong Yang, Sheng Liu, Shengyong Chen
- Link : [http://arxiv.org/abs/2206.06803](http://arxiv.org/abs/2206.06803)
> ABSTRACT  :  This work studies the joint rain and haze removal problem. In real-life scenarios, rain and haze, two often co-occurring common weather phenomena, can greatly degrade the clarity and quality of the scene images, leading to a performance drop in the visual applications, such as autonomous driving. However, jointly removing the rain and haze in scene images is ill-posed and challenging, where the existence of haze and rain and the change of atmosphere light, can both degrade the scene information. Current methods focus on the contamination removal part, thus ignoring the **restoration** of the scene information affected by the change of atmospheric light. We propose a novel deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address the aforementioned challenge. The ADU-Net produces both the contamination residual and the scene residual to efficiently remove the rain and haze while preserving the fidelity of the scene information. Extensive experiments show our work outperforms the existing state-of-the-art methods by a considerable margin in both synthetic data and real-world data benchmarks, including RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data, respectively.    Codes will be made available freely to the research community.  
### FWD: **Real-time** Novel View Synthesis with Forward Warping and Depth. (arXiv:2206.08355v2 [cs.CV] UPDATED)
- Authors : Ang Cao, Chris Rockwell, Justin Johnson
- Link : [http://arxiv.org/abs/2206.08355](http://arxiv.org/abs/2206.08355)
> ABSTRACT  :  Novel view synthesis (NVS) is a challenging task requiring systems to generate photorealistic images of scenes from new viewpoints, where both quality and speed are important for applications. Previous image-based rendering (IBR) methods are fast, but have poor quality when input views are sparse. Recent Neural Radiance Fields (**NeRF**) and generalizable variants give impressive results but are not real-time. In our paper, we propose a generalizable NVS method with sparse inputs, called FWD, which gives high-quality synthesis in real-time. With explicit depth and differentiable rendering, it achieves competitive results to the SOTA methods with 130-1000x speedup and better perceptual quality. If available, we can seamlessly integrate sensor depth during either training or inference to improve image quality while retaining real-time speed. With the growing prevalence of depths sensors, we hope that methods making use of depth will become increasingly useful.  
### A machine-generated catalogue of Charon's craters and implications for the Kuiper belt. (arXiv:2206.08277v1 [astro-ph.EP] CROSS LISTED)
- Authors : Mohamad Ali
- Link : [http://arxiv.org/abs/2206.08277](http://arxiv.org/abs/2206.08277)
> ABSTRACT  :  In this paper we investigate Charon's craters size distribution using a deep learning model. This is motivated by the recent results of Singer et al. (2019) who, using manual cataloging, found a change in the size distribution slope of craters smaller than 12 km in diameter, translating into a paucity of small Kuiper Belt objects. These results were corroborated by Robbins and Singer (2021), but opposed by Morbidelli et al. (2021), necessitating an independent review. Our MaskRCNN-based ensemble of models was trained on Lunar, Mercurian, and Martian crater catalogues and both optical and digital elevation images. We use a robust image augmentation scheme to force the model to generalize and transfer-learn into icy objects. With no prior bias or **exposure** to Charon, our model find best fit slopes of q =-1.47+-0.33 for craters smaller than 10 km, and q =-2.91+-0.51 for craters larger than 15 km. These values indicate a clear change in slope around 15 km as suggested by Singer et al. (2019) and thus independently confirm their conclusions. Our slopes however are both slightly flatter than those found more recently by Robbins and Singer (2021). Our trained models and relevant codes are available online on github.com/malidib/ACID .  
## eess.IV
---
### Analysis & Computational Complexity Reduction of Monocular and Stereo Depth Estimation Techniques. (arXiv:2206.09071v1 [cs.CV])
- Authors : Rajeev Patwari, Varo Ly
- Link : [http://arxiv.org/abs/2206.09071](http://arxiv.org/abs/2206.09071)
> ABSTRACT  :  Accurate depth estimation with lowest compute and energy cost is a crucial requirement for unmanned and battery operated autonomous systems. Robotic applications require **real time** depth estimation for navigation and decision making under rapidly changing 3D surroundings. A high accuracy algorithm may provide the best depth estimation but may consume tremendous compute and energy resources. A general trade-off is to choose less accurate methods for initial depth estimate and a more accurate yet compute intensive method when needed. Previous work has shown this trade-off can be improved by developing a state-of-the-art method (AnyNet) to improve stereo depth estimation.    We studied both the monocular and stereo vision depth estimation methods and investigated methods to reduce computational complexity of these methods. This was our baseline. Consequently, our experiments show reduction of monocular depth estimation model size by ~75% reduces accuracy by less than 2% (SSIM metric). Our experiments with the novel stereo vision method (AnyNet) show that accuracy of depth estimation does not degrade more than 3% (three pixel error metric) in spite of reduction in model size by ~20%. We have shown that smaller models can indeed perform competitively.  
### Perceptual Optimization of a Biologically-Inspired Tone Mapping Operator. (arXiv:2206.09146v1 [eess.IV])
- Authors : Peibei Cao, Chenyang Le, Yuming Fang, **Kede Ma**
- Link : [http://arxiv.org/abs/2206.09146](http://arxiv.org/abs/2206.09146)
> ABSTRACT  :  With the increasing popularity and accessibility of **high dynamic range** (**HDR**) photography, tone mapping operators (TMOs) for dynamic range compression and medium presentation are practically demanding. In this paper, we develop a two-stage neural network-based **HDR** image TMO that is biologically-inspired, computationally efficient, and perceptually optimized. In Stage one, motivated by the physiology of the early stages of the human visual system (HVS), we first decompose an **HDR** image into a normalized Laplacian pyramid. We then use two lightweight deep neural networks (DNNs) that take this normalized representation as input and estimate the Laplacian pyramid of the corresponding LDR image. We optimize the tone mapping network by minimizing the normalized Laplacian pyramid distance (NLPD), a perceptual metric calibrated against human judgments of tone-mapped image quality. In Stage two, we generate a pseudo-multi-**exposure** image stack with different color saturation and detail visibility by inputting an **HDR** image ``calibrated'' with different maximum luminances to the learned tone mapping network. We then train another lightweight DNN to fuse the LDR image stack into a desired LDR image by maximizing a variant of MEF-SSIM, another perceptually calibrated metric for image fusion. By doing so, the proposed TMO is fully automatic to tone map uncalibrated **HDR** images. Across an independent set of **HDR** images, we find that our method produces images with consistently better visual quality, and is among the fastest local TMOs.  
### Multi-Modality Image Super-Resolution using Generative Adversarial Networks. (arXiv:2206.09193v1 [eess.IV])
- Authors : Aref Abedjooy, Mehran Ebrahimi
- Link : [http://arxiv.org/abs/2206.09193](http://arxiv.org/abs/2206.09193)
> ABSTRACT  :  Over the past few years deep learning-based techniques such as Generative Adversarial Networks (GANs) have significantly improved solutions to image super-resolution and image-to-image translation problems. In this paper, we propose a solution to the joint problem of image super-resolution and multi-modality image-to-image translation. The problem can be stated as the recovery of a high-resolution image in a modality, given a low-resolution observation of the same image in an alternative modality. Our paper offers two models to address this problem and will be evaluated on the recovery of high-resolution day images given low-resolution **night** images of the same scene. Promising qualitative and quantitative results will be presented for each model.  
### Multi-Modality Image Inpainting using Generative Adversarial Networks. (arXiv:2206.09210v1 [eess.IV])
- Authors : Aref Abedjooy, Mehran Ebrahimi
- Link : [http://arxiv.org/abs/2206.09210](http://arxiv.org/abs/2206.09210)
> ABSTRACT  :  Deep learning techniques, especially Generative Adversarial Networks (GANs) have significantly improved image inpainting and image-to-image translation tasks over the past few years. To the best of our knowledge, the problem of combining the image inpainting task with the multi-modality image-to-image translation remains intact. In this paper, we propose a model to address this problem. The model will be evaluated on combined **night**-to-day image translation and inpainting, along with promising qualitative and quantitative results.  
### StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v1 [cs.CV])
- Authors : Minguk Kang, Joonghyuk Shin, Jaesik Park
- Link : [http://arxiv.org/abs/2206.09479](http://arxiv.org/abs/2206.09479)
> ABSTRACT  :  Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 13 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and **Swin** Transformer). Unlike other benchmarks used in the GAN community, we train representative GANs, including BigGAN, StyleGAN2, and StyleGAN3, in a unified training pipeline and quantify generation performance with 7 evaluation metrics. The benchmark evaluates other cutting-edge generative models(e.g., StyleGAN-XL, ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations, training, and evaluation scripts with the pre-trained weights. StudioGAN is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.  
### SJ-HD^2R: Selective Joint **High Dynamic Range** and Denoising Imaging for Dynamic Scenes. (arXiv:2206.09611v1 [eess.IV])
- Authors : Wei Li, Shuai Xiao, Tianhong Dai, Shanxin Yuan, Tao Wang, Cheng Li, Fenglong Song
- Link : [http://arxiv.org/abs/2206.09611](http://arxiv.org/abs/2206.09611)
> ABSTRACT  :  Ghosting artifacts, motion blur, and low fidelity in highlight are the main challenges in **High Dynamic Range** (**HDR**) imaging from multiple Low Dynamic Range (LDR) images. These issues come from using the medium-exposed image as the reference frame in previous methods. To deal with them, we propose to use the under-exposed image as the reference to avoid these issues. However, the heavy noise in **dark** regions of the under-exposed image becomes a new problem. Therefore, we propose a joint **HDR** and denoising pipeline, containing two sub-networks: (i) a pre-denoising network (PreDNNet) to adaptively denoise input LDRs by exploiting **exposure** priors; (ii) a pyramid cascading fusion network (PCFNet), introducing an attention mechanism and cascading structure in a multi-scale manner. To further leverage these two paradigms, we propose a selective and joint **HDR** and denoising (SJ-HD$^2$R) imaging framework, utilizing scenario-specific priors to conduct the path selection with an accuracy of more than 93.3$\%$. We create the first joint **HDR** and denoising benchmark dataset, which contains a variety of challenging **HDR** and denoising scenes and supports the switching of the reference image. Extensive experiment results show that our method achieves superior performance to previous methods.  
### An Integrated Representation & Compression Scheme Based on Convolutional Autoencoders with 4D DCT Perceptual Encoding for **High Dynamic Range** Light Fields. (arXiv:2206.10131v1 [cs.CV])
- Authors : Sally Khaidem, Mansi Sharma
- Link : [http://arxiv.org/abs/2206.10131](http://arxiv.org/abs/2206.10131)
> ABSTRACT  :  The emerging and existing light field displays are highly capable of realistic presentation of 3D scenes on auto-stereoscopic glasses-free platforms. The light field size is a major drawback while utilising 3D displays and streaming purposes. When a light field is of **high dynamic range**, the size increases drastically. In this paper, we propose a novel compression algorithm for a **high dynamic range** light field which yields a perceptually lossless compression. The algorithm exploits the inter and intra view correlations of the **HDR** light field by interpreting it to be a four-dimension volume. The **HDR** light field compression is based on a novel 4DDCT-UCS (4D-DCT Uniform Colour Space) algorithm. Additional encoding of 4DDCT-UCS acquired images by HEVC eliminates intra-frame, inter-frame and intrinsic redundancies in **HDR** light field data. Comparison with state-of-the-art coders like JPEG-XL and **HDR** video coding algorithm exhibits superior compression performance of the proposed scheme for real-world light fields.  
### covEcho Resource constrained lung ultrasound image analysis tool for faster triaging and active learning. (arXiv:2206.10183v1 [eess.IV])
- Authors : Jinu Joseph, Mahesh Raveendranatha, Yale Tung, Kesavadas Chandrasekharan, Vimal Chacko, Anoop Ayyappan, Jineesh Valakkada, Kiran Vishnu
- Link : [http://arxiv.org/abs/2206.10183](http://arxiv.org/abs/2206.10183)
> ABSTRACT  :  Lung ultrasound (LUS) is possibly the only medical imaging modality which could be used for continuous and periodic monitoring of the lung. This is extremely useful in tracking the lung manifestations either during the onset of lung infection or to track the effect of vaccination on lung as in pandemics such as COVID-19. There have been many attempts in automating the classification of severity of lung into various classes or automatic segmentation of various LUS landmarks and manifestations. However, all these approaches are based on training static machine learning models which require a significantly clinically annotated large dataset and are computationally heavy and most of the time non-**real time**. In this work, a real-time light weight active learning-based approach is presented for faster triaging in COVID-19 subjects in resource constrained settings. The tool, based on the you look only once (YOLO) network, has the capability of providing the quality of images based on the identification of various LUS landmarks, artefacts and manifestations, prediction of severity of lung infection, possibility of active learning based on the feedback from clinicians or on the image quality and a summarization of the significant frames which are having high severity of infection and high image quality for further analysis. The results show that the proposed tool has a mean average precision (mAP) of 66% at an Intersection over Union (IoU) threshold of 0.5 for the prediction of LUS landmarks. The 14MB lightweight YOLOv5s network achieves 123 FPS while running in a Quadro P4000 GPU. The tool is available for usage and analysis upon request from the authors.  
### Automated Coronary Calcium Scoring using U-Net Models through Semi-supervised Learning on Non-Gated CT Scans. (arXiv:2206.10455v1 [eess.IV])
- Authors : Sanskriti Singh
- Link : [http://arxiv.org/abs/2206.10455](http://arxiv.org/abs/2206.10455)
> ABSTRACT  :  Every year, thousands of innocent people die due to heart attacks. Often undiagnosed heart attacks can hit people by surprise since many current medical plans don't cover the costs to require the searching of calcification on these scans. Only if someone is suspected to have a heart problem, a gated CT scan is taken, otherwise, there's no way for the patient to be aware of a possible heart attack/disease. While nongated CT scans are more periodically taken, it is harder to detect calcification and is usually taken for a purpose other than locating calcification in arteries. In fact, in **real time** coronary artery calcification scores are only calculated on gated CT scans, not nongated CT scans. After training a unet model on the Coronary Calcium and chest CT's gated scans, it received a DICE coefficient of 0.95 on its untouched test set. This model was used to predict on nongated CT scans, performing with a mean absolute error (MAE) of 674.19 and bucket classification accuracy of 41% (5 classes). Through the analysis of the images and the information stored in the images, mathematical equations were derived and used to automatically crop the images around the location of the heart. By performing semi-supervised learning the new cropped nongated scans were able to closely resemble gated CT scans, improving the performance by 91% in MAE (62.38) and 23% in accuracy.  
### A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal. (arXiv:2206.05520v2 [cs.CV] UPDATED)
- Authors : Renwei Yang, YiKe Liu, Bing Zeng
- Link : [http://arxiv.org/abs/2206.05520](http://arxiv.org/abs/2206.05520)
> ABSTRACT  :  There are several previous methods based on neural network can have great performance in denoising salt and pepper noise. However, those methods are based on a hypothesis that the value of salt and pepper noise is exactly 0 and 255. It is not true in the real world. The result of those methods deviate sharply when the value is different from 0 and 255. To overcome this weakness, our method aims at designing a convolutional neural network to detect the noise pixels in a wider range of value and then a filter is used to modify pixel value to 0, which is beneficial for further filtering. Additionally, another convolutional neural network is used to conduct the denoising and **restoration** work.  
### Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal. (arXiv:2206.06803v2 [cs.CV] UPDATED)
- Authors : Yuan Feng, Yaojun Hu, Pengfei Fang, Yanhong Yang, Sheng Liu, Shengyong Chen
- Link : [http://arxiv.org/abs/2206.06803](http://arxiv.org/abs/2206.06803)
> ABSTRACT  :  This work studies the joint rain and haze removal problem. In real-life scenarios, rain and haze, two often co-occurring common weather phenomena, can greatly degrade the clarity and quality of the scene images, leading to a performance drop in the visual applications, such as autonomous driving. However, jointly removing the rain and haze in scene images is ill-posed and challenging, where the existence of haze and rain and the change of atmosphere light, can both degrade the scene information. Current methods focus on the contamination removal part, thus ignoring the **restoration** of the scene information affected by the change of atmospheric light. We propose a novel deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address the aforementioned challenge. The ADU-Net produces both the contamination residual and the scene residual to efficiently remove the rain and haze while preserving the fidelity of the scene information. Extensive experiments show our work outperforms the existing state-of-the-art methods by a considerable margin in both synthetic data and real-world data benchmarks, including RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data, respectively.    Codes will be made available freely to the research community.  
## cs.LG
---
### NASTAR: Noise Adaptive Speech **Enhancement** with Target-Conditional Resampling. (arXiv:2206.09058v1 [eess.AS])
- Authors : Chang Lee, Hung Hu, Chen Lin, Song Chen, Min Wang, Yu Tsao
- Link : [http://arxiv.org/abs/2206.09058](http://arxiv.org/abs/2206.09058)
> ABSTRACT  :  For deep learning-based speech **enhancement** (SE) systems, the training-test acoustic mismatch can cause notable performance degradation. To address the mismatch issue, numerous noise adaptation strategies have been derived. In this paper, we propose a novel method, called noise adaptive speech **enhancement** with target-conditional resampling (NASTAR), which reduces mismatches with only one sample (one-shot) of noisy speech in the target environment. NASTAR uses a feedback mechanism to simulate adaptive training data via a noise extractor and a retrieval model. The noise extractor estimates the target noise from the noisy speech, called pseudo-noise. The noise retrieval model retrieves relevant noise samples from a pool of noise signals according to the noisy speech, called relevant-cohort. The pseudo-noise and the relevant-cohort set are jointly sampled and mixed with the source speech corpus to prepare simulated training data for noise adaptation. Experimental results show that NASTAR can effectively use one noisy speech sample to adapt an SE model to a target condition. Moreover, both the noise extractor and the noise retrieval model contribute to model adaptation. To our best knowledge, NASTAR is the first work to perform one-shot noise adaptation through noise extraction and retrieval.  
### On Asymptotic Linear Convergence of Projected Gradient Descent for Constrained Least Squares. (arXiv:2112.11760v2 [math.OC] UPDATED)
- Authors : Trung Vu, Raviv Raich
- Link : [http://arxiv.org/abs/2112.11760](http://arxiv.org/abs/2112.11760)
> ABSTRACT  :  Many recent problems in signal processing and machine learning such as compressed sensing, image **restoration**, matrix/tensor recovery, and non-negative matrix factorization can be cast as constrained optimization. Projected gradient descent is a simple yet efficient method for solving such constrained optimization problems. Local convergence analysis furthers our understanding of its asymptotic behavior near the solution, offering sharper bounds on the convergence rate compared to global convergence analysis. However, local guarantees often appear scattered in problem-specific areas of machine learning and signal processing. This manuscript presents a unified framework for the local convergence analysis of projected gradient descent in the context of constrained least squares. The proposed analysis offers insights into pivotal local convergence properties such as the conditions for linear convergence, the region of convergence, the exact asymptotic rate of convergence, and the bound on the number of iterations needed to reach a certain level of accuracy. To demonstrate the applicability of the proposed approach, we present a recipe for the convergence analysis of projected gradient descent and demonstrate it via a beginning-to-end application of the recipe on four fundamental problems, namely, linear equality-constrained least squares, sparse recovery, least squares with the unit norm constraint, and matrix completion.  
### Particle Transformer for Jet Tagging. (arXiv:2202.03772v2 [hep-ph] UPDATED)
- Authors : Huilin Qu, Congqiao Li, Sitian Qian
- Link : [http://arxiv.org/abs/2202.03772](http://arxiv.org/abs/2202.03772)
> ABSTRACT  :  Jet tagging is a critical yet challenging classification task in particle physics. While deep learning has transformed jet tagging and significantly improved performance, the lack of a large-scale public dataset impedes further **enhancement**. In this work, we present JetClass, a new comprehensive dataset for jet tagging. The JetClass dataset consists of 100 M jets, about two orders of magnitude larger than existing public datasets. A total of 10 types of jets are simulated, including several types unexplored for tagging so far. Based on the large dataset, we propose a new Transformer-based architecture for jet tagging, called Particle Transformer (ParT). By incorporating pairwise particle interactions in the attention mechanism, ParT achieves higher tagging performance than a plain Transformer and surpasses the previous state-of-the-art, ParticleNet, by a large margin. The pre-trained ParT models, once fine-tuned, also substantially enhance the performance on two widely adopted jet tagging benchmarks. The dataset, code and models are publicly available at https://github.com/jet-universe/particle_transformer.  
### **Bilateral** Dependency Optimization: Defending Against Model-inversion Attacks. (arXiv:2206.05483v2 [cs.LG] UPDATED)
- Authors : Xiong Peng, Feng Liu, Jingfen Zhang, Long Lan, Junjie Ye, Tongliang Liu, Bo Han
- Link : [http://arxiv.org/abs/2206.05483](http://arxiv.org/abs/2206.05483)
> ABSTRACT  :  Through using only a well-trained classifier, model-inversion (MI) attacks can recover the data used for training the classifier, leading to the privacy leakage of the training data. To defend against MI attacks, previous work utilizes a unilateral dependency optimization strategy, i.e., minimizing the dependency between inputs (i.e., features) and outputs (i.e., labels) during training the classifier. However, such a minimization process conflicts with minimizing the supervised loss that aims to maximize the dependency between inputs and outputs, causing an explicit trade-off between model robustness against MI attacks and model utility on classification tasks. In this paper, we aim to minimize the dependency between the latent representations and the inputs while maximizing the dependency between latent representations and the outputs, named a **bilateral** dependency optimization (BiDO) strategy. In particular, we use the dependency constraints as a universally applicable regularizer in addition to commonly used losses for deep neural networks (e.g., cross-entropy), which can be instantiated with appropriate dependency criteria according to different tasks. To verify the efficacy of our strategy, we propose two implementations of BiDO, by using two different dependency measures: BiDO with constrained covariance (BiDO-COCO) and BiDO with Hilbert-Schmidt Independence Criterion (BiDO-HSIC). Experiments show that BiDO achieves the state-of-the-art defense performance for a variety of datasets, classifiers, and MI attacks while suffering a minor classification-accuracy drop compared to the well-trained classifier with no defense, which lights up a novel road to defend against MI attacks.  
### A machine-generated catalogue of Charon's craters and implications for the Kuiper belt. (arXiv:2206.08277v1 [astro-ph.EP] CROSS LISTED)
- Authors : Mohamad Ali
- Link : [http://arxiv.org/abs/2206.08277](http://arxiv.org/abs/2206.08277)
> ABSTRACT  :  In this paper we investigate Charon's craters size distribution using a deep learning model. This is motivated by the recent results of Singer et al. (2019) who, using manual cataloging, found a change in the size distribution slope of craters smaller than 12 km in diameter, translating into a paucity of small Kuiper Belt objects. These results were corroborated by Robbins and Singer (2021), but opposed by Morbidelli et al. (2021), necessitating an independent review. Our MaskRCNN-based ensemble of models was trained on Lunar, Mercurian, and Martian crater catalogues and both optical and digital elevation images. We use a robust image augmentation scheme to force the model to generalize and transfer-learn into icy objects. With no prior bias or **exposure** to Charon, our model find best fit slopes of q =-1.47+-0.33 for craters smaller than 10 km, and q =-2.91+-0.51 for craters larger than 15 km. These values indicate a clear change in slope around 15 km as suggested by Singer et al. (2019) and thus independently confirm their conclusions. Our slopes however are both slightly flatter than those found more recently by Robbins and Singer (2021). Our trained models and relevant codes are available online on github.com/malidib/ACID .  
## cs.AI
---
### Augmented Imagefication: A Data-driven Fault Detection Method for Aircraft Air Data Sensors. (arXiv:2206.09055v1 [cs.CV])
- Authors : Hang Zhao, Jinyi Ma, Zhongzhi Li, Yiqun Dong, Jianliang Ai
- Link : [http://arxiv.org/abs/2206.09055](http://arxiv.org/abs/2206.09055)
> ABSTRACT  :  In this paper, a novel data-driven approach named Augmented Imagefication for Fault detection (FD) of aircraft air data sensors (ADS) is proposed. Exemplifying the FD problem of aircraft air data sensors, an online FD scheme on edge device based on deep neural network (DNN) is developed. First, the aircraft inertial reference unit measurements is adopted as equivalent inputs, which is scalable to different aircraft/flight cases. Data associated with 6 different aircraft/flight conditions are collected to provide diversity (scalability) in the training/testing database. Then Augmented Imagefication is proposed for the DNN-based prediction of flying conditions. The raw data are reshaped as a grayscale image for convolutional operation, and the necessity of augmentation is analyzed and pointed out. Different kinds of augmented method, i.e. Flip, Repeat, Tile and their combinations are discussed, the result shows that the All Repeat operation in both axes of image matrix leads to the best performance of DNN. The interpretability of DNN is studied based on Grad-CAM, which provide a better understanding and further solidifies the robustness of DNN. Next the DNN model, VGG-16 with augmented imagefication data is optimized for mobile hardware deployment. After pruning of DNN, a lightweight model (98.79% smaller than original VGG-16) with high accuracy (slightly up by 0.27%) and fast speed (time delay is reduced by 87.54%) is obtained. And the hyperparameters optimization of DNN based on TPE is implemented and the best combination of hyperparameters is determined (learning rate 0.001, iterative epochs 600, and batch size 100 yields the highest accuracy at 0.987). Finally, a online FD deployment based on edge device, Jetson Nano, is developed and the **real time** monitoring of aircraft is achieved. We believe that this method is instructive for addressing the FD problems in other similar fields.  
# Paper List
---
## cs.CV
---
**113** new papers in cs.CV:-) 
1. Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency. (arXiv:2206.08936v1 [eess.IV])
2. CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation. (arXiv:2206.08948v1 [cs.CV])
3. Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding. (arXiv:2206.08954v1 [cs.CV])
4. KitBit: A New AI Model for Solving Intelligence Tests and Numerical Series. (arXiv:2206.08965v1 [cs.AI])
5. MultiEarth 2022 -- The Champion Solution for the Matrix Completion Challenge via Multimodal Regression and Generation. (arXiv:2206.08970v1 [cs.CV])
6. BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla Handwritten Text Recognition (HTR) and Line Segmentation. (arXiv:2206.08977v1 [cs.CV])
7. Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness. (arXiv:2206.08984v1 [eess.IV])
8. TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation. (arXiv:2206.08985v1 [eess.IV])
9. Shadows Shed Light on 3D Objects. (arXiv:2206.08990v1 [cs.CV])
10. Robust Group Synchronization via Quadratic Programming. (arXiv:2206.08994v1 [stat.ML])
11. Diffusion models as plug-and-play priors. (arXiv:2206.09012v1 [cs.LG])
12. Landscape Learning for Neural Network Inversion. (arXiv:2206.09027v1 [cs.CV])
13. Stop Overcomplicating Selective Classification: Use Max-Logit. (arXiv:2206.09034v1 [cs.LG])
14. Validation of Vector Data using Oblique Images. (arXiv:2206.09038v1 [cs.CV])
15. Augmented Imagefication: A Data-driven Fault Detection Method for Aircraft Air Data Sensors. (arXiv:2206.09055v1 [cs.CV])
16. CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks. (arXiv:2206.09059v1 [cs.CL])
17. Design of Supervision-Scalable Learning Systems: Methodology and Performance Benchmarking. (arXiv:2206.09061v1 [cs.CV])
18. Free-form Lesion Synthesis Using a Partial Convolution Generative Adversarial Network for Enhanced Deep Learning Liver Tumor Segmentation. (arXiv:2206.09065v1 [eess.IV])
19. Attention-based Dynamic Subspace Learners for Medical Image Analysis. (arXiv:2206.09068v1 [cs.CV])
20. SiamVGG: Visual Tracking using Deeper Siamese Networks. (arXiv:1902.02804v3 [cs.CV] UPDATED)
21. HERS: Homomorphically Encrypted Representation Search. (arXiv:2003.12197v3 [cs.CV] UPDATED)
22. SS-IL: Separated Softmax for Incremental Learning. (arXiv:2003.13947v3 [cs.CV] UPDATED)
23. Guided interactive image segmentation using machine learning and color based data set clustering. (arXiv:2005.07662v5 [cs.CV] UPDATED)
24. Joint Frequency and Image Space Learning for MRI Reconstruction and Analysis. (arXiv:2007.01441v4 [cs.CV] UPDATED)
25. Estimating Example Difficulty Using Variance of Gradients. (arXiv:2008.11600v4 [cs.CV] UPDATED)
26. Human Action Recognition from Various Data Modalities: A Review. (arXiv:2012.11866v5 [cs.CV] UPDATED)
27. Warping of Radar Data into Camera Image for Cross-Modal Supervision in Automotive Applications. (arXiv:2012.12809v2 [cs.CV] UPDATED)
28. Hyperspectral Image Denoising via Multi-modal and Double-weighted Tensor Nuclear Norm. (arXiv:2101.07681v3 [cs.CV] UPDATED)
29. Towards Continual, Online, Self-Supervised Depth. (arXiv:2103.00369v3 [cs.CV] UPDATED)
30. GANav: Efficient Terrain Segmentation for Robot Navigation in Unstructured Outdoor Environments. (arXiv:2103.04233v5 [cs.RO] UPDATED)
31. Highly Efficient Representation and Active Learning Framework and Its Application to Imbalanced Medical Image Classification. (arXiv:2103.05109v4 [cs.CV] UPDATED)
32. Multi-Person Extreme Motion Prediction. (arXiv:2105.08825v7 [cs.CV] UPDATED)
33. Scaling Vision Transformers. (arXiv:2106.04560v2 [cs.CV] UPDATED)
34. Knowledge distillation: A good teacher is patient and consistent. (arXiv:2106.05237v2 [cs.CV] UPDATED)
35. Physion: Evaluating Physical Prediction from Vision in Humans and Machines. (arXiv:2106.08261v3 [cs.AI] UPDATED)
36. Region-Aware Network: Model Human's Top-Down Visual Perception Mechanism for Crowd Counting. (arXiv:2106.12163v2 [cs.CV] UPDATED)
37. Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising. (arXiv:2106.12489v3 [eess.IV] UPDATED)
38. OPA: Object Placement Assessment Dataset. (arXiv:2107.01889v3 [cs.CV] UPDATED)
39. Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v3 [eess.IV] UPDATED)
40. Deep Anomaly Discovery From Unlabeled Videos via Normality Advantage and Self-Paced Refinement. (arXiv:2108.01975v3 [cs.CV] UPDATED)
41. Localized Shape Modelling with Global Coherence: An Inverse Spectral Approach. (arXiv:2108.02161v4 [cs.CV] UPDATED)
42. Data Acquisition and Preparation for Dual-reference Deep Learning of Image Super-Resolution. (arXiv:2108.02348v5 [eess.IV] UPDATED)
43. Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v3 [eess.IV] UPDATED)
44. Rapid Automated Analysis of Skull Base Tumor Specimens Using Intraoperative Optical Imaging and Artificial Intelligence. (arXiv:2108.03555v2 [cs.CV] UPDATED)
45. Ensemble CNN and Uncertainty Modeling to Improve Automatic Identification/Segmentation of Multiple Sclerosis Lesions in Magnetic Resonance Imaging. (arXiv:2108.11791v3 [eess.IV] UPDATED)
46. OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. (arXiv:2109.07644v5 [cs.CV] UPDATED)
47. The Hilti SLAM Challenge Dataset. (arXiv:2109.11316v3 [cs.RO] UPDATED)
48. MD Loss: Efficient Training of 3D Seismic Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v6 [cs.CV] UPDATED)
49. MixFace: Improving Face Verification Focusing on Fine-grained Conditions. (arXiv:2111.01717v3 [cs.CV] UPDATED)
50. TorchGeo: Deep Learning With Geospatial Data. (arXiv:2111.08872v3 [cs.CV] UPDATED)
51. Evaluating Self and Semi-Supervised Methods for Remote Sensing Segmentation Tasks. (arXiv:2111.10079v2 [cs.CV] UPDATED)
52. Exploring Segment-level Semantics for Online Phase Recognition from Surgical Videos. (arXiv:2111.11044v3 [cs.CV] UPDATED)
53. Self-Distilled Self-Supervised Representation Learning. (arXiv:2111.12958v2 [cs.CV] UPDATED)
54. **Swin**BERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v4 [cs.CV] UPDATED)
55. HEAT: Holistic Edge Attention Transformer for Structured Reconstruction. (arXiv:2111.15143v3 [cs.CV] UPDATED)
56. Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition. (arXiv:2111.15361v2 [cs.CV] UPDATED)
57. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v3 [cs.CV] UPDATED)
58. Pure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images. (arXiv:2112.08810v2 [cs.CV] UPDATED)
59. Constrained Gradient Descent: A Powerful and Principled Evasion Attack Against Neural Networks. (arXiv:2112.14232v2 [cs.LG] UPDATED)
60. Generalized Category Discovery. (arXiv:2201.02609v2 [cs.CV] UPDATED)
61. DynaMixer: A Vision MLP Architecture with Dynamic Mixing. (arXiv:2201.12083v3 [cs.CV] UPDATED)
62. Proximal Denoiser for Convergent Plug-and-Play Optimization with Nonconvex Regularization. (arXiv:2201.13256v4 [math.OC] UPDATED)
63. Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification. (arXiv:2202.00580v2 [cs.LG] UPDATED)
64. Make Some Noise: Reliable and Efficient Single-Step Adversarial Training. (arXiv:2202.01181v2 [cs.LG] UPDATED)
65. Beyond Images: Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features. (arXiv:2202.01273v2 [cs.LG] UPDATED)
66. Automatic defect segmentation by unsupervised anomaly learning. (arXiv:2202.02998v2 [cs.CV] UPDATED)
67. Consistency-Regularized Region-Growing Network for Semantic Segmentation of Urban Scenes with Point-Level Annotations. (arXiv:2202.03740v2 [cs.CV] UPDATED)
68. NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v4 [cs.CV] UPDATED)
69. VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v3 [cs.CV] UPDATED)
70. PointMatch: A Consistency Training Framework for Weakly Supervised Semantic Segmentation of 3D Point Clouds. (arXiv:2202.10705v2 [cs.CV] UPDATED)
71. Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v4 [cs.CV] UPDATED)
72. Self-Supervised Scene Flow Estimation with 4-D Automotive Radar. (arXiv:2203.01137v3 [cs.CV] UPDATED)
73. NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields. (arXiv:2203.01762v2 [cs.LG] UPDATED)
74. Object-centric and memory-guided normality reconstruction for video anomaly detection. (arXiv:2203.03677v2 [cs.CV] UPDATED)
75. Audio-Visual MLP for Scoring Sport. (arXiv:2203.03990v3 [cs.CV] UPDATED)
76. NaviAirway: a Bronchiole-sensitive Deep Learning-based Airway Segmentation Pipeline. (arXiv:2203.04294v2 [eess.IV] UPDATED)
77. The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks. (arXiv:2203.04466v3 [cs.LG] UPDATED)
78. Dynamic Instance Domain Adaptation. (arXiv:2203.05028v2 [cs.CV] UPDATED)
79. Supervised segmentation of NO2 plumes from individual ships using TROPOMI satellite data. (arXiv:2203.06993v2 [cs.CV] UPDATED)
80. Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v3 [cs.CV] UPDATED)
81. SepTr: Separable Transformer for Audio Spectrogram Processing. (arXiv:2203.09581v3 [cs.CV] UPDATED)
82. GriTS: Grid table similarity metric for table structure recognition. (arXiv:2203.12555v2 [cs.LG] UPDATED)
83. Style-Guided Domain Adaptation for Face Presentation Attack Detection. (arXiv:2203.14565v2 [cs.CV] UPDATED)
84. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v3 [cs.CV] UPDATED)
85. On the Generalization of BasicVSR++ to Video Deblurring and Denoising. (arXiv:2204.05308v2 [cs.CV] UPDATED)
86. ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v3 [cs.CV] UPDATED)
87. Video Moment Retrieval from Text Queries via Single Frame Annotation. (arXiv:2204.09409v3 [cs.CV] UPDATED)
88. SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v4 [cs.CV] UPDATED)
89. Self-Supervised Learning of Object Parts for Semantic Segmentation. (arXiv:2204.13101v2 [cs.CV] UPDATED)
90. Domino Saliency Metrics: Improving Existing Channel Saliency Metrics with Structural Information. (arXiv:2205.02131v2 [cs.CV] UPDATED)
91. ReFine: Re-randomization before Fine-tuning for Cross-domain Few-shot Learning. (arXiv:2205.05282v2 [cs.CV] UPDATED)
92. GR-GAN: Gradual Refinement Text-to-image Generation. (arXiv:2205.11273v2 [cs.CV] UPDATED)
93. Learning to Assemble Geometric Shapes. (arXiv:2205.11809v2 [cs.CV] UPDATED)
94. Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction. (arXiv:2206.00790v2 [cs.CV] UPDATED)
95. MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation. (arXiv:2206.01737v2 [eess.IV] UPDATED)
96. NORPPA: NOvel Ringed seal re-identification by Pelage Pattern Aggregation. (arXiv:2206.02498v3 [cs.CV] UPDATED)
97. SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v2 [cs.CV] UPDATED)
98. Patch-based Object-centric Transformers for Efficient Video Generation. (arXiv:2206.04003v2 [cs.CV] UPDATED)
99. A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal. (arXiv:2206.05520v2 [cs.CV] UPDATED)
100. TileGen: Tileable, Controllable Material Generation and Capture. (arXiv:2206.05649v2 [cs.GR] UPDATED)
101. DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement. (arXiv:2206.05687v2 [cs.HC] UPDATED)
102. Featurized Query R-CNN. (arXiv:2206.06258v3 [cs.CV] UPDATED)
103. Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images. (arXiv:2206.06665v2 [cs.CV] UPDATED)
104. CNN-based Classification Framework for Lung Tissues with Auxiliary Information. (arXiv:2206.06701v2 [eess.IV] UPDATED)
105. Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal. (arXiv:2206.06803v2 [cs.CV] UPDATED)
106. AnimeSR: Learning Real-World Super-Resolution Models for Animation Videos. (arXiv:2206.07038v2 [cs.CV] UPDATED)
107. Technical Report for Argoverse2 Challenge 2022 -- Motion Forecasting Task. (arXiv:2206.07934v2 [cs.CV] UPDATED)
108. Channel Importance Matters in Few-Shot Image Classification. (arXiv:2206.08126v2 [cs.CV] UPDATED)
109. FWD: **Real-time** Novel View Synthesis with Forward Warping and Depth. (arXiv:2206.08355v2 [cs.CV] UPDATED)
110. Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces. (arXiv:2206.08362v2 [cs.CV] UPDATED)
111. Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v2 [cs.CV] UPDATED)
112. TAVA: Template-free Animatable Volumetric Actors. (arXiv:2206.08929v2 [cs.CV] UPDATED)
113. A machine-generated catalogue of Charon's craters and implications for the Kuiper belt. (arXiv:2206.08277v1 [astro-ph.EP] CROSS LISTED)
## eess.IV
---
**54** new papers in eess.IV:-) 
1. Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency. (arXiv:2206.08936v1 [eess.IV])
2. Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness. (arXiv:2206.08984v1 [eess.IV])
3. TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation. (arXiv:2206.08985v1 [eess.IV])
4. Free-form Lesion Synthesis Using a Partial Convolution Generative Adversarial Network for Enhanced Deep Learning Liver Tumor Segmentation. (arXiv:2206.09065v1 [eess.IV])
5. Analysis & Computational Complexity Reduction of Monocular and Stereo Depth Estimation Techniques. (arXiv:2206.09071v1 [cs.CV])
6. A Combined PCA-MLP Network for Early Breast Cancer Detection. (arXiv:2206.09128v1 [eess.IV])
7. Perceptual Optimization of a Biologically-Inspired Tone Mapping Operator. (arXiv:2206.09146v1 [eess.IV])
8. Multi-Modality Image Super-Resolution using Generative Adversarial Networks. (arXiv:2206.09193v1 [eess.IV])
9. Multi-Modality Image Inpainting using Generative Adversarial Networks. (arXiv:2206.09210v1 [eess.IV])
10. Structured Light with Redundancy Codes. (arXiv:2206.09243v1 [cs.CV])
11. TBraTS: Trusted Brain Tumor Segmentation. (arXiv:2206.09309v1 [eess.IV])
12. JPEG Compression-Resistant Low-Mid Adversarial Perturbation against Unauthorized Face Recognition System. (arXiv:2206.09410v1 [cs.CV])
13. Extended field-of-view speckle-correlation imaging by estimating autocorrelation. (arXiv:2206.09417v1 [physics.optics])
14. QuDASH: Quantum-inspired rate adaptation approach for DASH video streaming. (arXiv:2206.09427v1 [cs.NI])
15. StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v1 [cs.CV])
16. SJ-HD^2R: Selective Joint **High Dynamic Range** and Denoising Imaging for Dynamic Scenes. (arXiv:2206.09611v1 [eess.IV])
17. Semantic Labeling of High Resolution Images Using EfficientUNets and Transformers. (arXiv:2206.09731v1 [cs.CV])
18. Geo-NI: Geometry-aware Neural Interpolation for Light Field Rendering. (arXiv:2206.09736v1 [cs.CV])
19. Time Gated Convolutional Neural Networks for Crop Classification. (arXiv:2206.09756v1 [cs.CV])
20. Quantitative CT texture-based method to predict diagnosis and prognosis of fibrosing interstitial lung disease patterns. (arXiv:2206.09766v1 [eess.IV])
21. On the benefit of parameter-driven approaches for the modeling and the prediction of Satisfied User Ratio for compressed video. (arXiv:2206.09854v1 [eess.IV])
22. Optimally Controllable Perceptual Lossy Compression. (arXiv:2206.10082v1 [cs.CV])
23. Memory-Efficient Learned Image Compression with Pruned Hyperprior Module. (arXiv:2206.10083v1 [eess.IV])
24. Transformers Improve Breast Cancer Diagnosis from Unregistered Multi-View Mammograms. (arXiv:2206.10096v1 [cs.CV])
25. DECAL: DEployable Clinical Active Learning. (arXiv:2206.10120v1 [eess.IV])
26. Fast image reverse filters through fixed point and gradient descent acceleration. (arXiv:2206.10124v1 [eess.IV])
27. An Integrated Representation & Compression Scheme Based on Convolutional Autoencoders with 4D DCT Perceptual Encoding for **High Dynamic Range** Light Fields. (arXiv:2206.10131v1 [cs.CV])
28. covEcho Resource constrained lung ultrasound image analysis tool for faster triaging and active learning. (arXiv:2206.10183v1 [eess.IV])
29. Position-prior Clustering-based Self-attention Module for Knee Cartilage Segmentation. (arXiv:2206.10286v1 [eess.IV])
30. Using the Polar Transform for Efficient Deep Learning-Based Aorta Segmentation in CTA Images. (arXiv:2206.10294v1 [eess.IV])
31. Confidence-Guided Unsupervised Domain Adaptation for Cerebellum Segmentation. (arXiv:2206.10357v1 [eess.IV])
32. Approximate Equivariance SO(3) Needlet Convolution. (arXiv:2206.10385v1 [eess.IV])
33. Automated Coronary Calcium Scoring using U-Net Models through Semi-supervised Learning on Non-Gated CT Scans. (arXiv:2206.10455v1 [eess.IV])
34. Faster Diffusion Cardiac MRI with Deep Learning-based breath hold reduction. (arXiv:2206.10543v1 [eess.IV])
35. Guided interactive image segmentation using machine learning and color based data set clustering. (arXiv:2005.07662v5 [cs.CV] UPDATED)
36. Joint Frequency and Image Space Learning for MRI Reconstruction and Analysis. (arXiv:2007.01441v4 [cs.CV] UPDATED)
37. Towards Continual, Online, Self-Supervised Depth. (arXiv:2103.00369v3 [cs.CV] UPDATED)
38. Highly Efficient Representation and Active Learning Framework and Its Application to Imbalanced Medical Image Classification. (arXiv:2103.05109v4 [cs.CV] UPDATED)
39. Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising. (arXiv:2106.12489v3 [eess.IV] UPDATED)
40. Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v3 [eess.IV] UPDATED)
41. Data Acquisition and Preparation for Dual-reference Deep Learning of Image Super-Resolution. (arXiv:2108.02348v5 [eess.IV] UPDATED)
42. Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v3 [eess.IV] UPDATED)
43. Ensemble CNN and Uncertainty Modeling to Improve Automatic Identification/Segmentation of Multiple Sclerosis Lesions in Magnetic Resonance Imaging. (arXiv:2108.11791v3 [eess.IV] UPDATED)
44. MD Loss: Efficient Training of 3D Seismic Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v6 [cs.CV] UPDATED)
45. NaviAirway: a Bronchiole-sensitive Deep Learning-based Airway Segmentation Pipeline. (arXiv:2203.04294v2 [eess.IV] UPDATED)
46. Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing. (arXiv:2204.07204v2 [eess.IV] UPDATED)
47. Fluorescent wavefront shaping using incoherent iterative phase conjugation. (arXiv:2205.06215v3 [physics.optics] UPDATED)
48. MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation. (arXiv:2206.01737v2 [eess.IV] UPDATED)
49. SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v2 [cs.CV] UPDATED)
50. A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal. (arXiv:2206.05520v2 [cs.CV] UPDATED)
51. DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement. (arXiv:2206.05687v2 [cs.HC] UPDATED)
52. CNN-based Classification Framework for Lung Tissues with Auxiliary Information. (arXiv:2206.06701v2 [eess.IV] UPDATED)
53. Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal. (arXiv:2206.06803v2 [cs.CV] UPDATED)
54. An incomplete taxonomy of self-assigned color specialties. (arXiv:2206.08779v2 [eess.IV] UPDATED)
## cs.LG
---
**268** new papers in cs.LG:-) 
1. CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowdsourced Dataset Annotation. (arXiv:2206.08931v1 [cs.HC])
2. A theory of learning with constrained weight-distribution. (arXiv:2206.08933v1 [q-bio.NC])
3. Photoelectric Factor Prediction Using Automated Learning and Uncertainty Quantification. (arXiv:2206.08950v1 [cs.LG])
4. The Impact of Variable Ordering on Bayesian Network Structure Learning. (arXiv:2206.08952v1 [cs.LG])
5. Bayesian neural networks for the probabilistic forecasting of wind direction and speed using ocean data. (arXiv:2206.08953v1 [physics.ao-ph])
6. Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding. (arXiv:2206.08954v1 [cs.CV])
7. Design of Multi-model Linear Inferential Sensors with SVM-based Switching Logic. (arXiv:2206.08961v1 [cs.LG])
8. Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks. (arXiv:2206.08966v1 [cs.CY])
9. Random Forest of Epidemiological Models for Influenza Forecasting. (arXiv:2206.08967v1 [cs.LG])
10. Shallow and Deep Nonparametric Convolutions for Gaussian Processes. (arXiv:2206.08972v1 [stat.ML])
11. DPDR: A novel machine learning method for the Decision Process for Dimensionality Reduction. (arXiv:2206.08974v1 [cs.LG])
12. Explainable Global Error Weighted on Feature Importance: The xGEWFI metric to evaluate the error of data imputation and data augmentation. (arXiv:2206.08980v1 [cs.LG])
13. ck-means, a novel unsupervised learning method that combines fuzzy and crispy clustering methods to extract intersecting data. (arXiv:2206.08982v1 [cs.LG])
14. Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness. (arXiv:2206.08984v1 [eess.IV])
15. Robust Group Synchronization via Quadratic Programming. (arXiv:2206.08994v1 [stat.ML])
16. A review of machine learning concepts and methods for addressing challenges in probabilistic hydrological post-processing and forecasting. (arXiv:2206.08998v1 [cs.LG])
17. Cluster Generation via Deep Energy-Based Model. (arXiv:2206.09002v1 [cond-mat.mtrl-sci])
18. Towards Efficient Active Learning of PDFA. (arXiv:2206.09004v1 [cs.FL])
19. LIMO: Latent Inceptionism for Targeted Molecule Generation. (arXiv:2206.09010v1 [cs.LG])
20. Diffusion models as plug-and-play priors. (arXiv:2206.09012v1 [cs.LG])
21. Path-Gradient Estimators for Continuous Normalizing Flows. (arXiv:2206.09016v1 [cs.LG])
22. Conditional Permutation Invariant Flows. (arXiv:2206.09021v1 [stat.ML])
23. Designing MacPherson Suspension Architectures using Bayesian Optimization. (arXiv:2206.09022v1 [cs.LG])
24. Landscape Learning for Neural Network Inversion. (arXiv:2206.09027v1 [cs.CV])
25. Binary Early-Exit Network for Adaptive Inference on Low-Resource Devices. (arXiv:2206.09029v1 [cs.LG])
26. Stop Overcomplicating Selective Classification: Use Max-Logit. (arXiv:2206.09034v1 [cs.LG])
27. Validation of Vector Data using Oblique Images. (arXiv:2206.09038v1 [cs.CV])
28. Energy reconstruction for large liquid scintillator detectors with machine learning techniques: aggregated features approach. (arXiv:2206.09040v1 [physics.ins-det])
29. Accelerating Machine Learning Training Time for Limit Order Book Prediction. (arXiv:2206.09041v1 [q-fin.TR])
30. Riemannian CUR Decompositions for Robust Principal Component Analysis. (arXiv:2206.09042v1 [stat.ML])
31. Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis. (arXiv:2206.09046v1 [cs.LG])
32. Learning the parameters of a differential equation from its trajectory via the adjoint equation. (arXiv:2206.09054v1 [cs.LG])
33. NASTAR: Noise Adaptive Speech **Enhancement** with Target-Conditional Resampling. (arXiv:2206.09058v1 [eess.AS])
34. CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks. (arXiv:2206.09059v1 [cs.CL])
35. Information-Theoretic Representation Learning for Positive-Unlabeled Classification. (arXiv:1710.05359v4 [stat.ML] UPDATED)
36. A General Framework for Abstention Under Label Shift. (arXiv:1802.07024v5 [stat.ML] UPDATED)
37. Thompson Sampling for Combinatorial Semi-Bandits. (arXiv:1803.04623v5 [cs.LG] UPDATED)
38. Large Scale Clustering with Variational EM for Gaussian Mixture Models. (arXiv:1810.00803v4 [stat.ML] UPDATED)
39. A Degeneracy Framework for Scalable Graph Autoencoders. (arXiv:1902.08813v3 [cs.LG] UPDATED)
40. Global Optimality Guarantees For Policy Gradient Methods. (arXiv:1906.01786v3 [cs.LG] UPDATED)
41. Towards Making the Most of BERT in Neural Machine Translation. (arXiv:1908.05672v5 [cs.CL] UPDATED)
42. Higher Criticism for Discriminating Word-Frequency Tables and Testing Authorship. (arXiv:1911.01208v5 [cs.CL] UPDATED)
43. TimeCaps: Capturing Time Series Data With Capsule Networks. (arXiv:1911.11800v4 [cs.LG] UPDATED)
44. Linear Convergence of Frank-Wolfe for Rank-One Matrix Recovery Without Strong Convexity. (arXiv:1912.01467v2 [math.OC] UPDATED)
45. Multi Type Mean Field Reinforcement Learning. (arXiv:2002.02513v7 [cs.MA] UPDATED)
46. HERS: Homomorphically Encrypted Representation Search. (arXiv:2003.12197v3 [cs.CV] UPDATED)
47. Approximation in shift-invariant spaces with deep ReLU neural networks. (arXiv:2005.11949v3 [cs.LG] UPDATED)
48. Hessian-Free High-Resolution Nesterov Acceleration for Sampling. (arXiv:2006.09230v4 [cs.LG] UPDATED)
49. On Multivariate Singular Spectrum Analysis and its Variants. (arXiv:2006.13448v5 [cs.LG] UPDATED)
50. Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees. (arXiv:2006.14591v4 [cs.LG] UPDATED)
51. Logarithmic regret for episodic continuous-time linear-quadratic reinforcement learning over a finite-time horizon. (arXiv:2006.15316v4 [math.OC] UPDATED)
52. Joint Frequency and Image Space Learning for MRI Reconstruction and Analysis. (arXiv:2007.01441v4 [cs.CV] UPDATED)
53. Estimating Example Difficulty Using Variance of Gradients. (arXiv:2008.11600v4 [cs.CV] UPDATED)
54. Optimal training of integer-valued neural networks with mixed integer programming. (arXiv:2009.03825v4 [cs.LG] UPDATED)
55. A Generic Methodology for the Statistically Uniform & Comparable Evaluation of Automated Trading Platform Components. (arXiv:2009.09993v4 [q-fin.TR] UPDATED)
56. Tight Lower Complexity Bounds for Strongly Convex Finite-Sum Optimization. (arXiv:2010.08766v2 [cs.LG] UPDATED)
57. Thresholded Lasso Bandit. (arXiv:2010.11994v4 [stat.ML] UPDATED)
58. Multi-Agent Reinforcement Learning for Channel Assignment and Power Allocation in Platoon-Based C-V2X Systems. (arXiv:2011.04555v2 [eess.SP] UPDATED)
59. Efficient and Transferable Adversarial Examples from Bayesian Neural Networks. (arXiv:2011.05074v4 [cs.LG] UPDATED)
60. Combating the Instability of Mutual Information-based Losses via Regularization. (arXiv:2011.07932v4 [cs.LG] UPDATED)
61. Optimal Algorithms for Convex Nested Stochastic Composite Optimization. (arXiv:2011.10076v5 [math.OC] UPDATED)
62. Combinatorial Bayesian Optimization with Random Mapping Functions to Convex Polytopes. (arXiv:2011.13094v2 [stat.ML] UPDATED)
63. A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit. (arXiv:2011.14033v4 [cs.LG] UPDATED)
64. Stability of Finite Horizon Optimisation based Control without Terminal Weight. (arXiv:2011.14193v3 [eess.SY] UPDATED)
65. Fairness-Oriented User Scheduling for Bursty Downlink Transmission Using Multi-Agent Reinforcement Learning. (arXiv:2012.15081v14 [cs.OS] UPDATED)
66. Unsupervised Imputation of Non-ignorably Missing Data Using Importance-Weighted Autoencoders. (arXiv:2101.07357v3 [cs.LG] UPDATED)
67. Weight Rescaling: Effective and Robust Regularization for Deep Neural Networks with Batch Normalization. (arXiv:2102.03497v2 [cs.LG] UPDATED)
68. Domain Adaptation for Time Series Forecasting via Attention Sharing. (arXiv:2102.06828v8 [cs.LG] UPDATED)
69. Evaluating Node Embeddings of Complex Networks. (arXiv:2102.08275v2 [cs.SI] UPDATED)
70. A Simple Unified Framework for High Dimensional Bandit Problems. (arXiv:2102.09626v3 [cs.LG] UPDATED)
71. Significance tests of feature relevance for a black-box learner. (arXiv:2103.04985v3 [stat.ML] UPDATED)
72. Highly Efficient Representation and Active Learning Framework and Its Application to Imbalanced Medical Image Classification. (arXiv:2103.05109v4 [cs.CV] UPDATED)
73. Sample-based and Feature-based Federated Learning for Unconstrained and Constrained Nonconvex Optimization via Mini-batch SSCA. (arXiv:2104.06011v3 [cs.LG] UPDATED)
74. Rethinking Image-Scaling Attacks: The Interplay Between Vulnerabilities in Machine Learning Systems. (arXiv:2104.08690v3 [cs.LG] UPDATED)
75. Multi-fairness under class-imbalance. (arXiv:2104.13312v3 [cs.LG] UPDATED)
76. Data Augmentation in High Dimensional Low Sample Size Setting Using a Geometry-Based Variational Autoencoder. (arXiv:2105.00026v2 [stat.ML] UPDATED)
77. Discrete-time Contraction-based Control of Nonlinear Systems with Parametric Uncertainties using Neural Networks. (arXiv:2105.05432v3 [eess.SY] UPDATED)
78. FedScale: Benchmarking Model and System Performance of Federated Learning at Scale. (arXiv:2105.11367v5 [cs.LG] UPDATED)
79. Estimating Instance-dependent Bayes-label Transition Matrix using a Deep Neural Network. (arXiv:2105.13001v2 [cs.LG] UPDATED)
80. Quantum Perceptron Revisited: Computational-Statistical Tradeoffs. (arXiv:2106.02496v3 [quant-ph] UPDATED)
81. Scaling Vision Transformers. (arXiv:2106.04560v2 [cs.CV] UPDATED)
82. Expectation Programming: Adapting Probabilistic Programming Systems to Estimate Expectations Efficiently. (arXiv:2106.04953v2 [cs.LG] UPDATED)
83. Knowledge distillation: A good teacher is patient and consistent. (arXiv:2106.05237v2 [cs.CV] UPDATED)
84. Identifiability of interaction kernels in mean-field equations of interacting particles. (arXiv:2106.05565v3 [stat.ML] UPDATED)
85. Metric Policy Representations for Opponent Modeling. (arXiv:2106.05802v2 [cs.LG] UPDATED)
86. Markov Decision Processes with Long-Term Average Constraints. (arXiv:2106.06680v2 [cs.LG] UPDATED)
87. Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness. (arXiv:2106.08161v3 [stat.ML] UPDATED)
88. Rayleigh-Gauss-Newton optimization with enhanced sampling for variational Monte Carlo. (arXiv:2106.10558v3 [stat.ML] UPDATED)
89. Robust Distributed Optimization With Randomly Corrupted Gradients. (arXiv:2106.14956v2 [math.OC] UPDATED)
90. The Values Encoded in Machine Learning Research. (arXiv:2106.15590v2 [cs.LG] UPDATED)
91. Preconditioning for Scalable Gaussian Process Hyperparameter Optimization. (arXiv:2107.00243v5 [cs.LG] UPDATED)
92. Supervised Off-Policy Ranking. (arXiv:2107.01360v2 [cs.LG] UPDATED)
93. Model Transferability With Responsive Decision Subjects. (arXiv:2107.05911v3 [cs.LG] UPDATED)
94. Fairer Machine Learning Software on Multiple Sensitive Attributes With Data Preprocessing. (arXiv:2107.08310v3 [cs.LG] UPDATED)
95. Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v3 [eess.IV] UPDATED)
96. Towards Data-Driven Synthesis of Autonomous Vehicle Safety Concepts. (arXiv:2107.14412v2 [cs.RO] UPDATED)
97. Model-Based Opponent Modeling. (arXiv:2108.01843v2 [cs.LG] UPDATED)
98. Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v3 [eess.IV] UPDATED)
99. Rapid Automated Analysis of Skull Base Tumor Specimens Using Intraoperative Optical Imaging and Artificial Intelligence. (arXiv:2108.03555v2 [cs.CV] UPDATED)
100. DQ-GAT: Towards Safe and Efficient Autonomous Driving with Deep Q-Learning and Graph Attention Networks. (arXiv:2108.05030v2 [cs.RO] UPDATED)
101. Cross-Silo Federated Learning for Multi-Tier Networks with Vertical and Horizontal Data Partitioning. (arXiv:2108.08930v3 [cs.LG] UPDATED)
102. Practical and Secure Federated Recommendation with Personalized Masks. (arXiv:2109.02464v2 [cs.IR] UPDATED)
103. Graph Neural Networks for Graph Drawing. (arXiv:2109.10061v2 [cs.LG] UPDATED)
104. Learning Generative Deception Strategies in Combinatorial Masking Games. (arXiv:2109.11637v2 [cs.GT] UPDATED)
105. A Graph Policy Network Approach for Volt-Var Control in Power Distribution Systems. (arXiv:2109.12073v2 [cs.LG] UPDATED)
106. LINDA: Multi-Agent Local Information Decomposition for Awareness of Teammates. (arXiv:2109.12508v3 [cs.LG] UPDATED)
107. Divergence-Regularized Multi-Agent Actor-Critic. (arXiv:2110.00304v2 [cs.LG] UPDATED)
108. 8-bit Optimizers via Block-wise Quantization. (arXiv:2110.02861v2 [cs.LG] UPDATED)
109. SCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition. (arXiv:2110.04187v3 [eess.AS] UPDATED)
110. Intriguing Properties of Input-dependent Randomized Smoothing. (arXiv:2110.05365v2 [cs.LG] UPDATED)
111. Action-Sufficient State Representation Learning for Control with Structural Constraints. (arXiv:2110.05721v2 [cs.LG] UPDATED)
112. Detecting Corrupted Labels Without Training a Model to Predict. (arXiv:2110.06283v3 [cs.LG] UPDATED)
113. More Efficient Sampling for Tensor Decomposition With Worst-Case Guarantees. (arXiv:2110.07631v2 [math.NA] UPDATED)
114. GradSign: Model Performance Inference with Theoretical Insights. (arXiv:2110.08616v2 [cs.LG] UPDATED)
115. Towards Noise-adaptive, Problem-adaptive (Accelerated) Stochastic Gradient Descent. (arXiv:2110.11442v3 [math.OC] UPDATED)
116. Using scientific machine learning for experimental bifurcation analysis of dynamic systems. (arXiv:2110.11854v3 [math.DS] UPDATED)
117. TOD: GPU-accelerated Outlier Detection via Tensor Operations. (arXiv:2110.14007v2 [cs.LG] UPDATED)
118. Scalable Bayesian Network Structure Learning with Splines. (arXiv:2110.14626v2 [cs.LG] UPDATED)
119. Self-supervised EEG Representation Learning for Automatic Sleep Staging. (arXiv:2110.15278v2 [eess.SP] UPDATED)
120. Faster Algorithms for Learning Convex Functions. (arXiv:2111.01348v4 [stat.ML] UPDATED)
121. DeepParticle: learning invariant measure by a deep neural network minimizing Wasserstein distance on data generated from an interacting particle method. (arXiv:2111.01356v3 [cs.LG] UPDATED)
122. TorchGeo: Deep Learning With Geospatial Data. (arXiv:2111.08872v3 [cs.CV] UPDATED)
123. Evaluating Self and Semi-Supervised Methods for Remote Sensing Segmentation Tasks. (arXiv:2111.10079v2 [cs.CV] UPDATED)
124. Controlling Conditional Language Models without Catastrophic Forgetting. (arXiv:2112.00791v2 [cs.LG] UPDATED)
125. First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach. (arXiv:2112.03432v3 [cs.LG] UPDATED)
126. Self-Organized Polynomial-Time Coordination Graphs. (arXiv:2112.03547v3 [cs.LG] UPDATED)
127. Modelling DDoS Attacks in IoT Networks using Machine Learning. (arXiv:2112.05477v2 [cs.DC] UPDATED)
128. A New Perspective on the Effects of Spectrum in Graph Neural Networks. (arXiv:2112.07160v2 [cs.LG] UPDATED)
129. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v3 [cs.CV] UPDATED)
130. Tree-based Focused Web Crawling with Reinforcement Learning. (arXiv:2112.07620v2 [cs.IR] UPDATED)
131. Learning to Share in Multi-Agent Reinforcement Learning. (arXiv:2112.08702v2 [cs.LG] UPDATED)
132. Discrete Probabilistic Inverse Optimal Transport. (arXiv:2112.09754v2 [stat.ML] UPDATED)
133. On Asymptotic Linear Convergence of Projected Gradient Descent for Constrained Least Squares. (arXiv:2112.11760v2 [math.OC] UPDATED)
134. Direct Behavior Specification via Constrained Reinforcement Learning. (arXiv:2112.12228v6 [cs.LG] UPDATED)
135. Black-Box Testing of Deep Neural Networks through Test Case Diversity. (arXiv:2112.12591v3 [cs.SE] UPDATED)
136. Constrained Gradient Descent: A Powerful and Principled Evasion Attack Against Neural Networks. (arXiv:2112.14232v2 [cs.LG] UPDATED)
137. Improving Out-of-Distribution Robustness via Selective Augmentation. (arXiv:2201.00299v3 [cs.LG] UPDATED)
138. Deep Reinforcement Learning, a textbook. (arXiv:2201.02135v4 [cs.AI] UPDATED)
139. Generalized Category Discovery. (arXiv:2201.02609v2 [cs.CV] UPDATED)
140. Black-box error diagnosis in deep neural networks for computer vision: a survey of tools. (arXiv:2201.06444v2 [cs.LG] UPDATED)
141. Encoding large information structures in linear algebra and statistical models. (arXiv:2201.08233v2 [cs.LG] UPDATED)
142. Bias in Automated Speaker Recognition. (arXiv:2201.09486v2 [cs.SD] UPDATED)
143. Analytic Mutual Information in Bayesian Neural Networks. (arXiv:2201.09815v3 [cs.IT] UPDATED)
144. Reward-Free RL is No Harder Than Reward-Aware RL in Linear Markov Decision Processes. (arXiv:2201.11206v2 [cs.LG] UPDATED)
145. Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks. (arXiv:2201.11729v3 [cs.LG] UPDATED)
146. DynaMixer: A Vision MLP Architecture with Dynamic Mixing. (arXiv:2201.12083v3 [cs.CV] UPDATED)
147. Approximate Bayesian Computation with Domain Expert in the Loop. (arXiv:2201.12090v2 [stat.ML] UPDATED)
148. Linear Adversarial Concept Erasure. (arXiv:2201.12091v2 [cs.LG] UPDATED)
149. A Simple Guard for Learned Optimizers. (arXiv:2201.12426v2 [cs.LG] UPDATED)
150. Investigating Why Contrastive Learning Benefits Robustness Against Label Noise. (arXiv:2201.12498v3 [cs.LG] UPDATED)
151. Why the Rich Get Richer? On the Balancedness of Random Partition Models. (arXiv:2201.12697v2 [stat.ML] UPDATED)
152. Lightweight Projective Derivative Codes for Compressed Asynchronous Gradient Descent. (arXiv:2201.12990v2 [cs.LG] UPDATED)
153. Optimal Estimation of Off-Policy Policy Gradient via Double Fitted Iteration. (arXiv:2202.00076v3 [stat.ML] UPDATED)
154. Machine learning to assess relatedness: the advantage of using firm-level data. (arXiv:2202.00458v3 [cs.LG] UPDATED)
155. Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification. (arXiv:2202.00580v2 [cs.LG] UPDATED)
156. Gradient Based Clustering. (arXiv:2202.00720v3 [cs.LG] UPDATED)
157. Modularity-Aware Graph Autoencoders for Joint Community Detection and Link Prediction. (arXiv:2202.00961v2 [cs.LG] UPDATED)
158. Make Some Noise: Reliable and Efficient Single-Step Adversarial Training. (arXiv:2202.01181v2 [cs.LG] UPDATED)
159. Beyond Images: Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features. (arXiv:2202.01273v2 [cs.LG] UPDATED)
160. ETSformer: Exponential Smoothing Transformers for Time-series Forecasting. (arXiv:2202.01381v2 [cs.LG] UPDATED)
161. Generalizing to New Physical Systems via Context-Informed Dynamics Model. (arXiv:2202.01889v2 [cs.LG] UPDATED)
162. Deep End-to-end Causal Inference. (arXiv:2202.02195v2 [stat.ML] UPDATED)
163. Versatile Offline Imitation from Observations and Examples via Regularized State-Occupancy Matching. (arXiv:2202.02433v2 [cs.LG] UPDATED)
164. Estimating the Euclidean quantum propagator with deep generative modeling of Feynman paths. (arXiv:2202.02750v2 [quant-ph] UPDATED)
165. Diversify and Disambiguate: Learning From Underspecified Data. (arXiv:2202.03418v2 [cs.LG] UPDATED)
166. Calibrated Learning to Defer with One-vs-All Classifiers. (arXiv:2202.03673v2 [cs.LG] UPDATED)
167. Particle Transformer for Jet Tagging. (arXiv:2202.03772v2 [hep-ph] UPDATED)
168. Improved Convergence Rates for Sparse Approximation Methods in Kernel-Based Learning. (arXiv:2202.04005v2 [cs.LG] UPDATED)
169. Generalized Strategic Classification and the Case of Aligned Incentives. (arXiv:2202.04357v2 [cs.LG] UPDATED)
170. New Projection-free Algorithms for Online Convex Optimization with Adaptive Regret Guarantees. (arXiv:2202.04721v2 [cs.LG] UPDATED)
171. REvolveR: Continuous Evolutionary Models for Robot-to-robot Policy Transfer. (arXiv:2202.05244v2 [cs.LG] UPDATED)
172. Invariance Principle Meets Out-of-Distribution Generalization on Graphs. (arXiv:2202.05441v2 [cs.LG] UPDATED)
173. A Differential Entropy Estimator for Training Neural Networks. (arXiv:2202.06618v2 [cs.LG] UPDATED)
174. TURF: A Two-factor, Universal, Robust, Fast Distribution Learning Algorithm. (arXiv:2202.07172v2 [stat.ML] UPDATED)
175. On a Variance-Reduction Correction for the Temporal-Difference Learning in the Stochastic Continuous Setting. (arXiv:2202.07960v2 [cs.LG] UPDATED)
176. Augment with Care: Contrastive Learning for Combinatorial Problems. (arXiv:2202.08396v2 [cs.LG] UPDATED)
177. What Functions Can Graph Neural Networks Generate?. (arXiv:2202.08833v2 [cs.LG] UPDATED)
178. Signal Decomposition Using Masked Proximal Operators. (arXiv:2202.09338v5 [cs.LG] UPDATED)
179. Knowledge Base Question Answering by Case-based Reasoning over Subgraphs. (arXiv:2202.10610v2 [cs.CL] UPDATED)
180. Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications. (arXiv:2202.12396v5 [math.OC] UPDATED)
181. Learning Multi-Task Gaussian Process Over Heterogeneous Input Domains. (arXiv:2202.12636v3 [stat.ML] UPDATED)
182. Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates. (arXiv:2202.12967v2 [cs.LG] UPDATED)
183. Variational Inference with Gaussian Mixture by Entropy Approximation. (arXiv:2202.13059v2 [stat.ML] UPDATED)
184. Safe Exploration for Efficient Policy Evaluation and Comparison. (arXiv:2202.13234v2 [cs.LG] UPDATED)
185. Resolving label uncertainty with implicit posterior models. (arXiv:2202.14000v2 [cs.LG] UPDATED)
186. Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks. (arXiv:2203.00199v4 [cs.LG] UPDATED)
187. Multi-task Learning Approach for Modulation and Wireless Signal Classification for 5G and Beyond: Edge Deployment via Model Compression. (arXiv:2203.00517v2 [eess.SP] UPDATED)
188. Partial Likelihood Thompson Sampling. (arXiv:2203.00820v2 [stat.ME] UPDATED)
189. Topological data analysis of truncated contagion maps. (arXiv:2203.01720v2 [math.AT] UPDATED)
190. NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields. (arXiv:2203.01762v2 [cs.LG] UPDATED)
191. AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators. (arXiv:2203.03673v3 [stat.ML] UPDATED)
192. The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks. (arXiv:2203.04466v3 [cs.LG] UPDATED)
193. ReVar: Strengthening Policy Evaluation via Reduced Variance Sampling. (arXiv:2203.04510v3 [cs.LG] UPDATED)
194. Supervised segmentation of NO2 plumes from individual ships using TROPOMI satellite data. (arXiv:2203.06993v2 [cs.CV] UPDATED)
195. SepTr: Separable Transformer for Audio Spectrogram Processing. (arXiv:2203.09581v3 [cs.CV] UPDATED)
196. Deep Multi-Modal Structural Equations For Causal Effect Estimation With Unstructured Proxies. (arXiv:2203.09672v3 [cs.LG] UPDATED)
197. Fair Federated Learning via Bounded Group Loss. (arXiv:2203.10190v2 [cs.LG] UPDATED)
198. VQ-Flows: Vector Quantized Local Normalizing Flows. (arXiv:2203.11556v2 [cs.LG] UPDATED)
199. Feature Distribution Matching for Federated Domain Generalization. (arXiv:2203.11635v2 [cs.LG] UPDATED)
200. On Supervised Feature Selection from High Dimensional Feature Spaces. (arXiv:2203.11924v3 [cs.LG] UPDATED)
201. GriTS: Grid table similarity metric for table structure recognition. (arXiv:2203.12555v2 [cs.LG] UPDATED)
202. SpeqNets: Sparsity-aware Permutation-equivariant Graph Networks. (arXiv:2203.13913v2 [cs.LG] UPDATED)
203. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v3 [cs.CV] UPDATED)
204. LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT. (arXiv:2203.15610v2 [eess.AS] UPDATED)
205. DELTA: Dynamically Optimizing GPU Memory beyond Tensor Recomputation. (arXiv:2203.15980v2 [cs.LG] UPDATED)
206. Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks. (arXiv:2203.16040v2 [cs.SD] UPDATED)
207. RICON: A ML framework for real-time and proactive intervention to prevent customer churn. (arXiv:2203.16155v2 [cs.LG] UPDATED)
208. Convergence of gradient descent for deep neural networks. (arXiv:2203.16462v3 [cs.LG] UPDATED)
209. A data-driven approach for the closure of RANS models by the divergence of the Reynolds Stress Tensor. (arXiv:2203.16944v2 [physics.flu-dyn] UPDATED)
210. Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data. (arXiv:2203.17113v2 [cs.SD] UPDATED)
211. SPECTRE: Spectral Conditioning Helps to Overcome the Expressivity Limits of One-shot Graph Generators. (arXiv:2204.01613v2 [cs.LG] UPDATED)
212. HCFL: A High Compression Approach for Communication-Efficient Federated Learning in Very Large Scale IoT Networks. (arXiv:2204.06760v2 [cs.LG] UPDATED)
213. CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v2 [cs.SI] UPDATED)
214. ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v3 [cs.CV] UPDATED)
215. On Distribution Shift in Learning-based Bug Detectors. (arXiv:2204.10049v2 [cs.LG] UPDATED)
216. A Novel Splitting Criterion Inspired by Geometric Mean Metric Learning for Decision Tree. (arXiv:2204.11011v2 [cs.LG] UPDATED)
217. Fast Sampling of Diffusion Models with Exponential Integrator. (arXiv:2204.13902v3 [cs.LG] UPDATED)
218. Orthogonal Statistical Learning with Self-Concordant Loss. (arXiv:2205.00350v2 [stat.ML] UPDATED)
219. OPT: Open Pre-trained Transformer Language Models. (arXiv:2205.01068v4 [cs.CL] UPDATED)
220. From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks. (arXiv:2205.01265v2 [cs.AI] UPDATED)
221. Explain to Not Forget: Defending Against Catastrophic Forgetting with XAI. (arXiv:2205.01929v3 [cs.LG] UPDATED)
222. Domino Saliency Metrics: Improving Existing Channel Saliency Metrics with Structural Information. (arXiv:2205.02131v2 [cs.CV] UPDATED)
223. Pessimism meets VCG: Learning Dynamic Mechanism Design via Offline Reinforcement Learning. (arXiv:2205.02450v2 [cs.LG] UPDATED)
224. Variance Reduction based Partial Trajectory Reuse to Accelerate Policy Gradient Optimization. (arXiv:2205.02976v2 [cs.LG] UPDATED)
225. Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition. (arXiv:2205.03433v2 [cs.SD] UPDATED)
226. Evaluating the Fairness Impact of Differentially Private Synthetic Data. (arXiv:2205.04321v2 [cs.LG] UPDATED)
227. Is calibration a fairness requirement? An argument from the point of view of moral philosophy and decision theory. (arXiv:2205.05512v3 [cs.LG] UPDATED)
228. A Safety Assurable Human-Inspired Perception Architecture. (arXiv:2205.07862v2 [cs.LG] UPDATED)
229. IIsy: Practical In-Network Classification. (arXiv:2205.08243v2 [cs.NI] UPDATED)
230. Universal characteristics of deep neural network loss surfaces from random matrix theory. (arXiv:2205.08601v2 [math-ph] UPDATED)
231. PAC-Wrap: Semi-Supervised PAC Anomaly Detection. (arXiv:2205.10798v2 [cs.LG] UPDATED)
232. Flexible and Hierarchical Prior for Bayesian Nonnegative Matrix Factorization. (arXiv:2205.11025v2 [cs.LG] UPDATED)
233. Learning to Assemble Geometric Shapes. (arXiv:2205.11809v2 [cs.CV] UPDATED)
234. Service Discovery in Social Internet of Things using Graph Neural Networks. (arXiv:2205.12711v2 [cs.LG] UPDATED)
235. Entropy Maximization with Depth: A Variational Principle for Random Neural Networks. (arXiv:2205.13076v2 [cs.LG] UPDATED)
236. Off-Beat Multi-Agent Reinforcement Learning. (arXiv:2205.13718v2 [cs.MA] UPDATED)
237. FedAUXfdp: Differentially Private One-Shot Federated Distillation. (arXiv:2205.14960v2 [cs.LG] UPDATED)
238. Superposing Many Tickets into One: A Performance Booster for Sparse Neural Network Training. (arXiv:2205.15322v2 [cs.LG] UPDATED)
239. Semi-Supervised Cross-Silo Advertising with Partial Knowledge Transfer. (arXiv:2205.15987v2 [cs.LG] UPDATED)
240. Indeterminacy in Latent Variable Models: Characterization and Strong Identifiability. (arXiv:2206.00801v2 [stat.ML] UPDATED)
241. Joint Energy Dispatch and Unit Commitment in Microgrids Based on Deep Reinforcement Learning. (arXiv:2206.01663v2 [cs.LG] UPDATED)
242. Generalized Data Distribution Iteration. (arXiv:2206.03192v4 [cs.LG] UPDATED)
243. A Benchmark for Federated Hetero-Task Learning. (arXiv:2206.03436v3 [cs.LG] UPDATED)
244. FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization. (arXiv:2206.03966v4 [cs.LG] UPDATED)
245. Patch-based Object-centric Transformers for Efficient Video Generation. (arXiv:2206.04003v2 [cs.CV] UPDATED)
246. What is a Good Metric to Study Generalization of Minimax Learners?. (arXiv:2206.04502v2 [stat.ML] UPDATED)
247. Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models. (arXiv:2206.04959v2 [cs.LG] UPDATED)
248. ROI-Constrained Bidding via Curriculum-Guided Bayesian Reinforcement Learning. (arXiv:2206.05240v3 [cs.LG] UPDATED)
249. **Bilateral** Dependency Optimization: Defending Against Model-inversion Attacks. (arXiv:2206.05483v2 [cs.LG] UPDATED)
250. Mathematical Theory of Bayesian Statistics for Unknown Information Source. (arXiv:2206.05630v2 [cs.LG] UPDATED)
251. PAC-Net: A Model Pruning Approach to Inductive Transfer Learning. (arXiv:2206.05703v2 [cs.LG] UPDATED)
252. SIXO: Smoothing Inference with Twisted Objectives. (arXiv:2206.05952v2 [cs.LG] UPDATED)
253. FreeKD: Free-direction Knowledge Distillation for Graph Neural Networks. (arXiv:2206.06561v2 [cs.LG] UPDATED)
254. Probabilistic Conformal Prediction Using Conditional Random Samples. (arXiv:2206.06584v2 [stat.ML] UPDATED)
255. Exploring speaker enrolment for few-shot personalisation in emotional vocalisation prediction. (arXiv:2206.06680v2 [cs.SD] UPDATED)
256. CNN-based Classification Framework for Lung Tissues with Auxiliary Information. (arXiv:2206.06701v2 [eess.IV] UPDATED)
257. Adversarial Audio Synthesis with Complex-valued Polynomial Networks. (arXiv:2206.06811v2 [eess.AS] UPDATED)
258. Exploring Representation of Horn Clauses using GNNs (technique report). (arXiv:2206.06986v2 [cs.AI] UPDATED)
259. GraphFM: Improving Large-Scale GNN Training via Feature Momentum. (arXiv:2206.07161v2 [cs.LG] UPDATED)
260. Resource-Constrained Edge AI with Early Exit Prediction. (arXiv:2206.07269v2 [cs.LG] UPDATED)
261. Double Sampling Randomized Smoothing. (arXiv:2206.07912v2 [cs.LG] UPDATED)
262. PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v2 [cs.LG] UPDATED)
263. Differentially Private Multi-Party Data Release for Linear Regression. (arXiv:2206.07998v2 [cs.CR] UPDATED)
264. Neural tangent kernel analysis of shallow $\alpha$-Stable ReLU neural networks. (arXiv:2206.08065v2 [cs.LG] UPDATED)
265. Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v2 [cs.CV] UPDATED)
266. Near-Optimal No-Regret Learning for General Convex Games. (arXiv:2206.08742v2 [cs.GT] UPDATED)
267. GINK: Graph-based Interaction-aware Kinodynamic Planning via Reinforcement Learning for Autonomous Driving. (arXiv:2206.01488v1 [cs.RO] CROSS LISTED)
268. A machine-generated catalogue of Charon's craters and implications for the Kuiper belt. (arXiv:2206.08277v1 [astro-ph.EP] CROSS LISTED)
## cs.AI
---
**108** new papers in cs.AI:-) 
1. Putting GPT-3's Creativity to the (Alternative Uses) Test. (arXiv:2206.08932v1 [cs.AI])
2. KitBit: A New AI Model for Solving Intelligence Tests and Numerical Series. (arXiv:2206.08965v1 [cs.AI])
3. Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks. (arXiv:2206.08966v1 [cs.CY])
4. Towards Efficient Active Learning of PDFA. (arXiv:2206.09004v1 [cs.FL])
5. Uniform and Modular Sequent Systems for Description Logics. (arXiv:2206.09020v1 [cs.LO])
6. Designing MacPherson Suspension Architectures using Bayesian Optimization. (arXiv:2206.09022v1 [cs.LG])
7. Stop Overcomplicating Selective Classification: Use Max-Logit. (arXiv:2206.09034v1 [cs.LG])
8. Augmented Imagefication: A Data-driven Fault Detection Method for Aircraft Air Data Sensors. (arXiv:2206.09055v1 [cs.CV])
9. CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks. (arXiv:2206.09059v1 [cs.CL])
10. Behavior Trees in Robotics and AI: An Introduction. (arXiv:1709.00084v5 [cs.RO] UPDATED)
11. Simplified decision making in the belief space using belief sparsification. (arXiv:1909.00885v5 [cs.AI] UPDATED)
12. TimeCaps: Capturing Time Series Data With Capsule Networks. (arXiv:1911.11800v4 [cs.LG] UPDATED)
13. Putting Ridesharing to the Test: Efficient and Scalable Solutions and the Power of Dynamic Vehicle Relocation. (arXiv:1912.08066v4 [cs.MA] UPDATED)
14. Multi Type Mean Field Reinforcement Learning. (arXiv:2002.02513v7 [cs.MA] UPDATED)
15. Optimal Algorithms for Convex Nested Stochastic Composite Optimization. (arXiv:2011.10076v5 [math.OC] UPDATED)
16. Stability of Finite Horizon Optimisation based Control without Terminal Weight. (arXiv:2011.14193v3 [eess.SY] UPDATED)
17. Hindsight and Sequential Rationality of Correlated Play. (arXiv:2012.05874v6 [cs.GT] UPDATED)
18. Warping of Radar Data into Camera Image for Cross-Modal Supervision in Automotive Applications. (arXiv:2012.12809v2 [cs.CV] UPDATED)
19. Fairness-Oriented User Scheduling for Bursty Downlink Transmission Using Multi-Agent Reinforcement Learning. (arXiv:2012.15081v14 [cs.OS] UPDATED)
20. Efficient Deviation Types and Learning for Hindsight Rationality in Extensive-Form Games. (arXiv:2102.06973v6 [cs.GT] UPDATED)
21. Towards Continual, Online, Self-Supervised Depth. (arXiv:2103.00369v3 [cs.CV] UPDATED)
22. EfficientTDNN: Efficient Architecture Search for Speaker Recognition. (arXiv:2103.13581v5 [eess.AS] UPDATED)
23. FedScale: Benchmarking Model and System Performance of Federated Learning at Scale. (arXiv:2105.11367v5 [cs.LG] UPDATED)
24. Scaling Vision Transformers. (arXiv:2106.04560v2 [cs.CV] UPDATED)
25. Knowledge distillation: A good teacher is patient and consistent. (arXiv:2106.05237v2 [cs.CV] UPDATED)
26. Metric Policy Representations for Opponent Modeling. (arXiv:2106.05802v2 [cs.LG] UPDATED)
27. Markov Decision Processes with Long-Term Average Constraints. (arXiv:2106.06680v2 [cs.LG] UPDATED)
28. Physion: Evaluating Physical Prediction from Vision in Humans and Machines. (arXiv:2106.08261v3 [cs.AI] UPDATED)
29. The Values Encoded in Machine Learning Research. (arXiv:2106.15590v2 [cs.LG] UPDATED)
30. Model-Based Opponent Modeling. (arXiv:2108.01843v2 [cs.LG] UPDATED)
31. Rapid Automated Analysis of Skull Base Tumor Specimens Using Intraoperative Optical Imaging and Artificial Intelligence. (arXiv:2108.03555v2 [cs.CV] UPDATED)
32. DQ-GAT: Towards Safe and Efficient Autonomous Driving with Deep Q-Learning and Graph Attention Networks. (arXiv:2108.05030v2 [cs.RO] UPDATED)
33. Ensemble CNN and Uncertainty Modeling to Improve Automatic Identification/Segmentation of Multiple Sclerosis Lesions in Magnetic Resonance Imaging. (arXiv:2108.11791v3 [eess.IV] UPDATED)
34. Practical and Secure Federated Recommendation with Personalized Masks. (arXiv:2109.02464v2 [cs.IR] UPDATED)
35. Learning Generative Deception Strategies in Combinatorial Masking Games. (arXiv:2109.11637v2 [cs.GT] UPDATED)
36. A Graph Policy Network Approach for Volt-Var Control in Power Distribution Systems. (arXiv:2109.12073v2 [cs.LG] UPDATED)
37. Divergence-Regularized Multi-Agent Actor-Critic. (arXiv:2110.00304v2 [cs.LG] UPDATED)
38. Intriguing Properties of Input-dependent Randomized Smoothing. (arXiv:2110.05365v2 [cs.LG] UPDATED)
39. Action-Sufficient State Representation Learning for Control with Structural Constraints. (arXiv:2110.05721v2 [cs.LG] UPDATED)
40. Scalable Bayesian Network Structure Learning with Splines. (arXiv:2110.14626v2 [cs.LG] UPDATED)
41. Self-supervised EEG Representation Learning for Automatic Sleep Staging. (arXiv:2110.15278v2 [eess.SP] UPDATED)
42. Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition. (arXiv:2111.15361v2 [cs.CV] UPDATED)
43. Lecture Notes on Partially Known MDPs. (arXiv:2112.02976v2 [cs.AI] UPDATED)
44. Self-Organized Polynomial-Time Coordination Graphs. (arXiv:2112.03547v3 [cs.LG] UPDATED)
45. Fast and scalable neuroevolution deep learning architecture search for multivariate anomaly detection. (arXiv:2112.05640v4 [cs.NE] UPDATED)
46. A New Perspective on the Effects of Spectrum in Graph Neural Networks. (arXiv:2112.07160v2 [cs.LG] UPDATED)
47. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v3 [cs.CV] UPDATED)
48. A First Mathematical Runtime Analysis of the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). (arXiv:2112.08581v3 [cs.NE] UPDATED)
49. Making sense of electrical vehicle discussions using sentiment analysis on closely related news and user comments. (arXiv:2112.12327v4 [cs.CL] UPDATED)
50. Black-Box Testing of Deep Neural Networks through Test Case Diversity. (arXiv:2112.12591v3 [cs.SE] UPDATED)
51. MISO hierarchical inference engine with fuzzy implication satisfying I(A(x, y), z) = I(x, I(y, z)). (arXiv:2112.12808v2 [cs.AI] UPDATED)
52. Deep Reinforcement Learning, a textbook. (arXiv:2201.02135v4 [cs.AI] UPDATED)
53. Black-box error diagnosis in deep neural networks for computer vision: a survey of tools. (arXiv:2201.06444v2 [cs.LG] UPDATED)
54. Summarising and Comparing Agent Dynamics with Contrastive Spatiotemporal Abstraction. (arXiv:2201.07749v2 [cs.AI] UPDATED)
55. NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis. (arXiv:2201.08277v3 [cs.CL] UPDATED)
56. Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks. (arXiv:2201.11729v3 [cs.LG] UPDATED)
57. Generalizing to New Physical Systems via Context-Informed Dynamics Model. (arXiv:2202.01889v2 [cs.LG] UPDATED)
58. Versatile Offline Imitation from Observations and Examples via Regularized State-Occupancy Matching. (arXiv:2202.02433v2 [cs.LG] UPDATED)
59. Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill Diversity. (arXiv:2202.02886v3 [cs.AI] UPDATED)
60. Generalized Strategic Classification and the Case of Aligned Incentives. (arXiv:2202.04357v2 [cs.LG] UPDATED)
61. REvolveR: Continuous Evolutionary Models for Robot-to-robot Policy Transfer. (arXiv:2202.05244v2 [cs.LG] UPDATED)
62. HousE: Knowledge Graph Embedding with Householder Parameterization. (arXiv:2202.07919v3 [cs.AI] UPDATED)
63. On a Variance-Reduction Correction for the Temporal-Difference Learning in the Stochastic Continuous Setting. (arXiv:2202.07960v2 [cs.LG] UPDATED)
64. Augment with Care: Contrastive Learning for Combinatorial Problems. (arXiv:2202.08396v2 [cs.LG] UPDATED)
65. Knowledge Base Question Answering by Case-based Reasoning over Subgraphs. (arXiv:2202.10610v2 [cs.CL] UPDATED)
66. Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates. (arXiv:2202.12967v2 [cs.LG] UPDATED)
67. Multi-task Learning Approach for Modulation and Wireless Signal Classification for 5G and Beyond: Edge Deployment via Model Compression. (arXiv:2203.00517v2 [eess.SP] UPDATED)
68. NaviAirway: a Bronchiole-sensitive Deep Learning-based Airway Segmentation Pipeline. (arXiv:2203.04294v2 [eess.IV] UPDATED)
69. PMIC: Improving Multi-Agent Reinforcement Learning with Progressive Mutual Information Collaboration. (arXiv:2203.08553v2 [cs.MA] UPDATED)
70. VQ-Flows: Vector Quantized Local Normalizing Flows. (arXiv:2203.11556v2 [cs.LG] UPDATED)
71. SpeqNets: Sparsity-aware Permutation-equivariant Graph Networks. (arXiv:2203.13913v2 [cs.LG] UPDATED)
72. BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v3 [cs.CV] UPDATED)
73. Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition. (arXiv:2204.00465v3 [eess.AS] UPDATED)
74. SPECTRE: Spectral Conditioning Helps to Overcome the Expressivity Limits of One-shot Graph Generators. (arXiv:2204.01613v2 [cs.LG] UPDATED)
75. Distributed Transition Systems with Tags for Privacy Analysis. (arXiv:2204.02602v2 [cs.CL] UPDATED)
76. Iterative Depth-First Search for Fully Observable Non-Deterministic Planning. (arXiv:2204.04322v3 [cs.AI] UPDATED)
77. Why did I fail? A Causal-based Method to Find Explanations for Robot Failures. (arXiv:2204.04483v2 [cs.RO] UPDATED)
78. HCFL: A High Compression Approach for Communication-Efficient Federated Learning in Very Large Scale IoT Networks. (arXiv:2204.06760v2 [cs.LG] UPDATED)
79. Cognitive Architecture for Decision-Making Based on Brain Principles Programming. (arXiv:2204.07919v3 [cs.AI] UPDATED)
80. CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v2 [cs.SI] UPDATED)
81. SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v4 [cs.CV] UPDATED)
82. HarmoF0: Logarithmic Scale Dilated Convolution For Pitch Estimation. (arXiv:2205.01019v2 [cs.SD] UPDATED)
83. From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks. (arXiv:2205.01265v2 [cs.AI] UPDATED)
84. Towards Improved Zero-shot Voice Conversion with Conditional DSVAE. (arXiv:2205.05227v2 [eess.AS] UPDATED)
85. A Safety Assurable Human-Inspired Perception Architecture. (arXiv:2205.07862v2 [cs.LG] UPDATED)
86. GR-GAN: Gradual Refinement Text-to-image Generation. (arXiv:2205.11273v2 [cs.CV] UPDATED)
87. Entropy Maximization with Depth: A Variational Principle for Random Neural Networks. (arXiv:2205.13076v2 [cs.LG] UPDATED)
88. Off-Beat Multi-Agent Reinforcement Learning. (arXiv:2205.13718v2 [cs.MA] UPDATED)
89. Superposing Many Tickets into One: A Performance Booster for Sparse Neural Network Training. (arXiv:2205.15322v2 [cs.LG] UPDATED)
90. Fast-Spanning Ant Colony Optimisation (FaSACO) for Mobile Robot Coverage Path Planning. (arXiv:2205.15691v2 [cs.RO] UPDATED)
91. Joint Energy Dispatch and Unit Commitment in Microgrids Based on Deep Reinforcement Learning. (arXiv:2206.01663v2 [cs.LG] UPDATED)
92. Design and Implementation of an Heuristic-Enhanced Branch-and-Bound Solver for MILP. (arXiv:2206.01857v2 [cs.AI] UPDATED)
93. CAISAR: A platform for Characterizing Artificial Intelligence Safety and Robustness. (arXiv:2206.03044v2 [cs.AI] UPDATED)
94. Generalized Data Distribution Iteration. (arXiv:2206.03192v4 [cs.LG] UPDATED)
95. Position Paper: Online Modeling for Offline Planning. (arXiv:2206.03356v3 [cs.AI] UPDATED)
96. SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v2 [cs.CV] UPDATED)
97. ROI-Constrained Bidding via Curriculum-Guided Bayesian Reinforcement Learning. (arXiv:2206.05240v3 [cs.LG] UPDATED)
98. PAC-Net: A Model Pruning Approach to Inductive Transfer Learning. (arXiv:2206.05703v2 [cs.LG] UPDATED)
99. X-Risk Analysis for AI Research. (arXiv:2206.05862v3 [cs.CY] UPDATED)
100. SIXO: Smoothing Inference with Twisted Objectives. (arXiv:2206.05952v2 [cs.LG] UPDATED)
101. Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images. (arXiv:2206.06665v2 [cs.CV] UPDATED)
102. Exploring Representation of Horn Clauses using GNNs (technique report). (arXiv:2206.06986v2 [cs.AI] UPDATED)
103. AnimeSR: Learning Real-World Super-Resolution Models for Animation Videos. (arXiv:2206.07038v2 [cs.CV] UPDATED)
104. GraphFM: Improving Large-Scale GNN Training via Feature Momentum. (arXiv:2206.07161v2 [cs.LG] UPDATED)
105. PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v2 [cs.LG] UPDATED)
106. Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v2 [cs.SI] UPDATED)
107. TAVA: Template-free Animatable Volumetric Actors. (arXiv:2206.08929v2 [cs.CV] UPDATED)
108. GINK: Graph-based Interaction-aware Kinodynamic Planning via Reinforcement Learning for Autonomous Driving. (arXiv:2206.01488v1 [cs.RO] CROSS LISTED)

