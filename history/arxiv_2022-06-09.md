# Your interest papers
---
## cs.CV
---
### ObPose: Leveraging Canonical Pose for Object-Centric Scene Inference in 3D. (arXiv:2206.03591v1 [cs.CV])
- Authors : Yizhe Wu, Oiwi Parker, Ingmar Posner
- Link : [http://arxiv.org/abs/2206.03591](http://arxiv.org/abs/2206.03591)
> ABSTRACT  :  We present ObPose, an unsupervised object-centric generative model that learns to segment 3D objects from RGB-D video in an unsupervised manner. Inspired by prior art in 2D representation learning, ObPose considers a factorised latent space, separately encoding object-wise location (where) and appearance (what) information. In particular, ObPose leverages an object's canonical pose, defined via a minimum volume principle, as a novel inductive bias for learning the where component. To achieve this, we propose an efficient, voxelised approximation approach to recover the object shape directly from a neural radiance field (**NeRF**). As a consequence, ObPose models scenes as compositions of **NeRF**s representing individual objects. When evaluated on the YCB dataset for unsupervised scene segmentation, ObPose outperforms the current state-of-the-art in 3D scene inference (ObSuRF) by a significant margin in terms of segmentation quality for both video inputs as well as for multi-view static scenes. In addition, the design choices made in the ObPose encoder are validated with relevant ablations.  
### UHD Image Deblurring via Multi-scale Cubic-Mixer. (arXiv:2206.03678v1 [cs.CV])
- Authors : Zhuoran Zheng, Xiuyi Jia
- Link : [http://arxiv.org/abs/2206.03678](http://arxiv.org/abs/2206.03678)
> ABSTRACT  :  Currently, transformer-based algorithms are making a splash in the domain of image deblurring. Their achievement depends on the self-attention mechanism with CNN stem to model long range dependencies between tokens. Unfortunately, this ear-pleasing pipeline introduces high computational complexity and makes it difficult to run an ultra-high-definition image on a single GPU in **real time**. To trade-off accuracy and efficiency, the input degraded image is computed cyclically over three dimensional ($C$, $W$, and $H$) signals without a self-attention mechanism. We term this deep network as Multi-scale Cubic-Mixer, which is acted on both the real and imaginary components after fast Fourier transform to estimate the Fourier coefficients and thus obtain a deblurred image. Furthermore, we combine the multi-scale cubic-mixer with a slicing strategy to generate high-quality results at a much lower computational cost. Experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art deblurring approaches on the several benchmarks and a new ultra-high-definition dataset in terms of accuracy and speed.  
### Robust Deep Ensemble Method for Real-world Image Denoising. (arXiv:2206.03691v1 [cs.CV])
- Authors : Pengju Liu, Hongzhi Zhang, Jinghui Wang, Yuzhi Wang, Dongwei Ren, Wangmeng Zuo
- Link : [http://arxiv.org/abs/2206.03691](http://arxiv.org/abs/2206.03691)
> ABSTRACT  :  Recently, deep learning-based image denoising methods have achieved promising performance on test data with the same distribution as training set, where various denoising models based on synthetic or collected real-world training data have been learned. However, when handling real-world noisy images, the denoising performance is still limited. In this paper, we propose a simple yet effective Bayesian deep ensemble (BDE) method for real-world image denoising, where several representative deep denoisers pre-trained with various training data settings can be fused to improve robustness. The foundation of BDE is that real-world image noises are highly signal-dependent, and heterogeneous noises in a real-world noisy image can be separately handled by different denoisers. In particular, we take well-trained CBDNet, NBNet, HINet, Uformer and GMSNet into denoiser pool, and a U-Net is adopted to predict pixel-wise weighting maps to fuse these denoisers. Instead of solely learning pixel-wise weighting maps, Bayesian deep learning strategy is introduced to predict weighting uncertainty as well as weighting map, by which prediction variance can be modeled for improving robustness on real-world noisy images. Extensive experiments have shown that real-world noises can be better removed by fusing existing denoisers instead of training a big denoiser with expensive cost. On DND dataset, our BDE achieves +0.28~dB PSNR gain over the state-of-the-art denoising method. Moreover, we note that our BDE denoiser based on different Gaussian noise levels outperforms state-of-the-art CBDNet when applying to real-world noisy images. Furthermore, our BDE can be extended to other image **restoration** tasks, and achieves +0.30dB, +0.18dB and +0.12dB PSNR gains on benchmark datasets for image deblurring, image deraining and single image super-resolution, respectively.  
### Blind Face **Restoration**: Benchmark Datasets and a Baseline Model. (arXiv:2206.03697v1 [cs.CV])
- Authors : Puyang Zhang, Kaihao Zhang, Wenhan Luo, Changsheng Li, Guoren Wang
- Link : [http://arxiv.org/abs/2206.03697](http://arxiv.org/abs/2206.03697)
> ABSTRACT  :  Blind Face **Restoration** (BFR) aims to construct a high-quality (HQ) face image from its corresponding low-quality (LQ) input. Recently, many BFR methods have been proposed and they have achieved remarkable success. However, these methods are trained or evaluated on privately synthesized datasets, which makes it infeasible for the subsequent approaches to fairly compare with them. To address this problem, we first synthesize two blind face **restoration** benchmark datasets called EDFace-Celeb-1M (BFR128) and EDFace-Celeb-150K (BFR512). State-of-the-art methods are benchmarked on them under five settings including blur, noise, low resolution, JPEG compression artifacts, and the combination of them (full degradation). To make the comparison more comprehensive, five widely-used quantitative metrics and two task-driven metrics including Average Face Landmark Distance (AFLD) and Average Face ID Cosine Similarity (AFICS) are applied. Furthermore, we develop an effective baseline model called **Swin** Transformer U-Net (STUNet). The STUNet with U-net architecture applies an attention mechanism and a shifted windowing scheme to capture long-range pixel interactions and focus more on significant features while still being trained efficiently. Experimental results show that the proposed baseline method performs favourably against the SOTA methods on various BFR tasks.  
### Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation. (arXiv:2206.03789v1 [cs.CV])
- Authors : Zihan Ding, Tianrui Hui, Junshi Huang, Xiaoming Wei, Jizhong Han, Si Liu
- Link : [http://arxiv.org/abs/2206.03789](http://arxiv.org/abs/2206.03789)
> ABSTRACT  :  Referring video object segmentation aims to predict foreground labels for objects referred by natural language expressions in videos. Previous methods either depend on 3D ConvNets or incorporate additional 2D ConvNets as encoders to extract mixed spatial-temporal features. However, these methods suffer from spatial misalignment or false distractors due to delayed and implicit spatial-temporal interaction occurring in the decoding phase. To tackle these limitations, we propose a Language-Bridged Duplex Transfer (LBDT) module which utilizes language as an intermediary bridge to accomplish explicit and adaptive spatial-temporal interaction earlier in the encoding phase. Concretely, cross-modal attention is performed among the temporal encoder, referring words and the spatial encoder to aggregate and transfer language-relevant motion and appearance information. In addition, we also propose a **Bilateral** Channel Activation (BCA) module in the decoding phase for further denoising and highlighting the spatial-temporal consistent features via channel-wise activation. Extensive experiments show our method achieves new state-of-the-art performances on four popular benchmarks with 6.8% and 6.9% absolute AP gains on A2D Sentences and J-HMDB Sentences respectively, while consuming around 7x less computational overhead.  
### Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior. (arXiv:2206.03858v1 [cs.CV])
- Authors : Bernhard Egger
- Link : [http://arxiv.org/abs/2206.03858](http://arxiv.org/abs/2206.03858)
> ABSTRACT  :  Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. We propose a conditional neural field representation based on a variational auto-decoder with a SIREN network and, extending Vector Neurons, build equivariance directly into the network. Using this we develop a rotation-equivariant, **high dynamic range** (**HDR**) neural illumination model that is compact and able to express complex, high-frequency features of natural environment maps. Training our model on a curated dataset of 1.6K **HDR** environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations. A PyTorch implementation, our dataset and trained models can be found at jadgardner.github.io/RENI.  
### Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset, and Toolkit. (arXiv:2106.14922v2 [astro-ph.IM] UPDATED)
- Authors : Chengyuan Xu, Curtis McCully, Boning Dong, Andrew Howell, Pradeep Sen
- Link : [http://arxiv.org/abs/2106.14922](http://arxiv.org/abs/2106.14922)
> ABSTRACT  :  Rejecting cosmic rays (CRs) is essential for the scientific interpretation of CCD-captured data, but detecting CRs in single-**exposure** images has remained challenging. Conventional CR detectors require experimental parameter tuning for different instruments, and recent deep learning methods only produce instrument-specific models that suffer from performance loss on telescopes not included in the training data. In this work, we present Cosmic-CoNN, a generic CR detector deployed for 24 telescopes at the Las Cumbres Observatory (LCO). We first leverage thousands of images from LCO's global telescope network to build a large, diverse ground-based CR dataset for rich coverage of instruments and CR features. We then optimize a neural network and propose a novel Median-Weighted loss function for CR detection to train a generic model that achieves a 99.91% true-positive detection rate on LCO imaging data and maintains over 96.40% on unseen data from Gemini GMOS-N/S, with a false-positive rate of 0.01%. We also build a suite of tools including an interactive CR mask visualization and editing interface, console commands, and Python APIs to make automatic, robust CR detection widely accessible by the community of astronomers. Our dataset, open-source codebase, and trained models are available at https://github.com/cy-xu/cosmic-conn.  
### Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v4 [cs.CV] UPDATED)
- Authors : Christoph Angermann, Matthias Schwab, Markus Haltmeier, Christian Laubichler
- Link : [http://arxiv.org/abs/2201.12170](http://arxiv.org/abs/2201.12170)
> ABSTRACT  :  **Real-time** estimation of actual object depth is an essential module for various autonomous system tasks such as 3D reconstruction, scene understanding and condition assessment. During the last decade of machine learning, extensive deployment of deep learning methods to computer vision tasks has yielded approaches that succeed in achieving realistic depth synthesis out of a simple RGB modality. Most of these models are based on paired RGB-depth data and/or the availability of video sequences and stereo images. The lack of sequences, stereo data and RGB-depth pairs makes depth estimation a fully unsupervised single-image transfer problem that has barely been explored so far. This study builds on recent advances in the field of generative neural networks in order to establish fully unsupervised single-shot depth estimation. Two generators for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously optimized using the Wasserstein-1 distance, a novel perceptual reconstruction term and hand-crafted image filters. We comprehensively evaluate the models using industrial surface depth data as well as the Texas 3D Face Recognition Database, the CelebAMask-HQ database of human portraits and the SURREAL dataset that records body depth. For each evaluation dataset the proposed method shows a significant increase in depth accuracy compared to state-of-the-art single-image transfer methods.  
### Residual Mixture of Experts. (arXiv:2204.09636v2 [cs.CV] UPDATED)
- Authors : Lemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, Lu Yuan
- Link : [http://arxiv.org/abs/2204.09636](http://arxiv.org/abs/2204.09636)
> ABSTRACT  :  Mixture of Experts (MoE) is able to scale up vision transformers effectively. However, it requires prohibiting computation resources to train a large MoE transformer. In this paper, we propose Residual Mixture of Experts (RMoE), an efficient training pipeline for MoE vision transformers on downstream tasks, such as segmentation and detection. RMoE achieves comparable results with the upper-bound MoE training, while only introducing minor additional training cost than the lower-bound non-MoE training pipelines. The efficiency is supported by our key observation: the weights of an MoE transformer can be factored into an input-independent core and an input-dependent residual. Compared with the weight core, the weight residual can be efficiently trained with much less computation resource, e.g., finetuning on the downstream data. We show that, compared with the current MoE training pipeline, we get comparable results while saving over 30% training cost. When compared with state-of-the-art non- MoE transformers, such as **Swin**-T / CvT-13 / **Swin**-L, we get +1.1 / 0.9 / 1.0 mIoU gain on ADE20K segmentation and +1.4 / 1.6 / 0.6 AP gain on MS-COCO object detection task with less than 3% additional training cost.  
### Differentiable Point-Based Radiance Fields for Efficient View Synthesis. (arXiv:2205.14330v2 [cs.CV] UPDATED)
- Authors : Qiang Zhang, Hwan Baek, Szymon Rusinkiewicz, Felix Heide
- Link : [http://arxiv.org/abs/2205.14330](http://arxiv.org/abs/2205.14330)
> ABSTRACT  :  We propose a differentiable rendering algorithm for efficient novel view synthesis. By departing from volume-based representations in favor of a learned point representation, we improve on existing methods more than an order of magnitude in memory and runtime, both in training and inference. The method begins with a uniformly-sampled random point cloud and learns per-point position and view-dependent appearance, using a differentiable splat-based renderer to evolve the model to match a set of input images. Our method is up to 300x faster than **NeRF** in both training and inference, with only a marginal sacrifice in quality, while using less than 10~MB of memory for a static scene. For dynamic scenes, our method trains two orders of magnitude faster than ST**NeRF** and renders at near interactive rate, while maintaining high image quality and temporal coherence even without imposing any temporal-coherency regularizers.  
## eess.IV
---
### Robust Deep Ensemble Method for Real-world Image Denoising. (arXiv:2206.03691v1 [cs.CV])
- Authors : Pengju Liu, Hongzhi Zhang, Jinghui Wang, Yuzhi Wang, Dongwei Ren, Wangmeng Zuo
- Link : [http://arxiv.org/abs/2206.03691](http://arxiv.org/abs/2206.03691)
> ABSTRACT  :  Recently, deep learning-based image denoising methods have achieved promising performance on test data with the same distribution as training set, where various denoising models based on synthetic or collected real-world training data have been learned. However, when handling real-world noisy images, the denoising performance is still limited. In this paper, we propose a simple yet effective Bayesian deep ensemble (BDE) method for real-world image denoising, where several representative deep denoisers pre-trained with various training data settings can be fused to improve robustness. The foundation of BDE is that real-world image noises are highly signal-dependent, and heterogeneous noises in a real-world noisy image can be separately handled by different denoisers. In particular, we take well-trained CBDNet, NBNet, HINet, Uformer and GMSNet into denoiser pool, and a U-Net is adopted to predict pixel-wise weighting maps to fuse these denoisers. Instead of solely learning pixel-wise weighting maps, Bayesian deep learning strategy is introduced to predict weighting uncertainty as well as weighting map, by which prediction variance can be modeled for improving robustness on real-world noisy images. Extensive experiments have shown that real-world noises can be better removed by fusing existing denoisers instead of training a big denoiser with expensive cost. On DND dataset, our BDE achieves +0.28~dB PSNR gain over the state-of-the-art denoising method. Moreover, we note that our BDE denoiser based on different Gaussian noise levels outperforms state-of-the-art CBDNet when applying to real-world noisy images. Furthermore, our BDE can be extended to other image **restoration** tasks, and achieves +0.30dB, +0.18dB and +0.12dB PSNR gains on benchmark datasets for image deblurring, image deraining and single image super-resolution, respectively.  
## cs.LG
---
### Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v4 [cs.CV] UPDATED)
- Authors : Christoph Angermann, Matthias Schwab, Markus Haltmeier, Christian Laubichler
- Link : [http://arxiv.org/abs/2201.12170](http://arxiv.org/abs/2201.12170)
> ABSTRACT  :  **Real-time** estimation of actual object depth is an essential module for various autonomous system tasks such as 3D reconstruction, scene understanding and condition assessment. During the last decade of machine learning, extensive deployment of deep learning methods to computer vision tasks has yielded approaches that succeed in achieving realistic depth synthesis out of a simple RGB modality. Most of these models are based on paired RGB-depth data and/or the availability of video sequences and stereo images. The lack of sequences, stereo data and RGB-depth pairs makes depth estimation a fully unsupervised single-image transfer problem that has barely been explored so far. This study builds on recent advances in the field of generative neural networks in order to establish fully unsupervised single-shot depth estimation. Two generators for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously optimized using the Wasserstein-1 distance, a novel perceptual reconstruction term and hand-crafted image filters. We comprehensively evaluate the models using industrial surface depth data as well as the Texas 3D Face Recognition Database, the CelebAMask-HQ database of human portraits and the SURREAL dataset that records body depth. For each evaluation dataset the proposed method shows a significant increase in depth accuracy compared to state-of-the-art single-image transfer methods.  
## cs.AI
---
### ObPose: Leveraging Canonical Pose for Object-Centric Scene Inference in 3D. (arXiv:2206.03591v1 [cs.CV])
- Authors : Yizhe Wu, Oiwi Parker, Ingmar Posner
- Link : [http://arxiv.org/abs/2206.03591](http://arxiv.org/abs/2206.03591)
> ABSTRACT  :  We present ObPose, an unsupervised object-centric generative model that learns to segment 3D objects from RGB-D video in an unsupervised manner. Inspired by prior art in 2D representation learning, ObPose considers a factorised latent space, separately encoding object-wise location (where) and appearance (what) information. In particular, ObPose leverages an object's canonical pose, defined via a minimum volume principle, as a novel inductive bias for learning the where component. To achieve this, we propose an efficient, voxelised approximation approach to recover the object shape directly from a neural radiance field (**NeRF**). As a consequence, ObPose models scenes as compositions of **NeRF**s representing individual objects. When evaluated on the YCB dataset for unsupervised scene segmentation, ObPose outperforms the current state-of-the-art in 3D scene inference (ObSuRF) by a significant margin in terms of segmentation quality for both video inputs as well as for multi-view static scenes. In addition, the design choices made in the ObPose encoder are validated with relevant ablations.  
### Combining Monte-Carlo Tree Search with Proof-Number Search. (arXiv:2206.03965v1 [cs.AI])
- Authors : Elliot Doe, Cameron Browne
- Link : [http://arxiv.org/abs/2206.03965](http://arxiv.org/abs/2206.03965)
> ABSTRACT  :  Proof-Number Search (PNS) and Monte-Carlo Tree Search (MCTS) have been successfully applied for decision making in a range of games. This paper proposes a new approach called PN-MCTS that combines these two tree-search methods by incorporating the concept of proof and disproof numbers into the UCT formula of MCTS. Experimental results demonstrate that PN-MCTS outperforms basic MCTS in several games including Lines of Action, MiniShogi, K**night**through, and Awari, achieving win rates up to 94.0%.  
# Paper List
---
## cs.CV
---
**98** new papers in cs.CV:-) 
1. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v1 [cs.AI])
2. A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v1 [cs.CV])
3. Contributor-Aware Defenses Against Adversarial Backdoor Attacks. (arXiv:2206.03583v1 [cs.CR])
4. White-box Membership Attack Against Machine Learning Based Retinopathy Classification. (arXiv:2206.03584v1 [cs.CR])
5. ObPose: Leveraging Canonical Pose for Object-Centric Scene Inference in 3D. (arXiv:2206.03591v1 [cs.CV])
6. Neural Network Compression via Effective Filter Analysis and Hierarchical Pruning. (arXiv:2206.03596v1 [cs.LG])
7. One Ring to Bring Them All: Towards Open-Set Recognition under Domain Shift. (arXiv:2206.03600v1 [cs.CV])
8. A new method incorporating deep learning with shape priors for left ventricular segmentation in myocardial perfusion SPECT images. (arXiv:2206.03603v1 [eess.IV])
9. Predictive Modeling of Charge Levels for Battery Electric Vehicles using CNN EfficientNet and IGTD Algorithm. (arXiv:2206.03612v1 [cs.CV])
10. Delving into the Pre-training Paradigm of Monocular 3D Object Detection. (arXiv:2206.03657v1 [cs.CV])
11. One Hyper-Initializer for All Network Architectures in Medical Image Analysis. (arXiv:2206.03661v1 [cs.CV])
12. Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking. (arXiv:2206.03666v1 [cs.CV])
13. COVIDx CXR-3: A Large-Scale, Open-Source Benchmark Dataset of Chest X-ray Images for Computer-Aided COVID-19 Diagnostics. (arXiv:2206.03671v1 [eess.IV])
14. Unsupervised Learning of 3D Scene Flow from Monocular Camera. (arXiv:2206.03673v1 [cs.CV])
15. UHD Image Deblurring via Multi-scale Cubic-Mixer. (arXiv:2206.03678v1 [cs.CV])
16. DebiasBench: Benchmark for Fair Comparison of Debiasing in Image Classification. (arXiv:2206.03680v1 [cs.CV])
17. A Unified Model for Multi-class Anomaly Detection. (arXiv:2206.03687v1 [cs.CV])
18. Robust Deep Ensemble Method for Real-world Image Denoising. (arXiv:2206.03691v1 [cs.CV])
19. Blind Face **Restoration**: Benchmark Datasets and a Baseline Model. (arXiv:2206.03697v1 [cs.CV])
20. What do we learn? Debunking the Myth of Unsupervised Outlier Detection. (arXiv:2206.03698v1 [cs.CV])
21. Hypernetwork-based Personalized Federated Learning for Multi-Institutional CT Imaging. (arXiv:2206.03709v1 [eess.IV])
22. Wavelet Regularization Benefits Adversarial Training. (arXiv:2206.03727v1 [cs.CV])
23. Disentangled Ontology Embedding for Zero-shot Learning. (arXiv:2206.03739v1 [cs.AI])
24. Large Loss Matters in Weakly Supervised Multi-Label Classification. (arXiv:2206.03740v1 [cs.CV])
25. Learning Task Agnostic Temporal Consistency Correction. (arXiv:2206.03753v1 [cs.CV])
26. PixSelect: Less but Reliable Pixels for Accurate and Efficient Localization. (arXiv:2206.03775v1 [cs.CV])
27. Learning Digital Terrain Models from Point Clouds: ALS2DTM Dataset and Rasterization-based GAN. (arXiv:2206.03778v1 [cs.CV])
28. Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation. (arXiv:2206.03789v1 [cs.CV])
29. Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps. (arXiv:2206.03799v1 [cs.CV])
30. Dual Windows Are Significant: Learning from Mediastinal Window and Focusing on Lung Window. (arXiv:2206.03803v1 [eess.IV])
31. SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v1 [cs.CV])
32. Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v1 [cs.LG])
33. Generative Myocardial Motion Tracking via Latent Space Exploration with Biomechanics-informed Prior. (arXiv:2206.03830v1 [eess.IV])
34. Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior. (arXiv:2206.03858v1 [cs.CV])
35. Orthonormal Convolutions for the Rotation Based Iterative Gaussianization. (arXiv:2206.03860v1 [cs.CV])
36. Perceptual Quality Assessment for Fine-Grained Compressed Images. (arXiv:2206.03862v1 [cs.CV])
37. Progressive GANomaly: Anomaly detection with progressively growing GANs. (arXiv:2206.03876v1 [cs.CV])
38. ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical Image Segmentation. (arXiv:2206.03888v1 [cs.CV])
39. PrivHAR: Recognizing Human Actions From Privacy-preserving Lens. (arXiv:2206.03891v1 [cs.CV])
40. Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans. (arXiv:2206.03900v1 [eess.IV])
41. Direct Triangulation with Spherical Projection for Omnidirectional Cameras. (arXiv:2206.03928v1 [cs.CV])
42. Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays. (arXiv:2206.03935v1 [eess.IV])
43. Depth-Adapted CNNs for RGB-D Semantic Segmentation. (arXiv:2206.03939v1 [cs.CV])
44. Robust Environment Perception for Automated Driving: A Unified Learning Pipeline for Visual-Infrared Object Detection. (arXiv:2206.03943v1 [cs.CV])
45. Out-of-Distribution Detection with Class Ratio Estimation. (arXiv:2206.03955v1 [stat.ML])
46. Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v1 [cs.CV])
47. Patch-based Object-centric Transformers for Efficient Video Generation. (arXiv:2206.04003v1 [cs.CV])
48. Few-Shot Audio-Visual Learning of Environment Acoustics. (arXiv:2206.04006v1 [cs.SD])
49. SYNERgy between SYNaptic consolidation and Experience Replay for general continual learning. (arXiv:2206.04016v1 [cs.NE])
50. CO^3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving. (arXiv:2206.04028v1 [cs.CV])
51. Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v1 [cs.CV])
52. An Improved One millisecond Mobile Backbone. (arXiv:2206.04040v1 [cs.CV])
53. Learning Ego 3D Representation as Ray Tracing. (arXiv:2206.04042v1 [cs.CV])
54. Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v1 [cs.CV])
55. An Efficient Target Detection and Recognition Method in Aerial Remote-sensing Images Based on Multiangle Regions-of-Interest. (arXiv:1907.09320v2 [cs.CV] UPDATED)
56. Blacklight: Defending Black-Box Adversarial Attacks on Deep Neural Networks. (arXiv:2006.14042v2 [cs.CR] UPDATED)
57. Surface Topography Characterization Using a Simple Optical Device and Artificial Neural Networks. (arXiv:2103.08482v3 [cs.CV] UPDATED)
58. Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v4 [cs.CV] UPDATED)
59. Convolutions for Spatial Interaction Modeling. (arXiv:2104.07182v3 [cs.CV] UPDATED)
60. Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure DNN Accelerators. (arXiv:2104.08323v2 [cs.LG] UPDATED)
61. GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathological Image Detection. (arXiv:2104.14528v7 [cs.CV] UPDATED)
62. An Online Learning System for Wireless Charging Alignment using Surround-view Fisheye Cameras. (arXiv:2105.12763v4 [cs.CV] UPDATED)
63. Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset, and Toolkit. (arXiv:2106.14922v2 [astro-ph.IM] UPDATED)
64. PU-Flow: a Point Cloud Upsampling Network with Normalizing Flows. (arXiv:2107.05893v4 [cs.CV] UPDATED)
65. Attribution of Predictive Uncertainties in Classification Models. (arXiv:2107.08756v3 [cs.LG] UPDATED)
66. Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images. (arXiv:2109.10778v2 [cs.CV] UPDATED)
67. EDFace-Celeb-1M: Benchmarking Face Hallucination with a Million-scale Dataset. (arXiv:2110.05031v2 [cs.CV] UPDATED)
68. Practical Galaxy Morphology Tools from Deep Supervised Representation Learning. (arXiv:2110.12735v2 [astro-ph.GA] UPDATED)
69. Learning Pruned Structure and Weights Simultaneously from Scratch: an Attention based Approach. (arXiv:2111.02399v2 [cs.LG] UPDATED)
70. HEAT: Holistic Edge Attention Transformer for Structured Reconstruction. (arXiv:2111.15143v2 [cs.CV] UPDATED)
71. Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides. (arXiv:2112.02222v4 [eess.IV] UPDATED)
72. 3D Hierarchical Refinement and Augmentation for Unsupervised Learning of Depth and Pose from Monocular Video. (arXiv:2112.03045v2 [cs.CV] UPDATED)
73. Smoothness and effective regularizations in learned embeddings for shape matching. (arXiv:2112.07289v2 [cs.CV] UPDATED)
74. General Greedy De-bias Learning. (arXiv:2112.10572v3 [cs.LG] UPDATED)
75. Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v5 [cs.CV] UPDATED)
76. Density Estimation from Schlieren Images through Machine Learning. (arXiv:2201.05233v2 [physics.flu-dyn] UPDATED)
77. Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v4 [cs.CV] UPDATED)
78. Probabilistically Robust Learning: Balancing Average- and Worst-case Performance. (arXiv:2202.01136v3 [cs.LG] UPDATED)
79. Dataset Condensation with Contrastive Signals. (arXiv:2202.02916v2 [cs.CV] UPDATED)
80. MOST-Net: A Memory Oriented Style Transfer Network for Face Sketch Synthesis. (arXiv:2202.03596v2 [cs.CV] UPDATED)
81. How Do Vision Transformers Work?. (arXiv:2202.06709v4 [cs.CV] UPDATED)
82. What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v2 [cs.CV] UPDATED)
83. iPLAN: Interactive and Procedural Layout Planning. (arXiv:2203.14412v2 [cs.CV] UPDATED)
84. Unified Transformer Tracker for Object Tracking. (arXiv:2203.15175v2 [cs.CV] UPDATED)
85. Residual Mixture of Experts. (arXiv:2204.09636v2 [cs.CV] UPDATED)
86. An Iterative Labeling Method for Annotating Fisheries Imagery. (arXiv:2204.12934v2 [cs.LG] UPDATED)
87. Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning. (arXiv:2205.00272v2 [cs.CV] UPDATED)
88. Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning. (arXiv:2205.04363v2 [cs.CV] UPDATED)
89. Real-Time Video Deblurring via Lightweight Motion Compensation. (arXiv:2205.12634v3 [cs.CV] UPDATED)
90. Scalable Interpretability via Polynomials. (arXiv:2205.14108v2 [cs.LG] UPDATED)
91. Neural Basis Models for Interpretability. (arXiv:2205.14120v2 [cs.LG] UPDATED)
92. Point RCNN: An Angle-Free Framework for Rotated Object Detection. (arXiv:2205.14328v2 [cs.CV] UPDATED)
93. Differentiable Point-Based Radiance Fields for Efficient View Synthesis. (arXiv:2205.14330v2 [cs.CV] UPDATED)
94. CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v3 [cs.CV] UPDATED)
95. Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning. (arXiv:2206.02261v2 [cs.CV] UPDATED)
96. Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data. (arXiv:2206.02353v2 [cs.LG] UPDATED)
97. Deep Learning Techniques for Visual Counting. (arXiv:2206.03033v2 [cs.CV] UPDATED)
98. Fast and Robust Non-Rigid Registration Using Accelerated Majorization-Minimization. (arXiv:2206.03410v2 [cs.CV] UPDATED)
## eess.IV
---
**15** new papers in eess.IV:-) 
1. Neural Network Compression via Effective Filter Analysis and Hierarchical Pruning. (arXiv:2206.03596v1 [cs.LG])
2. A new method incorporating deep learning with shape priors for left ventricular segmentation in myocardial perfusion SPECT images. (arXiv:2206.03603v1 [eess.IV])
3. COVIDx CXR-3: A Large-Scale, Open-Source Benchmark Dataset of Chest X-ray Images for Computer-Aided COVID-19 Diagnostics. (arXiv:2206.03671v1 [eess.IV])
4. Robust Deep Ensemble Method for Real-world Image Denoising. (arXiv:2206.03691v1 [cs.CV])
5. What do we learn? Debunking the Myth of Unsupervised Outlier Detection. (arXiv:2206.03698v1 [cs.CV])
6. Hypernetwork-based Personalized Federated Learning for Multi-Institutional CT Imaging. (arXiv:2206.03709v1 [eess.IV])
7. Learning Digital Terrain Models from Point Clouds: ALS2DTM Dataset and Rasterization-based GAN. (arXiv:2206.03778v1 [cs.CV])
8. Dual Windows Are Significant: Learning from Mediastinal Window and Focusing on Lung Window. (arXiv:2206.03803v1 [eess.IV])
9. Generative Myocardial Motion Tracking via Latent Space Exploration with Biomechanics-informed Prior. (arXiv:2206.03830v1 [eess.IV])
10. Progressive GANomaly: Anomaly detection with progressively growing GANs. (arXiv:2206.03876v1 [cs.CV])
11. Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans. (arXiv:2206.03900v1 [eess.IV])
12. Direct Triangulation with Spherical Projection for Omnidirectional Cameras. (arXiv:2206.03928v1 [cs.CV])
13. Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays. (arXiv:2206.03935v1 [eess.IV])
14. An Efficient Target Detection and Recognition Method in Aerial Remote-sensing Images Based on Multiangle Regions-of-Interest. (arXiv:1907.09320v2 [cs.CV] UPDATED)
15. Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides. (arXiv:2112.02222v4 [eess.IV] UPDATED)
## cs.LG
---
**175** new papers in cs.LG:-) 
1. Towards Practical Differential Privacy in Data Analysis: Understanding the Effect of Epsilon on Utility in Private ERM. (arXiv:2206.03488v1 [cs.CR])
2. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v1 [cs.AI])
3. A Privacy-Preserving Subgraph-Level Federated Graph Neural Network via Differential Privacy. (arXiv:2206.03492v1 [cs.CR])
4. DeepCAVE: An Interactive Analysis Tool for Automated Machine Learning. (arXiv:2206.03493v1 [cs.LG])
5. How does overparametrization affect performance on minority groups?. (arXiv:2206.03515v1 [cs.LG])
6. Finite-Time Regret of Thompson Sampling Algorithms for Exponential Family Multi-Armed Bandits. (arXiv:2206.03520v1 [stat.ML])
7. A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v1 [cs.CV])
8. NOMAD: Nonlinear Manifold Decoders for Operator Learning. (arXiv:2206.03551v1 [cs.LG])
9. A generative recommender system with GMM prior for cancer drug generation and sensitivity prediction. (arXiv:2206.03555v1 [cs.LG])
10. Two Ways of Understanding Social Dynamics: Analyzing the Predictability of Emergent of Objects in Reddit r/place Dependent on Locality in Space and Time. (arXiv:2206.03563v1 [physics.soc-ph])
11. Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement Learning with Latent Low-Rank Structure. (arXiv:2206.03569v1 [cs.LG])
12. Certifying Data-Bias Robustness in Linear Regression. (arXiv:2206.03575v1 [cs.LG])
13. Contributor-Aware Defenses Against Adversarial Backdoor Attacks. (arXiv:2206.03583v1 [cs.CR])
14. White-box Membership Attack Against Machine Learning Based Retinopathy Classification. (arXiv:2206.03584v1 [cs.CR])
15. Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation. (arXiv:2206.03588v1 [cs.LG])
16. Click Prediction Boosting via Ensemble Learning Pipelines. (arXiv:2206.03592v1 [cs.LG])
17. Neural Network Compression via Effective Filter Analysis and Hierarchical Pruning. (arXiv:2206.03596v1 [cs.LG])
18. Meta-Learning Transferable Parameterized Skills. (arXiv:2206.03597v1 [cs.LG])
19. One Ring to Bring Them All: Towards Open-Set Recognition under Domain Shift. (arXiv:2206.03600v1 [cs.CV])
20. Decoupled Self-supervised Learning for Non-Homophilous Graphs. (arXiv:2206.03601v1 [cs.LG])
21. Towards Scalable Hyperbolic Neural Networks using Taylor Series Approximations. (arXiv:2206.03610v1 [cs.LG])
22. FedPop: A Bayesian Approach for Personalised Federated Learning. (arXiv:2206.03611v1 [cs.LG])
23. Predictive Modeling of Charge Levels for Battery Electric Vehicles using CNN EfficientNet and IGTD Algorithm. (arXiv:2206.03612v1 [cs.CV])
24. Subject Granular Differential Privacy in Federated Learning. (arXiv:2206.03617v1 [cs.LG])
25. Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping. (arXiv:2206.03633v1 [cs.LG])
26. Network Report: A Structured Description for Network Datasets. (arXiv:2206.03635v1 [cs.SI])
27. Alternately Optimized Graph Neural Networks. (arXiv:2206.03638v1 [cs.LG])
28. Neural Bandit with Arm Group Graph. (arXiv:2206.03644v1 [cs.LG])
29. Solving the Spike Feature Information Vanishing Problem in Spiking Deep Q Network with Potential Based Normalization. (arXiv:2206.03654v1 [cs.NE])
30. pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning. (arXiv:2206.03655v1 [cs.LG])
31. Joint Adversarial Learning for Cross-domain Fair Classification. (arXiv:2206.03656v1 [cs.LG])
32. Scalable Online Disease Diagnosis via Multi-Model-Fused Actor-Critic Reinforcement Learning. (arXiv:2206.03659v1 [cs.LG])
33. Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with Communication Compression. (arXiv:2206.03665v1 [cs.LG])
34. Toward Certified Robustness Against Real-World Distribution Shifts. (arXiv:2206.03669v1 [cs.LG])
35. Integrating Symmetry into Differentiable Planning. (arXiv:2206.03674v1 [cs.LG])
36. Progress Report: A Deep Learning Guided Exploration of Affine Unimodular Loop Transformations. (arXiv:2206.03684v1 [cs.PL])
37. Identifying good directions to escape the NTK regime and efficiently learn low-degree plus sparse polynomials. (arXiv:2206.03688v1 [cs.LG])
38. Autoregressive Perturbations for Data Poisoning. (arXiv:2206.03693v1 [cs.LG])
39. Metric Based Few-Shot Graph Classification. (arXiv:2206.03695v1 [cs.LG])
40. What do we learn? Debunking the Myth of Unsupervised Outlier Detection. (arXiv:2206.03698v1 [cs.CV])
41. Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning. (arXiv:2206.03715v1 [cs.AI])
42. Performance, Transparency and Time. Feature selection to speed up the diagnosis of Parkinson's disease. (arXiv:2206.03716v1 [cs.LG])
43. Latent Boundary-guided Adversarial Training. (arXiv:2206.03717v1 [cs.LG])
44. Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach. (arXiv:2206.03718v1 [cs.LG])
45. Set Interdependence Transformer: Set-to-Sequence Neural Networks for Permutation Learning and Structure Prediction. (arXiv:2206.03720v1 [cs.LG])
46. Stabilizing Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning with Transformer. (arXiv:2206.03721v1 [cs.MA])
47. Hub-Pathway: Transfer Learning from A Hub of Pre-trained Models. (arXiv:2206.03726v1 [cs.LG])
48. On gradient descent training under data augmentation with on-line noisy copies. (arXiv:2206.03734v1 [stat.ML])
49. Motiflets -- Fast and Accurate Detection of Motifs in Time Series. (arXiv:2206.03735v1 [cs.LG])
50. Disentangled Ontology Embedding for Zero-shot Learning. (arXiv:2206.03739v1 [cs.AI])
51. Using Mixed-Effect Models to Learn Bayesian Networks from Related Data Sets. (arXiv:2206.03743v1 [stat.ML])
52. Machine learning-based patient selection in an emergency department. (arXiv:2206.03752v1 [cs.LG])
53. Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v1 [cs.LG])
54. Entropic Convergence of Random Batch Methods for Interacting Particle Diffusion. (arXiv:2206.03792v1 [math.PR])
55. Dual Windows Are Significant: Learning from Mediastinal Window and Focusing on Lung Window. (arXiv:2206.03803v1 [eess.IV])
56. Multi-channel neural networks for predicting influenza A virus hosts and antigenic types. (arXiv:2206.03823v1 [q-bio.QM])
57. Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v1 [cs.LG])
58. $p$-Sparsified Sketches for Fast Multiple Output Kernel Methods. (arXiv:2206.03827v1 [stat.ML])
59. Boosting the Confidence of Generalization for $L_2$-Stable Randomized Learning Algorithms. (arXiv:2206.03834v1 [stat.ML])
60. Scalable Joint Learning of Wireless Multiple-Access Policies and their Signaling. (arXiv:2206.03844v1 [cs.IT])
61. Sim2real for Reinforcement Learning Driven Next Generation Networks. (arXiv:2206.03846v1 [cs.LG])
62. Towards Bridging Algorithm and Theory for Unbiased Recommendation. (arXiv:2206.03851v1 [cs.IR])
63. FEL: High Capacity Learning for Recommendation and Ranking via Federated Ensemble Learning. (arXiv:2206.03852v1 [cs.IR])
64. An Analysis of Selection Bias Issue for Online Advertising. (arXiv:2206.03853v1 [cs.IR])
65. Asymptotic Stability in Reservoir Computing. (arXiv:2206.03854v1 [cs.NE])
66. Decentralized Online Regularized Learning Over Random Time-Varying Graphs. (arXiv:2206.03861v1 [cs.LG])
67. "GAN I hire you?" -- A System for Personalized Virtual Job Interview Training. (arXiv:2206.03869v1 [cs.HC])
68. Efficient Resource Allocation with Fairness Constraints in Restless Multi-Armed Bandits. (arXiv:2206.03883v1 [cs.LG])
69. ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical Image Segmentation. (arXiv:2206.03888v1 [cs.CV])
70. PrivHAR: Recognizing Human Actions From Privacy-preserving Lens. (arXiv:2206.03891v1 [cs.CV])
71. To remove or not remove Mobile Apps? A data-driven predictive model approach. (arXiv:2206.03905v1 [cs.CR])
72. A Unified Convergence Theorem for Stochastic Optimization Methods. (arXiv:2206.03907v1 [math.OC])
73. Learning in games from a stochastic approximation viewpoint. (arXiv:2206.03922v1 [cs.GT])
74. Sequential Density Estimation via NCWFAs Sequential Density Estimation via Nonlinear Continuous Weighted Finite Automata. (arXiv:2206.03923v1 [cs.LG])
75. Boundary between noise and information applied to filtering neural network weight matrices. (arXiv:2206.03927v1 [cond-mat.dis-nn])
76. Few-shot Prompting Toward Controllable Response Generation. (arXiv:2206.03931v1 [cs.CL])
77. TURJUMAN: A Public Toolkit for Neural Arabic Machine Translation. (arXiv:2206.03933v1 [cs.CL])
78. A Study of Continual Learning Methods for Q-Learning. (arXiv:2206.03934v1 [cs.LG])
79. Designing Reinforcement Learning Algorithms for Digital Interventions: Pre-implementation Guidelines. (arXiv:2206.03944v1 [cs.LG])
80. Transfer learning to decode brain states reflecting the relationship between cognitive tasks. (arXiv:2206.03950v1 [q-bio.NC])
81. Mathematical model bridges disparate timescales of lifelong learning. (arXiv:2206.03954v1 [physics.soc-ph])
82. Out-of-Distribution Detection with Class Ratio Estimation. (arXiv:2206.03955v1 [stat.ML])
83. Predict better with less training data using a QNN. (arXiv:2206.03960v1 [quant-ph])
84. FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization. (arXiv:2206.03966v1 [cs.LG])
85. Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v1 [cs.CV])
86. Classification of Stochastic Processes with Topological Data Analysis. (arXiv:2206.03973v1 [stat.ML])
87. Diffusion Curvature for Estimating Local Curvature in High Dimensional Data. (arXiv:2206.03977v1 [cs.LG])
88. How unfair is private learning ?. (arXiv:2206.03985v1 [cs.LG])
89. Data-driven hysteretic behavior simulation based on weighted stacked pyramid neural network architecture. (arXiv:2206.03990v1 [cs.LG])
90. Neural Diffusion Processes. (arXiv:2206.03992v1 [stat.ML])
91. Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning. (arXiv:2206.03996v1 [cs.LG])
92. Patch-based Object-centric Transformers for Efficient Video Generation. (arXiv:2206.04003v1 [cs.CV])
93. Few-Shot Audio-Visual Learning of Environment Acoustics. (arXiv:2206.04006v1 [cs.SD])
94. Robust Semantic Communications with Masked VQ-VAE Enabled Codebook. (arXiv:2206.04011v1 [eess.SP])
95. Improving trajectory calculations using deep learning inspired single image superresolution. (arXiv:2206.04015v1 [physics.ao-ph])
96. SYNERgy between SYNaptic consolidation and Experience Replay for general continual learning. (arXiv:2206.04016v1 [cs.NE])
97. Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v1 [cs.CV])
98. High-dimensional limit theorems for SGD: Effective dynamics and critical scaling. (arXiv:2206.04030v1 [stat.ML])
99. Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting. (arXiv:2206.04038v1 [cs.LG])
100. Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v1 [cs.CY])
101. Neural Collapse: A Review on Modelling Principles and Generalization. (arXiv:2206.04041v1 [cs.LG])
102. Model-Based Reinforcement Learning Is Minimax-Optimal for Offline Zero-Sum Markov Games. (arXiv:2206.04044v1 [cs.LG])
103. STable: Table Generation Framework for Encoder-Decoder Models. (arXiv:2206.04045v1 [cs.CL])
104. Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v1 [cs.CV])
105. Blacklight: Defending Black-Box Adversarial Attacks on Deep Neural Networks. (arXiv:2006.14042v2 [cs.CR] UPDATED)
106. Automatic Personality Prediction; an Enhanced Method Using Ensemble Modeling. (arXiv:2007.04571v3 [cs.CL] UPDATED)
107. A Two-Timescale Framework for Bilevel Optimization: Complexity Analysis and Application to Actor-Critic. (arXiv:2007.05170v4 [math.OC] UPDATED)
108. Improve Generalization and Robustness of Neural Networks via Weight Scale Shifting Invariant Regularizations. (arXiv:2008.02965v2 [cs.LG] UPDATED)
109. Dissipative Deep Neural Dynamical Systems. (arXiv:2011.13492v3 [cs.LG] UPDATED)
110. Causal inference for observational longitudinal studies using deep survival models. (arXiv:2101.10643v12 [stat.ML] UPDATED)
111. COVIDHunter: An Accurate, Flexible, and Environment-Aware Open-Source COVID-19 Outbreak Simulation Model. (arXiv:2102.03667v2 [q-bio.PE] UPDATED)
112. Fairness-Aware PAC Learning from Corrupted Data. (arXiv:2102.06004v3 [cs.LG] UPDATED)
113. Hybrid Physics and Deep Learning Model for Interpretable Vehicle State Prediction. (arXiv:2103.06727v3 [cs.LG] UPDATED)
114. Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v4 [cs.CV] UPDATED)
115. Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure DNN Accelerators. (arXiv:2104.08323v2 [cs.LG] UPDATED)
116. Mapping the Internet: Modelling Entity Interactions in Complex Heterogeneous Networks. (arXiv:2104.09650v2 [cs.LG] UPDATED)
117. Principal Components Bias in Over-parameterized Linear Models, and its Manifestation in Deep Neural Networks. (arXiv:2105.05553v8 [cs.LG] UPDATED)
118. SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v2 [cs.IR] UPDATED)
119. Inverse Contextual Bandits: Learning How Behavior Evolves over Time. (arXiv:2107.06317v3 [cs.LG] UPDATED)
120. Attribution of Predictive Uncertainties in Classification Models. (arXiv:2107.08756v3 [cs.LG] UPDATED)
121. Geometry of Linear Convolutional Networks. (arXiv:2108.01538v2 [cs.LG] UPDATED)
122. Anarchic Federated Learning. (arXiv:2108.09875v3 [cs.LG] UPDATED)
123. Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v2 [cs.CL] UPDATED)
124. Predicting Census Survey Response Rates via Interpretable Nonparametric Additive Models with Structured Interactions. (arXiv:2108.11328v2 [stat.ML] UPDATED)
125. Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images. (arXiv:2109.10778v2 [cs.CV] UPDATED)
126. Federated Learning Algorithms for Generalized Mixed-effects Model (GLMM) on Horizontally Partitioned Data from Distributed Sources. (arXiv:2109.14046v2 [stat.ML] UPDATED)
127. One-Bit Matrix Completion with Differential Privacy. (arXiv:2110.00719v4 [cs.CR] UPDATED)
128. FedSEAL: Semi-Supervised Federated Learning with Self-Ensemble Learning and Negative Learning. (arXiv:2110.07829v2 [cs.LG] UPDATED)
129. Option Transfer and SMDP Abstraction with Successor Features. (arXiv:2110.09196v2 [cs.LG] UPDATED)
130. Deeper-GXX: Deepening Arbitrary GNNs. (arXiv:2110.13798v2 [cs.LG] UPDATED)
131. Learning Pruned Structure and Weights Simultaneously from Scratch: an Attention based Approach. (arXiv:2111.02399v2 [cs.LG] UPDATED)
132. Model-Free $\mu$ Synthesis via Adversarial Reinforcement Learning. (arXiv:2111.15537v2 [cs.LG] UPDATED)
133. General Greedy De-bias Learning. (arXiv:2112.10572v3 [cs.LG] UPDATED)
134. Machine-Learning the Classification of Spacetimes. (arXiv:2201.01644v2 [gr-qc] UPDATED)
135. Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v4 [cs.CV] UPDATED)
136. Error Rates for Kernel Classification under Source and Capacity Conditions. (arXiv:2201.12655v2 [stat.ML] UPDATED)
137. Probabilistically Robust Learning: Balancing Average- and Worst-case Performance. (arXiv:2202.01136v3 [cs.LG] UPDATED)
138. Selective Network Linearization for Efficient Private Inference. (arXiv:2202.02340v2 [cs.CR] UPDATED)
139. Dataset Condensation with Contrastive Signals. (arXiv:2202.02916v2 [cs.CV] UPDATED)
140. Structure-Aware Transformer for Graph Representation Learning. (arXiv:2202.03036v2 [stat.ML] UPDATED)
141. Stop Oversampling for Class Imbalance Learning: A Critical Review. (arXiv:2202.03579v2 [cs.LG] UPDATED)
142. How Do Vision Transformers Work?. (arXiv:2202.06709v4 [cs.CV] UPDATED)
143. Between Stochastic and Adversarial Online Convex Optimization: Improved Regret Bounds via Smoothness. (arXiv:2202.07554v2 [cs.LG] UPDATED)
144. Inferring Lexicographically-Ordered Rewards from Preferences. (arXiv:2202.10153v2 [cs.LG] UPDATED)
145. Decentralized Safe Multi-agent Stochastic Optimal Control using Deep FBSDEs and ADMM. (arXiv:2202.10658v2 [cs.MA] UPDATED)
146. Generative modeling via tensor train sketching. (arXiv:2202.11788v3 [math.NA] UPDATED)
147. An Information-Theoretic Framework for Supervised Learning. (arXiv:2203.00246v5 [cs.LG] UPDATED)
148. A Primal-Dual Approach to Bilevel Optimization with Multiple Inner Minima. (arXiv:2203.01123v2 [math.OC] UPDATED)
149. What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v2 [cs.CV] UPDATED)
150. Quantum continual learning of quantum data realizing knowledge backward transfer. (arXiv:2203.14032v2 [quant-ph] UPDATED)
151. Decision-Focused Learning without Decision-Making: Learning Locally Optimized Decision Losses. (arXiv:2203.16067v2 [cs.LG] UPDATED)
152. Continuous LWE is as Hard as LWE & Applications to Learning Gaussian Mixtures. (arXiv:2204.02550v2 [cs.CR] UPDATED)
153. An Iterative Labeling Method for Annotating Fisheries Imagery. (arXiv:2204.12934v2 [cs.LG] UPDATED)
154. Poisoning Deep Learning Based Recommender Model in Federated Learning Scenarios. (arXiv:2204.13594v2 [cs.IR] UPDATED)
155. Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning. (arXiv:2205.04363v2 [cs.CV] UPDATED)
156. Generic and Trend-aware Curriculum Learning for Relation Extraction in Graph Neural Networks. (arXiv:2205.08625v2 [cs.CL] UPDATED)
157. The Sufficiency of Off-policyness: PPO is insufficient according an Off-policy Measure. (arXiv:2205.10047v2 [cs.LG] UPDATED)
158. Diversity vs. Recognizability: Human-like generalization in one-shot generative models. (arXiv:2205.10370v2 [cs.AI] UPDATED)
159. Standalone Neural ODEs with Sensitivity Analysis. (arXiv:2205.13933v2 [cs.LG] UPDATED)
160. Scalable Interpretability via Polynomials. (arXiv:2205.14108v2 [cs.LG] UPDATED)
161. Neural Basis Models for Interpretability. (arXiv:2205.14120v2 [cs.LG] UPDATED)
162. Experience report of physics-informed neural networks in fluid simulations: pitfalls and frustration. (arXiv:2205.14249v2 [physics.flu-dyn] UPDATED)
163. Modeling Disagreement in Automatic Data Labelling for Semi-Supervised Learning in Clinical Natural Language Processing. (arXiv:2205.14761v2 [cs.LG] UPDATED)
164. Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models. (arXiv:2205.15223v2 [cs.CL] UPDATED)
165. k-Means Maximum Entropy Exploration. (arXiv:2205.15623v3 [cs.LG] UPDATED)
166. Model Generation with Provable Coverability for Offline Reinforcement Learning. (arXiv:2206.00316v3 [cs.LG] UPDATED)
167. Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions. (arXiv:2206.01255v2 [math.NA] UPDATED)
168. Optimal Weak to Strong Learning. (arXiv:2206.01563v2 [cs.LG] UPDATED)
169. A Learning-Based Method for Automatic Operator Selection in the Fanoos XAI System. (arXiv:2206.01722v2 [cs.LG] UPDATED)
170. Beyond Value: CHECKLIST for Testing Inferences in Planning-Based RL. (arXiv:2206.02039v2 [cs.AI] UPDATED)
171. Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning. (arXiv:2206.02261v2 [cs.CV] UPDATED)
172. Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data. (arXiv:2206.02353v2 [cs.LG] UPDATED)
173. Spam Detection Using BERT. (arXiv:2206.02443v2 [cs.CR] UPDATED)
174. Risk Measures and Upper Probabilities: Coherence and Stratification. (arXiv:2206.03183v2 [cs.LG] UPDATED)
175. Goal-Space Planning with Subgoal Models. (arXiv:2206.02902v2 [cs.LG] CROSS LISTED)
## cs.AI
---
**69** new papers in cs.AI:-) 
1. Formalization of the principles of brain Programming (Brain Principles Programming). (arXiv:2206.03487v1 [cs.AI])
2. Towards Practical Differential Privacy in Data Analysis: Understanding the Effect of Epsilon on Utility in Private ERM. (arXiv:2206.03488v1 [cs.CR])
3. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v1 [cs.AI])
4. A Privacy-Preserving Subgraph-Level Federated Graph Neural Network via Differential Privacy. (arXiv:2206.03492v1 [cs.CR])
5. A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v1 [cs.CV])
6. Code-DKT: A Code-based Knowledge Tracing Model for Programming Tasks. (arXiv:2206.03545v1 [cs.SE])
7. Guidelines and a Corpus for Extracting Biographical Events. (arXiv:2206.03547v1 [cs.CL])
8. A generative recommender system with GMM prior for cancer drug generation and sensitivity prediction. (arXiv:2206.03555v1 [cs.LG])
9. Compromised account detection using authorship verification: a novel approach. (arXiv:2206.03581v1 [cs.CR])
10. Contributor-Aware Defenses Against Adversarial Backdoor Attacks. (arXiv:2206.03583v1 [cs.CR])
11. White-box Membership Attack Against Machine Learning Based Retinopathy Classification. (arXiv:2206.03584v1 [cs.CR])
12. XAI for Cybersecurity: State of the Art, Challenges, Open Issues and Future Directions. (arXiv:2206.03585v1 [cs.CR])
13. ObPose: Leveraging Canonical Pose for Object-Centric Scene Inference in 3D. (arXiv:2206.03591v1 [cs.CV])
14. Meta-Learning Transferable Parameterized Skills. (arXiv:2206.03597v1 [cs.LG])
15. Predictive Modeling of Charge Levels for Battery Electric Vehicles using CNN EfficientNet and IGTD Algorithm. (arXiv:2206.03612v1 [cs.CV])
16. Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping. (arXiv:2206.03633v1 [cs.LG])
17. Solving the Spike Feature Information Vanishing Problem in Spiking Deep Q Network with Potential Based Normalization. (arXiv:2206.03654v1 [cs.NE])
18. Joint Adversarial Learning for Cross-domain Fair Classification. (arXiv:2206.03656v1 [cs.LG])
19. Toward Certified Robustness Against Real-World Distribution Shifts. (arXiv:2206.03669v1 [cs.LG])
20. Integrating Symmetry into Differentiable Planning. (arXiv:2206.03674v1 [cs.LG])
21. Metric Based Few-Shot Graph Classification. (arXiv:2206.03695v1 [cs.LG])
22. Sampling-based techniques for designing school boundaries. (arXiv:2206.03703v1 [cs.AI])
23. Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning. (arXiv:2206.03715v1 [cs.AI])
24. Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach. (arXiv:2206.03718v1 [cs.LG])
25. Motiflets -- Fast and Accurate Detection of Motifs in Time Series. (arXiv:2206.03735v1 [cs.LG])
26. Disentangled Ontology Embedding for Zero-shot Learning. (arXiv:2206.03739v1 [cs.AI])
27. Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v1 [cs.LG])
28. SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v1 [cs.CV])
29. Scalable Joint Learning of Wireless Multiple-Access Policies and their Signaling. (arXiv:2206.03844v1 [cs.IT])
30. Fault-Aware Neural Code Rankers. (arXiv:2206.03865v1 [cs.PL])
31. Sequential Density Estimation via NCWFAs Sequential Density Estimation via Nonlinear Continuous Weighted Finite Automata. (arXiv:2206.03923v1 [cs.LG])
32. Few-shot Prompting Toward Controllable Response Generation. (arXiv:2206.03931v1 [cs.CL])
33. TURJUMAN: A Public Toolkit for Neural Arabic Machine Translation. (arXiv:2206.03933v1 [cs.CL])
34. Designing Reinforcement Learning Algorithms for Digital Interventions: Pre-implementation Guidelines. (arXiv:2206.03944v1 [cs.LG])
35. Transfer learning to decode brain states reflecting the relationship between cognitive tasks. (arXiv:2206.03950v1 [q-bio.NC])
36. Combining Monte-Carlo Tree Search with Proof-Number Search. (arXiv:2206.03965v1 [cs.AI])
37. Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v1 [cs.CV])
38. SYNERgy between SYNaptic consolidation and Experience Replay for general continual learning. (arXiv:2206.04016v1 [cs.NE])
39. Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v1 [cs.CY])
40. Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v1 [cs.CV])
41. A multiple criteria methodology for priority based portfolio selection. (arXiv:1812.10410v4 [cs.CY] UPDATED)
42. Automatic Personality Prediction; an Enhanced Method Using Ensemble Modeling. (arXiv:2007.04571v3 [cs.CL] UPDATED)
43. Causal inference for observational longitudinal studies using deep survival models. (arXiv:2101.10643v12 [stat.ML] UPDATED)
44. Human-AI Interactions in Public Sector Decision-Making: "Automation Bias" and "Selective Adherence" to Algorithmic Advice. (arXiv:2103.02381v3 [cs.HC] UPDATED)
45. Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings. (arXiv:2106.14361v2 [cs.CL] UPDATED)
46. SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v2 [cs.IR] UPDATED)
47. Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v2 [cs.CL] UPDATED)
48. Risk-averse autonomous systems: A brief history and recent developments from the perspective of optimal control. (arXiv:2109.08947v2 [cs.AI] UPDATED)
49. Option Transfer and SMDP Abstraction with Successor Features. (arXiv:2110.09196v2 [cs.LG] UPDATED)
50. Towards artificial general intelligence via a multimodal foundation model. (arXiv:2110.14378v2 [cs.AI] UPDATED)
51. Cross-language Information Retrieval. (arXiv:2111.05988v2 [cs.IR] UPDATED)
52. Stop Oversampling for Class Imbalance Learning: A Critical Review. (arXiv:2202.03579v2 [cs.LG] UPDATED)
53. MOST-Net: A Memory Oriented Style Transfer Network for Face Sketch Synthesis. (arXiv:2202.03596v2 [cs.CV] UPDATED)
54. Inferring Lexicographically-Ordered Rewards from Preferences. (arXiv:2202.10153v2 [cs.LG] UPDATED)
55. An Information-Theoretic Framework for Supervised Learning. (arXiv:2203.00246v5 [cs.LG] UPDATED)
56. Scalable Verification of GNN-based Job Schedulers. (arXiv:2203.03153v3 [cs.AI] UPDATED)
57. What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v2 [cs.CV] UPDATED)
58. Decision-Focused Learning without Decision-Making: Learning Locally Optimized Decision Losses. (arXiv:2203.16067v2 [cs.LG] UPDATED)
59. Graph Enhanced Contrastive Learning for Radiology Findings Summarization. (arXiv:2204.00203v2 [cs.CL] UPDATED)
60. Disappeared Command: Spoofing Attack On Automatic Speech Recognition Systems with Sound Masking. (arXiv:2204.08977v2 [cs.SD] UPDATED)
61. Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning. (arXiv:2205.04363v2 [cs.CV] UPDATED)
62. Generic and Trend-aware Curriculum Learning for Relation Extraction in Graph Neural Networks. (arXiv:2205.08625v2 [cs.CL] UPDATED)
63. Diversity vs. Recognizability: Human-like generalization in one-shot generative models. (arXiv:2205.10370v2 [cs.AI] UPDATED)
64. Experience report of physics-informed neural networks in fluid simulations: pitfalls and frustration. (arXiv:2205.14249v2 [physics.flu-dyn] UPDATED)
65. A Learning-Based Method for Automatic Operator Selection in the Fanoos XAI System. (arXiv:2206.01722v2 [cs.LG] UPDATED)
66. CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v3 [cs.CV] UPDATED)
67. Beyond Value: CHECKLIST for Testing Inferences in Planning-Based RL. (arXiv:2206.02039v2 [cs.AI] UPDATED)
68. TSFEDL: A Python Library for Time Series Spatio-Temporal Feature Extraction and Prediction using Deep Learning (with Appendices on Detailed Network Architectures and Experimental Cases of Study). (arXiv:2206.03179v2 [cs.NE] UPDATED)
69. Goal-Space Planning with Subgoal Models. (arXiv:2206.02902v2 [cs.LG] CROSS LISTED)

