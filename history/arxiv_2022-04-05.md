# Your interest papers
---
## cs.CV
---
### Bayesian Image Super-Resolution with Deep Modeling of Image Statistics. (arXiv:2204.00623v1 [eess.IV])
- Authors : Shangqi Gao, Xiahai Zhuang
- Link : [http://arxiv.org/abs/2204.00623](http://arxiv.org/abs/2204.00623)
> ABSTRACT  :  Modeling statistics of image priors is useful for image super-resolution, but little attention has been paid from the massive works of deep learning-based methods. In this work, we propose a Bayesian image **restoration** framework, where natural image statistics are modeled with the combination of smoothness and sparsity priors. Concretely, firstly we consider an ideal image as the sum of a smoothness component and a sparsity residual, and model real image degradation including blurring, downscaling, and noise corruption. Then, we develop a variational Bayesian approach to infer their posteriors. Finally, we implement the variational approach for single image super-resolution (SISR) using deep neural networks, and propose an unsupervised training strategy. The experiments on three image **restoration** tasks, \textit{i.e.,} ideal SISR, realistic SISR, and real-world SISR, demonstrate that our method has superior model generalizability against varying noise levels and degradation kernels and is effective in unsupervised SISR. The code and resulting models are released via \url{https://zmiclab.github.io/projects.html}.  
### Extremely **Low-light** Image **Enhancement** with Scene Text **Restoration**. (arXiv:2204.00630v1 [eess.IV])
- Authors : Pohao Hsu, Tsung Lin, Chun Chet, Long Kew, Mei Yih, Hong Lai, Chee Seng, Christopher Zach
- Link : [http://arxiv.org/abs/2204.00630](http://arxiv.org/abs/2204.00630)
> ABSTRACT  :  Deep learning-based methods have made impressive progress in enhancing extremely **low-light** images - the image quality of the reconstructed images has generally improved. However, we found out that most of these methods could not sufficiently recover the image details, for instance, the texts in the scene. In this paper, a novel image **enhancement** framework is proposed to precisely restore the scene texts, as well as the overall quality of the image simultaneously under extremely **low-light** images conditions. Mainly, we employed a self-regularised attention map, an edge map, and a novel text detection loss. In addition, leveraging synthetic **low-light** images is beneficial for image **enhancement** on the genuine ones in terms of text detection. The quantitative and qualitative experimental results have shown that the proposed model outperforms state-of-the-art methods in image **restoration**, text detection, and text spotting on See In the **Dark** and ICDAR15 datasets.  
### UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])
- Authors : Ali Hatamizadeh, Ziyue Xu, Dong Yang, Wenqi Li, Holger Roth, Daguang Xu
- Link : [http://arxiv.org/abs/2204.00631](http://arxiv.org/abs/2204.00631)
> ABSTRACT  :  Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D **Swin** Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions  
### Robust Neonatal Face Detection in Real-world Clinical Settings. (arXiv:2204.00655v1 [cs.CV])
- Authors : Jacqueline Hausmann, Md Sirajus, Ghada Zamzmi, Dmitry Goldgof, Yu Sun
- Link : [http://arxiv.org/abs/2204.00655](http://arxiv.org/abs/2204.00655)
> ABSTRACT  :  Current face detection algorithms are extremely generalized and can obtain decent accuracy when detecting the adult faces. These approaches are insufficient when handling outlier cases, for example when trying to detect the face of a neonate infant whose face composition and expressions are relatively different than that of the adult. It is furthermore difficult when applied to detect faces in a complicated setting such as the Neonate Intensive Care Unit. By training a state-of-the-art face detection model, You-Only-Look-Once, on a proprietary dataset containing labelled neonate faces in a clinical setting, this work achieves near **real time** neonate face detection. Our preliminary findings show an accuracy of 68.7%, compared to the off the shelf solution which detected neonate faces with an accuracy of 7.37%. Although further experiments are needed to validate our model, our results are promising and prove the feasibility of detecting neonatal faces in challenging real-world settings. The robust and real-time detection of neonatal faces would benefit wide range of automated systems (e.g., pain recognition and surveillance) who currently suffer from the time and effort due to the necessity of manual annotations. To benefit the research community, we make our trained weights publicly available at github(https://github.com/ja05haus/trained_neonate_face).  
### What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions. (arXiv:2204.00746v1 [cs.CV])
- Authors : Hao Chen, Kaustav Kundu, Xinyu Li, Joseph Tighe, Davide Modolo
- Link : [http://arxiv.org/abs/2204.00746](http://arxiv.org/abs/2204.00746)
> ABSTRACT  :  We propose a novel one-stage Transformer-based semantic and spatial refined transformer (SSRT) to solve the Human-Object Interaction detection task, which requires to localize humans and objects, and predicts their interactions. Differently from previous Transformer-based HOI approaches, which mostly focus at improving the design of the decoder outputs for the final detection, SSRT introduces two new modules to help select the most relevant object-action pairs within an image and refine the queries' representation using rich semantic and spatial features. These **enhancement**s lead to state-of-the-art results on the two most popular HOI benchmarks: V-COCO and HICO-DET.  
### Sin**NeRF**: Training Neural Radiance Fields on Complex Scenes from a Single Image. (arXiv:2204.00928v1 [cs.CV])
- Authors : Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, Zhangyang Wang
- Link : [http://arxiv.org/abs/2204.00928](http://arxiv.org/abs/2204.00928)
> ABSTRACT  :  Despite the rapid development of Neural Radiance Field (**NeRF**), the necessity of dense covers largely prohibits its wider applications. While several recent works have attempted to address this issue, they either operate with sparse views (yet still, a few of them) or on simple objects/scenes. In this work, we consider a more ambitious task: training neural radiance field, over realistically complex visual scenes, by "looking only once", i.e., using only a single view. To attain this goal, we present a Single View **NeRF** (Sin**NeRF**) framework consisting of thoughtfully designed semantic and geometry regularizations. Specifically, Sin**NeRF** constructs a semi-supervised learning process, where we introduce and propagate geometry pseudo labels and semantic pseudo labels to guide the progressive training process. Extensive experiments are conducted on complex scene benchmarks, including **NeRF** synthetic dataset, Local Light Field Fusion dataset, and DTU dataset. We show that even without pre-training on multi-view datasets, Sin**NeRF** can yield photo-realistic novel-view synthesis results. Under the single image setting, Sin**NeRF** significantly outperforms the current state-of-the-art **NeRF** baselines in all cases. Project page: https://vita-group.github.io/Sin**NeRF**/  
### Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v1 [cs.CV])
- Authors : Jiawang Bai, Li Yuan, Tao Xia, Shuicheng Yan, Zhifeng Li, Wei Liu
- Link : [http://arxiv.org/abs/2204.00993](http://arxiv.org/abs/2204.00993)
> ABSTRACT  :  The transformer models have shown promising effectiveness in dealing with various vision tasks. However, compared with training Convolutional Neural Network (CNN) models, training Vision Transformer (ViT) models is more difficult and relies on the large-scale training set. To explain this observation we make a hypothesis that ViT models are less effective in capturing the high-frequency components of images than CNN models, and verify it by a frequency analysis. Inspired by this finding, we first investigate the effects of existing techniques for improving ViT models from a new frequency perspective, and find that the success of some techniques (e.g., RandAugment) can be attributed to the better usage of the high-frequency components. Then, to compensate for this insufficient ability of ViT models, we propose HAT, which directly augments high-frequency components of images via adversarial training. We show that HAT can consistently boost the performance of various ViT models (e.g., +1.2% for ViT-B, +0.5% for **Swin**-B), and especially enhance the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the superiority can also be maintained on out-of-distribution data and transferred to downstream tasks.  
### Adjusting for Bias with Procedural Data. (arXiv:2204.01108v1 [cs.CV])
- Authors : Shesh Narayan, Nicholas Bear
- Link : [http://arxiv.org/abs/2204.01108](http://arxiv.org/abs/2204.01108)
> ABSTRACT  :  3D softwares are now capable of producing highly realistic images that look nearly indistinguishable from the real images. This raises the question: can real datasets be enhanced with 3D rendered data? We investigate this question. In this paper we demonstrate the use of 3D rendered data, procedural, data for the adjustment of bias in image datasets. We perform error analysis of images of animals which shows that the misclassification of some animal breeds is largely a data issue. We then create procedural images of the poorly classified breeds and that model further trained on procedural data can better classify poorly performing breeds on real data. We believe that this approach can be used for the **enhancement** of visual data for any underrepresented group, including rare diseases, or any data bias potentially improving the accuracy and fairness of models. We find that the resulting representations rival or even out-perform those learned directly from real data, but that good performance requires care in the 3D rendered procedural data generation. 3D image dataset can be viewed as a compressed and organized copy of a real dataset, and we envision a future where more and more procedural data proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future.  
### Dynamic Focus-aware Positional Queries for Semantic Segmentation. (arXiv:2204.01244v1 [cs.CV])
- Authors : Haoyu He, Jianfei Cai, Zizheng Pan, Jing Liu, Jing Zhang, Dacheng Tao, Bohan Zhuang
- Link : [http://arxiv.org/abs/2204.01244](http://arxiv.org/abs/2204.01244)
> ABSTRACT  :  Most of the latest top semantic segmentation approaches are based on vision Transformers, particularly DETR-like frameworks, which employ a set of queries in the Transformer decoder. Each query is composed of a content query that preserves semantic information and a positional query that provides positional guidance for aggregating the query-specific context. However, the positional queries in the Transformer decoder layers are typically represented as fixed learnable weights, which often encode dataset statistics for segments and can be inaccurate for individual samples. Therefore, in this paper, we propose to generate positional queries dynamically conditioned on the cross-attention scores and the localization information of the preceding layer. By doing so, each query is aware of its previous focus, thus providing more accurate positional guidance and encouraging the cross-attention consistency across the decoder layers. In addition, we also propose an efficient way to deal with high-resolution cross-attention by dynamically determining the contextual tokens based on the low-resolution cross-attention maps to perform local relation aggregation. Our overall framework termed FASeg (Focus-Aware semantic Segmentation) provides a simple yet effective solution for semantic segmentation. Extensive experiments on ADE20K and Cityscapes show that our FASeg achieves state-of-the-art performance, e.g., obtaining 48.3% and 49.6% mIoU respectively for single-scale inference on ADE20K validation set with ResNet-50 and **Swin**-T backbones, and barely increases the computation consumption from Mask2former. Source code will be made publicly available at https://github.com/zip-group/FASeg.  
### Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer. (arXiv:2109.08024v2 [cs.CV] UPDATED)
- Authors : Fushun Zhu, Shan Zhao, Peng Wang, Hao Wang, Hua Yan, Shuaicheng Liu
- Link : [http://arxiv.org/abs/2109.08024](http://arxiv.org/abs/2109.08024)
> ABSTRACT  :  We propose a semi-supervised network for wide-angle portraits correction. Wide-angle images often suffer from skew and distortion affected by perspective distortion, especially noticeable at the face regions. Previous deep learning based approaches need the ground-truth correction flow maps for training guidance. However, such labels are expensive, which can only be obtained manually. In this work, we design a semi-supervised scheme and build a high-quality unlabeled dataset with rich scenarios, allowing us to simultaneously use labeled and unlabeled data to improve performance. Specifically, our semi-supervised scheme takes advantage of the consistency mechanism, with several novel components such as direction and range consistency (DRC) and regression consistency (RC). Furthermore, different from the existing methods, we propose the Multi-Scale **Swin**-Unet (MS-Unet) based on the multi-scale swin transformer block (MSTB), which can simultaneously learn short-distance and long-distance information to avoid artifacts. Extensive experiments demonstrate that the proposed method is superior to the state-of-the-art methods and other representative baselines. The source code and dataset are available at: https://github.com/megvii-research/Portraits_Correction.  
### MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v2 [eess.IV] UPDATED)
- Authors : Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, **Peyman Milanfar**, Alan Bovik, Yinxiao Li
- Link : [http://arxiv.org/abs/2201.02973](http://arxiv.org/abs/2201.02973)
> ABSTRACT  :  Recent progress on Transformers and multi-layer perceptron (MLP) models provide new network architectural designs for computer vision tasks. Although these models proved to be effective in many vision tasks such as image recognition, there remain challenges in adapting them for low-level vision. The inflexibility to support high-resolution images and limitations of local attention are perhaps the main bottlenecks. In this work, we present a multi-axis MLP based architecture called MAXIM, that can serve as an efficient and flexible general-purpose vision backbone for image processing tasks. MAXIM uses a UNet-shaped hierarchical structure and supports long-range interactions enabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based building blocks: a multi-axis gated MLP that allows for efficient and scalable spatial mixing of local and global visual cues, and a cross-gating block, an alternative to cross-attention, which accounts for cross-feature conditioning. Both these modules are exclusively based on MLPs, but also benefit from being both global and `fully-convolutional', two properties that are desirable for image processing. Our extensive experimental results show that the proposed MAXIM model achieves state-of-the-art performance on more than ten benchmarks across a range of image processing tasks, including denoising, deblurring, deraining, dehazing, and **enhancement** while requiring fewer or comparable numbers of parameters and FLOPs than competitive models. The source code and trained models will be available at \url{https://github.com/google-research/maxim}.  
### Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v2 [eess.IV] UPDATED)
- Authors : Jiahao Huang, Yinzhe Wu, Huanjun Wu, Guang Yang
- Link : [http://arxiv.org/abs/2201.09400](http://arxiv.org/abs/2201.09400)
> ABSTRACT  :  Magnetic resonance imaging (MRI) is a widely used non-radiative and non-invasive method for clinical interrogation of organ structures and metabolism, with an inherently long scanning time. Methods by k-space undersampling and deep learning based reconstruction have been popularised to accelerate the scanning process. This work focuses on investigating how powerful transformers are for fast MRI by exploiting and comparing different novel network architectures. In particular, a generative adversarial network (GAN) based **Swin** transformer (ST-GAN) was introduced for the fast MRI reconstruction. To further preserve the edge and texture information, edge enhanced GAN based **Swin** transformer (EES-GAN) and texture enhanced GAN based **Swin** transformer (TES-GAN) were also developed, where a dual-discriminator GAN structure was applied. We compared our proposed GAN based transformers, standalone **Swin** transformer and other convolutional neural networks based GAN model in terms of the evaluation metrics PSNR, SSIM and FID. We showed that transformers work well for the MRI reconstruction from different undersampling conditions. The utilisation of GAN's adversarial structure improves the quality of images reconstructed when undersampled for 30% or higher. The code is publicly available at https://github.com/ayanglab/**Swin**GANMR.  
### Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v4 [cs.CV] UPDATED)
- Authors : Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, Jian Sun
- Link : [http://arxiv.org/abs/2203.06717](http://arxiv.org/abs/2203.06717)
> ABSTRACT  :  We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than **Swin** Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code &amp; models at https://github.com/megvii-research/RepLKNet.  
### Rethinking Semantic Segmentation: A Prototype View. (arXiv:2203.15102v2 [cs.CV] UPDATED)
- Authors : Tianfei Zhou, Wenguan Wang, Ender Konukoglu, Luc Van
- Link : [http://arxiv.org/abs/2203.15102](http://arxiv.org/abs/2203.15102)
> ABSTRACT  :  Prevalent semantic segmentation solutions, despite their different network designs (FCN based or attention based) and mask decoding strategies (parametric softmax based or pixel-query based), can be placed in one category, by considering the softmax weights or query vectors as learnable class prototypes. In light of this prototype view, this study uncovers several limitations of such parametric segmentation regime, and proposes a nonparametric alternative based on non-learnable prototypes. Instead of prior methods learning a single weight/query vector for each class in a fully parametric manner, our model represents each class as a set of non-learnable prototypes, relying solely on the mean features of several training pixels within that class. The dense prediction is thus achieved by nonparametric nearest prototype retrieving. This allows our model to directly shape the pixel embedding space, by optimizing the arrangement between embedded pixels and anchored prototypes. It is able to handle arbitrary number of classes with a constant amount of learnable parameters. We empirically show that, with FCN based and attention based segmentation models (i.e., HRNet, **Swin**, SegFormer) and backbones (i.e., ResNet, HRNet, **Swin**, MiT), our nonparametric framework yields compelling results over several datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in the large-vocabulary situation. We expect this work will provoke a rethink of the current de facto semantic segmentation model design.  
### DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v2 [cs.CV] UPDATED)
- Authors : Shuai Chen, Xinghui Li, Zirui Wang, Victor Adrian
- Link : [http://arxiv.org/abs/2204.00559](http://arxiv.org/abs/2204.00559)
> ABSTRACT  :  We introduce a camera relocalization pipeline that combines absolute pose regression (APR) and direct feature matching. Existing photometric-based methods have trouble on scenes with large photometric distortions, e.g. outdoor environments. By incorporating an **exposure**-adaptive novel view synthesis, our methods can successfully address the challenges. Moreover, by introducing domain-invariant feature matching, our solution can improve pose regression accuracy while using semi-supervised learning on unlabeled data. In particular, the pipeline consists of two components, Novel View Synthesizer and FeatureNet (DFNet). The former synthesizes novel views compensating for changes in **exposure** and the latter regresses camera poses and extracts robust features that bridge the domain gap between real images and synthetic ones. We show that domain invariant feature matching effectively enhances camera pose estimation both in indoor and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by outperforming existing single-image APR methods by as much as 56%, comparable to 3D structure-based methods.  
## eess.IV
---
### Bayesian Image Super-Resolution with Deep Modeling of Image Statistics. (arXiv:2204.00623v1 [eess.IV])
- Authors : Shangqi Gao, Xiahai Zhuang
- Link : [http://arxiv.org/abs/2204.00623](http://arxiv.org/abs/2204.00623)
> ABSTRACT  :  Modeling statistics of image priors is useful for image super-resolution, but little attention has been paid from the massive works of deep learning-based methods. In this work, we propose a Bayesian image **restoration** framework, where natural image statistics are modeled with the combination of smoothness and sparsity priors. Concretely, firstly we consider an ideal image as the sum of a smoothness component and a sparsity residual, and model real image degradation including blurring, downscaling, and noise corruption. Then, we develop a variational Bayesian approach to infer their posteriors. Finally, we implement the variational approach for single image super-resolution (SISR) using deep neural networks, and propose an unsupervised training strategy. The experiments on three image **restoration** tasks, \textit{i.e.,} ideal SISR, realistic SISR, and real-world SISR, demonstrate that our method has superior model generalizability against varying noise levels and degradation kernels and is effective in unsupervised SISR. The code and resulting models are released via \url{https://zmiclab.github.io/projects.html}.  
### Extremely **Low-light** Image **Enhancement** with Scene Text **Restoration**. (arXiv:2204.00630v1 [eess.IV])
- Authors : Pohao Hsu, Tsung Lin, Chun Chet, Long Kew, Mei Yih, Hong Lai, Chee Seng, Christopher Zach
- Link : [http://arxiv.org/abs/2204.00630](http://arxiv.org/abs/2204.00630)
> ABSTRACT  :  Deep learning-based methods have made impressive progress in enhancing extremely **low-light** images - the image quality of the reconstructed images has generally improved. However, we found out that most of these methods could not sufficiently recover the image details, for instance, the texts in the scene. In this paper, a novel image **enhancement** framework is proposed to precisely restore the scene texts, as well as the overall quality of the image simultaneously under extremely **low-light** images conditions. Mainly, we employed a self-regularised attention map, an edge map, and a novel text detection loss. In addition, leveraging synthetic **low-light** images is beneficial for image **enhancement** on the genuine ones in terms of text detection. The quantitative and qualitative experimental results have shown that the proposed model outperforms state-of-the-art methods in image **restoration**, text detection, and text spotting on See In the **Dark** and ICDAR15 datasets.  
### UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])
- Authors : Ali Hatamizadeh, Ziyue Xu, Dong Yang, Wenqi Li, Holger Roth, Daguang Xu
- Link : [http://arxiv.org/abs/2204.00631](http://arxiv.org/abs/2204.00631)
> ABSTRACT  :  Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D **Swin** Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions  
### MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v2 [eess.IV] UPDATED)
- Authors : Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, **Peyman Milanfar**, Alan Bovik, Yinxiao Li
- Link : [http://arxiv.org/abs/2201.02973](http://arxiv.org/abs/2201.02973)
> ABSTRACT  :  Recent progress on Transformers and multi-layer perceptron (MLP) models provide new network architectural designs for computer vision tasks. Although these models proved to be effective in many vision tasks such as image recognition, there remain challenges in adapting them for low-level vision. The inflexibility to support high-resolution images and limitations of local attention are perhaps the main bottlenecks. In this work, we present a multi-axis MLP based architecture called MAXIM, that can serve as an efficient and flexible general-purpose vision backbone for image processing tasks. MAXIM uses a UNet-shaped hierarchical structure and supports long-range interactions enabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based building blocks: a multi-axis gated MLP that allows for efficient and scalable spatial mixing of local and global visual cues, and a cross-gating block, an alternative to cross-attention, which accounts for cross-feature conditioning. Both these modules are exclusively based on MLPs, but also benefit from being both global and `fully-convolutional', two properties that are desirable for image processing. Our extensive experimental results show that the proposed MAXIM model achieves state-of-the-art performance on more than ten benchmarks across a range of image processing tasks, including denoising, deblurring, deraining, dehazing, and **enhancement** while requiring fewer or comparable numbers of parameters and FLOPs than competitive models. The source code and trained models will be available at \url{https://github.com/google-research/maxim}.  
### Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v2 [eess.IV] UPDATED)
- Authors : Jiahao Huang, Yinzhe Wu, Huanjun Wu, Guang Yang
- Link : [http://arxiv.org/abs/2201.09400](http://arxiv.org/abs/2201.09400)
> ABSTRACT  :  Magnetic resonance imaging (MRI) is a widely used non-radiative and non-invasive method for clinical interrogation of organ structures and metabolism, with an inherently long scanning time. Methods by k-space undersampling and deep learning based reconstruction have been popularised to accelerate the scanning process. This work focuses on investigating how powerful transformers are for fast MRI by exploiting and comparing different novel network architectures. In particular, a generative adversarial network (GAN) based **Swin** transformer (ST-GAN) was introduced for the fast MRI reconstruction. To further preserve the edge and texture information, edge enhanced GAN based **Swin** transformer (EES-GAN) and texture enhanced GAN based **Swin** transformer (TES-GAN) were also developed, where a dual-discriminator GAN structure was applied. We compared our proposed GAN based transformers, standalone **Swin** transformer and other convolutional neural networks based GAN model in terms of the evaluation metrics PSNR, SSIM and FID. We showed that transformers work well for the MRI reconstruction from different undersampling conditions. The utilisation of GAN's adversarial structure improves the quality of images reconstructed when undersampled for 30% or higher. The code is publicly available at https://github.com/ayanglab/**Swin**GANMR.  
## cs.LG
---
### Bayesian Image Super-Resolution with Deep Modeling of Image Statistics. (arXiv:2204.00623v1 [eess.IV])
- Authors : Shangqi Gao, Xiahai Zhuang
- Link : [http://arxiv.org/abs/2204.00623](http://arxiv.org/abs/2204.00623)
> ABSTRACT  :  Modeling statistics of image priors is useful for image super-resolution, but little attention has been paid from the massive works of deep learning-based methods. In this work, we propose a Bayesian image **restoration** framework, where natural image statistics are modeled with the combination of smoothness and sparsity priors. Concretely, firstly we consider an ideal image as the sum of a smoothness component and a sparsity residual, and model real image degradation including blurring, downscaling, and noise corruption. Then, we develop a variational Bayesian approach to infer their posteriors. Finally, we implement the variational approach for single image super-resolution (SISR) using deep neural networks, and propose an unsupervised training strategy. The experiments on three image **restoration** tasks, \textit{i.e.,} ideal SISR, realistic SISR, and real-world SISR, demonstrate that our method has superior model generalizability against varying noise levels and degradation kernels and is effective in unsupervised SISR. The code and resulting models are released via \url{https://zmiclab.github.io/projects.html}.  
### UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])
- Authors : Ali Hatamizadeh, Ziyue Xu, Dong Yang, Wenqi Li, Holger Roth, Daguang Xu
- Link : [http://arxiv.org/abs/2204.00631](http://arxiv.org/abs/2204.00631)
> ABSTRACT  :  Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D **Swin** Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions  
### Why **Exposure** Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v1 [cs.CL])
- Authors : Kushal Arora, Layla El, Hareesh Bahuleyan, Jackie Chi, Kit Cheung
- Link : [http://arxiv.org/abs/2204.01171](http://arxiv.org/abs/2204.01171)
> ABSTRACT  :  Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis is that this brittleness of generation models is caused by the training and the generation procedure mismatch, also referred to as **exposure** bias. In this paper, we verify this hypothesis by analyzing **exposure** bias from an imitation learning perspective. We show that **exposure** bias leads to an accumulation of errors, analyze why perplexity fails to capture this accumulation, and empirically show that this accumulation results in poor generation quality. Source code to reproduce these experiments is available at https://github.com/kushalarora/quantifying_**exposure**_bias  
### Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v2 [eess.IV] UPDATED)
- Authors : Jiahao Huang, Yinzhe Wu, Huanjun Wu, Guang Yang
- Link : [http://arxiv.org/abs/2201.09400](http://arxiv.org/abs/2201.09400)
> ABSTRACT  :  Magnetic resonance imaging (MRI) is a widely used non-radiative and non-invasive method for clinical interrogation of organ structures and metabolism, with an inherently long scanning time. Methods by k-space undersampling and deep learning based reconstruction have been popularised to accelerate the scanning process. This work focuses on investigating how powerful transformers are for fast MRI by exploiting and comparing different novel network architectures. In particular, a generative adversarial network (GAN) based **Swin** transformer (ST-GAN) was introduced for the fast MRI reconstruction. To further preserve the edge and texture information, edge enhanced GAN based **Swin** transformer (EES-GAN) and texture enhanced GAN based **Swin** transformer (TES-GAN) were also developed, where a dual-discriminator GAN structure was applied. We compared our proposed GAN based transformers, standalone **Swin** transformer and other convolutional neural networks based GAN model in terms of the evaluation metrics PSNR, SSIM and FID. We showed that transformers work well for the MRI reconstruction from different undersampling conditions. The utilisation of GAN's adversarial structure improves the quality of images reconstructed when undersampled for 30% or higher. The code is publicly available at https://github.com/ayanglab/**Swin**GANMR.  
### Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v4 [cs.CV] UPDATED)
- Authors : Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, Jian Sun
- Link : [http://arxiv.org/abs/2203.06717](http://arxiv.org/abs/2203.06717)
> ABSTRACT  :  We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than **Swin** Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code &amp; models at https://github.com/megvii-research/RepLKNet.  
## cs.AI
---
### UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])
- Authors : Ali Hatamizadeh, Ziyue Xu, Dong Yang, Wenqi Li, Holger Roth, Daguang Xu
- Link : [http://arxiv.org/abs/2204.00631](http://arxiv.org/abs/2204.00631)
> ABSTRACT  :  Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D **Swin** Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions  
### Why **Exposure** Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v1 [cs.CL])
- Authors : Kushal Arora, Layla El, Hareesh Bahuleyan, Jackie Chi, Kit Cheung
- Link : [http://arxiv.org/abs/2204.01171](http://arxiv.org/abs/2204.01171)
> ABSTRACT  :  Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis is that this brittleness of generation models is caused by the training and the generation procedure mismatch, also referred to as **exposure** bias. In this paper, we verify this hypothesis by analyzing **exposure** bias from an imitation learning perspective. We show that **exposure** bias leads to an accumulation of errors, analyze why perplexity fails to capture this accumulation, and empirically show that this accumulation results in poor generation quality. Source code to reproduce these experiments is available at https://github.com/kushalarora/quantifying_**exposure**_bias  
### Monte Carlo Physarum Machine: Characteristics of Pattern Formation in Continuous Stochastic Transport Networks. (arXiv:2204.01256v1 [astro-ph.CO])
- Authors : Oskar Elek, Xavier Prochaska
- Link : [http://arxiv.org/abs/2204.01256](http://arxiv.org/abs/2204.01256)
> ABSTRACT  :  We present Monte Carlo Physarum Machine: a computational model suitable for reconstructing continuous transport networks from sparse 2D and 3D data. MCPM is a probabilistic generalization of Jones's 2010 agent-based model for simulating the growth of Physarum polycephalum slime mold. We compare MCPM to Jones's work on theoretical grounds, and describe a task-specific variant designed for reconstructing the large-scale distribution of gas and **dark** matter in the Universe known as the Cosmic web. To analyze the new model, we first explore MCPM's self-patterning behavior, showing a wide range of continuous network-like morphologies -- called "polyphorms" -- that the model produces from geometrically intuitive parameters. Applying MCPM to both simulated and observational cosmological datasets, we then evaluate its ability to produce consistent 3D density maps of the Cosmic web. Finally, we examine other possible tasks where MCPM could be useful, along with several examples of fitting to domain-specific data as proofs of concept.  
### A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation. (arXiv:2104.08704v2 [cs.CL] UPDATED)
- Authors : Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, Bill Dolan
- Link : [http://arxiv.org/abs/2104.08704](http://arxiv.org/abs/2104.08704)
> ABSTRACT  :  Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in **real time**. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDes (HAllucination DEtection dataSet). To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.  
### Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v4 [cs.CV] UPDATED)
- Authors : Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, Jian Sun
- Link : [http://arxiv.org/abs/2203.06717](http://arxiv.org/abs/2203.06717)
> ABSTRACT  :  We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than **Swin** Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code &amp; models at https://github.com/megvii-research/RepLKNet.  
# Paper List
---
## cs.CV
---
**180** new papers in cs.CV:-) 
1. Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations. (arXiv:2204.00617v1 [eess.IV])
2. Universal Lymph Node Detection in T2 MRI using Neural Networks. (arXiv:2204.00622v1 [eess.IV])
3. Bayesian Image Super-Resolution with Deep Modeling of Image Statistics. (arXiv:2204.00623v1 [eess.IV])
4. Explainable and Interpretable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning. (arXiv:2204.00624v1 [cs.LG])
5. Learning Neural Acoustic Fields. (arXiv:2204.00628v1 [cs.SD])
6. TopTemp: Parsing Precipitate Structure from Temper Topology. (arXiv:2204.00629v1 [cond-mat.mtrl-sci])
7. Extremely **Low-light** Image **Enhancement** with Scene Text **Restoration**. (arXiv:2204.00630v1 [eess.IV])
8. UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])
9. SIMBAR: Single Image-Based Scene Relighting For Effective Data Augmentation For Automated Driving Vision Tasks. (arXiv:2204.00644v1 [cs.CV])
10. Robust Neonatal Face Detection in Real-world Clinical Settings. (arXiv:2204.00655v1 [cs.CV])
11. Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes. (arXiv:2204.00656v1 [cs.CV])
12. Hazard Detection And Avoidance For The Nova-C Lander. (arXiv:2204.00660v1 [cs.CV])
13. Learning Audio-Video Modalities from Image Captions. (arXiv:2204.00679v1 [cs.CV])
14. SkeleVision: Towards Adversarial Resiliency of Person Tracking with Multi-Task Learning. (arXiv:2204.00734v1 [cs.CV])
15. What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions. (arXiv:2204.00746v1 [cs.CV])
16. Homography Loss for Monocular 3D Object Detection. (arXiv:2204.00754v1 [cs.CV])
17. Do learned representations respect causal relationships?. (arXiv:2204.00762v1 [cs.CV])
18. SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images. (arXiv:2204.00790v1 [cs.CV])
19. IR-GAN: Image Manipulation with Linguistic Instruction by Increment Reasoning. (arXiv:2204.00792v1 [cs.CV])
20. R(Det)^2: Randomized Decision Routing for Object Detection. (arXiv:2204.00794v1 [cs.CV])
21. Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency. (arXiv:2204.00795v1 [cs.CV])
22. RFVTM: A Recovery and Filtering Vertex Trichotomy Matching for Remote Sensing Image Registration. (arXiv:2204.00818v1 [eess.IV])
23. Semantic-Aware Domain Generalized Segmentation. (arXiv:2204.00822v1 [cs.CV])
24. Online Convolutional Re-parameterization. (arXiv:2204.00826v1 [cs.CV])
25. Automatic Registration of Images with Inconsistent Content Through Line-Support Region Segmentation and Geometrical Outlier Removal. (arXiv:2204.00832v1 [eess.IV])
26. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v1 [cs.CV])
27. Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images. (arXiv:2204.00840v1 [cs.CV])
28. Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs. (arXiv:2204.00853v1 [cs.CV])
29. Acoustic-to-articulatory Inversion based on Speech Decomposition and Auxiliary Feature. (arXiv:2204.00873v1 [cs.SD])
30. Moment-based Adversarial Training for Embodied Language Comprehension. (arXiv:2204.00889v1 [cs.RO])
31. A Free Lunch to Person Re-identification: Learning from Automatically Generated Noisy Tracklets. (arXiv:2204.00891v1 [cs.CV])
32. Mix-up Self-Supervised Learning for Contrast-agnostic Applications. (arXiv:2204.00901v1 [cs.CV])
33. Deep Algebraic Fitting for Multiple Circle Primitives Extraction from Raw Point Clouds. (arXiv:2204.00920v1 [cs.CV])
34. Word separation in continuous sign language using isolated signs and post-processing. (arXiv:2204.00923v1 [cs.CV])
35. Sin**NeRF**: Training Neural Radiance Fields on Complex Scenes from a Single Image. (arXiv:2204.00928v1 [cs.CV])
36. A-ACT: Action Anticipation through Cycle Transformations. (arXiv:2204.00942v1 [cs.CV])
37. TripleNet: A Low Computing Power Platform of Low-Parameter Network. (arXiv:2204.00943v1 [cs.CV])
38. Progressive Minimal Path Method with Embedded CNN. (arXiv:2204.00944v1 [cs.CV])
39. Matching Feature Sets for Few-Shot Image Classification. (arXiv:2204.00949v1 [cs.CV])
40. A Sentinel-2 multi-year, multi-country benchmark dataset for crop classification and segmentation with deep learning. (arXiv:2204.00951v1 [cs.CV])
41. AdaFace: Quality Adaptive Margin for Face Recognition. (arXiv:2204.00964v1 [cs.CV])
42. DST: Dynamic Substitute Training for Data-free Black-box Attack. (arXiv:2204.00972v1 [cs.CV])
43. Kernel Extreme Learning Machine Optimized by the Sparrow Search Algorithm for Hyperspectral Image Classification. (arXiv:2204.00973v1 [cs.CV])
44. Neural Global Shutter: Learn to Restore Video from a Rolling Shutter Camera with Global Reset Feature. (arXiv:2204.00974v1 [cs.CV])
45. Question-Driven Graph Fusion Network For Visual Question Answering. (arXiv:2204.00975v1 [cs.CV])
46. BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation. (arXiv:2204.00987v1 [cs.CV])
47. POS-BERT: Point Cloud One-Stage BERT Pre-Training. (arXiv:2204.00989v1 [cs.CV])
48. Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v1 [cs.CV])
49. Region-aware Attention for Image Inpainting. (arXiv:2204.01004v1 [cs.CV])
50. Gastrointestinal Polyps and Tumors Detection Based on Multi-scale Feature-fusion with WCE Sequences. (arXiv:2204.01012v1 [eess.IV])
51. TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting. (arXiv:2204.01018v1 [cs.CV])
52. STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes. (arXiv:2204.01026v1 [cs.CV])
53. Distortion-Aware Self-Supervised 360{\deg} Depth Estimation from A Single Equirectangular Projection Image. (arXiv:2204.01027v1 [cs.CV])
54. Style-Based Global Appearance Flow for Virtual Try-On. (arXiv:2204.01046v1 [cs.CV])
55. In Rain or Shine: Understanding and Overcoming Dataset Bias for Improving Robustness Against Weather Corruptions for Autonomous Vehicles. (arXiv:2204.01062v1 [cs.CV])
56. ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework. (arXiv:2204.01080v1 [cs.CV])
57. Faces: AI Blitz XIII Solutions. (arXiv:2204.01081v1 [cs.CV])
58. Adversarially robust segmentation models learn perceptually-aligned gradients. (arXiv:2204.01099v1 [cs.CV])
59. Adjusting for Bias with Procedural Data. (arXiv:2204.01108v1 [cs.CV])
60. BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion. (arXiv:2204.01139v1 [cs.CV])
61. Indoor Navigation Assistance for Visually Impaired People via Dynamic SLAM and Panoptic Segmentation with an RGB-D Sensor. (arXiv:2204.01154v1 [cs.CV])
62. Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons. (arXiv:2204.01159v1 [cs.CV])
63. Exploiting Temporal Relations on Radar Perception for Autonomous Driving. (arXiv:2204.01184v1 [cs.CV])
64. Revisiting a kNN-based Image Classification System with High-capacity Storage. (arXiv:2204.01186v1 [cs.CV])
65. Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution. (arXiv:2204.01188v1 [cs.CV])
66. Unsupervised Change Detection Based on Image Reconstruction Loss. (arXiv:2204.01200v1 [cs.CV])
67. A Novel Mask R-CNN Model to Segment Heterogeneous Brain Tumors through Image Subtraction. (arXiv:2204.01201v1 [eess.IV])
68. Attribute Prototype Network for Any-Shot Learning. (arXiv:2204.01208v1 [cs.CV])
69. Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection. (arXiv:2204.01209v1 [cs.CV])
70. Co-Teaching for Unsupervised Domain Adaptation and Expansion. (arXiv:2204.01210v1 [cs.CV])
71. Animatable Neural Radiance Fields from Monocular RGB-D. (arXiv:2204.01218v1 [cs.CV])
72. Soft Threshold Ternary Networks. (arXiv:2204.01234v1 [cs.CV])
73. Dynamic Focus-aware Positional Queries for Semantic Segmentation. (arXiv:2204.01244v1 [cs.CV])
74. Differentiable Rendering for Synthetic Aperture Radar Imagery. (arXiv:2204.01248v1 [eess.IV])
75. BatchFormerV2: Exploring Sample Relationships for Dense Representation Learning. (arXiv:2204.01254v1 [cs.CV])
76. Direct Dense Pose Estimation. (arXiv:2204.01263v1 [cs.CV])
77. Probabilistic Implicit Scene Completion. (arXiv:2204.01264v1 [cs.CV])
78. Multi-modality Associative Bridging through Memory: Speech Sound Recollected from Face Video. (arXiv:2204.01265v1 [cs.CV])
79. FoV-Net: Field-of-View Extrapolation Using Self-Attention and Uncertainty. (arXiv:2204.01267v1 [cs.CV])
80. Improving Monocular Visual Odometry Using Learned Depth. (arXiv:2204.01268v1 [cs.CV])
81. Towards Robust and Reproducible Active Learning Using Neural Networks. (arXiv:2002.09564v2 [cs.LG] UPDATED)
82. Critical Assessment of Transfer Learning for Medical Image Segmentation with Fully Convolutional Neural Networks. (arXiv:2006.00356v2 [cs.CV] UPDATED)
83. Tropical time series, iterated-sums signatures and quasisymmetric functions. (arXiv:2009.08443v3 [math.RA] UPDATED)
84. MCW-Net: Single Image Deraining with Multi-level Connections and Wide Regional Non-local Blocks. (arXiv:2009.13990v4 [cs.CV] UPDATED)
85. Towards Accurate Active Camera Localization. (arXiv:2012.04263v3 [cs.CV] UPDATED)
86. Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v4 [eess.IV] UPDATED)
87. Nonlinear Evolutionary PDE-Based Refinement of Optical Flow. (arXiv:2102.00487v4 [cs.CV] UPDATED)
88. Subspace-Based Feature Fusion From Hyperspectral And Multispectral Image For Land Cover Classification. (arXiv:2102.11228v2 [eess.IV] UPDATED)
89. Convolution-Free Medical Image Segmentation using Transformers. (arXiv:2102.13645v2 [eess.IV] UPDATED)
90. Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles. (arXiv:2103.14098v2 [cs.CV] UPDATED)
91. Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings. (arXiv:2103.14572v3 [cs.CV] UPDATED)
92. Quantum Self-Supervised Learning. (arXiv:2103.14653v3 [quant-ph] UPDATED)
93. Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring. (arXiv:2104.12665v2 [eess.IV] UPDATED)
94. Revisiting Skeleton-based Action Recognition. (arXiv:2104.13586v2 [cs.CV] UPDATED)
95. When Does Contrastive Visual Representation Learning Work?. (arXiv:2105.05837v2 [cs.CV] UPDATED)
96. Multi-Person Extreme Motion Prediction. (arXiv:2105.08825v6 [cs.CV] UPDATED)
97. Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v2 [cs.CV] UPDATED)
98. Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v6 [cs.LG] UPDATED)
99. BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v3 [cs.LG] UPDATED)
100. Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v3 [cs.CV] UPDATED)
101. TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?. (arXiv:2106.11297v4 [cs.CV] UPDATED)
102. Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis. (arXiv:2106.11485v3 [cs.CV] UPDATED)
103. Attention Bottlenecks for Multimodal Fusion. (arXiv:2107.00135v2 [cs.CV] UPDATED)
104. Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v3 [physics.optics] UPDATED)
105. Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset. (arXiv:2107.04286v2 [cs.CV] UPDATED)
106. Multi-Modal MRI Reconstruction Assisted with Spatial Alignment Network. (arXiv:2108.05603v3 [eess.IV] UPDATED)
107. Full-resolution quality assessment for pansharpening. (arXiv:2108.06144v3 [cs.CV] UPDATED)
108. Vision-Language Navigation: A Survey and Taxonomy. (arXiv:2108.11544v3 [cs.CV] UPDATED)
109. M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v5 [cs.CV] UPDATED)
110. Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer. (arXiv:2109.08024v2 [cs.CV] UPDATED)
111. AutoPhaseNN: Unsupervised Physics-aware Deep Learning of 3D Nanoscale Bragg Coherent Diffraction Imaging. (arXiv:2109.14053v2 [physics.app-ph] UPDATED)
112. Optical Flow Estimation for Spiking Camera. (arXiv:2110.03916v3 [cs.CV] UPDATED)
113. Real-Time Learning from An Expert in Deep Recommendation Systems with Marginal Distance Probability Distribution. (arXiv:2110.06287v2 [cs.LG] UPDATED)
114. Trigger Hunting with a Topological Prior for Trojan Detection. (arXiv:2110.08335v2 [cs.CV] UPDATED)
115. SynCoLFinGer: Synthetic Contactless Fingerprint Generator. (arXiv:2110.09144v2 [cs.CV] UPDATED)
116. Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups. (arXiv:2110.13059v2 [cs.CV] UPDATED)
117. CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v3 [cs.CV] UPDATED)
118. Pansharpening by convolutional neural networks in the full resolution framework. (arXiv:2111.08334v3 [cs.CV] UPDATED)
119. It's About Time: Analog Clock Reading in the Wild. (arXiv:2111.09162v3 [cs.CV] UPDATED)
120. Discrete Representations Strengthen Vision Transformer Robustness. (arXiv:2111.10493v2 [cs.CV] UPDATED)
121. Transferability Estimation using Bhattacharyya Class Separability. (arXiv:2111.12780v2 [cs.CV] UPDATED)
122. SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation with Learnt Surface Embeddings. (arXiv:2111.13489v2 [cs.CV] UPDATED)
123. Not All Relations are Equal: Mining Informative Labels for Scene Graph Generation. (arXiv:2111.13517v2 [cs.CV] UPDATED)
124. End-to-End Referring Video Object Segmentation with Multimodal Transformers. (arXiv:2111.14821v2 [cs.CV] UPDATED)
125. Assessment of Data Consistency through Cascades of Independently Recurrent Inference Machines for fast and robust accelerated MRI reconstruction. (arXiv:2111.15498v2 [eess.IV] UPDATED)
126. Background Activation Suppression for Weakly Supervised Object Localization. (arXiv:2112.00580v2 [cs.CV] UPDATED)
127. HyperInverter: Improving StyleGAN Inversion via Hypernetwork. (arXiv:2112.00719v2 [cs.CV] UPDATED)
128. GANSeg: Learning to Segment by Unsupervised Hierarchical Image Generation. (arXiv:2112.01036v2 [cs.CV] UPDATED)
129. OW-DETR: Open-world Detection Transformer. (arXiv:2112.01513v3 [cs.CV] UPDATED)
130. E$^2$(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition. (arXiv:2112.03596v3 [cs.CV] UPDATED)
131. What's Behind the Couch? Directed Ray Distance Functions (DRDF) for 3D Scene Reconstruction. (arXiv:2112.04481v2 [cs.CV] UPDATED)
132. Fast Point Transformer. (arXiv:2112.04702v2 [cs.CV] UPDATED)
133. Contextualized Spatio-Temporal Contrastive Learning with Self-Supervision. (arXiv:2112.05181v2 [cs.CV] UPDATED)
134. IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes. (arXiv:2112.05298v3 [cs.CV] UPDATED)
135. Multimedia Datasets for Anomaly Detection: A Review. (arXiv:2112.05410v3 [cs.CV] UPDATED)
136. I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v4 [cs.CV] UPDATED)
137. Decomposing the Deep: Finding Class Specific Filters in Deep CNNs. (arXiv:2112.07719v3 [cs.CV] UPDATED)
138. Object Pursuit: Building a Space of Objects via Discriminative Weight Generation. (arXiv:2112.07954v3 [cs.CV] UPDATED)
139. UnweaveNet: Unweaving Activity Stories. (arXiv:2112.10194v2 [cs.CV] UPDATED)
140. Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis. (arXiv:2201.01683v2 [cs.CV] UPDATED)
141. Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v3 [eess.IV] UPDATED)
142. Relieving Long-tailed Instance Segmentation via Pairwise Class Balance. (arXiv:2201.02784v2 [cs.CV] UPDATED)
143. MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v2 [eess.IV] UPDATED)
144. Language-driven Semantic Segmentation. (arXiv:2201.03546v2 [cs.CV] UPDATED)
145. A Saliency based Feature Fusion Model for EEG Emotion Estimation. (arXiv:2201.03891v3 [cs.CV] UPDATED)
146. Captcha Attack: Turning Captchas Against Humanity. (arXiv:2201.04014v3 [cs.CR] UPDATED)
147. Parameter-free Online Test-time Adaptation. (arXiv:2201.05718v2 [cs.CV] UPDATED)
148. Revisiting Weakly Supervised Pre-Training of Visual Perception Models. (arXiv:2201.08371v2 [cs.CV] UPDATED)
149. Learning Pixel Trajectories with Multiscale Contrastive Random Walks. (arXiv:2201.08379v2 [cs.CV] UPDATED)
150. Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v2 [eess.IV] UPDATED)
151. Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning. (arXiv:2201.09671v2 [cs.LG] UPDATED)
152. A Deep Learning Approach for Digital Color Reconstruction of Lenticular Films. (arXiv:2202.05270v2 [eess.IV] UPDATED)
153. A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron. (arXiv:2202.06372v2 [eess.IV] UPDATED)
154. Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v2 [eess.IV] UPDATED)
155. Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data. (arXiv:2203.00734v2 [cs.CV] UPDATED)
156. 3D Common Corruptions and Data Augmentation. (arXiv:2203.01441v2 [cs.CV] UPDATED)
157. AugHover-Net: Augmenting Hover-net for Nucleus Segmentation and Classification. (arXiv:2203.03415v3 [eess.IV] UPDATED)
158. Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language. (arXiv:2203.03598v2 [cs.CV] UPDATED)
159. Learning Distinctive Margin toward Active Domain Adaptation. (arXiv:2203.05738v2 [cs.LG] UPDATED)
160. Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v4 [cs.CV] UPDATED)
161. Rethinking Minimal Sufficient Representation in Contrastive Learning. (arXiv:2203.07004v2 [cs.CV] UPDATED)
162. Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation. (arXiv:2203.09811v2 [cs.CV] UPDATED)
163. Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v2 [cs.CV] UPDATED)
164. Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis. (arXiv:2203.10977v2 [eess.IV] UPDATED)
165. Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v3 [eess.IV] UPDATED)
166. A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v2 [cs.LG] UPDATED)
167. Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v2 [cs.LG] UPDATED)
168. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v2 [cs.LG] UPDATED)
169. Federated Learning with Position-Aware Neurons. (arXiv:2203.14666v2 [cs.CV] UPDATED)
170. A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition. (arXiv:2203.14779v2 [cs.CV] UPDATED)
171. Rethinking Semantic Segmentation: A Prototype View. (arXiv:2203.15102v2 [cs.CV] UPDATED)
172. Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification. (arXiv:2203.15210v2 [cs.CV] UPDATED)
173. Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation. (arXiv:2203.15227v2 [cs.CV] UPDATED)
174. SepViT: Separable Vision Transformer. (arXiv:2203.15380v2 [cs.CV] UPDATED)
175. Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection. (arXiv:2203.15793v2 [cs.CV] UPDATED)
176. PP-YOLOE: An evolved version of YOLO. (arXiv:2203.16250v2 [cs.CV] UPDATED)
177. An Improved Lightweight YOLOv5 Model Based on Attention Mechanism for Face Mask Detection. (arXiv:2203.16506v2 [cs.CV] UPDATED)
178. Ball 3D localization from a single calibrated image. (arXiv:2204.00003v2 [cs.CV] UPDATED)
179. CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection. (arXiv:2204.00325v2 [cs.CV] UPDATED)
180. DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v2 [cs.CV] UPDATED)
## eess.IV
---
**42** new papers in eess.IV:-) 
1. Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations. (arXiv:2204.00617v1 [eess.IV])
2. Universal Lymph Node Detection in T2 MRI using Neural Networks. (arXiv:2204.00622v1 [eess.IV])
3. Bayesian Image Super-Resolution with Deep Modeling of Image Statistics. (arXiv:2204.00623v1 [eess.IV])
4. Explainable and Interpretable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning. (arXiv:2204.00624v1 [cs.LG])
5. Extremely **Low-light** Image **Enhancement** with Scene Text **Restoration**. (arXiv:2204.00630v1 [eess.IV])
6. UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])
7. RFVTM: A Recovery and Filtering Vertex Trichotomy Matching for Remote Sensing Image Registration. (arXiv:2204.00818v1 [eess.IV])
8. Automatic Registration of Images with Inconsistent Content Through Line-Support Region Segmentation and Geometrical Outlier Removal. (arXiv:2204.00832v1 [eess.IV])
9. Optical modelling of accommodative light field display system and prediction of human eye responses. (arXiv:2204.00884v1 [eess.IV])
10. Gastrointestinal Polyps and Tumors Detection Based on Multi-scale Feature-fusion with WCE Sequences. (arXiv:2204.01012v1 [eess.IV])
11. Sparse Tensor-based Point Cloud Attribute Compression. (arXiv:2204.01023v1 [eess.IV])
12. Indoor Navigation Assistance for Visually Impaired People via Dynamic SLAM and Panoptic Segmentation with an RGB-D Sensor. (arXiv:2204.01154v1 [cs.CV])
13. Unsupervised Change Detection Based on Image Reconstruction Loss. (arXiv:2204.01200v1 [cs.CV])
14. A Novel Mask R-CNN Model to Segment Heterogeneous Brain Tumors through Image Subtraction. (arXiv:2204.01201v1 [eess.IV])
15. Differentiable Rendering for Synthetic Aperture Radar Imagery. (arXiv:2204.01248v1 [eess.IV])
16. Computer-Aided Extraction of Select MRI Markers of Cerebral Small Vessel Disease: A Systematic Review. (arXiv:2204.01411v1 [eess.IV])
17. Satellite Monitoring of Terrestrial Plastic Waste. (arXiv:2204.01485v1 [cs.CY])
18. Bi-directional Loop Closure for Visual SLAM. (arXiv:2204.01524v1 [cs.CV])
19. Optimize Deep Learning Models for Prediction of Gene Mutations Using Unsupervised Clustering. (arXiv:2204.01593v1 [q-bio.QM])
20. Three-dimensional Microstructural Image Synthesis from 2D Backscattered Electron Image of Cement Paste. (arXiv:2204.01645v1 [eess.IV])
21. Critical Assessment of Transfer Learning for Medical Image Segmentation with Fully Convolutional Neural Networks. (arXiv:2006.00356v2 [cs.CV] UPDATED)
22. Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v4 [eess.IV] UPDATED)
23. Subspace-Based Feature Fusion From Hyperspectral And Multispectral Image For Land Cover Classification. (arXiv:2102.11228v2 [eess.IV] UPDATED)
24. Convolution-Free Medical Image Segmentation using Transformers. (arXiv:2102.13645v2 [eess.IV] UPDATED)
25. Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring. (arXiv:2104.12665v2 [eess.IV] UPDATED)
26. Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text. (arXiv:2106.14014v3 [eess.IV] UPDATED)
27. Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v3 [physics.optics] UPDATED)
28. Multi-Modal MRI Reconstruction Assisted with Spatial Alignment Network. (arXiv:2108.05603v3 [eess.IV] UPDATED)
29. Pansharpening by convolutional neural networks in the full resolution framework. (arXiv:2111.08334v3 [cs.CV] UPDATED)
30. Calibrated Diffusion Tensor Estimation. (arXiv:2111.10847v2 [cs.LG] UPDATED)
31. Assessment of Data Consistency through Cascades of Independently Recurrent Inference Machines for fast and robust accelerated MRI reconstruction. (arXiv:2111.15498v2 [eess.IV] UPDATED)
32. Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v3 [eess.IV] UPDATED)
33. MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v2 [eess.IV] UPDATED)
34. Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v2 [eess.IV] UPDATED)
35. A method for virtual optical sectioning and tomography utilizing shallow depth of field. (arXiv:2202.02692v2 [physics.optics] UPDATED)
36. A Deep Learning Approach for Digital Color Reconstruction of Lenticular Films. (arXiv:2202.05270v2 [eess.IV] UPDATED)
37. A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron. (arXiv:2202.06372v2 [eess.IV] UPDATED)
38. Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v2 [eess.IV] UPDATED)
39. AugHover-Net: Augmenting Hover-net for Nucleus Segmentation and Classification. (arXiv:2203.03415v3 [eess.IV] UPDATED)
40. Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis. (arXiv:2203.10977v2 [eess.IV] UPDATED)
41. Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v3 [eess.IV] UPDATED)
42. Ball 3D localization from a single calibrated image. (arXiv:2204.00003v2 [cs.CV] UPDATED)
## cs.LG
---
**194** new papers in cs.LG:-) 
1. Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations. (arXiv:2204.00617v1 [eess.IV])
2. CogNGen: Constructing the Kernel of a Hyperdimensional Predictive Processing Cognitive Architecture. (arXiv:2204.00619v1 [cs.AI])
3. Universal Lymph Node Detection in T2 MRI using Neural Networks. (arXiv:2204.00622v1 [eess.IV])
4. Bayesian Image Super-Resolution with Deep Modeling of Image Statistics. (arXiv:2204.00623v1 [eess.IV])
5. Explainable and Interpretable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning. (arXiv:2204.00624v1 [cs.LG])
6. Learning Neural Acoustic Fields. (arXiv:2204.00628v1 [cs.SD])
7. TopTemp: Parsing Precipitate Structure from Temper Topology. (arXiv:2204.00629v1 [cond-mat.mtrl-sci])
8. UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])
9. Application of Dimensional Reduction in Artificial Neural Networks to Improve Emergency Department Triage During Chemical Mass Casualty Incidents. (arXiv:2204.00642v1 [cs.NE])
10. SIMBAR: Single Image-Based Scene Relighting For Effective Data Augmentation For Automated Driving Vision Tasks. (arXiv:2204.00644v1 [cs.CV])
11. Cluster-based ensemble learning for wind power modeling with meteorological wind data. (arXiv:2204.00646v1 [cs.LG])
12. Knowledge distillation with error-correcting transfer learning for wind power prediction. (arXiv:2204.00649v1 [cs.LG])
13. Hysteresis-Based RL: Robustifying Reinforcement Learning-based Control Policies via Hybrid Control. (arXiv:2204.00654v1 [cs.LG])
14. Learnable latent embeddings for joint behavioral and neural analysis. (arXiv:2204.00673v1 [cs.LG])
15. Assimilation of Satellite Active Fires Data. (arXiv:2204.00686v1 [cs.LG])
16. Testing Feedforward Neural Networks Training Programs. (arXiv:2204.00694v1 [cs.SE])
17. A Reinforcement Learning Approach to Sensing Design in Resource-Constrained Wireless Networked Control Systems. (arXiv:2204.00703v1 [eess.SY])
18. Strategies for Safe Multi-Armed Bandits with Logarithmic Regret and Risk. (arXiv:2204.00706v1 [cs.LG])
19. Identifying Exoplanets with Machine Learning Methods: A Preliminary Study. (arXiv:2204.00721v1 [astro-ph.EP])
20. Analysis of Sparse Subspace Clustering: Experiments and Random Projection. (arXiv:2204.00723v1 [cs.LG])
21. SkeleVision: Towards Adversarial Resiliency of Person Tracking with Multi-Task Learning. (arXiv:2204.00734v1 [cs.CV])
22. Path Development Network with Finite-dimensional Lie Group Representation. (arXiv:2204.00740v1 [cs.LG])
23. Modeling Dynamic User Preference via Dictionary Learning for Sequential Recommendation. (arXiv:2204.00752v1 [cs.IR])
24. Variational message passing for online polynomial NARMAX identification. (arXiv:2204.00769v1 [stat.ML])
25. Speaker adaptation for Wav2vec2 based dysarthric ASR. (arXiv:2204.00770v1 [cs.SD])
26. Revealing the real-world CO2 emission reduction of ridesplitting and its determinants based on machine learning. (arXiv:2204.00777v1 [cs.LG])
27. Distributional Gradient Boosting Machines. (arXiv:2204.00778v1 [stat.ML])
28. Paoding: Supervised Robustness-preserving Data-free Neural Network Pruning. (arXiv:2204.00783v1 [cs.LG])
29. HLDC: Hindi Legal Documents Corpus. (arXiv:2204.00806v1 [cs.CL])
30. Efficient comparison of sentence embeddings. (arXiv:2204.00820v1 [cs.CL])
31. AdaSmooth: An Adaptive Learning Rate Method based on Effective Ratio. (arXiv:2204.00825v1 [cs.LG])
32. Intelligence at the Extreme Edge: A Survey on Reformable TinyML. (arXiv:2204.00827v1 [cs.LG])
33. Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks. (arXiv:2204.00846v1 [cs.LG])
34. Production of Categorical Data Verifying Differential Privacy: Conception and Applications to Machine Learning. (arXiv:2204.00850v1 [cs.CR])
35. Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs. (arXiv:2204.00853v1 [cs.CV])
36. A Differential Evolution-Enhanced Latent Factor Analysis Model for High-dimensional and Sparse Data. (arXiv:2204.00861v1 [cs.LG])
37. Accurate Online Posterior Alignments for Principled Lexically-Constrained Decoding. (arXiv:2204.00871v1 [cs.CL])
38. Dimensionless machine learning: Imposing exact units equivariance. (arXiv:2204.00887v1 [stat.ML])
39. Learning List-wise Representation in Reinforcement Learning for Ads Allocation with Multiple Auxiliary Tasks. (arXiv:2204.00888v1 [cs.LG])
40. Class-Incremental Learning by Knowledge Distillation with Adaptive Feature Consolidation. (arXiv:2204.00895v1 [cs.LG])
41. AutoProtoNet: Interpretability for Prototypical Networks. (arXiv:2204.00929v1 [cs.LG])
42. Exploiting Local and Global Features in Transformer-based Extreme Multi-label Text Classification. (arXiv:2204.00933v1 [cs.CL])
43. Risk-Aware Control and Optimization for High-Renewable Power Grids. (arXiv:2204.00950v1 [math.OC])
44. Model-Free and Model-Based Policy Evaluation when Causality is Uncertain. (arXiv:2204.00956v1 [cs.LG])
45. Long-tailed Extreme Multi-label Text Classification with Generated Pseudo Label Descriptions. (arXiv:2204.00958v1 [cs.LG])
46. Dynamic physical activity recommendation on personalised mobile health information service: A deep reinforcement learning approach. (arXiv:2204.00961v1 [cs.LG])
47. A Dynamic Meta-Learning Model for Time-Sensitive Cold-Start Recommendations. (arXiv:2204.00970v1 [cs.IR])
48. Kernel Extreme Learning Machine Optimized by the Sparrow Search Algorithm for Hyperspectral Image Classification. (arXiv:2204.00973v1 [cs.CV])
49. FedGBF: An efficient vertical federated learning framework via gradient boosting and bagging. (arXiv:2204.00976v1 [cs.LG])
50. Towards Web Phishing Detection Limitations and Mitigation. (arXiv:2204.00985v1 [cs.CR])
51. Bi-fidelity Modeling of Uncertain and Partially Unknown Systems using DeepONets. (arXiv:2204.00997v1 [stat.ML])
52. A Computational Analysis of Pitch Drift in Unaccompanied Solo Singing using DBSCAN Clustering. (arXiv:2204.01009v1 [cs.SD])
53. On Efficiently Acquiring Annotations for Multilingual Models. (arXiv:2204.01016v1 [cs.CL])
54. A Differentially Private Framework for Deep Learning with Convexified Loss Functions. (arXiv:2204.01049v1 [cs.CR])
55. Understanding the unstable convergence of gradient descent. (arXiv:2204.01050v1 [math.OC])
56. Learning-Based Approaches for Graph Problems: A Survey. (arXiv:2204.01057v1 [math.CO])
57. Correlation Functions in Random Fully Connected Neural Networks at Finite Width. (arXiv:2204.01058v1 [math.PR])
58. Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI. (arXiv:2204.01075v1 [cs.HC])
59. Faces: AI Blitz XIII Solutions. (arXiv:2204.01081v1 [cs.CV])
60. Breaking the De-Pois Poisoning Defense. (arXiv:2204.01090v1 [cs.LG])
61. pmuBAGE: The Benchmarking Assortment of Generated PMU Data for Power System Events -- Part I: Overview and Results. (arXiv:2204.01095v1 [cs.LG])
62. Adversarially robust segmentation models learn perceptually-aligned gradients. (arXiv:2204.01099v1 [cs.CV])
63. Fitting an immersed submanifold to data via Sussmann's orbit theorem. (arXiv:2204.01119v1 [cs.LG])
64. A System for Interactive Examination of Learned Security Policies. (arXiv:2204.01126v1 [cs.CR])
65. Proceedings of TDA: Applications of Topological Data Analysis to Data Science, Artificial Intelligence, and Machine Learning Workshop at SDM 2022. (arXiv:2204.01142v1 [math.AT])
66. Proactive Anomaly Detection for Robot Navigation with Multi-Sensor Fusion. (arXiv:2204.01146v1 [cs.RO])
67. Byzantine-Robust Federated Linear Bandits. (arXiv:2204.01155v1 [cs.LG])
68. Best-Response Bayesian Reinforcement Learning with Bayes-adaptive POMDPs for Centaurs. (arXiv:2204.01160v1 [cs.AI])
69. Seemo: A new tool for early design window view satisfaction evaluation in residential buildings. (arXiv:2204.01164v1 [cs.LG])
70. Why **Exposure** Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v1 [cs.CL])
71. Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution. (arXiv:2204.01188v1 [cs.CV])
72. Continuous Variable Quantum MNIST Classifiers. (arXiv:2204.01194v1 [quant-ph])
73. Towards Large-Scale Learned Solvers for Parametric PDEs with Model-Parallel Fourier Neural Operators. (arXiv:2204.01205v1 [cs.LG])
74. Learning Linear Symmetries in Data Using Moment Matching. (arXiv:2204.01213v1 [cs.LG])
75. Capturing positive utilities during the estimation of recursive logit models: A prism-based approach. (arXiv:2204.01215v1 [econ.EM])
76. MLPro: A System for Hosting Crowdsourced Machine Learning Challenges for Open-Ended Research Problems. (arXiv:2204.01216v1 [cs.HC])
77. Analysis of Joint Speech-Text Embeddings for Semantic Matching. (arXiv:2204.01235v1 [cs.CL])
78. Differentiable Rendering for Synthetic Aperture Radar Imagery. (arXiv:2204.01248v1 [eess.IV])
79. Into-TTS : Intonation Template based Prosody Control System. (arXiv:2204.01271v1 [eess.AS])
80. On The Identifiability of Mixture Models from Grouped Samples. (arXiv:1502.06644v2 [stat.ML] UPDATED)
81. Possibility results for graph clustering: A novel consistency axiom. (arXiv:1806.06142v6 [cs.LG] UPDATED)
82. A Geometric Approach of Gradient Descent Algorithms in Linear Neural Networks. (arXiv:1811.03568v3 [cs.LG] UPDATED)
83. Removing Malicious Nodes from Networks. (arXiv:1812.11448v7 [cs.SI] UPDATED)
84. First-Order Bayesian Regret Analysis of Thompson Sampling. (arXiv:1902.00681v3 [cs.LG] UPDATED)
85. Estimating heterogeneous treatment effects with right-censored data via causal survival forests. (arXiv:2001.09887v3 [stat.ME] UPDATED)
86. Towards Robust and Reproducible Active Learning Using Neural Networks. (arXiv:2002.09564v2 [cs.LG] UPDATED)
87. Deep Reinforcement Learning for FlipIt Security Game. (arXiv:2002.12909v2 [cs.LG] UPDATED)
88. Critical Assessment of Transfer Learning for Medical Image Segmentation with Fully Convolutional Neural Networks. (arXiv:2006.00356v2 [cs.CV] UPDATED)
89. Random Hyperboxes. (arXiv:2006.00695v4 [cs.LG] UPDATED)
90. Exploiting Higher Order Smoothness in Derivative-free Optimization and Continuous Bandits. (arXiv:2006.07862v3 [cs.LG] UPDATED)
91. The reinforcement learning-based multi-agent cooperative approach for the adaptive speed regulation on a metallurgical pickling line. (arXiv:2008.06933v2 [cs.LG] UPDATED)
92. Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism. (arXiv:2009.04544v3 [cs.LG] UPDATED)
93. Online nonnegative CP-dictionary learning for Markovian data. (arXiv:2009.07612v4 [stat.ML] UPDATED)
94. Tropical time series, iterated-sums signatures and quasisymmetric functions. (arXiv:2009.08443v3 [math.RA] UPDATED)
95. Cognitive Learning-Aided Multi-Antenna Communications. (arXiv:2010.03131v3 [eess.SP] UPDATED)
96. Precise Statistical Analysis of Classification Accuracies for Adversarial Training. (arXiv:2010.11213v2 [stat.ML] UPDATED)
97. Adversarial Robust Low Rank Matrix Estimation: Compressed Sensing and Matrix Completion. (arXiv:2010.13018v4 [stat.ML] UPDATED)
98. Greedy k-Center from Noisy Distance Samples. (arXiv:2011.01973v3 [cs.DS] UPDATED)
99. Graph Neural Networks in Recommender Systems: A Survey. (arXiv:2011.02260v4 [cs.IR] UPDATED)
100. On the Convergence of Continuous Constrained Optimization for Structure Learning. (arXiv:2011.11150v3 [cs.LG] UPDATED)
101. MLPerf Mobile Inference Benchmark. (arXiv:2012.02328v3 [cs.LG] UPDATED)
102. Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v4 [eess.IV] UPDATED)
103. Joint Continuous and Discrete Model Selection via Submodularity. (arXiv:2102.09029v2 [math.OC] UPDATED)
104. Scalable federated machine learning with FEDn. (arXiv:2103.00148v2 [cs.LG] UPDATED)
105. Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings. (arXiv:2103.14572v3 [cs.CV] UPDATED)
106. Quantum Self-Supervised Learning. (arXiv:2103.14653v3 [quant-ph] UPDATED)
107. Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx. (arXiv:2104.05522v6 [cs.LG] UPDATED)
108. When Does Contrastive Visual Representation Learning Work?. (arXiv:2105.05837v2 [cs.CV] UPDATED)
109. OpReg-Boost: Learning to Accelerate Online Algorithms with Operator Regression. (arXiv:2105.13271v3 [cs.LG] UPDATED)
110. Informing Geometric Deep Learning with Electronic Interactions to Accelerate Quantum Chemistry. (arXiv:2105.14655v4 [cs.LG] UPDATED)
111. Dynamic-Deep: Tune ECG Task Performance and Optimize Compression in IoT Architectures. (arXiv:2106.00606v2 [eess.SP] UPDATED)
112. Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v2 [cs.CV] UPDATED)
113. Sum of Ranked Range Loss for Supervised Learning. (arXiv:2106.03300v2 [cs.LG] UPDATED)
114. Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v6 [cs.LG] UPDATED)
115. BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v3 [cs.LG] UPDATED)
116. Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v3 [cs.CV] UPDATED)
117. TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?. (arXiv:2106.11297v4 [cs.CV] UPDATED)
118. Supporting AI Engineering on the IoT Edge through Model-Driven TinyML. (arXiv:2107.02690v2 [cs.SE] UPDATED)
119. Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage. (arXiv:2107.03920v3 [stat.ML] UPDATED)
120. A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v4 [cs.SE] UPDATED)
121. A general sample complexity analysis of vanilla policy gradient. (arXiv:2107.11433v4 [cs.LG] UPDATED)
122. Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search. (arXiv:2108.01036v4 [cs.AI] UPDATED)
123. Unsupervised Learning of Debiased Representations with Pseudo-Attributes. (arXiv:2108.02943v2 [cs.LG] UPDATED)
124. Settling the Variance of Multi-Agent Policy Gradients. (arXiv:2108.08612v3 [cs.LG] UPDATED)
125. Multi-agent Natural Actor-critic Reinforcement Learning Algorithms. (arXiv:2109.01654v3 [cs.LG] UPDATED)
126. Investigating and Modeling the Dynamics of Long Ties. (arXiv:2109.10523v3 [cs.SI] UPDATED)
127. Accelerated nonlinear primal-dual hybrid gradient methods with applications to supervised machine learning. (arXiv:2109.12222v2 [math.OC] UPDATED)
128. Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v4 [cs.CL] UPDATED)
129. IGLU: Efficient GCN Training via Lazy Updates. (arXiv:2109.13995v2 [cs.LG] UPDATED)
130. Real-Time Learning from An Expert in Deep Recommendation Systems with Marginal Distance Probability Distribution. (arXiv:2110.06287v2 [cs.LG] UPDATED)
131. An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. (arXiv:2110.08527v3 [cs.CL] UPDATED)
132. Towards Federated Bayesian Network Structure Learning with Continuous Optimization. (arXiv:2110.09356v2 [cs.LG] UPDATED)
133. Joint Gaussian Graphical Model Estimation: A Survey. (arXiv:2110.10281v3 [stat.ME] UPDATED)
134. Projection-Free Algorithm for Stochastic Bi-level Optimization. (arXiv:2110.11721v2 [math.OC] UPDATED)
135. Off-Policy Evaluation in Partially Observed Markov Decision Processes. (arXiv:2110.12343v2 [cs.LG] UPDATED)
136. CS-Rep: Making Speaker Verification Networks Embracing Re-parameterization. (arXiv:2110.13465v2 [cs.SD] UPDATED)
137. Unbiased Graph Embedding with Biased Graph Observations. (arXiv:2110.13957v3 [cs.LG] UPDATED)
138. CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v3 [cs.CV] UPDATED)
139. Power Allocation for Wireless Federated Learning using Graph Neural Networks. (arXiv:2111.07480v2 [cs.LG] UPDATED)
140. It's About Time: Analog Clock Reading in the Wild. (arXiv:2111.09162v3 [cs.CV] UPDATED)
141. Calibrated Diffusion Tensor Estimation. (arXiv:2111.10847v2 [cs.LG] UPDATED)
142. Plan Better Amid Conservatism: Offline Multi-Agent Reinforcement Learning with Actor Rectification. (arXiv:2111.11188v3 [cs.LG] UPDATED)
143. SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation with Learnt Surface Embeddings. (arXiv:2111.13489v2 [cs.CV] UPDATED)
144. An Optimization Framework for Federated Edge Learning. (arXiv:2111.13526v2 [cs.LG] UPDATED)
145. Dynamic Network-Assisted D2D-Aided Coded Distributed Learning. (arXiv:2111.14789v2 [cs.LG] UPDATED)
146. End-to-End Referring Video Object Segmentation with Multimodal Transformers. (arXiv:2111.14821v2 [cs.CV] UPDATED)
147. Assessment of Data Consistency through Cascades of Independently Recurrent Inference Machines for fast and robust accelerated MRI reconstruction. (arXiv:2111.15498v2 [eess.IV] UPDATED)
148. Remixing Functionally Graded Structures: Data-Driven Topology Optimization with Multiclass Shape Blending. (arXiv:2112.00648v2 [cs.CE] UPDATED)
149. A More Stable Accelerated Gradient Method Inspired by Continuous-Time Perspective. (arXiv:2112.04922v2 [math.OC] UPDATED)
150. ElegantRL-Podracer: Scalable and Elastic Library for Cloud-Native Deep Reinforcement Learning. (arXiv:2112.05923v2 [cs.LG] UPDATED)
151. An Introduction to Quantum Computing for Statisticians and Data Scientists. (arXiv:2112.06587v2 [stat.CO] UPDATED)
152. A cross-domain recommender system using deep coupled autoencoders. (arXiv:2112.07617v3 [cs.IR] UPDATED)
153. DISTREAL: Distributed Resource-Aware Learning in Heterogeneous Systems. (arXiv:2112.08761v2 [cs.LG] UPDATED)
154. Meta Propagation Networks for Graph Few-shot Semi-supervised Learning. (arXiv:2112.09810v2 [cs.LG] UPDATED)
155. Improving Subgraph Recognition with Variational Graph Information Bottleneck. (arXiv:2112.09899v3 [cs.LG] UPDATED)
156. Deep Learning Models for Knowledge Tracing: Review and Empirical Evaluation. (arXiv:2112.15072v3 [cs.LG] UPDATED)
157. Direct multi-modal inversion of geophysical logs using deep learning. (arXiv:2201.01871v2 [physics.geo-ph] UPDATED)
158. Language-driven Semantic Segmentation. (arXiv:2201.03546v2 [cs.CV] UPDATED)
159. Captcha Attack: Turning Captchas Against Humanity. (arXiv:2201.04014v3 [cs.CR] UPDATED)
160. Impact of Stop Sets on Stopping Active Learning for Text Classification. (arXiv:2201.05460v2 [cs.IR] UPDATED)
161. Formula graph self-attention network for representation-domain independent materials discovery. (arXiv:2201.05649v2 [cs.LG] UPDATED)
162. Minimax risk classifiers with 0-1 loss. (arXiv:2201.06487v2 [stat.ML] UPDATED)
163. Learning Wave Propagation with Attention-Based Convolutional Recurrent Autoencoder Net. (arXiv:2201.06628v3 [physics.flu-dyn] UPDATED)
164. Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v2 [eess.IV] UPDATED)
165. Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning. (arXiv:2201.09671v2 [cs.LG] UPDATED)
166. Adaptive and Robust Multi-task Learning. (arXiv:2202.05250v2 [stat.ML] UPDATED)
167. Efficient Natural Gradient Descent Methods for Large-Scale Optimization Problems. (arXiv:2202.06236v2 [math.OC] UPDATED)
168. Benign Overfitting in Two-layer Convolutional Neural Networks. (arXiv:2202.06526v2 [cs.LG] UPDATED)
169. Learning and Evaluating Graph Neural Network Explanations based on Counterfactual and Factual Reasoning. (arXiv:2202.08816v2 [cs.IR] UPDATED)
170. Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v2 [eess.IV] UPDATED)
171. Constant matters: Fine-grained Complexity of Differentially Private Continual Observation. (arXiv:2202.11205v2 [cs.DS] UPDATED)
172. An optimal scheduled learning rate for a randomized Kaczmarz algorithm. (arXiv:2202.12224v3 [math.NA] UPDATED)
173. Graph Attention Retrospective. (arXiv:2202.13060v2 [cs.LG] UPDATED)
174. Parameter-free Mirror Descent. (arXiv:2203.00444v2 [cs.LG] UPDATED)
175. 3D Common Corruptions and Data Augmentation. (arXiv:2203.01441v2 [cs.CV] UPDATED)
176. Data-driven detector signal characterization with constrained bottleneck autoencoders. (arXiv:2203.04604v3 [physics.ins-det] UPDATED)
177. Learning Distinctive Margin toward Active Domain Adaptation. (arXiv:2203.05738v2 [cs.LG] UPDATED)
178. Symbolic Learning to Optimize: Towards Interpretability and Scalability. (arXiv:2203.06578v3 [cs.LG] UPDATED)
179. Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v4 [cs.CV] UPDATED)
180. Contrastive Learning with Positive-Negative Frame Mask for Music Representation. (arXiv:2203.09129v2 [cs.SD] UPDATED)
181. Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v2 [cs.CV] UPDATED)
182. Forecast Evaluation for Data Scientists: Common Pitfalls and Best Practices. (arXiv:2203.10716v2 [cs.LG] UPDATED)
183. A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v2 [cs.LG] UPDATED)
184. Mix and Match: Learning-free Controllable Text Generation using Energy Language Models. (arXiv:2203.13299v2 [cs.CL] UPDATED)
185. Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v2 [cs.LG] UPDATED)
186. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v2 [cs.LG] UPDATED)
187. A Roadmap for Big Model. (arXiv:2203.14101v3 [cs.LG] UPDATED)
188. Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v2 [cs.CL] UPDATED)
189. Federated Learning with Position-Aware Neurons. (arXiv:2203.14666v2 [cs.CV] UPDATED)
190. AUC Maximization in the Era of Big Data and AI: A Survey. (arXiv:2203.15046v2 [cs.LG] UPDATED)
191. Consistency regularization-based Deep Polynomial Chaos Neural Network Method for Reliability Analysis. (arXiv:2203.15655v2 [cs.LG] UPDATED)
192. A Simple Yet Effective Pretraining Strategy for Graph Few-shot Learning. (arXiv:2203.15936v2 [cs.LG] UPDATED)
193. Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v3 [cs.CL] UPDATED)
194. An Improved Lightweight YOLOv5 Model Based on Attention Mechanism for Face Mask Detection. (arXiv:2203.16506v2 [cs.CV] UPDATED)
## cs.AI
---
**89** new papers in cs.AI:-) 
1. Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations. (arXiv:2204.00617v1 [eess.IV])
2. CogNGen: Constructing the Kernel of a Hyperdimensional Predictive Processing Cognitive Architecture. (arXiv:2204.00619v1 [cs.AI])
3. Universal Lymph Node Detection in T2 MRI using Neural Networks. (arXiv:2204.00622v1 [eess.IV])
4. Explainable and Interpretable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning. (arXiv:2204.00624v1 [cs.LG])
5. UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])
6. Hysteresis-Based RL: Robustifying Reinforcement Learning-based Control Policies via Hybrid Control. (arXiv:2204.00654v1 [cs.LG])
7. Testing Feedforward Neural Networks Training Programs. (arXiv:2204.00694v1 [cs.SE])
8. RFID-Based Indoor Spatial Query Evaluation with Bayesian Filtering Techniques. (arXiv:2204.00747v1 [cs.AI])
9. Safe Reinforcement Learning via Shielding for POMDPs. (arXiv:2204.00755v1 [cs.AI])
10. Speaker adaptation for Wav2vec2 based dysarthric ASR. (arXiv:2204.00770v1 [cs.SD])
11. SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images. (arXiv:2204.00790v1 [cs.CV])
12. Introduction to the Artificial Intelligence that can be applied to the Network Automation Journey. (arXiv:2204.00800v1 [cs.NI])
13. HLDC: Hindi Legal Documents Corpus. (arXiv:2204.00806v1 [cs.CL])
14. Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images. (arXiv:2204.00840v1 [cs.CV])
15. CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation. (arXiv:2204.00862v1 [cs.CL])
16. SciNoBo : A Hierarchical Multi-Label Classifier of Scientific Publications. (arXiv:2204.00880v1 [cs.DL])
17. Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging. (arXiv:2204.00885v1 [cs.CL])
18. FedGBF: An efficient vertical federated learning framework via gradient boosting and bagging. (arXiv:2204.00976v1 [cs.LG])
19. AutoOpt: A Methodological Framework of Automatically Designing Metaheuristics for Optimization Problems. (arXiv:2204.00998v1 [cs.NE])
20. Selective Kernel Attention for Robust Speaker Verification. (arXiv:2204.01005v1 [eess.AS])
21. Task2Dial: A Novel Task and Dataset for Commonsense enhanced Task-based Dialogue Grounded in Documents. (arXiv:2204.01061v1 [cs.CL])
22. Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI. (arXiv:2204.01075v1 [cs.HC])
23. Virtual Relational Knowledge Graphs for Recommendation. (arXiv:2204.01089v1 [cs.IR])
24. A sequence-to-sequence approach for document-level relation extraction. (arXiv:2204.01098v1 [cs.CL])
25. Proactive Anomaly Detection for Robot Navigation with Multi-Sensor Fusion. (arXiv:2204.01146v1 [cs.RO])
26. Best-Response Bayesian Reinforcement Learning with Bayes-adaptive POMDPs for Centaurs. (arXiv:2204.01160v1 [cs.AI])
27. Pragmatic constraints and pronoun reference disambiguation: the possible and the impossible. (arXiv:2204.01166v1 [cs.CL])
28. Few Shot Protein Generation. (arXiv:2204.01168v1 [q-bio.BM])
29. Why **Exposure** Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v1 [cs.CL])
30. BatchFormerV2: Exploring Sample Relationships for Dense Representation Learning. (arXiv:2204.01254v1 [cs.CV])
31. Monte Carlo Physarum Machine: Characteristics of Pattern Formation in Continuous Stochastic Transport Networks. (arXiv:2204.01256v1 [astro-ph.CO])
32. Multi-modality Associative Bridging through Memory: Speech Sound Recollected from Face Video. (arXiv:2204.01265v1 [cs.CV])
33. Deep Reinforcement Learning for FlipIt Security Game. (arXiv:2002.12909v2 [cs.LG] UPDATED)
34. Construction and Elicitation of a Black Box Model in the Game of Bridge. (arXiv:2005.01633v2 [cs.AI] UPDATED)
35. Cognitive Learning-Aided Multi-Antenna Communications. (arXiv:2010.03131v3 [eess.SP] UPDATED)
36. Reinforcement Learning Based Temporal Logic Control with Soft Constraints Using Limit-deterministic Generalized Buchi Automata. (arXiv:2101.10284v4 [cs.RO] UPDATED)
37. Uncertainty Maximization in Partially Observable Domains: A Cognitive Perspective. (arXiv:2102.11232v4 [cs.AI] UPDATED)
38. PROVED: A Tool for Graph Representation and Analysis of Uncertain Event Data. (arXiv:2103.05564v2 [cs.AI] UPDATED)
39. A Benchmark and Comprehensive Survey on Knowledge Graph Entity Alignment via Representation Learning. (arXiv:2103.15059v4 [cs.AI] UPDATED)
40. Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx. (arXiv:2104.05522v6 [cs.LG] UPDATED)
41. A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation. (arXiv:2104.08704v2 [cs.CL] UPDATED)
42. BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v3 [cs.LG] UPDATED)
43. Neuroevolution-Enhanced Multi-Objective Optimization for Mixed-Precision Quantization. (arXiv:2106.07611v2 [cs.NE] UPDATED)
44. Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis. (arXiv:2106.11485v3 [cs.CV] UPDATED)
45. Supporting AI Engineering on the IoT Edge through Model-Driven TinyML. (arXiv:2107.02690v2 [cs.SE] UPDATED)
46. MarIA: Spanish Language Models. (arXiv:2107.07253v4 [cs.CL] UPDATED)
47. A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v4 [cs.SE] UPDATED)
48. Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search. (arXiv:2108.01036v4 [cs.AI] UPDATED)
49. Settling the Variance of Multi-Agent Policy Gradients. (arXiv:2108.08612v3 [cs.LG] UPDATED)
50. Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning. (arXiv:2109.11251v2 [cs.AI] UPDATED)
51. Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v4 [cs.CL] UPDATED)
52. AutoPhaseNN: Unsupervised Physics-aware Deep Learning of 3D Nanoscale Bragg Coherent Diffraction Imaging. (arXiv:2109.14053v2 [physics.app-ph] UPDATED)
53. Real-Time Learning from An Expert in Deep Recommendation Systems with Marginal Distance Probability Distribution. (arXiv:2110.06287v2 [cs.LG] UPDATED)
54. Towards Social Situation Awareness in Support Agents. (arXiv:2110.09829v3 [cs.AI] UPDATED)
55. Privacy in Open Search: A Review of Challenges and Solutions. (arXiv:2110.10720v4 [cs.CR] UPDATED)
56. Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups. (arXiv:2110.13059v2 [cs.CV] UPDATED)
57. Unbiased Graph Embedding with Biased Graph Observations. (arXiv:2110.13957v3 [cs.LG] UPDATED)
58. CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v3 [cs.CV] UPDATED)
59. Diagnosing Data from ICTs to Provide Focused Assistance in Agricultural Adoptions. (arXiv:2111.00052v3 [cs.CY] UPDATED)
60. Calibrated Diffusion Tensor Estimation. (arXiv:2111.10847v2 [cs.LG] UPDATED)
61. SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation with Learnt Surface Embeddings. (arXiv:2111.13489v2 [cs.CV] UPDATED)
62. ViF-SD2E: A Robust Weakly-Supervised Method for Neural Decoding. (arXiv:2112.01261v2 [cs.NE] UPDATED)
63. Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?. (arXiv:2112.02125v2 [cs.CR] UPDATED)
64. IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes. (arXiv:2112.05298v3 [cs.CV] UPDATED)
65. ElegantRL-Podracer: Scalable and Elastic Library for Cloud-Native Deep Reinforcement Learning. (arXiv:2112.05923v2 [cs.LG] UPDATED)
66. Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge. (arXiv:2112.08619v2 [cs.CL] UPDATED)
67. Improving Subgraph Recognition with Variational Graph Information Bottleneck. (arXiv:2112.09899v3 [cs.LG] UPDATED)
68. A Saliency based Feature Fusion Model for EEG Emotion Estimation. (arXiv:2201.03891v3 [cs.CV] UPDATED)
69. Subgoal-Based Explanations for Unreliable Intelligent Decision Support Systems. (arXiv:2201.04204v2 [cs.AI] UPDATED)
70. Machine-Learning enabled analysis of ELM filament dynamics in KSTAR. (arXiv:2201.07941v2 [physics.plasm-ph] UPDATED)
71. Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v2 [eess.IV] UPDATED)
72. Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v5 [cs.CL] UPDATED)
73. Conflict-Based Search for Explainable Multi-Agent Path Finding. (arXiv:2202.09930v2 [cs.AI] UPDATED)
74. Brain Principles Programming. (arXiv:2202.12710v3 [q-bio.NC] UPDATED)
75. Rethinking and Refining the Distinct Metric. (arXiv:2202.13587v3 [cs.CL] UPDATED)
76. Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data. (arXiv:2203.00734v2 [cs.CV] UPDATED)
77. Symbolic Learning to Optimize: Towards Interpretability and Scalability. (arXiv:2203.06578v3 [cs.LG] UPDATED)
78. Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v4 [cs.CV] UPDATED)
79. HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2203.07376v2 [cs.DB] UPDATED)
80. Prioritized Variable-length Test Cases Generation for Finite State Machines. (arXiv:2203.09596v2 [cs.SE] UPDATED)
81. Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation. (arXiv:2203.09811v2 [cs.CV] UPDATED)
82. Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v2 [cs.CV] UPDATED)
83. Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v2 [cs.LG] UPDATED)
84. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v2 [cs.LG] UPDATED)
85. A Roadmap for Big Model. (arXiv:2203.14101v3 [cs.LG] UPDATED)
86. Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v2 [cs.CL] UPDATED)
87. Subjective Evaluation of Deep Learning Models for Symbolic Music Composition. (arXiv:2203.14641v2 [cs.SD] UPDATED)
88. AUC Maximization in the Era of Big Data and AI: A Survey. (arXiv:2203.15046v2 [cs.LG] UPDATED)
89. Adaptive Learning with Artificial Barriers Yielding Nash Equilibria in General Games. (arXiv:2203.15780v2 [cs.GT] UPDATED)

