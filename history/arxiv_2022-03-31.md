# Your interest papers
---
## cs.CV
---
### Towards Learning Neural Representations from Shadows. (arXiv:2203.15946v1 [cs.CV])
- Authors : Kushagra Tiwary, Tzofi Klinghoffer, Ramesh Raskar
- Link : [http://arxiv.org/abs/2203.15946](http://arxiv.org/abs/2203.15946)
> ABSTRACT  :  We present a method that learns neural scene representations from only shadows present in the scene. While traditional shape-from-shadow (SfS) algorithms reconstruct geometry from shadows, they assume a fixed scanning setup and fail to generalize to complex scenes. Neural rendering algorithms, on the other hand, rely on photometric consistency between RGB images but largely ignore physical cues such as shadows, which have been shown to provide valuable information about the scene. We observe that shadows are a powerful cue that can constrain neural scene representations to learn SfS, and even outperform **NeRF** to reconstruct otherwise hidden geometry. We propose a graphics-inspired differentiable approach to render accurate shadows with volumetric rendering, predicting a shadow map that can be compared to the ground truth shadow. Even with just binary shadow maps, we show that neural rendering can localize the object and estimate coarse geometry. Our approach reveals that sparse cues in images can be used to estimate geometry using differentiable volumetric rendering. Moreover, our framework is highly generalizable and can work alongside existing 3D reconstruction techniques that otherwise only use photometric consistency. Our code is made available in our supplementary materials.  
### AdaMixer: A Fast-Converging Query-Based Object Detector. (arXiv:2203.16507v1 [cs.CV])
- Authors : Ziteng Gao, Limin Wang, Bing Han, Sheng Guo
- Link : [http://arxiv.org/abs/2203.16507](http://arxiv.org/abs/2203.16507)
> ABSTRACT  :  Traditional object detectors employ the dense paradigm of scanning over locations and scales in an image. The recent query-based object detectors break this convention by decoding image features with a set of learnable queries. However, this paradigm still suffers from slow convergence, limited performance, and design complexity of extra networks between backbone and decoder. In this paper, we find that the key to these issues is the adaptability of decoders for casting queries to varying objects. Accordingly, we propose a fast-converging query-based detector, named AdaMixer, by improving the adaptability of query-based decoding processes in two aspects. First, each query adaptively samples features over space and scales based on estimated offsets, which allows AdaMixer to efficiently attend to the coherent regions of objects. Then, we dynamically decode these sampled features with an adaptive MLP-Mixer under the guidance of each query. Thanks to these two critical designs, AdaMixer enjoys architectural simplicity without requiring dense attentional encoders or explicit pyramid networks. On the challenging MS COCO benchmark, AdaMixer with ResNet-50 as the backbone, with 12 training epochs, reaches up to 45.0 AP on the validation set along with 27.9 APs in detecting small objects. With the longer training scheme, AdaMixer with ResNeXt-101-DCN and **Swin**-S reaches 49.5 and 51.3 AP. Our work sheds light on a simple, accurate, and fast converging architecture for query-based object detectors. The code is made available at https://github.com/MCG-NJU/AdaMixer  
### Large-Scale Pre-training for Person Re-identification with Noisy Labels. (arXiv:2203.16533v1 [cs.CV])
- Authors : Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao, Lu Yuan, **Lei Zhang**, Houqiang Li, Fang Wen, Dong Chen
- Link : [http://arxiv.org/abs/2203.16533](http://arxiv.org/abs/2203.16533)
> ABSTRACT  :  This paper aims to address the problem of pre-training for person re-identification (Re-ID) with noisy labels. To setup the pre-training task, we apply a simple online multi-object tracking system on raw videos of an existing unlabeled Re-ID dataset "LUPerson" nd build the Noisy Labeled variant called "LUPerson-NL". Since theses ID labels automatically derived from tracklets inevitably contain noises, we develop a large-scale Pre-training framework utilizing Noisy Labels (PNL), which consists of three learning modules: supervised Re-ID learning, prototype-based contrastive learning, and label-guided contrastive learning. In principle, joint learning of these three modules not only clusters similar examples to one prototype, but also rectifies noisy labels based on the prototype assignment. We demonstrate that learning directly from raw videos is a promising alternative for pre-training, which utilizes spatial and temporal correlations as weak supervision. This simple pre-training task provides a scalable way to learn SOTA Re-ID representations from scratch on "LUPerson-NL" without bells and whistles. For example, by applying on the same supervised Re-ID method MGN, our pre-trained model improves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%, 2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or few-shot setting, the performance gain is even more significant, suggesting a better transferability of the learned representation. Code is available at https://github.com/DengpanFu/LUPerson-NL  
### Language-Level Semantics Conditioned 3D Point Cloud Segmentation. (arXiv:2107.00430v3 [cs.CV] UPDATED)
- Authors : Bo Liu, Shuang Deng, Qiulei Dong, Zhanyi Hu
- Link : [http://arxiv.org/abs/2107.00430](http://arxiv.org/abs/2107.00430)
> ABSTRACT  :  In this work, a language-level Semantics Conditioned framework for 3D Point cloud segmentation, called SeCondPoint, is proposed, where language-level semantics are introduced to condition the modeling of point feature distribution as well as the pseudo-feature generation, and a feature-geometry-based mixup approach is further proposed to facilitate the distribution learning. To our knowledge, this is the first attempt in literature to introduce language-level semantics to the 3D point cloud segmentation task. Since a large number of point features could be generated from the learned distribution thanks to the semantics conditioned modeling, any existing segmentation network could be embedded into the proposed framework to boost its performance. In addition, the proposed framework has the inherent advantage of dealing with novel classes, which seems an impossible feat for the current segmentation networks. Extensive experimental results on two public datasets demonstrate that three typical segmentation networks could achieve significant improvements over their original performances after **enhancement** by the proposed framework in the conventional 3D segmentation task. Two benchmarks are also introduced for a newly introduced zero-shot 3D segmentation task, and the results also validate the proposed framework.  
### ISNAS-DIP: Image-Specific Neural Architecture Search for Deep Image Prior. (arXiv:2111.15362v2 [cs.CV] UPDATED)
- Authors : Metin Ersin, Ozgur Kara, Gustav Bredell, Ender Konukoglu
- Link : [http://arxiv.org/abs/2111.15362](http://arxiv.org/abs/2111.15362)
> ABSTRACT  :  Recent works show that convolutional neural network (CNN) architectures have a spectral bias towards lower frequencies, which has been leveraged for various image **restoration** tasks in the Deep Image Prior (DIP) framework. The benefit of the inductive bias the network imposes in the DIP framework depends on the architecture. Therefore, researchers have studied how to automate the search to determine the best-performing model. However, common neural architecture search (NAS) techniques are resource and time-intensive. Moreover, best-performing models are determined for a whole dataset of images instead of for each image independently, which would be prohibitively expensive. In this work, we first show that optimal neural architectures in the DIP framework are image-dependent. Leveraging this insight, we then propose an image-specific NAS strategy for the DIP framework that requires substantially less training than typical NAS approaches, effectively enabling image-specific NAS. We justify the proposed strategy's effectiveness by (1) demonstrating its performance on a NAS Dataset for DIP that includes 522 models from a particular search space (2) conducting extensive experiments on image denoising, inpainting, and super-resolution tasks. Our experiments show that image-specific metrics can reduce the search space to a small cohort of models, of which the best model outperforms current NAS approaches for image **restoration**. Codes and datasets are available at https://github.com/ozgurkara99/ISNAS-DIP.  
### Geometry-Guided Progressive **NeRF** for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v3 [cs.CV] UPDATED)
- Authors : Mingfei Chen, Jianfeng Zhang, Xiangyu Xu, Lijuan Liu, Yujun Cai, Jiashi Feng, Shuicheng Yan
- Link : [http://arxiv.org/abs/2112.04312](http://arxiv.org/abs/2112.04312)
> ABSTRACT  :  In this work we develop a generalizable and efficient Neural Radiance Field (**NeRF**) pipeline for high-fidelity free-viewpoint human body synthesis under settings with sparse camera views. Though existing **NeRF**-based methods can synthesize rather realistic details for human body, they tend to produce poor results when the input has self-occlusion, especially for unseen humans under sparse views. Moreover, these methods often require a large number of sampling points for rendering, which leads to low efficiency and limits their real-world applicability. To address these challenges, we propose a Geometry-guided Progressive **NeRF** (GP-**NeRF**). In particular, to better tackle self-occlusion, we devise a geometry-guided multi-view feature integration approach that utilizes the estimated geometry prior to integrate the incomplete information from input views and construct a complete geometry volume for the target human body. Meanwhile, for achieving higher rendering efficiency, we introduce a progressive rendering pipeline through geometry guidance, which leverages the geometric feature volume and the predicted density values to progressively reduce the number of sampling points and speed up the rendering process. Experiments on the ZJU-MoCap and THUman datasets show that our method outperforms the state-of-the-arts significantly across multiple generalization settings, while the time cost is reduced &gt; 70% via applying our efficient progressive rendering pipeline.  
### Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v3 [cs.CV] UPDATED)
- Authors : Lin Zhu, Xiao Wang, Yi Chang, Jianing Li, Tiejun Huang, Yonghong Tian
- Link : [http://arxiv.org/abs/2201.10943](http://arxiv.org/abs/2201.10943)
> ABSTRACT  :  Neuromorphic vision sensor is a new bio-inspired imaging paradigm that reports asynchronous, continuously per-pixel brightness changes called `events' with high temporal resolution and **high dynamic range**. So far, the event-based image reconstruction methods are based on artificial neural networks (ANN) or hand-crafted spatiotemporal smoothing techniques. In this paper, we first implement the image reconstruction work via fully spiking neural network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. We propose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Membrane Potential (MP) neuron. We find that the spiking neurons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Furthermore, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AMP) neuron, which adaptively updates the membrane potential according to the input spikes. The experimental results demonstrate that our models achieve comparable performance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are 19.36$\times$ and 7.75$\times$ more computationally efficient than their ANN architectures, respectively.  
### DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v4 [cs.CV] UPDATED)
- Authors : Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, **Lei Zhang**
- Link : [http://arxiv.org/abs/2201.12329](http://arxiv.org/abs/2201.12329)
> ABSTRACT  :  We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \url{https://github.com/SlongLiu/DAB-DETR}.  
### Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v2 [eess.IV] UPDATED)
- Authors : Dezhao Wang, **Wenhan Yang**, Yueyu Hu, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2203.04963](http://arxiv.org/abs/2203.04963)
> ABSTRACT  :  Learned image compression has achieved great success due to its excellent modeling capacity, but seldom further considers the Rate-Distortion Optimization (RDO) of each input image. To explore this potential in the learned codec, we make the first attempt to build a neural data-dependent transform and introduce a continuous online mode decision mechanism to jointly optimize the coding efficiency for each individual image. Specifically, apart from the image content stream, we employ an additional model stream to generate the transform parameters at the decoder side. The presence of a model stream enables our model to learn more abstract neural-syntax, which helps cluster the latent representations of images more compactly. Beyond the transform stage, we also adopt neural-syntax based post-processing for the scenarios that require higher quality reconstructions regardless of extra decoding overhead. Moreover, the involvement of the model stream further makes it possible to optimize both the representation and the decoder in an online way, i.e. RDO at the testing time. It is equivalent to a continuous online mode decision, like coding modes in the traditional codecs, to improve the coding efficiency based on the individual input image. The experimental results show the effectiveness of the proposed neural-syntax design and the continuous online mode decision mechanism, demonstrating the superiority of our method in coding efficiency compared to the latest conventional standard Versatile Video Coding (VVC) and other state-of-the-art learning-based methods.  
### Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v3 [cs.CV] UPDATED)
- Authors : Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, Jian Sun
- Link : [http://arxiv.org/abs/2203.06717](http://arxiv.org/abs/2203.06717)
> ABSTRACT  :  We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than **Swin** Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code &amp; models at https://github.com/megvii-research/RepLKNet.  
### Fast Autofocusing using Tiny Transformer Networks for Digital Holographic Microscopy. (arXiv:2203.07772v3 [eess.IV] UPDATED)
- Authors : phane Cuenat, Louis Andr, Patrick Sandoz, Maxime Jacquot
- Link : [http://arxiv.org/abs/2203.07772](http://arxiv.org/abs/2203.07772)
> ABSTRACT  :  The numerical wavefront backpropagation principle of digital holography confers unique extended focus capabilities, without mechanical displacements along z-axis. However, the determination of the correct focusing distance is a non-trivial and time consuming issue. A deep learning (DL) solution is proposed to cast the autofocusing as a regression problem and tested over both experimental and simulated holograms. Single wavelength digital holograms were recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$ microscope objective from a patterned target moving in 3D over an axial range of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision Transformer (TViT), tiny VGG16 (TVGG) and a tiny **Swin**-Transfomer (T**Swin**T). The experiments show that the predicted focusing distance $Z_R^{\mathrm{Pred}}$ is accurately inferred with an accuracy of 1.2 $\mu$m in average in comparison with the DHM depth of field of 15 $\mu$m. Numerical simulations show that all tiny models give the $Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such a prospect would significantly improve the current capabilities of computer vision position sensing in applications such as 3D microscopy for life sciences or micro-robotics. Moreover, all models reach state of the art inference time on CPU, less than 25 ms per inference.  
### Unsupervised Domain Adaptation for **Night**time Aerial Tracking. (arXiv:2203.10541v2 [cs.CV] UPDATED)
- Authors : Junjie Ye, Changhong Fu, Guangze Zheng, Danda Pani, Guang Chen
- Link : [http://arxiv.org/abs/2203.10541](http://arxiv.org/abs/2203.10541)
> ABSTRACT  :  Previous advances in object tracking mostly reported on favorable illumination circumstances while neglecting performance at **night**time, which significantly impeded the development of related aerial robot applications. This work instead develops a novel unsupervised domain adaptation framework for **night**time aerial tracking (named UDAT). Specifically, a unique object discovery approach is provided to generate training patches from raw **night**time tracking videos. To tackle the domain discrepancy, we employ a Transformer-based bridging layer post to the feature extractor to align image features from both domains. With a Transformer day/**night** feature discriminator, the daytime tracking model is adversarially trained to track at **night**. Moreover, we construct a pioneering benchmark namely NAT2021 for unsupervised domain adaptive **night**time tracking, which comprises a test set of 180 manually annotated tracking sequences and a train set of over 276k unlabelled **night**time tracking frames. Exhaustive experiments demonstrate the robustness and domain adaptability of the proposed framework in **night**time aerial tracking. The code and benchmark are available at https://github.com/vision4robotics/UDAT.  
### Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v2 [cs.CV] UPDATED)
- Authors : Pratik Katte, Siva Teja, Geetha Manjunath
- Link : [http://arxiv.org/abs/2203.14128](http://arxiv.org/abs/2203.14128)
> ABSTRACT  :  In the last two years, millions of lives have been lost due to COVID-19. Despite the vaccination programmes for a year, hospitalization rates and deaths are still high due to the new variants of COVID-19. Stringent guidelines and COVID-19 screening measures such as temperature check and mask check at all public places are helping reduce the spread of COVID-19. Visual inspections to ensure these screening measures can be taxing and erroneous. Automated inspection ensures an effective and accurate screening. Traditional approaches involve identification of faces and masks from visual camera images followed by extraction of temperature values from thermal imaging cameras. Use of visual imaging as a primary modality limits these applications only for good-lighting conditions. The use of thermal imaging alone for these screening measures makes the system invariant to illumination. However, lack of open source datasets is an issue to develop such systems. In this paper, we discuss our work on using machine learning over thermal video streams for face and mask detection and subsequent temperature screening in a passive non-invasive way that enables an effective automated COVID-19 screening method in public places. We open source our NTIC dataset that was used for training our models and was collected at 8 different locations. Our results show that the use of thermal imaging is as effective as visual imaging in the presence of high illumination. This performance stays the same for thermal images even under **low-light**ing conditions, whereas the performance with visual trained classifiers show more than 50% degradation.  
## eess.IV
---
### Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v2 [eess.IV] UPDATED)
- Authors : Dezhao Wang, **Wenhan Yang**, Yueyu Hu, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2203.04963](http://arxiv.org/abs/2203.04963)
> ABSTRACT  :  Learned image compression has achieved great success due to its excellent modeling capacity, but seldom further considers the Rate-Distortion Optimization (RDO) of each input image. To explore this potential in the learned codec, we make the first attempt to build a neural data-dependent transform and introduce a continuous online mode decision mechanism to jointly optimize the coding efficiency for each individual image. Specifically, apart from the image content stream, we employ an additional model stream to generate the transform parameters at the decoder side. The presence of a model stream enables our model to learn more abstract neural-syntax, which helps cluster the latent representations of images more compactly. Beyond the transform stage, we also adopt neural-syntax based post-processing for the scenarios that require higher quality reconstructions regardless of extra decoding overhead. Moreover, the involvement of the model stream further makes it possible to optimize both the representation and the decoder in an online way, i.e. RDO at the testing time. It is equivalent to a continuous online mode decision, like coding modes in the traditional codecs, to improve the coding efficiency based on the individual input image. The experimental results show the effectiveness of the proposed neural-syntax design and the continuous online mode decision mechanism, demonstrating the superiority of our method in coding efficiency compared to the latest conventional standard Versatile Video Coding (VVC) and other state-of-the-art learning-based methods.  
### Fast Autofocusing using Tiny Transformer Networks for Digital Holographic Microscopy. (arXiv:2203.07772v3 [eess.IV] UPDATED)
- Authors : phane Cuenat, Louis Andr, Patrick Sandoz, Maxime Jacquot
- Link : [http://arxiv.org/abs/2203.07772](http://arxiv.org/abs/2203.07772)
> ABSTRACT  :  The numerical wavefront backpropagation principle of digital holography confers unique extended focus capabilities, without mechanical displacements along z-axis. However, the determination of the correct focusing distance is a non-trivial and time consuming issue. A deep learning (DL) solution is proposed to cast the autofocusing as a regression problem and tested over both experimental and simulated holograms. Single wavelength digital holograms were recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$ microscope objective from a patterned target moving in 3D over an axial range of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision Transformer (TViT), tiny VGG16 (TVGG) and a tiny **Swin**-Transfomer (T**Swin**T). The experiments show that the predicted focusing distance $Z_R^{\mathrm{Pred}}$ is accurately inferred with an accuracy of 1.2 $\mu$m in average in comparison with the DHM depth of field of 15 $\mu$m. Numerical simulations show that all tiny models give the $Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such a prospect would significantly improve the current capabilities of computer vision position sensing in applications such as 3D microscopy for life sciences or micro-robotics. Moreover, all models reach state of the art inference time on CPU, less than 25 ms per inference.  
### Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v2 [cs.CV] UPDATED)
- Authors : Pratik Katte, Siva Teja, Geetha Manjunath
- Link : [http://arxiv.org/abs/2203.14128](http://arxiv.org/abs/2203.14128)
> ABSTRACT  :  In the last two years, millions of lives have been lost due to COVID-19. Despite the vaccination programmes for a year, hospitalization rates and deaths are still high due to the new variants of COVID-19. Stringent guidelines and COVID-19 screening measures such as temperature check and mask check at all public places are helping reduce the spread of COVID-19. Visual inspections to ensure these screening measures can be taxing and erroneous. Automated inspection ensures an effective and accurate screening. Traditional approaches involve identification of faces and masks from visual camera images followed by extraction of temperature values from thermal imaging cameras. Use of visual imaging as a primary modality limits these applications only for good-lighting conditions. The use of thermal imaging alone for these screening measures makes the system invariant to illumination. However, lack of open source datasets is an issue to develop such systems. In this paper, we discuss our work on using machine learning over thermal video streams for face and mask detection and subsequent temperature screening in a passive non-invasive way that enables an effective automated COVID-19 screening method in public places. We open source our NTIC dataset that was used for training our models and was collected at 8 different locations. Our results show that the use of thermal imaging is as effective as visual imaging in the presence of high illumination. This performance stays the same for thermal images even under **low-light**ing conditions, whereas the performance with visual trained classifiers show more than 50% degradation.  
## cs.LG
---
### Phase-Aware Deep Speech **Enhancement**: It's All About The Frame Length. (arXiv:2203.16222v1 [eess.AS])
- Authors : Tal Peer, Timo Gerkmann
- Link : [http://arxiv.org/abs/2203.16222](http://arxiv.org/abs/2203.16222)
> ABSTRACT  :  While phase-aware speech processing has been receiving increasing attention in recent years, most narrowband STFT approaches with frame lengths of about 32ms show a rather modest impact of phase on overall performance. At the same time, modern deep neural network (DNN)-based approaches, like Conv-TasNet, that implicitly modify both magnitude and phase yield great performance on very short frames (2ms). Motivated by this observation, in this paper we systematically investigate the role of phase and magnitude in DNN-based speech **enhancement** for different frame lengths. The results show that a phase-aware DNN can take advantage of what previous studies concerning reconstruction of clean speech have shown: When using short frames, the phase spectrum becomes more important while the importance of the magnitude spectrum decreases. Furthermore, our experiments show that when both magnitude and phase are estimated, shorter frames result in a considerably improved performance in a DNN with explicit phase estimation. Contrarily, in the phase-blind case, where only magnitudes are processed, 32ms frames lead to the best performance. We conclude that DNN-based phase estimation benefits from the use of shorter frames and recommend a frame length of about 4ms for future phase-aware deep speech **enhancement** methods.  
### Low-complexity Near-optimum Symbol Detection Based on Neural **Enhancement** of Factor Graphs. (arXiv:2203.16417v1 [cs.IT])
- Authors : Luca Schmid, Laurent Schmalen
- Link : [http://arxiv.org/abs/2203.16417](http://arxiv.org/abs/2203.16417)
> ABSTRACT  :  We consider the application of the factor graph framework for symbol detection on linear inter-symbol interference channels. Based on the Ungerboeck observation model, a detection algorithm with appealing complexity properties can be derived. However, since the underlying factor graph contains cycles, the sum-product algorithm (SPA) yields a suboptimal algorithm. In this paper, we develop and evaluate efficient strategies to improve the performance of the factor graph-based symbol detection by means of neural **enhancement**. In particular, we consider neural belief propagation as an effective way to mitigate the effect of cycles within the factor graph. We also investigate the application of factor node generalizations and pruning techniques. By applying a generic preprocessor to the channel output, we propose a simple technique to vary the underlying factor graph in every SPA iteration. Using this dynamic factor graph transition, we intend to preserve the extrinsic nature of the SPA messages which is otherwise impaired due to cycles. Simulation results show that the proposed methods can massively improve the detection performance, even approaching the maximum a posteriori performance for various transmission scenarios, while preserving a complexity which is linear in both the block length and the channel memory.  
### Intelligent Blockage Prediction and Proactive Handover for Seamless Connectivity in Vision-Aided 5G/6G UDNs. (arXiv:2203.16419v1 [cs.NI])
- Authors : Mohammad Al, Ahsan Khan, Lina Mohjazi, Anthony Centeno, Ahmed Zoha, Muhammad Ali
- Link : [http://arxiv.org/abs/2203.16419](http://arxiv.org/abs/2203.16419)
> ABSTRACT  :  The upsurge in wireless devices and real-time service demands force the move to a higher frequency spectrum. Millimetre-wave (mmWave) and terahertz (THz) bands combined with the beamforming technology offer significant performance **enhancement**s for ultra-dense networks (UDNs). Unfortunately, shrinking cell coverage and severe penetration loss experienced at higher spectrum render mobility management a critical issue in UDNs, especially optimizing beam blockages and frequent handover (HO). Mobility management challenges have become prevalent in city centres and urban areas. To address this, we propose a novel mechanism driven by exploiting wireless signals and on-road surveillance systems to intelligently predict possible blockages in advance and perform timely HO. This paper employs computer vision (CV) to determine obstacles and users' location and speed. In addition, this study introduces a new HO event, called block event {BLK}, defined by the presence of a blocking object and a user moving towards the blocked area. Moreover, the multivariate regression technique predicts the remaining time until the user reaches the blocked area, hence determining best HO decision. Compared to typical wireless networks without blockage prediction, simulation results show that our BLK detection and PHO algorithm achieves 40\% improvement in maintaining user connectivity and the required quality of experience (QoE).  
### Learning Continuous Representation of Audio for Arbitrary Scale Super Resolution. (arXiv:2111.00195v2 [cs.SD] UPDATED)
- Authors : Jaechang Kim, Yunjoo Lee, Seunghoon Hong, Jungseul Ok
- Link : [http://arxiv.org/abs/2111.00195](http://arxiv.org/abs/2111.00195)
> ABSTRACT  :  Audio super resolution aims to predict the missing high resolution components of the low resolution audio signals. While audio in nature is a continuous signal, current approaches treat it as discrete data (i.e., input is defined on discrete time domain), and consider the super resolution over a fixed scale factor (i.e., it is required to train a new neural network to change output resolution). To obtain a continuous representation of audio and enable super resolution for arbitrary scale factor, we propose a method of **implicit neural representation**, coined Local Implicit representation for Super resolution of Arbitrary scale (LISA). Our method locally parameterizes a chunk of audio as a function of continuous time, and represents each chunk with the local latent codes of neighboring chunks so that the function can extrapolate the signal at any time coordinate, i.e., infinite resolution. To learn a continuous representation for audio, we design a self-supervised learning strategy to practice super resolution tasks up to the original resolution by stochastic selection. Our numerical evaluation shows that LISA outperforms the previous fixed-scale methods with a fraction of parameters, but also is capable of arbitrary scale super resolution even beyond the resolution of training data.  
### Geometry-Guided Progressive **NeRF** for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v3 [cs.CV] UPDATED)
- Authors : Mingfei Chen, Jianfeng Zhang, Xiangyu Xu, Lijuan Liu, Yujun Cai, Jiashi Feng, Shuicheng Yan
- Link : [http://arxiv.org/abs/2112.04312](http://arxiv.org/abs/2112.04312)
> ABSTRACT  :  In this work we develop a generalizable and efficient Neural Radiance Field (**NeRF**) pipeline for high-fidelity free-viewpoint human body synthesis under settings with sparse camera views. Though existing **NeRF**-based methods can synthesize rather realistic details for human body, they tend to produce poor results when the input has self-occlusion, especially for unseen humans under sparse views. Moreover, these methods often require a large number of sampling points for rendering, which leads to low efficiency and limits their real-world applicability. To address these challenges, we propose a Geometry-guided Progressive **NeRF** (GP-**NeRF**). In particular, to better tackle self-occlusion, we devise a geometry-guided multi-view feature integration approach that utilizes the estimated geometry prior to integrate the incomplete information from input views and construct a complete geometry volume for the target human body. Meanwhile, for achieving higher rendering efficiency, we introduce a progressive rendering pipeline through geometry guidance, which leverages the geometric feature volume and the predicted density values to progressively reduce the number of sampling points and speed up the rendering process. Experiments on the ZJU-MoCap and THUman datasets show that our method outperforms the state-of-the-arts significantly across multiple generalization settings, while the time cost is reduced &gt; 70% via applying our efficient progressive rendering pipeline.  
### Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v3 [cs.CV] UPDATED)
- Authors : Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, Jian Sun
- Link : [http://arxiv.org/abs/2203.06717](http://arxiv.org/abs/2203.06717)
> ABSTRACT  :  We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than **Swin** Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code &amp; models at https://github.com/megvii-research/RepLKNet.  
### Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v2 [cs.CV] UPDATED)
- Authors : Pratik Katte, Siva Teja, Geetha Manjunath
- Link : [http://arxiv.org/abs/2203.14128](http://arxiv.org/abs/2203.14128)
> ABSTRACT  :  In the last two years, millions of lives have been lost due to COVID-19. Despite the vaccination programmes for a year, hospitalization rates and deaths are still high due to the new variants of COVID-19. Stringent guidelines and COVID-19 screening measures such as temperature check and mask check at all public places are helping reduce the spread of COVID-19. Visual inspections to ensure these screening measures can be taxing and erroneous. Automated inspection ensures an effective and accurate screening. Traditional approaches involve identification of faces and masks from visual camera images followed by extraction of temperature values from thermal imaging cameras. Use of visual imaging as a primary modality limits these applications only for good-lighting conditions. The use of thermal imaging alone for these screening measures makes the system invariant to illumination. However, lack of open source datasets is an issue to develop such systems. In this paper, we discuss our work on using machine learning over thermal video streams for face and mask detection and subsequent temperature screening in a passive non-invasive way that enables an effective automated COVID-19 screening method in public places. We open source our NTIC dataset that was used for training our models and was collected at 8 different locations. Our results show that the use of thermal imaging is as effective as visual imaging in the presence of high illumination. This performance stays the same for thermal images even under **low-light**ing conditions, whereas the performance with visual trained classifiers show more than 50% degradation.  
## cs.AI
---
### Towards Learning Neural Representations from Shadows. (arXiv:2203.15946v1 [cs.CV])
- Authors : Kushagra Tiwary, Tzofi Klinghoffer, Ramesh Raskar
- Link : [http://arxiv.org/abs/2203.15946](http://arxiv.org/abs/2203.15946)
> ABSTRACT  :  We present a method that learns neural scene representations from only shadows present in the scene. While traditional shape-from-shadow (SfS) algorithms reconstruct geometry from shadows, they assume a fixed scanning setup and fail to generalize to complex scenes. Neural rendering algorithms, on the other hand, rely on photometric consistency between RGB images but largely ignore physical cues such as shadows, which have been shown to provide valuable information about the scene. We observe that shadows are a powerful cue that can constrain neural scene representations to learn SfS, and even outperform **NeRF** to reconstruct otherwise hidden geometry. We propose a graphics-inspired differentiable approach to render accurate shadows with volumetric rendering, predicting a shadow map that can be compared to the ground truth shadow. Even with just binary shadow maps, we show that neural rendering can localize the object and estimate coarse geometry. Our approach reveals that sparse cues in images can be used to estimate geometry using differentiable volumetric rendering. Moreover, our framework is highly generalizable and can work alongside existing 3D reconstruction techniques that otherwise only use photometric consistency. Our code is made available in our supplementary materials.  
### Research topic trend prediction of scientific papers based on spatial **enhancement** and dynamic graph convolution network. (arXiv:2203.16256v1 [cs.IR])
- Authors : Changwei Zheng, Zhe Xue, Meiyu Liang, Feifei Kou
- Link : [http://arxiv.org/abs/2203.16256](http://arxiv.org/abs/2203.16256)
> ABSTRACT  :  In recent years, with the increase of social investment in scientific research, the number of research results in various fields has increased significantly. Accurately and effectively predicting the trends of future research topics can help researchers discover future research hotspots. However, due to the increasingly close correlation between various research themes, there is a certain dependency relationship between a large number of research themes. Viewing a single research theme in isolation and using traditional sequence problem processing methods cannot effectively explore the spatial dependencies between these research themes. To simultaneously capture the spatial dependencies and temporal changes between research topics, we propose a deep neural network-based research topic hotness prediction algorithm, a spatiotemporal convolutional network model. Our model combines a graph convolutional neural network (GCN) and Temporal Convolutional Network (TCN), specifically, GCNs are used to learn the spatial dependencies of research topics a and use space dependence to strengthen spatial characteristics. TCN is used to learn the dynamics of research topics' trends. Optimization is based on the calculation of weighted losses based on time distance. Compared with the current mainstream sequence prediction models and similar spatiotemporal models on the paper datasets, experiments show that, in research topic prediction tasks, our model can effectively capture spatiotemporal relationships and the predictions outperform state-of-art baselines.  
### Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v3 [cs.CV] UPDATED)
- Authors : Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, Jian Sun
- Link : [http://arxiv.org/abs/2203.06717](http://arxiv.org/abs/2203.06717)
> ABSTRACT  :  We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than **Swin** Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code &amp; models at https://github.com/megvii-research/RepLKNet.  
# Paper List
---
## cs.CV
---
**194** new papers in cs.CV:-) 
1. An EEG-Based Multi-Modal Emotion Database with Both Posed and Authentic Facial Actions for Emotion Analysis. (arXiv:2203.15829v1 [cs.CV])
2. ACR Loss: Adaptive Coordinate-based Regression Loss for Face Alignment. (arXiv:2203.15835v1 [cs.CV])
3. VPTR: Efficient Transformers for Video Prediction. (arXiv:2203.15836v1 [cs.CV])
4. NNLander-VeriF: A Neural Network Formal Verification Framework for Vision-Based Autonomous Aircraft Landing. (arXiv:2203.15841v1 [cs.LG])
5. Neural Inertial Localization. (arXiv:2203.15851v1 [cs.RO])
6. OdontoAI: A human-in-the-loop labeled data set and an online platform to boost research on dental panoramic radiographs. (arXiv:2203.15856v1 [cs.CV])
7. NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models. (arXiv:2203.15859v1 [cs.CV])
8. On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation. (arXiv:2203.15865v1 [cs.CV])
9. Image Retrieval from Contextual Descriptions. (arXiv:2203.15867v1 [cs.CV])
10. A deep learning model for burn depth classification using ultrasound imaging. (arXiv:2203.15879v1 [eess.IV])
11. Proactive Image Manipulation Detection. (arXiv:2203.15880v1 [cs.CV])
12. Learning to Detect Mobile Objects from LiDAR Scans Without Labels. (arXiv:2203.15882v1 [cs.CV])
13. Deep Equilibrium Assisted Block Sparse Coding of Inter-dependent Signals: Application to Hyperspectral Imaging. (arXiv:2203.15901v1 [cs.CV])
14. Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images. (arXiv:2203.15926v1 [cs.CV])
15. Self-Supervised Leaf Segmentation under Complex Lighting Conditions. (arXiv:2203.15943v1 [cs.CV])
16. Towards Learning Neural Representations from Shadows. (arXiv:2203.15946v1 [cs.CV])
17. High-resolution Face Swapping via Latent Semantics Disentanglement. (arXiv:2203.15958v1 [cs.CV])
18. Using Active Speaker Faces for Diarization in TV shows. (arXiv:2203.15961v1 [cs.MM])
19. PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation. (arXiv:2203.15965v1 [cs.CV])
20. Deeply Interleaved Two-Stream Encoder for Referring Video Segmentation. (arXiv:2203.15969v1 [cs.CV])
21. Iterative Deep Homography Estimation. (arXiv:2203.15982v1 [cs.CV])
22. VI-IKD: High-Speed Accurate Off-Road Navigation using Learned Visual-Inertial Inverse Kinodynamics. (arXiv:2203.15983v1 [cs.RO])
23. Fine-Grained Object Classification via Self-Supervised Pose Alignment. (arXiv:2203.15987v1 [cs.CV])
24. The Sound of Bounding-Boxes. (arXiv:2203.15991v1 [cs.CV])
25. StyleFool: Fooling Video Classification Systems via Style Transfer. (arXiv:2203.16000v1 [cs.CV])
26. Meta-Sampler: Almost-Universal yet Task-Oriented Sampling for Point Clouds. (arXiv:2203.16001v1 [cs.CV])
27. ITTR: Unpaired Image-to-Image Translation with Transformers. (arXiv:2203.16015v1 [cs.CV])
28. ReplaceBlock: An improved regularization method based on background information. (arXiv:2203.16029v1 [cs.CV])
29. How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v1 [cs.CV])
30. Monitored Distillation for Positive Congruent Depth Completion. (arXiv:2203.16034v1 [cs.CV])
31. Semi-Supervised Learning of Semantic Correspondence with Pseudo-Labels. (arXiv:2203.16038v1 [cs.CV])
32. An Iterative Co-Training Transductive Framework for Zero Shot Learning. (arXiv:2203.16041v1 [cs.CV])
33. Threshold Matters in WSSS: Manipulating the Activation for the Robust and Accurate Segmentation Model Against Thresholds. (arXiv:2203.16045v1 [cs.CV])
34. Progressively Generating Better Initial Guesses Towards Next Stages for High-Quality Human Motion Prediction. (arXiv:2203.16051v1 [cs.CV])
35. Automatic Facial Skin Feature Detection for Everyone. (arXiv:2203.16056v1 [cs.CV])
36. Self-supervised 360$^{\circ}$ Room Layout Estimation. (arXiv:2203.16057v1 [cs.CV])
37. AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval. (arXiv:2203.16062v1 [cs.CV])
38. Pay Attention to Hidden States for Video Deblurring: Ping-Pong Recurrent Neural Networks and Selective Non-Local Attention. (arXiv:2203.16063v1 [cs.CV])
39. Learning Program Representations for Food Images and Cooking Recipes. (arXiv:2203.16071v1 [cs.CV])
40. An Efficient Anchor-free Universal Lesion Detection in CT-scans. (arXiv:2203.16074v1 [cs.CV])
41. STRPM: A Spatiotemporal Residual Predictive Model for High-Resolution Video Prediction. (arXiv:2203.16084v1 [cs.CV])
42. Omni-DETR: Omni-Supervised Object Detection with Transformers. (arXiv:2203.16089v1 [cs.CV])
43. Global Tracking via Ensemble of Local Trackers. (arXiv:2203.16092v1 [cs.CV])
44. Contribution of the Temperature of the Objects to the Problem of Thermal Imaging Focusing. (arXiv:2203.16106v1 [cs.CV])
45. Preliminary experiments on thermal emissivity adjustment for face images. (arXiv:2203.16107v1 [cs.CV])
46. SIT: A Bionic and Non-Linear Neuron for Spiking Neural Network. (arXiv:2203.16117v1 [cs.CV])
47. Sensor Data Validation and Driving Safety in Autonomous Driving Systems. (arXiv:2203.16130v1 [cs.CV])
48. Tampered VAE for Improved Satellite Image Time Series Classification. (arXiv:2203.16149v1 [cs.CV])
49. Recommendation of Compatible Outfits Conditioned on Style. (arXiv:2203.16161v1 [cs.IR])
50. Rabbit, toad, and the Moon: Can machine categorize them into one class?. (arXiv:2203.16163v1 [cs.CV])
51. FLOAT: Factorized Learning of Object Attributes for Improved Multi-object Multi-part Scene Parsing. (arXiv:2203.16168v1 [cs.CV])
52. Self-Distillation from the Last Mini-Batch for Consistency Regularization. (arXiv:2203.16172v1 [cs.CV])
53. FlowFormer: A Transformer Architecture for Optical Flow. (arXiv:2203.16194v1 [cs.CV])
54. On the Road to Online Adaptation for Semantic Image Segmentation. (arXiv:2203.16195v1 [cs.CV])
55. Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation. (arXiv:2203.16202v1 [cs.CV])
56. Fair Contrastive Learning for Facial Attribute Classification. (arXiv:2203.16209v1 [cs.CV])
57. Learning of Global Objective for Network Flow in Multi-Object Tracking. (arXiv:2203.16210v1 [cs.CV])
58. Acknowledging the Unknown for Multi-label Learning with Single Positive Labels. (arXiv:2203.16219v1 [cs.CV])
59. Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection. (arXiv:2203.16220v1 [cs.CV])
60. End to End Lip Synchronization with a Temporal AutoEncoder. (arXiv:2203.16224v1 [cs.CV])
61. Biclustering Algorithms Based on Metaheuristics: A Review. (arXiv:2203.16241v1 [cs.LG])
62. CycDA: Unsupervised Cycle Domain Adaptation from Image to Video. (arXiv:2203.16244v1 [cs.CV])
63. InstaFormer: Instance-Aware Image-to-Image Translation with Transformer. (arXiv:2203.16248v1 [cs.CV])
64. PP-YOLOE: An evolved version of YOLO. (arXiv:2203.16250v1 [cs.CV])
65. Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data. (arXiv:2203.16258v1 [cs.CV])
66. SeqTR: A Simple yet Universal Network for Visual Grounding. (arXiv:2203.16265v1 [cs.CV])
67. Interactive Multi-scale Fusion of 2D and 3D Features for Multi-object Tracking. (arXiv:2203.16268v1 [cs.CV])
68. Interpretable Vertebral Fracture Diagnosis. (arXiv:2203.16273v1 [eess.IV])
69. HDSDF: Hybrid Directional and Signed Distance Functions for Fast Inverse Rendering. (arXiv:2203.16284v1 [cs.CV])
70. Region of Interest focused MRI to Synthetic CT Translation using Regression and Classification Multi-task Network. (arXiv:2203.16288v1 [eess.IV])
71. AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift. (arXiv:2203.16291v1 [cs.CV])
72. Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v1 [cs.CV])
73. PEGG-Net: Background Agnostic Pixel-Wise Efficient Grasp Generation Under Closed-Loop Conditions. (arXiv:2203.16301v1 [cs.CV])
74. PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection. (arXiv:2203.16317v1 [cs.CV])
75. Multi-Robot Active Mapping via Neural Bipartite Graph Matching. (arXiv:2203.16319v1 [cs.CV])
76. A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v1 [cs.CV])
77. Smooth Robust Tensor Completion for Background/Foreground Separation with Missing Pixels: Novel Algorithm with Convergence Guarantee. (arXiv:2203.16328v1 [cs.CV])
78. Parameter-efficient Fine-tuning for Vision Transformers. (arXiv:2203.16329v1 [cs.CV])
79. On handwriting pressure normalization for interoperability of different acquisition stylus. (arXiv:2203.16337v1 [eess.SP])
80. Stack operation of tensor networks. (arXiv:2203.16338v1 [cs.LG])
81. Practical Learned Lossless JPEG Recompression with Multi-Level Cross-Channel Entropy Model in the DCT Domain. (arXiv:2203.16357v1 [eess.IV])
82. CardioID: Mitigating the Effects of Irregular Cardiac Signals for Biometric Identification. (arXiv:2203.16381v1 [cs.CR])
83. On learning adaptive acquisition policies for undersampled multi-coil MRI reconstruction. (arXiv:2203.16392v1 [cs.CV])
84. Online Motion Style Transfer for Interactive Character Control. (arXiv:2203.16393v1 [cs.GR])
85. Recognition of polar lows in Sentinel-1 SAR images with deep learning. (arXiv:2203.16401v1 [cs.CV])
86. Surface Vision Transformers: Attention-Based Modelling applied to Cortical Analysis. (arXiv:2203.16414v1 [cs.CV])
87. The impact of using voxel-level segmentation metrics on evaluating multifocal prostate cancer localisation. (arXiv:2203.16415v1 [eess.IV])
88. OPD: Single-view 3D Openable Part Detection. (arXiv:2203.16421v1 [cs.CV])
89. Balanced MSE for Imbalanced Visual Regression. (arXiv:2203.16427v1 [cs.CV])
90. TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v1 [cs.CV])
91. ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v1 [cs.LG])
92. RFNet-4D: Joint Object Reconstruction and Flow Estimation from 4D Point Clouds. (arXiv:2203.16482v1 [cs.CV])
93. Foveation-based Deep Video Compression without Motion Search. (arXiv:2203.16490v1 [eess.IV])
94. Fast, Accurate and Memory-Efficient Partial Permutation Synchronization. (arXiv:2203.16505v1 [cs.CV])
95. An Improved Lightweight YOLOv5 Model Based on Attention Mechanism for Face Mask Detection. (arXiv:2203.16506v1 [cs.CV])
96. AdaMixer: A Fast-Converging Query-Based Object Detector. (arXiv:2203.16507v1 [cs.CV])
97. PromptDet: Expand Your Detector Vocabulary with Uncurated Images. (arXiv:2203.16513v1 [cs.CV])
98. Fast Light-Weight Near-Field Photometric Stereo. (arXiv:2203.16515v1 [cs.CV])
99. Unseen Classes at a Later Time? No Problem. (arXiv:2203.16517v1 [cs.CV])
100. Collaborative Transformers for Grounded Situation Recognition. (arXiv:2203.16518v1 [cs.CV])
101. CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs. (arXiv:2203.16521v1 [cs.CV])
102. Exploring Plain Vision Transformer Backbones for Object Detection. (arXiv:2203.16527v1 [cs.CV])
103. L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors. (arXiv:2203.16528v1 [cs.CV])
104. CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism. (arXiv:2203.16529v1 [cs.CV])
105. Learning Instance-Specific Adaptation for Cross-Domain Segmentation. (arXiv:2203.16530v1 [cs.CV])
106. Understanding 3D Object Articulation in Internet Videos. (arXiv:2203.16531v1 [cs.CV])
107. Large-Scale Pre-training for Person Re-identification with Noisy Labels. (arXiv:2203.16533v1 [cs.CV])
108. Least Squares Normalized Cross Correlation. (arXiv:1810.04320v3 [cs.CV] UPDATED)
109. Cross-Sensor Periocular Biometrics in a Global Pandemic: Comparative Benchmark and Novel Multialgorithmic Approach. (arXiv:1902.08123v5 [cs.CV] UPDATED)
110. Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse Surgical Instrument Usage for Context-aware Assistance. (arXiv:2007.00548v4 [cs.CV] UPDATED)
111. Improved Knowledge Distillation via Full Kernel Matrix Transfer. (arXiv:2009.14416v2 [cs.LG] UPDATED)
112. Fooling the primate brain with minimal, targeted image manipulation. (arXiv:2011.05623v3 [q-bio.NC] UPDATED)
113. TCLR: Temporal Contrastive Learning for Video Representation. (arXiv:2101.07974v4 [cs.CV] UPDATED)
114. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v5 [cs.CV] UPDATED)
115. Learning ABCs: Approximate Bijective Correspondence for isolating factors of variation with weak supervision. (arXiv:2103.03240v4 [cs.LG] UPDATED)
116. The hidden label-marginal biases of segmentation losses. (arXiv:2104.08717v3 [cs.CV] UPDATED)
117. RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition. (arXiv:2105.01883v3 [cs.CV] UPDATED)
118. Multi-Person Extreme Motion Prediction. (arXiv:2105.08825v5 [cs.CV] UPDATED)
119. FairCal: Fairness Calibration for Face Verification. (arXiv:2106.03761v4 [cs.CV] UPDATED)
120. Spectral Unsupervised Domain Adaptation for Visual Recognition. (arXiv:2106.06112v2 [cs.CV] UPDATED)
121. Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v3 [cs.LG] UPDATED)
122. Cross-domain Few-shot Learning with Task-specific Adapters. (arXiv:2107.00358v3 [cs.CV] UPDATED)
123. Language-Level Semantics Conditioned 3D Point Cloud Segmentation. (arXiv:2107.00430v3 [cs.CV] UPDATED)
124. Robust outlier detection by de-biasing VAE likelihoods. (arXiv:2108.08760v2 [cs.LG] UPDATED)
125. Progressive Hard-case Mining across Pyramid Levels for Object Detection. (arXiv:2109.07217v2 [cs.CV] UPDATED)
126. Translating Images into Maps. (arXiv:2110.00966v2 [cs.CV] UPDATED)
127. Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v4 [cs.LG] UPDATED)
128. EditVAE: Unsupervised Part-Aware Controllable 3D Point Cloud Shape Generation. (arXiv:2110.06679v2 [cs.CV] UPDATED)
129. Projective Manifold Gradient Layer for Deep Rotation Regression. (arXiv:2110.11657v3 [cs.CV] UPDATED)
130. Advances in Neural Rendering. (arXiv:2111.05849v2 [cs.GR] UPDATED)
131. Enabling equivariance for arbitrary Lie groups. (arXiv:2111.08251v2 [cs.CV] UPDATED)
132. HARA: A Hierarchical Approach for Robust Rotation Averaging. (arXiv:2111.08831v2 [cs.CV] UPDATED)
133. Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing. (arXiv:2111.12608v2 [cs.CV] UPDATED)
134. Amortized Prompt: Guide CLIP to Domain Transfer Learning. (arXiv:2111.12853v2 [cs.CV] UPDATED)
135. Learning Multiple Dense Prediction Tasks from Partially Annotated Data. (arXiv:2111.14893v2 [cs.CV] UPDATED)
136. ISNAS-DIP: Image-Specific Neural Architecture Search for Deep Image Prior. (arXiv:2111.15362v2 [cs.CV] UPDATED)
137. The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v3 [cs.CV] UPDATED)
138. ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation. (arXiv:2111.15483v2 [cs.CV] UPDATED)
139. Neural Emotion Director: Speech-preserving semantic control of facial expressions in "in-the-wild" videos. (arXiv:2112.00585v2 [cs.CV] UPDATED)
140. GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras. (arXiv:2112.01524v2 [cs.CV] UPDATED)
141. MViTv2: Improved Multiscale Vision Transformers for Classification and Detection. (arXiv:2112.01526v2 [cs.CV] UPDATED)
142. Forward Compatible Training for Large-Scale Embedding Retrieval Systems. (arXiv:2112.02805v2 [cs.CV] UPDATED)
143. PTTR: Relational 3D Point Cloud Object Tracking with Transformer. (arXiv:2112.02857v5 [cs.CV] UPDATED)
144. VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation. (arXiv:2112.04177v2 [cs.CV] UPDATED)
145. Geometry-Guided Progressive **NeRF** for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v3 [cs.CV] UPDATED)
146. FLAVA: A Foundational Language And Vision Alignment Model. (arXiv:2112.04482v3 [cs.CV] UPDATED)
147. UNIST: Unpaired Neural Implicit Shape Translation Network. (arXiv:2112.05381v2 [cs.CV] UPDATED)
148. PartGlot: Learning Shape Part Segmentation from Language Reference Games. (arXiv:2112.06390v2 [cs.CV] UPDATED)
149. I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v3 [cs.CV] UPDATED)
150. A-ViT: Adaptive Tokens for Efficient Vision Transformer. (arXiv:2112.07658v2 [cs.CV] UPDATED)
151. CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v5 [cs.CV] UPDATED)
152. Weakly Supervised Semantic Segmentation via Alternative Self-Dual Teaching. (arXiv:2112.09459v2 [cs.CV] UPDATED)
153. Image Segmentation Using Text and Image Prompts. (arXiv:2112.10003v2 [cs.CV] UPDATED)
154. Topology Preserving Local Road Network Estimation from Single Onboard Camera Image. (arXiv:2112.10155v2 [cs.CV] UPDATED)
155. RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality. (arXiv:2112.11081v2 [cs.CV] UPDATED)
156. StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. (arXiv:2112.11427v2 [cs.CV] UPDATED)
157. Sketch-based Facial Synthesis: A New Challenge. (arXiv:2112.15439v3 [cs.CV] UPDATED)
158. POCO: Point Convolution for Surface Reconstruction. (arXiv:2201.01831v2 [cs.CV] UPDATED)
159. ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v4 [eess.IV] UPDATED)
160. Nonlinear Unknown Input Observability and Unknown Input Reconstruction: The General Analytical Solution. (arXiv:2201.07610v2 [math.OC] UPDATED)
161. Importance of Textlines in Historical Document Classification. (arXiv:2201.09575v2 [cs.CV] UPDATED)
162. Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v3 [cs.CV] UPDATED)
163. DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v4 [cs.CV] UPDATED)
164. Crafting Better Contrastive Views for Siamese Representation Learning. (arXiv:2202.03278v3 [cs.CV] UPDATED)
165. DDL-MVS: Depth Discontinuity Learning for MVS Networks. (arXiv:2203.01391v2 [cs.CV] UPDATED)
166. Automatic Facial Paralysis Estimation with Facial Action Units. (arXiv:2203.01800v2 [cs.CV] UPDATED)
167. CoNIC Solution. (arXiv:2203.03415v2 [eess.IV] UPDATED)
168. Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v2 [eess.IV] UPDATED)
169. Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v2 [cs.CL] UPDATED)
170. AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v3 [cs.CV] UPDATED)
171. Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v3 [cs.CV] UPDATED)
172. Fast Autofocusing using Tiny Transformer Networks for Digital Holographic Microscopy. (arXiv:2203.07772v3 [eess.IV] UPDATED)
173. ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation. (arXiv:2203.09418v2 [cs.CV] UPDATED)
174. Imaging-based histological features are predictive of MET alterations in Non-Small Cell Lung Cancer. (arXiv:2203.10062v2 [cs.CV] UPDATED)
175. Unsupervised Domain Adaptation for **Night**time Aerial Tracking. (arXiv:2203.10541v2 [cs.CV] UPDATED)
176. Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization. (arXiv:2203.11471v2 [cs.CV] UPDATED)
177. WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v3 [cs.CV] UPDATED)
178. SSD-KD: A Self-supervised Diverse Knowledge Distillation Method for Lightweight Skin Lesion Classification Using Dermoscopic Images. (arXiv:2203.11490v2 [cs.CV] UPDATED)
179. Continuous Emotion Recognition using Visual-audio-linguistic information: A Technical Report for ABAW3. (arXiv:2203.13031v2 [cs.MM] UPDATED)
180. Does Monocular Depth Estimation Provide Better Pre-training than Classification for Semantic Segmentation?. (arXiv:2203.13987v2 [cs.CV] UPDATED)
181. Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds. (arXiv:2203.14048v2 [cs.CV] UPDATED)
182. Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v2 [cs.CV] UPDATED)
183. Single-Stream Multi-Level Alignment for Vision-Language Pretraining. (arXiv:2203.14395v2 [cs.CV] UPDATED)
184. ObjectFormer for Image Manipulation Detection and Localization. (arXiv:2203.14681v2 [cs.CV] UPDATED)
185. WSEBP: A Novel Width-depth Synchronous Extension-based Basis Pursuit Algorithm for Multi-Layer Convolutional Sparse Coding. (arXiv:2203.14856v2 [cs.CV] UPDATED)
186. Affine Medical Image Registration with Coarse-to-Fine Vision Transformer. (arXiv:2203.15216v2 [cs.CV] UPDATED)
187. Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection. (arXiv:2203.15221v2 [cs.CV] UPDATED)
188. Fine-tuning Image Transformers using Learnable Memory. (arXiv:2203.15243v2 [cs.CV] UPDATED)
189. MAT: Mask-Aware Transformer for Large Hole Image Inpainting. (arXiv:2203.15270v2 [cs.CV] UPDATED)
190. SIOD: Single Instance Annotated Per Category Per Image for Object Detection. (arXiv:2203.15353v2 [cs.CV] UPDATED)
191. Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v2 [cs.CV] UPDATED)
192. TransductGAN: a Transductive Adversarial Model for Novelty Detection. (arXiv:2203.15406v2 [cs.LG] UPDATED)
193. OSOP: A Multi-Stage One Shot Object Pose Estimation Framework. (arXiv:2203.15533v2 [cs.CV] UPDATED)
194. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v2 [cs.CV] UPDATED)
## eess.IV
---
**20** new papers in eess.IV:-) 
1. A deep learning model for burn depth classification using ultrasound imaging. (arXiv:2203.15879v1 [eess.IV])
2. Deep Equilibrium Assisted Block Sparse Coding of Inter-dependent Signals: Application to Hyperspectral Imaging. (arXiv:2203.15901v1 [cs.CV])
3. Self-Supervised Leaf Segmentation under Complex Lighting Conditions. (arXiv:2203.15943v1 [cs.CV])
4. Interpretable Vertebral Fracture Diagnosis. (arXiv:2203.16273v1 [eess.IV])
5. Region of Interest focused MRI to Synthetic CT Translation using Regression and Classification Multi-task Network. (arXiv:2203.16288v1 [eess.IV])
6. Practical Learned Lossless JPEG Recompression with Multi-Level Cross-Channel Entropy Model in the DCT Domain. (arXiv:2203.16357v1 [eess.IV])
7. Incorporating Gradient Similarity for Robust Time Delay Estimation in Ultrasound Elastography. (arXiv:2203.16398v1 [eess.IV])
8. Hybrid Diffractive Optics Design via Hardware-in-the-Loop Methodology for Achromatic Extended-Depth-of-Field Imaging. (arXiv:2203.16407v1 [physics.optics])
9. Surface Vision Transformers: Attention-Based Modelling applied to Cortical Analysis. (arXiv:2203.16414v1 [cs.CV])
10. The impact of using voxel-level segmentation metrics on evaluating multifocal prostate cancer localisation. (arXiv:2203.16415v1 [eess.IV])
11. Foveation-based Deep Video Compression without Motion Search. (arXiv:2203.16490v1 [eess.IV])
12. L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors. (arXiv:2203.16528v1 [cs.CV])
13. Fooling the primate brain with minimal, targeted image manipulation. (arXiv:2011.05623v3 [q-bio.NC] UPDATED)
14. COL0RME: Super-resolution microscopy based on sparse blinking/fluctuating fluorophore localization and intensity estimation. (arXiv:2108.07095v2 [math.OC] UPDATED)
15. ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation. (arXiv:2111.15483v2 [cs.CV] UPDATED)
16. ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v4 [eess.IV] UPDATED)
17. CoNIC Solution. (arXiv:2203.03415v2 [eess.IV] UPDATED)
18. Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v2 [eess.IV] UPDATED)
19. Fast Autofocusing using Tiny Transformer Networks for Digital Holographic Microscopy. (arXiv:2203.07772v3 [eess.IV] UPDATED)
20. Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v2 [cs.CV] UPDATED)
## cs.LG
---
**170** new papers in cs.LG:-) 
1. Shift-Robust Node Classification via Graph Adversarial Clustering. (arXiv:2203.15802v1 [cs.LG])
2. Improving The Diagnosis of Thyroid Cancer by Machine Learning and Clinical Data. (arXiv:2203.15804v1 [cs.LG])
3. LinkBERT: Pretraining Language Models with Document Links. (arXiv:2203.15827v1 [cs.CL])
4. Learning to Collide: Recommendation System Model Compression with Learned Hash Functions. (arXiv:2203.15837v1 [cs.IR])
5. NNLander-VeriF: A Neural Network Formal Verification Framework for Vision-Based Autonomous Aircraft Landing. (arXiv:2203.15841v1 [cs.LG])
6. Topological Experience Replay. (arXiv:2203.15845v1 [cs.LG])
7. Near-optimality for infinite-horizon restless bandits with many arms. (arXiv:2203.15853v1 [cs.LG])
8. Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics. (arXiv:2203.15858v1 [cs.CL])
9. NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models. (arXiv:2203.15859v1 [cs.CV])
10. Visualizing the Relationship Between Encoded Linguistic Information and Task Performance. (arXiv:2203.15860v1 [cs.CL])
11. An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion. (arXiv:2203.15873v1 [cs.SD])
12. Radial Autoencoders for Enhanced Anomaly Detection. (arXiv:2203.15884v1 [cs.LG])
13. Pretraining Graph Neural Networks for few-shot Analog Circuit Modeling and Design. (arXiv:2203.15913v1 [cs.LG])
14. Multi-Agent Asynchronous Cooperation with Hierarchical Reinforcement Learning. (arXiv:2203.15925v1 [cs.RO])
15. Self-Contrastive Learning based Semi-Supervised Radio Modulation Classification. (arXiv:2203.15932v1 [cs.LG])
16. Graph Neural Networks in IoT: A Survey. (arXiv:2203.15935v1 [cs.LG])
17. A Simple Yet Effective Pretraining Strategy for Graph Few-shot Learning. (arXiv:2203.15936v1 [cs.LG])
18. Robust, Automated, and Accurate Black-box Variational Inference. (arXiv:2203.15945v1 [stat.ML])
19. 4-bit Conformer with Native Quantization Aware Training for Speech Recognition. (arXiv:2203.15952v1 [eess.AS])
20. Investigating the Properties of Neural Network Representations in Reinforcement Learning. (arXiv:2203.15955v1 [cs.LG])
21. Entity-driven Fact-aware Abstractive Summarization of Biomedical Literature. (arXiv:2203.15959v1 [cs.CL])
22. Higher-Order Generalization Bounds: Learning Deep Probabilistic Programs via PAC-Bayes Objectives. (arXiv:2203.15972v1 [cs.LG])
23. Device-Directed Speech Detection: Regularization via Distillation for Weakly-Supervised Models. (arXiv:2203.15975v1 [eess.AS])
24. DELTA: Dynamically Optimizing GPU Memory beyond Tensor Recomputation. (arXiv:2203.15980v1 [cs.LG])
25. VI-IKD: High-Speed Accurate Off-Road Navigation using Learned Visual-Inertial Inverse Kinodynamics. (arXiv:2203.15983v1 [cs.RO])
26. Optimal Learning. (arXiv:2203.15994v1 [cs.LG])
27. Meta-Sampler: Almost-Universal yet Task-Oriented Sampling for Point Clouds. (arXiv:2203.16001v1 [cs.CV])
28. Theory of Acceleration of Decision Making by Correlated Times Sequences. (arXiv:2203.16004v1 [cs.LG])
29. Prognosis of Rotor Parts Fly-off Based on Cascade Classification and Online Prediction Ability Index. (arXiv:2203.16006v1 [eess.SY])
30. Towards Collaborative Intelligence: Routability Estimation based on Decentralized Private Data. (arXiv:2203.16009v1 [cs.LG])
31. Longitudinal Fairness with Censorship. (arXiv:2203.16024v1 [cs.LG])
32. How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v1 [cs.CV])
33. Monitored Distillation for Positive Congruent Depth Completion. (arXiv:2203.16034v1 [cs.CV])
34. Enhancing Zero-Shot Many to Many Voice Conversion with Self-Attention VAE. (arXiv:2203.16037v1 [cs.SD])
35. Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks. (arXiv:2203.16040v1 [cs.SD])
36. Coarse-to-Fine Recursive Speech Separation for Unknown Number of Speakers. (arXiv:2203.16054v1 [cs.SD])
37. Learning (Local) Surrogate Loss Functions for Predict-Then-Optimize Problems. (arXiv:2203.16067v1 [cs.LG])
38. Explainable Artificial Intelligence in Process Mining: Assessing the Explainability-Performance Trade-Off in Outcome-Oriented Predictive Process Monitoring. (arXiv:2203.16073v1 [cs.LG])
39. STRPM: A Spatiotemporal Residual Predictive Model for High-Resolution Video Prediction. (arXiv:2203.16084v1 [cs.CV])
40. Polarized deep diffractive neural network for classification, generation, multiplexing and de-multiplexing of orbital angular momentum modes. (arXiv:2203.16087v1 [physics.optics])
41. Neighbor Enhanced Graph Convolutional Networks for Node Classification and Recommendation. (arXiv:2203.16097v1 [cs.LG])
42. Adaptive Private-K-Selection with Adaptive K and Application to Multi-label PATE. (arXiv:2203.16100v1 [cs.LG])
43. Continual Normalization: Rethinking Batch Normalization for Online Continual Learning. (arXiv:2203.16102v1 [cs.LG])
44. Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation. (arXiv:2203.16104v1 [cs.SD])
45. Weakly-supervised Temporal Path Representation Learning with Contrastive Curriculum Learning -- Extended Version. (arXiv:2203.16110v1 [cs.LG])
46. A Fast Transformer-based General-Purpose Lossless Compressor. (arXiv:2203.16114v1 [cs.LG])
47. Example-based Explanations with Adversarial Attacks for Respiratory Sound Analysis. (arXiv:2203.16141v1 [cs.SD])
48. Tampered VAE for Improved Satellite Image Time Series Classification. (arXiv:2203.16149v1 [cs.CV])
49. RICON: A ML framework for real-time and proactive intervention to prevent customer churn. (arXiv:2203.16155v1 [cs.LG])
50. Recommendation of Compatible Outfits Conditioned on Style. (arXiv:2203.16161v1 [cs.IR])
51. AdaGrid: Adaptive Grid Search for Link Prediction Training Objective. (arXiv:2203.16162v1 [cs.LG])
52. Marginalized Operators for Off-policy Reinforcement Learning. (arXiv:2203.16177v1 [cs.LG])
53. Dynamic Model Tree for Interpretable Data Stream Learning. (arXiv:2203.16181v1 [cs.LG])
54. Automatic Identification of Chemical Moieties. (arXiv:2203.16205v1 [physics.chem-ph])
55. Adaptive Divergence-based Non-negative Latent Factor Analysis. (arXiv:2203.16214v1 [cs.LG])
56. Improved Convergence Rate of Stochastic Gradient Langevin Dynamics with Variance Reduction and its Application to Optimization. (arXiv:2203.16217v1 [cs.LG])
57. APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction. (arXiv:2203.16218v1 [cs.IR])
58. Phase-Aware Deep Speech **Enhancement**: It's All About The Frame Length. (arXiv:2203.16222v1 [eess.AS])
59. Hypergraphon Mean Field Games. (arXiv:2203.16223v1 [cs.GT])
60. Biclustering Algorithms Based on Metaheuristics: A Review. (arXiv:2203.16241v1 [cs.LG])
61. Co-Membership-based Generic Anomalous Communities Detection. (arXiv:2203.16246v1 [cs.SI])
62. Physics Community Needs, Tools, and Resources for Machine Learning. (arXiv:2203.16255v1 [cs.LG])
63. Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data. (arXiv:2203.16258v1 [cs.CV])
64. How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning. (arXiv:2203.16262v1 [cs.LG])
65. Does Audio Deepfake Detection Generalize?. (arXiv:2203.16263v1 [cs.SD])
66. Reinforcement Learning Guided by Provable Normative Compliance. (arXiv:2203.16275v1 [cs.AI])
67. The Weak Supervision Landscape. (arXiv:2203.16282v1 [cs.LG])
68. An Offset-Free Nonlinear MPC scheme for systems learned by Neural NARX models. (arXiv:2203.16290v1 [eess.SY])
69. Context-aware Automatic Music Transcription. (arXiv:2203.16294v1 [cs.SD])
70. Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v1 [cs.CV])
71. Zero-shot meta-learning for small-scale data from human subjects. (arXiv:2203.16309v1 [cs.LG])
72. When to Go, and When to Explore: The Benefit of Post-Exploration in Intrinsic Motivation. (arXiv:2203.16311v1 [cs.LG])
73. A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v1 [cs.CV])
74. Smooth Robust Tensor Completion for Background/Foreground Separation with Missing Pixels: Novel Algorithm with Convergence Guarantee. (arXiv:2203.16328v1 [cs.CV])
75. FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata. (arXiv:2203.16331v1 [cs.LG])
76. Instantaneous Frequency Estimation In Multi-Component Signals Using Stochastic EM Algorithm. (arXiv:2203.16334v1 [eess.SP])
77. TraHGR: Few-shot Learning for Hand Gesture Recognition via ElectroMyography. (arXiv:2203.16336v1 [eess.SP])
78. Stack operation of tensor networks. (arXiv:2203.16338v1 [cs.LG])
79. Robust and Energy-efficient PPG-based Heart-Rate Monitoring. (arXiv:2203.16339v1 [eess.SP])
80. Optimization for Classical Machine Learning Problems on the GPU. (arXiv:2203.16340v1 [cs.LG])
81. Learning multiobjective rough terrain traversability. (arXiv:2203.16354v1 [cs.RO])
82. Generative Adversarial Networks for the fast simulation of the Time Projection Chamber responses at the MPD detector. (arXiv:2203.16355v1 [physics.ins-det])
83. IGRF-RFE: A Hybrid Feature Selection Method for MLP-based Network Intrusion Detection on UNSW-NB15 Dataset. (arXiv:2203.16365v1 [cs.LG])
84. Slow-varying Dynamics Assisted Temporal Capsule Network for Machinery Remaining Useful Life Estimation. (arXiv:2203.16373v1 [cs.LG])
85. Online Motion Style Transfer for Interactive Character Control. (arXiv:2203.16393v1 [cs.GR])
86. PerfectDou: Dominating DouDizhu with Perfect Information Distillation. (arXiv:2203.16406v1 [cs.AI])
87. Learning Fair Models without Sensitive Attributes: A Generative Approach. (arXiv:2203.16413v1 [cs.LG])
88. Low-complexity Near-optimum Symbol Detection Based on Neural **Enhancement** of Factor Graphs. (arXiv:2203.16417v1 [cs.IT])
89. Intelligent Blockage Prediction and Proactive Handover for Seamless Connectivity in Vision-Aided 5G/6G UDNs. (arXiv:2203.16419v1 [cs.NI])
90. TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v1 [cs.CV])
91. Weakly supervised causal representation learning. (arXiv:2203.16437v1 [stat.ML])
92. Reliability and Validity of the Polar V800 Sports Watch for Estimating Vertical Jump Height. (arXiv:2203.16442v1 [physics.med-ph])
93. AI Gone Astray: Technical Supplement. (arXiv:2203.16452v1 [cs.LG])
94. Explicitising The Implicit Intrepretability of Deep Neural Networks Via Duality. (arXiv:2203.16455v1 [cs.LG])
95. Convergence of gradient descent for deep neural networks. (arXiv:2203.16462v1 [cs.LG])
96. Perfectly Accurate Membership Inference by a Dishonest Central Server in Federated Learning. (arXiv:2203.16463v1 [cs.LG])
97. Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning. (arXiv:2203.16464v1 [cs.LG])
98. Remember to correct the bias when using deep learning for regression!. (arXiv:2203.16470v1 [cs.LG])
99. Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models. (arXiv:2203.16474v1 [cs.CL])
100. ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v1 [cs.LG])
101. On Uncertainty, Tempering, and Data Augmentation in Bayesian Classification. (arXiv:2203.16481v1 [cs.LG])
102. Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v1 [cs.CL])
103. Generative Spoken Dialogue Language Modeling. (arXiv:2203.16502v1 [cs.CL])
104. An Improved Lightweight YOLOv5 Model Based on Attention Mechanism for Face Mask Detection. (arXiv:2203.16506v1 [cs.CV])
105. Collaborative Transformers for Grounded Situation Recognition. (arXiv:2203.16518v1 [cs.CV])
106. Forecast Aggregation via Peer Prediction. (arXiv:1910.03779v8 [stat.ME] UPDATED)
107. Bayesian Deep Learning and a Probabilistic Perspective of Generalization. (arXiv:2002.08791v4 [cs.LG] UPDATED)
108. Sparse recovery by reduced variance stochastic approximation. (arXiv:2006.06365v3 [stat.ML] UPDATED)
109. Improved Knowledge Distillation via Full Kernel Matrix Transfer. (arXiv:2009.14416v2 [cs.LG] UPDATED)
110. Near Real-Time Social Distance Estimation in London. (arXiv:2012.07751v3 [cs.CY] UPDATED)
111. Connecting ansatz expressibility to gradient magnitudes and barren plateaus. (arXiv:2101.02138v2 [quant-ph] UPDATED)
112. Learning ABCs: Approximate Bijective Correspondence for isolating factors of variation with weak supervision. (arXiv:2103.03240v4 [cs.LG] UPDATED)
113. RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition. (arXiv:2105.01883v3 [cs.CV] UPDATED)
114. SpecRepair: Counter-Example Guided Safety Repair of Deep Neural Networks. (arXiv:2106.01917v3 [cs.LG] UPDATED)
115. FairCal: Fairness Calibration for Face Verification. (arXiv:2106.03761v4 [cs.CV] UPDATED)
116. Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v3 [cs.LG] UPDATED)
117. Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search. (arXiv:2108.01036v3 [cs.AI] UPDATED)
118. Robust outlier detection by de-biasing VAE likelihoods. (arXiv:2108.08760v2 [cs.LG] UPDATED)
119. Unbiased Single-scale and Multi-scale Quantizers for Distributed Optimization. (arXiv:2109.12497v2 [cs.LG] UPDATED)
120. CTC Variations Through New WFST Topologies. (arXiv:2110.03098v2 [eess.AS] UPDATED)
121. Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v4 [cs.LG] UPDATED)
122. Computational Graph Completion. (arXiv:2110.10323v2 [stat.ML] UPDATED)
123. Sensing Cox Processes via Posterior Sampling and Positive Bases. (arXiv:2110.11181v2 [cs.LG] UPDATED)
124. Learning Continuous Representation of Audio for Arbitrary Scale Super Resolution. (arXiv:2111.00195v2 [cs.SD] UPDATED)
125. Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning. (arXiv:2111.03189v2 [cs.LG] UPDATED)
126. Learning equilibria with personalized incentives in a class of nonmonotone games. (arXiv:2111.03854v2 [math.OC] UPDATED)
127. Enabling equivariance for arbitrary Lie groups. (arXiv:2111.08251v2 [cs.CV] UPDATED)
128. Prediction of Large Magnetic Moment Materials With Graph Neural Networks and Random Forests. (arXiv:2111.14712v2 [cond-mat.mtrl-sci] UPDATED)
129. The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v3 [cs.CV] UPDATED)
130. GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras. (arXiv:2112.01524v2 [cs.CV] UPDATED)
131. ALX: Large Scale Matrix Factorization on TPUs. (arXiv:2112.02194v2 [cs.LG] UPDATED)
132. D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. (arXiv:2112.03028v2 [cs.RO] UPDATED)
133. Geometry-Guided Progressive **NeRF** for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v3 [cs.CV] UPDATED)
134. Prediction of Adverse Biological Effects of Chemicals Using Knowledge Graph Embeddings. (arXiv:2112.04605v2 [cs.AI] UPDATED)
135. i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery. (arXiv:2112.04905v2 [cs.LG] UPDATED)
136. A cross-domain recommender system using deep coupled autoencoders. (arXiv:2112.07617v2 [cs.IR] UPDATED)
137. A-ViT: Adaptive Tokens for Efficient Vision Transformer. (arXiv:2112.07658v2 [cs.CV] UPDATED)
138. RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality. (arXiv:2112.11081v2 [cs.CV] UPDATED)
139. StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. (arXiv:2112.11427v2 [cs.CV] UPDATED)
140. Computationally Efficient Approximations for Matrix-based Renyi's Entropy. (arXiv:2112.13720v2 [stat.ML] UPDATED)
141. Relationship extraction for knowledge graph creation from biomedical literature. (arXiv:2201.01647v2 [cs.AI] UPDATED)
142. Using Deep Learning with Large Aggregated Datasets for COVID-19 Classification from Cough. (arXiv:2201.01669v3 [eess.AS] UPDATED)
143. POCO: Point Convolution for Surface Reconstruction. (arXiv:2201.01831v2 [cs.CV] UPDATED)
144. PocketNN: Integer-only Training and Inference of Neural Networks without Quantization via Direct Feedback Alignment and Pocket Activations in Pure C++. (arXiv:2201.02863v5 [cs.LG] UPDATED)
145. ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v4 [eess.IV] UPDATED)
146. CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery. (arXiv:2202.00161v3 [cs.LG] UPDATED)
147. Efficient Memory Partitioning in Software Defined Hardware. (arXiv:2202.01261v3 [cs.AR] UPDATED)
148. SpeechPainter: Text-conditioned Speech Inpainting. (arXiv:2202.07273v2 [cs.SD] UPDATED)
149. Parallel MCMC Without Embarrassing Failures. (arXiv:2202.11154v2 [stat.ML] UPDATED)
150. Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice. (arXiv:2202.12780v2 [stat.ML] UPDATED)
151. An Analytical Approach to Compute the Exact Preimage of Feed-Forward Neural Networks. (arXiv:2203.00438v3 [cs.LG] UPDATED)
152. DDL-MVS: Depth Discontinuity Learning for MVS Networks. (arXiv:2203.01391v2 [cs.CV] UPDATED)
153. IAE-Net: Integral Autoencoders for Discretization-Invariant Learning. (arXiv:2203.05142v2 [cs.LG] UPDATED)
154. Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v3 [cs.CV] UPDATED)
155. A novel sampler for Gauss-Hermite determinantal point processes with application to Monte Carlo integration. (arXiv:2203.08061v2 [cs.LG] UPDATED)
156. Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v2 [cs.CL] UPDATED)
157. A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v3 [cs.LG] UPDATED)
158. Offline Reinforcement Learning Under Value and Density-Ratio Realizability: the Power of Gaps. (arXiv:2203.13935v2 [cs.LG] UPDATED)
159. A Roadmap for Big Model. (arXiv:2203.14101v2 [cs.LG] UPDATED)
160. Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v2 [cs.CV] UPDATED)
161. 5G Routing Interfered Environment. (arXiv:2203.14790v2 [cs.NI] UPDATED)
162. WSEBP: A Novel Width-depth Synchronous Extension-based Basis Pursuit Algorithm for Multi-Layer Convolutional Sparse Coding. (arXiv:2203.14856v2 [cs.CV] UPDATED)
163. Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks. (arXiv:2203.15030v2 [cs.AI] UPDATED)
164. Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation. (arXiv:2203.15319v2 [cs.CL] UPDATED)
165. Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v2 [cs.CV] UPDATED)
166. TransductGAN: a Transductive Adversarial Model for Novelty Detection. (arXiv:2203.15406v2 [cs.LG] UPDATED)
167. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v2 [cs.CV] UPDATED)
168. Towards Spatio-Temporal Aware Traffic Time Series Forecasting--Full Version. (arXiv:2203.15737v2 [cs.LG] UPDATED)
169. AlloST: Low-resource Speech Translation without Source Transcription. (arXiv:2105.00171v3 [cs.CL] CROSS LISTED)
170. Adaptative Perturbation Patterns: Realistic Adversarial Learning for Robust Intrusion Detection. (arXiv:2203.04234v2 [cs.CR] CROSS LISTED)
## cs.AI
---
**84** new papers in cs.AI:-) 
1. A Metaheuristic Algorithm for Large Maximum Weight Independent Set Problems. (arXiv:2203.15805v1 [cs.AI])
2. Learning to Collide: Recommendation System Model Compression with Learned Hash Functions. (arXiv:2203.15837v1 [cs.IR])
3. Topological Experience Replay. (arXiv:2203.15845v1 [cs.LG])
4. NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models. (arXiv:2203.15859v1 [cs.CV])
5. WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models. (arXiv:2203.15863v1 [eess.AS])
6. Indoor SLAM Using a Foot-mounted IMU and the local Magnetic Field. (arXiv:2203.15866v1 [cs.RO])
7. An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion. (arXiv:2203.15873v1 [cs.SD])
8. Radial Autoencoders for Enhanced Anomaly Detection. (arXiv:2203.15884v1 [cs.LG])
9. Pretraining Graph Neural Networks for few-shot Analog Circuit Modeling and Design. (arXiv:2203.15913v1 [cs.LG])
10. Multi-Agent Asynchronous Cooperation with Hierarchical Reinforcement Learning. (arXiv:2203.15925v1 [cs.RO])
11. Towards Learning Neural Representations from Shadows. (arXiv:2203.15946v1 [cs.CV])
12. High-resolution Face Swapping via Latent Semantics Disentanglement. (arXiv:2203.15958v1 [cs.CV])
13. A meta-probabilistic-programming language for bisimulation of probabilistic and non-well-founded type systems. (arXiv:2203.15970v1 [cs.AI])
14. VI-IKD: High-Speed Accurate Off-Road Navigation using Learned Visual-Inertial Inverse Kinodynamics. (arXiv:2203.15983v1 [cs.RO])
15. Does Configuration Encoding Matter in Learning Software Performance? An Empirical Study on Encoding Schemes. (arXiv:2203.15988v1 [cs.SE])
16. Longitudinal Fairness with Censorship. (arXiv:2203.16024v1 [cs.LG])
17. Monitored Distillation for Positive Congruent Depth Completion. (arXiv:2203.16034v1 [cs.CV])
18. Learning (Local) Surrogate Loss Functions for Predict-Then-Optimize Problems. (arXiv:2203.16067v1 [cs.LG])
19. Explainable Artificial Intelligence in Process Mining: Assessing the Explainability-Performance Trade-Off in Outcome-Oriented Predictive Process Monitoring. (arXiv:2203.16073v1 [cs.LG])
20. Weakly-supervised Temporal Path Representation Learning with Contrastive Curriculum Learning -- Extended Version. (arXiv:2203.16110v1 [cs.LG])
21. Recommendation of Compatible Outfits Conditioned on Style. (arXiv:2203.16161v1 [cs.IR])
22. Rabbit, toad, and the Moon: Can machine categorize them into one class?. (arXiv:2203.16163v1 [cs.CV])
23. Symbolic music generation conditioned on continuous-valued emotions. (arXiv:2203.16165v1 [eess.AS])
24. Anticipatory Counterplanning. (arXiv:2203.16171v1 [cs.AI])
25. APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction. (arXiv:2203.16218v1 [cs.IR])
26. Exploring ML testing in practice -- Lessons learned from an interactive rapid review with Axis Communications. (arXiv:2203.16225v1 [cs.SE])
27. Automatic generation of semantic corpora for improving intent estimation of taxonomy-driven search engines. (arXiv:2203.16230v1 [cs.CL])
28. Research topic trend prediction of scientific papers based on spatial **enhancement** and dynamic graph convolution network. (arXiv:2203.16256v1 [cs.IR])
29. How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning. (arXiv:2203.16262v1 [cs.LG])
30. Reinforcement Learning Guided by Provable Normative Compliance. (arXiv:2203.16275v1 [cs.AI])
31. CMMD: Cross-Metric Multi-Dimensional Root Cause Analysis. (arXiv:2203.16280v1 [cs.AI])
32. Region of Interest focused MRI to Synthetic CT Translation using Regression and Classification Multi-task Network. (arXiv:2203.16288v1 [eess.IV])
33. One-Step Two-Critic Deep Reinforcement Learning for Inverter-based Volt-Var Control in Active Distribution Networks. (arXiv:2203.16289v1 [cs.AI])
34. Zero-shot meta-learning for small-scale data from human subjects. (arXiv:2203.16309v1 [cs.LG])
35. When to Go, and When to Explore: The Benefit of Post-Exploration in Intrinsic Motivation. (arXiv:2203.16311v1 [cs.LG])
36. PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection. (arXiv:2203.16317v1 [cs.CV])
37. A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v1 [cs.CV])
38. Parameter-efficient Fine-tuning for Vision Transformers. (arXiv:2203.16329v1 [cs.CV])
39. Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction. (arXiv:2203.16346v1 [cs.PL])
40. Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis. (arXiv:2203.16369v1 [cs.CL])
41. Online Motion Style Transfer for Interactive Character Control. (arXiv:2203.16393v1 [cs.GR])
42. PerfectDou: Dominating DouDizhu with Perfect Information Distillation. (arXiv:2203.16406v1 [cs.AI])
43. Learning Fair Models without Sensitive Attributes: A Generative Approach. (arXiv:2203.16413v1 [cs.LG])
44. Perfectly Accurate Membership Inference by a Dishonest Central Server in Federated Learning. (arXiv:2203.16463v1 [cs.LG])
45. Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning. (arXiv:2203.16464v1 [cs.LG])
46. An Artificial Intelligence Browser Architecture (AIBA) For Our Kind and Others: A Voice Name System Speech implementation with two warrants, Wake Neutrality and Value Preservation of Privately Identifiable Information. (arXiv:2203.16497v1 [cs.HC])
47. Collaborative Transformers for Grounded Situation Recognition. (arXiv:2203.16518v1 [cs.CV])
48. L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors. (arXiv:2203.16528v1 [cs.CV])
49. Randomized Wagering Mechanisms. (arXiv:1809.04136v5 [cs.GT] UPDATED)
50. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v5 [cs.CV] UPDATED)
51. SAT-based Circuit Local Improvement. (arXiv:2102.12579v2 [cs.AI] UPDATED)
52. RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition. (arXiv:2105.01883v3 [cs.CV] UPDATED)
53. Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey. (arXiv:2105.04387v5 [cs.CL] UPDATED)
54. Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v3 [cs.LG] UPDATED)
55. Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search. (arXiv:2108.01036v3 [cs.AI] UPDATED)
56. Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v4 [cs.LG] UPDATED)
57. Retrieval-guided Counterfactual Generation for QA. (arXiv:2110.07596v2 [cs.CL] UPDATED)
58. Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning. (arXiv:2111.03189v2 [cs.LG] UPDATED)
59. GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras. (arXiv:2112.01524v2 [cs.CV] UPDATED)
60. Prediction of Adverse Biological Effects of Chemicals Using Knowledge Graph Embeddings. (arXiv:2112.04605v2 [cs.AI] UPDATED)
61. i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery. (arXiv:2112.04905v2 [cs.LG] UPDATED)
62. CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v5 [cs.CV] UPDATED)
63. Weakly Supervised Semantic Segmentation via Alternative Self-Dual Teaching. (arXiv:2112.09459v2 [cs.CV] UPDATED)
64. RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality. (arXiv:2112.11081v2 [cs.CV] UPDATED)
65. StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. (arXiv:2112.11427v2 [cs.CV] UPDATED)
66. Relationship extraction for knowledge graph creation from biomedical literature. (arXiv:2201.01647v2 [cs.AI] UPDATED)
67. PocketNN: Integer-only Training and Inference of Neural Networks without Quantization via Direct Feedback Alignment and Pocket Activations in Pure C++. (arXiv:2201.02863v5 [cs.LG] UPDATED)
68. Benchmarking Subset Selection from Large Candidate Solution Sets in Evolutionary Multi-objective Optimization. (arXiv:2201.06700v2 [cs.NE] UPDATED)
69. The Mathematics of Comparing Objects. (arXiv:2201.07032v2 [cs.AI] UPDATED)
70. CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery. (arXiv:2202.00161v3 [cs.LG] UPDATED)
71. Chord-Conditioned Melody Choralization with Controllable Harmonicity and Polyphonicity. (arXiv:2202.08423v2 [cs.SD] UPDATED)
72. Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v2 [cs.CL] UPDATED)
73. Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v3 [cs.CV] UPDATED)
74. Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI. (arXiv:2203.10525v2 [cs.AI] UPDATED)
75. Voltage-Dependent Synaptic Plasticity (VDSP): Unsupervised probabilistic Hebbian plasticity rule based on neurons membrane potential. (arXiv:2203.11022v2 [cs.NE] UPDATED)
76. MERLIN -- Malware Evasion with Reinforcement LearnINg. (arXiv:2203.12980v4 [cs.CR] UPDATED)
77. Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion. (arXiv:2203.13224v2 [cs.CL] UPDATED)
78. Code Smells for Machine Learning Applications. (arXiv:2203.13746v2 [cs.SE] UPDATED)
79. A Roadmap for Big Model. (arXiv:2203.14101v2 [cs.LG] UPDATED)
80. Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks. (arXiv:2203.15030v2 [cs.AI] UPDATED)
81. Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v2 [cs.CV] UPDATED)
82. ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v2 [cs.CV] UPDATED)
83. AlloST: Low-resource Speech Translation without Source Transcription. (arXiv:2105.00171v3 [cs.CL] CROSS LISTED)
84. Adaptative Perturbation Patterns: Realistic Adversarial Learning for Robust Intrusion Detection. (arXiv:2203.04234v2 [cs.CR] CROSS LISTED)

