# Your interest papers
---
## cs.CV
---
### **Swin**FIR: Revisiting the **Swin**IR with Fast Fourier Convolution and Improved Training for Image Super-Resolution. (arXiv:2208.11247v1 [cs.CV])
- Authors : Dafeng Zhang, Feiyu Huang, Shizhuo Liu, Xiaobing Wang, Zhezhu Jin
- Link : [http://arxiv.org/abs/2208.11247](http://arxiv.org/abs/2208.11247)
> ABSTRACT  :  Transformer-based methods have achieved impressive image **restoration** performance due to their capacities to model long-range dependency compared to CNN-based methods. However, advances like **Swin**IR adopts the window-based and local attention strategy to balance the performance and computational overhead, which restricts employing large receptive fields to capture global information and establish long dependencies in the early layers. To further improve the efficiency of capturing global information, in this work, we propose **Swin**FIR to extend **Swin**IR by replacing Fast Fourier Convolution (FFC) components, which have the image-wide receptive field. We also revisit other advanced techniques, i.e, data augmentation, pre-training, and feature ensemble to improve the effect of image reconstruction. And our feature ensemble method enables the performance of the model to be considerably enhanced without increasing the training and testing time. We applied our algorithm on multiple popular large-scale benchmarks and achieved state-of-the-art performance comparing to the existing methods. For example, our **Swin**FIR achieves the PSNR of 32.83 dB on Manga109 dataset, which is 0.8 dB higher than the state-of-the-art **Swin**IR method.  
### E-**NeRF**: Neural Radiance Fields from a Moving Event Camera. (arXiv:2208.11300v1 [cs.CV])
- Authors : Simon Klenk, Lukas Koestler, Davide Scaramuzza, Daniel Cremers
- Link : [http://arxiv.org/abs/2208.11300](http://arxiv.org/abs/2208.11300)
> ABSTRACT  :  Estimating neural radiance fields (**NeRF**s) from ideal images has been extensively studied in the computer vision community. Most approaches assume optimal illumination and slow camera motion. These assumptions are often violated in robotic applications, where images contain motion blur and the scene may not have suitable illumination. This can cause significant problems for downstream tasks such as navigation, inspection or visualization of the scene. To alleviate these problems we present E-**NeRF**, the first method which estimates a volumetric scene representation in the form of a **NeRF** from a fast-moving event camera. Our method can recover **NeRF**s during very fast motion and in **high dynamic range** conditions, where frame-based approaches fail. We show that rendering high-quality frames is possible by only providing an event stream as input. Furthermore, by combining events and frames, we can estimate **NeRF**s of higher quality than state-of-the-art approaches under severe motion blur. We also show that combining events and frames can overcome failure cases of **NeRF** estimation in scenarios where only few input views are available, without requiring additional regularization.  
### Event-based Image Deblurring with Dynamic Motion Awareness. (arXiv:2208.11398v1 [cs.CV])
- Authors : Patricia Vitoria, Stamatios Georgoulis, Stepan Tulyakov, Alfredo Bochicchio, Julius Erbach, Yuanyou Li
- Link : [http://arxiv.org/abs/2208.11398](http://arxiv.org/abs/2208.11398)
> ABSTRACT  :  Non-uniform image deblurring is a challenging task due to the lack of temporal and textural information in the blurry image itself. Complementary information from auxiliary sensors such event sensors are being explored to address these limitations. The latter can record changes in a logarithmic intensity asynchronously, called events, with high temporal resolution and **high dynamic range**. Current event-based deblurring methods combine the blurry image with events to jointly estimate per-pixel motion and the deblur operator. In this paper, we argue that a divide-and-conquer approach is more suitable for this task. To this end, we propose to use modulated deformable convolutions, whose kernel offsets and modulation masks are dynamically estimated from events to encode the motion in the scene, while the deblur operator is learned from the combination of blurry image and corresponding events. Furthermore, we employ a coarse-to-fine multi-scale reconstruction approach to cope with the inherent sparsity of events in low contrast regions. Importantly, we introduce the first dataset containing pairs of real RGB blur images and related events during the **exposure** time. Our results show better overall robustness when using events, with improvements in PSNR by up to 1.57dB on synthetic data and 1.08 dB on real event data.  
### A Deep Learning Approach Using Masked Image Modeling for Reconstruction of Undersampled K-spaces. (arXiv:2208.11472v1 [eess.IV])
- Authors : Kyler Larsen, Arghya Pal, Yogesh Rathi
- Link : [http://arxiv.org/abs/2208.11472](http://arxiv.org/abs/2208.11472)
> ABSTRACT  :  Magnetic Resonance Imaging (MRI) scans are time consuming and precarious, since the patients remain still in a confined space for extended periods of time. To reduce scanning time, some experts have experimented with undersampled k spaces, trying to use deep learning to predict the fully sampled result. These studies report that as many as 20 to 30 minutes could be saved off a scan that takes an hour or more. However, none of these studies have explored the possibility of using masked image modeling (MIM) to predict the missing parts of MRI k spaces. This study makes use of 11161 reconstructed MRI and k spaces of knee MRI images from Facebook's fastmri dataset. This tests a modified version of an existing model using baseline shifted window (**Swin**) and vision transformer architectures that makes use of MIM on undersampled k spaces to predict the full k space and consequently the full MRI image. Modifications were made using pytorch and numpy libraries, and were published to a github repository. After the model reconstructed the k space images, the basic Fourier transform was applied to determine the actual MRI image. Once the model reached a steady state, experimentation with hyperparameters helped to achieve pinpoint accuracy for the reconstructed images. The model was evaluated through L1 loss, gradient normalization, and structural similarity values. The model produced reconstructed images with L1 loss values averaging to &lt;0.01 and gradient normalization values &lt;0.1 after training finished. The reconstructed k spaces yielded structural similarity values of over 99% for both training and validation with the fully sampled k spaces, while validation loss continually decreased under 0.01. These data strongly support the idea that the algorithm works for MRI reconstruction, as they indicate the model's reconstructed image aligns extremely well with the original, fully sampled k space.  
### An End-to-End OCR Framework for Robust Arabic-Handwriting Recognition using a Novel Transformers-based Model and an Innovative 270 Million-Words Multi-Font Corpus of Classical Arabic with Diacritics. (arXiv:2208.11484v1 [cs.CV])
- Authors : Aly Mostafa, Omar Mohamed, Ali Ashraf, Ahmed Elbehery, Salma Jamal, Anas Salah
- Link : [http://arxiv.org/abs/2208.11484](http://arxiv.org/abs/2208.11484)
> ABSTRACT  :  This research is the second phase in a series of investigations on developing an Optical Character Recognition (OCR) of Arabic historical documents and examining how different modeling procedures interact with the problem. The first research studied the effect of Transformers on our custom-built Arabic dataset. One of the downsides of the first research was the size of the training data, a mere 15000 images from our 30 million images, due to lack of resources. Also, we add an image **enhancement** layer, time and space optimization, and Post-Correction layer to aid the model in predicting the correct word for the correct context. Notably, we propose an end-to-end text recognition approach using Vision Transformers as an encoder, namely BEIT, and vanilla Transformer as a decoder, eliminating CNNs for feature extraction and reducing the model's complexity. The experiments show that our end-to-end model outperforms Convolutions Backbones. The model attained a CER of 4.46%.  
### PeRFception: Perception using Radiance Fields. (arXiv:2208.11537v1 [cs.CV])
- Authors : Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park
- Link : [http://arxiv.org/abs/2208.11537](http://arxiv.org/abs/2208.11537)
> ABSTRACT  :  The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (**NeRF**s), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of **NeRF** called Plenoxels, we create the first large-scale implicit representation datasets for perception tasks, called the PeRFception, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4\%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the classification and segmentation models that directly take as input this implicit format and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data are publicly available in https://postech-cvlab.github.io/PeRFception .  
### AGO-Net: Association-Guided 3D Point Cloud Object Detection Network. (arXiv:2208.11658v1 [cs.CV])
- Authors : Liang Du, Xiaoqing Ye, Xiao Tan, Edward Johns, Bo Chen, Errui Ding, Xiangyang Xue, Jianfeng Feng
- Link : [http://arxiv.org/abs/2208.11658](http://arxiv.org/abs/2208.11658)
> ABSTRACT  :  The human brain can effortlessly recognize and localize objects, whereas current 3D object detection methods based on LiDAR point clouds still report inferior performance for detecting occluded and distant objects: the point cloud appearance varies greatly due to occlusion, and has inherent variance in point densities along the distance to sensors. Therefore, designing feature representations robust to such point clouds is critical. Inspired by human associative recognition, we propose a novel 3D detection framework that associates intact features for objects via domain adaptation. We bridge the gap between the perceptual domain, where features are derived from real scenes with sub-optimal representations, and the conceptual domain, where features are extracted from augmented scenes that consist of non-occlusion objects with rich detailed information. A feasible method is investigated to construct conceptual scenes without external datasets. We further introduce an attention-based re-weighting module that adaptively strengthens the feature adaptation of more informative regions. The network's feature **enhancement** ability is exploited without introducing extra cost during inference, which is plug-and-play in various 3D detection frameworks. We achieve new state-of-the-art performance on the KITTI 3D detection benchmark in both accuracy and speed. Experiments on nuScenes and Waymo datasets also validate the versatility of our method.  
### ForestEyes Project: Conception, **Enhancement**s, and Challenges. (arXiv:2208.11687v1 [cs.CV])
- Authors : lvaro Luiz
- Link : [http://arxiv.org/abs/2208.11687](http://arxiv.org/abs/2208.11687)
> ABSTRACT  :  Rainforests play an important role in the global ecosystem. However, significant regions of them are facing deforestation and degradation due to several reasons. Diverse government and private initiatives were created to monitor and alert for deforestation increases from remote sensing images, using different ways to deal with the notable amount of generated data. Citizen Science projects can also be used to reach the same goal. Citizen Science consists of scientific research involving nonprofessional volunteers for analyzing, collecting data, and using their computational resources to outcome advancements in science and to increase the public's understanding of problems in specific knowledge areas such as astronomy, chemistry, mathematics, and physics. In this sense, this work presents a Citizen Science project called ForestEyes, which uses volunteer's answers through the analysis and classification of remote sensing images to monitor deforestation regions in rainforests. To evaluate the quality of those answers, different campaigns/workflows were launched using remote sensing images from Brazilian Legal Amazon and their results were compared to an official groundtruth from the Amazon Deforestation Monitoring Project PRODES. In this work, the first two workflows that enclose the State of Rond\^onia in the years 2013 and 2016 received more than $35,000$ answers from $383$ volunteers in the $2,050$ created tasks in only two and a half weeks after their launch. For the other four workflows, even enclosing the same area (Rond\^onia) and different setups (e.g., image segmentation method, image resolution, and detection target), they received $51,035$ volunteers' answers gathered from $281$ volunteers in $3,358$ tasks. In the performed experiments...  
### **Real-time**, low-cost multi-person 3D pose estimation. (arXiv:2110.11414v3 [cs.CV] UPDATED)
- Authors : Alice Ruget, Max Tyler, Mora Mart, Stirling Scholes, Feng Zhu, Istvan Gyongy, Brent Hearn, Steve McLaughlin, Abderrahim Halimi, Jonathan Leach
- Link : [http://arxiv.org/abs/2110.11414](http://arxiv.org/abs/2110.11414)
> ABSTRACT  :  The process of tracking human anatomy in computer vision is referred to pose estimation, and it is used in fields ranging from gaming to surveillance. Three-dimensional pose estimation traditionally requires advanced equipment, such as multiple linked intensity cameras or high-resolution time-of-flight cameras to produce depth images. However, there are applications, e.g.~consumer electronics, where significant constraints are placed on the size, power consumption, weight and cost of the usable technology. Here, we demonstrate that computational imaging methods can achieve accurate pose estimation and overcome the apparent limitations of time-of-flight sensors designed for much simpler tasks. The sensor we use is already widely integrated in consumer-grade mobile devices, and despite its low spatial resolution, only 4$\times$4 pixels, our proposed Pixels2Pose system transforms its data into accurate depth maps and 3D pose data of multiple people up to a distance of 3 m from the sensor. We are able to generate depth maps at a resolution of 32$\times$32 and 3D localization of a body parts with an error of only $\approx$10 cm at a frame rate of 7 fps. This work opens up promising real-life applications in scenarios that were previously restricted by the advanced hardware requirements and cost of time-of-flight technology.  
### Learn from Unpaired Data for Image **Restoration**: A Variational Bayes Approach. (arXiv:2204.10090v2 [cs.CV] UPDATED)
- Authors : Dihan Zheng, Xiaowen Zhang, Kaisheng Ma, Chenglong Bao
- Link : [http://arxiv.org/abs/2204.10090](http://arxiv.org/abs/2204.10090)
> ABSTRACT  :  Collecting paired training data is difficult in practice, but the unpaired samples broadly exist. Current approaches aim at generating synthesized training data from the unpaired samples by exploring the relationship between the corrupted and clean data. This work proposes LUD-VAE, a deep generative method to learn the joint probability density function from data sampled from marginal distributions. Our approach is based on a carefully designed probabilistic graphical model in which the clean and corrupted data domains are conditionally independent. Using variational inference, we maximize the evidence lower bound (ELBO) to estimate the joint probability density function. Furthermore, we show that the ELBO is computable without paired samples under the inference invariant assumption. This property provides the mathematical rationale of our approach in the unpaired setting. Finally, we apply our method to real-world image denoising, super-resolution, and **low-light** image **enhancement** tasks and train the models using the synthetic data generated by the LUD-VAE. Experimental results validate the advantages of our method over other approaches.  
### Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v3 [cs.CV] UPDATED)
- Authors : Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, Baining Guo
- Link : [http://arxiv.org/abs/2205.14141](http://arxiv.org/abs/2205.14141)
> ABSTRACT  :  Masked image modeling (MIM) learns representations with remarkably good fine-tuning performances, overshadowing previous prevalent pre-training approaches such as image classification, instance contrastive learning, and image-text alignment. In this paper, we show that the inferior fine-tuning performance of these pre-training approaches can be significantly improved by a simple post-processing in the form of feature distillation (FD). The feature distillation converts the old representations to new representations that have a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by a set of attention- and optimization-related diagnosis tools. With these properties, the new representations show strong fine-tuning performance. Specifically, the contrastive self-supervised learning methods are made as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is also significantly improved, with a CLIP ViT-L model reaching 89.0% top-1 accuracy on ImageNet-1K classification. On the 3-billion-parameter **Swin**V2-G model, the fine-tuning accuracy is improved by +1.5 mIoU / +1.1 mAP to 61.4 mIoU / 64.2 mAP on ADE20K semantic segmentation and COCO object detection, respectively, creating new records on both benchmarks. More importantly, our work provides a way for the future research to focus more effort on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness since it can be enhanced rather easily. The code will be available at https://github.com/**Swin**Transformer/Feature-Distillation.  
### Two Decades of Bengali Handwritten Digit Recognition: A Survey. (arXiv:2206.02234v2 [cs.CV] UPDATED)
- Authors : Ashikur Rahman, Bakhtiar Hasan, Sabbir Ahmed, Tasnim Ahmed, Hamjajul Ashmafee, Mohammad Ridwan, Hasanul Kabir
- Link : [http://arxiv.org/abs/2206.02234](http://arxiv.org/abs/2206.02234)
> ABSTRACT  :  Handwritten Digit Recognition (**HDR**) is one of the most challenging tasks in the domain of Optical Character Recognition (OCR). Irrespective of language, there are some inherent challenges of **HDR**, which mostly arise due to the variations in writing styles across individuals, writing medium and environment, inability to maintain the same strokes while writing any digit repeatedly, etc. In addition to that, the structural complexities of the digits of a particular language may lead to ambiguous scenarios of **HDR**. Over the years, researchers have developed numerous offline and online **HDR** pipelines, where different image processing techniques are combined with traditional Machine Learning (ML)-based and/or Deep Learning (DL)-based architectures. Although evidence of extensive review studies on **HDR** exists in the literature for languages, such as: English, Arabic, Indian, Farsi, Chinese, etc., few surveys on Bengali **HDR** (B**HDR**) can be found, which lack a comprehensive analysis of the challenges, the underlying recognition process, and possible future directions. In this paper, the characteristics and inherent ambiguities of Bengali handwritten digits along with a comprehensive insight of two decades of the state-of-the-art datasets and approaches towards offline B**HDR** have been analyzed. Furthermore, several real-life application-specific studies, which involve B**HDR**, have also been discussed in detail. This paper will also serve as a compendium for researchers interested in the science behind offline B**HDR**, instigating the exploration of newer avenues of relevant research that may further lead to better offline recognition of Bengali handwritten digits in different application areas.  
### L2E: Lasers to Events for 6-DoF Extrinsic Calibration of Lidars and Event Cameras. (arXiv:2207.01009v2 [cs.CV] UPDATED)
- Authors : Kevin Ta, David Bruggemann, Tim Br, Christos Sakaridis, Luc Van
- Link : [http://arxiv.org/abs/2207.01009](http://arxiv.org/abs/2207.01009)
> ABSTRACT  :  As neuromorphic technology is maturing, its application to robotics and autonomous vehicle systems has become an area of active research. In particular, event cameras have emerged as a compelling alternative to frame-based cameras in low-power and latency-demanding applications. To enable event cameras to operate alongside staple sensors like lidar in perception tasks, we propose a direct, temporally-decoupled extrinsic calibration method between event cameras and lidars. The **high dynamic range**, high temporal resolution, and low-latency operation of event cameras are exploited to directly register lidar laser returns, allowing information-based correlation methods to optimize for the 6-DoF extrinsic calibration between the two sensors. This paper presents the first direct calibration method between event cameras and lidars, removing dependencies on frame-based camera intermediaries and/or highly-accurate hand measurements. Code will be made publicly available.  
### Combining Self-Training and Hybrid Architecture for Semi-supervised Abdominal Organ Segmentation. (arXiv:2207.11512v3 [cs.CV] UPDATED)
- Authors : Wentao Liu, Weijin Xu, Songlin Yan, Lemeng Wang, Huihua Yang, Haoyuan Li
- Link : [http://arxiv.org/abs/2207.11512](http://arxiv.org/abs/2207.11512)
> ABSTRACT  :  Abdominal organ segmentation has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis. However, manually annotating organs from CT scans is time-consuming and labor-intensive. Semi-supervised learning has shown the potential to alleviate this challenge by learning from a large set of unlabeled images and limited labeled samples. In this work, we follow the self-training strategy and employ a high-performance hybrid architecture (PHTrans) consisting of CNN and **Swin** Transformer for the teacher model to generate precise pseudo labels for unlabeled data. Afterward, we introduce them with labeled data together into a two-stage segmentation framework with lightweight PHTrans for training to improve the performance and generalization ability of the model while remaining efficient. Experiments on the validation set of FLARE2022 demonstrate that our method achieves excellent segmentation performance as well as fast and low-resource model inference. The average DSC and HSD are 0.8956 and 0.9316, respectively. Under our development environments, the average inference time is 18.62 s, the average maximum GPU memory is 1995.04 MB, and the area under the GPU memory-time curve and the average area under the CPU utilization-time curve are 23196.84 and 319.67.  
## eess.IV
---
### Direct Optimisation of $\boldsymbol\lambda$ for **HDR** Content Adaptive Transcoding in AV1. (arXiv:2208.11150v1 [eess.IV])
- Authors : ois Piti, Angeliki Katsenou, Daniel Joseph, Yeping Su, Neil Birkbeck, Jessie Lin, Balu Adsumilli, Anil Kokaram
- Link : [http://arxiv.org/abs/2208.11150](http://arxiv.org/abs/2208.11150)
> ABSTRACT  :  Since the adoption of VP9 by Netflix in 2016, royalty-free coding standards continued to gain prominence through the activities of the AOMedia consortium. AV1, the latest open source standard, is now widely supported. In the early years after standardisation, **HDR** video tends to be under served in open source encoders for a variety of reasons including the relatively small amount of true **HDR** content being broadcast and the challenges in RD optimisation with that material. AV1 codec optimisation has been ongoing since 2020 including consideration of the computational load. In this paper, we explore the idea of direct optimisation of the Lagrangian $\lambda$ parameter used in the rate control of the encoders to estimate the optimal Rate-Distortion trade-off achievable for a **High Dynamic Range** signalled video clip. We show that by adjusting the Lagrange multiplier in the RD optimisation process on a frame-hierarchy basis, we are able to increase the Bjontegaard difference rate gains by more than 3.98$\times$ on average without visually affecting the quality.  
### A Deep Learning Approach Using Masked Image Modeling for Reconstruction of Undersampled K-spaces. (arXiv:2208.11472v1 [eess.IV])
- Authors : Kyler Larsen, Arghya Pal, Yogesh Rathi
- Link : [http://arxiv.org/abs/2208.11472](http://arxiv.org/abs/2208.11472)
> ABSTRACT  :  Magnetic Resonance Imaging (MRI) scans are time consuming and precarious, since the patients remain still in a confined space for extended periods of time. To reduce scanning time, some experts have experimented with undersampled k spaces, trying to use deep learning to predict the fully sampled result. These studies report that as many as 20 to 30 minutes could be saved off a scan that takes an hour or more. However, none of these studies have explored the possibility of using masked image modeling (MIM) to predict the missing parts of MRI k spaces. This study makes use of 11161 reconstructed MRI and k spaces of knee MRI images from Facebook's fastmri dataset. This tests a modified version of an existing model using baseline shifted window (**Swin**) and vision transformer architectures that makes use of MIM on undersampled k spaces to predict the full k space and consequently the full MRI image. Modifications were made using pytorch and numpy libraries, and were published to a github repository. After the model reconstructed the k space images, the basic Fourier transform was applied to determine the actual MRI image. Once the model reached a steady state, experimentation with hyperparameters helped to achieve pinpoint accuracy for the reconstructed images. The model was evaluated through L1 loss, gradient normalization, and structural similarity values. The model produced reconstructed images with L1 loss values averaging to &lt;0.01 and gradient normalization values &lt;0.1 after training finished. The reconstructed k spaces yielded structural similarity values of over 99% for both training and validation with the fully sampled k spaces, while validation loss continually decreased under 0.01. These data strongly support the idea that the algorithm works for MRI reconstruction, as they indicate the model's reconstructed image aligns extremely well with the original, fully sampled k space.  
### Learn from Unpaired Data for Image **Restoration**: A Variational Bayes Approach. (arXiv:2204.10090v2 [cs.CV] UPDATED)
- Authors : Dihan Zheng, Xiaowen Zhang, Kaisheng Ma, Chenglong Bao
- Link : [http://arxiv.org/abs/2204.10090](http://arxiv.org/abs/2204.10090)
> ABSTRACT  :  Collecting paired training data is difficult in practice, but the unpaired samples broadly exist. Current approaches aim at generating synthesized training data from the unpaired samples by exploring the relationship between the corrupted and clean data. This work proposes LUD-VAE, a deep generative method to learn the joint probability density function from data sampled from marginal distributions. Our approach is based on a carefully designed probabilistic graphical model in which the clean and corrupted data domains are conditionally independent. Using variational inference, we maximize the evidence lower bound (ELBO) to estimate the joint probability density function. Furthermore, we show that the ELBO is computable without paired samples under the inference invariant assumption. This property provides the mathematical rationale of our approach in the unpaired setting. Finally, we apply our method to real-world image denoising, super-resolution, and **low-light** image **enhancement** tasks and train the models using the synthetic data generated by the LUD-VAE. Experimental results validate the advantages of our method over other approaches.  
## cs.LG
---
### An End-to-End OCR Framework for Robust Arabic-Handwriting Recognition using a Novel Transformers-based Model and an Innovative 270 Million-Words Multi-Font Corpus of Classical Arabic with Diacritics. (arXiv:2208.11484v1 [cs.CV])
- Authors : Aly Mostafa, Omar Mohamed, Ali Ashraf, Ahmed Elbehery, Salma Jamal, Anas Salah
- Link : [http://arxiv.org/abs/2208.11484](http://arxiv.org/abs/2208.11484)
> ABSTRACT  :  This research is the second phase in a series of investigations on developing an Optical Character Recognition (OCR) of Arabic historical documents and examining how different modeling procedures interact with the problem. The first research studied the effect of Transformers on our custom-built Arabic dataset. One of the downsides of the first research was the size of the training data, a mere 15000 images from our 30 million images, due to lack of resources. Also, we add an image **enhancement** layer, time and space optimization, and Post-Correction layer to aid the model in predicting the correct word for the correct context. Notably, we propose an end-to-end text recognition approach using Vision Transformers as an encoder, namely BEIT, and vanilla Transformer as a decoder, eliminating CNNs for feature extraction and reducing the model's complexity. The experiments show that our end-to-end model outperforms Convolutions Backbones. The model attained a CER of 4.46%.  
### Debias the Black-box: A Fair Ranking Framework via Knowledge Distillation. (arXiv:2208.11628v1 [cs.IR])
- Authors : Zhitao Zhu, Shijing Si, Jianzong Wang, Yaodong Yang, Jing Xiao
- Link : [http://arxiv.org/abs/2208.11628](http://arxiv.org/abs/2208.11628)
> ABSTRACT  :  Deep neural networks can capture the intricate interaction history information between queries and documents, because of their many complicated nonlinear units, allowing them to provide correct search recommendations. However, service providers frequently face more complex obstacles in real-world circumstances, such as deployment cost constraints and fairness requirements. Knowledge distillation, which transfers the knowledge of a well-trained complex model (teacher) to a simple model (student), has been proposed to alleviate the former concern, but the best current distillation methods focus only on how to make the student model imitate the predictions of the teacher model. To better facilitate the application of deep models, we propose a fair information retrieval framework based on knowledge distillation. This framework can improve the **exposure**-based fairness of models while considerably decreasing model size. Our extensive experiments on three huge datasets show that our proposed framework can reduce the model size to a minimum of 1% of its original size while maintaining its black-box state. It also improves fairness performance by 15%~46% while keeping a high level of recommendation effectiveness.  
### Fractional SDE-Net: Generation of Time Series Data with Long-term Memory. (arXiv:2201.05974v2 [cs.LG] UPDATED)
- Authors : Kohei Hayashi, Kei Nakagawa
- Link : [http://arxiv.org/abs/2201.05974](http://arxiv.org/abs/2201.05974)
> ABSTRACT  :  In this paper, we focus on the generation of time-series data using neural networks. It is often the case that input time-series data have only one realized (and usually irregularly sampled) path, which makes it difficult to extract time-series characteristics, and its noise structure is more complicated than i.i.d. type. Time series data, especially from hydrology, telecommunications, economics, and finance, exhibit long-term memory also called long-range dependency (LRD). The main purpose of this paper is to artificially generate time series with the help of neural networks, making the LRD of paths into account. We propose fSDE-Net: neural fractional Stochastic Differential Equation Network. It generalizes the neural stochastic differential equation model by using fractional Brownian motion with a Hurst index larger than half, which exhibits the LRD property. We derive the solver of fSDE-Net and theoretically analyze the existence and uniqueness of the solution to fSDE-Net. Our experiments with artificial and **real time**-series data demonstrate that the fSDE-Net model can replicate distributional properties well.  
### BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection. (arXiv:2205.13084v2 [cs.LG] UPDATED)
- Authors : Mingxuan Lu, Zhichao Han, Susie Xi, Zitao Zhang, Yang Zhao, Yinan Shan, Ramesh Raghunathan, Ce Zhang, Jiawei Jiang
- Link : [http://arxiv.org/abs/2205.13084](http://arxiv.org/abs/2205.13084)
> ABSTRACT  :  Detecting fraudulent transactions is an essential component to control risk in e-commerce marketplaces. Apart from rule-based and machine learning filters that are already deployed in production, we want to enable efficient real-time inference with graph neural networks (GNNs), which is useful to catch multihop risk propagation in a transaction graph. However, two challenges arise in the implementation of GNNs in production. First, future information in a dynamic graph should not be considered in message passing to predict the past. Second, the latency of graph query and GNN model inference is usually up to hundreds of milliseconds, which is costly for some critical online services. To tackle these challenges, we propose a Batch and **Real-time** Inception GrapH Topology (BRIGHT) framework to conduct an end-to-end GNN learning that allows efficient online real-time inference. BRIGHT framework consists of a graph transformation module (Two-Stage Directed Graph) and a corresponding GNN architecture (Lambda Neural Network). The Two-Stage Directed Graph guarantees that the information passed through neighbors is only from the historical payment transactions. It consists of two subgraphs representing historical relationships and real-time links, respectively. The Lambda Neural Network decouples inference into two stages: batch inference of entity embeddings and real-time inference of transaction prediction. Our experiments show that BRIGHT outperforms the baseline models by &gt;2\% in average w.r.t.~precision. Furthermore, BRIGHT is computationally efficient for real-time fraud detection. Regarding end-to-end performance (including neighbor query and inference), BRIGHT can reduce the P99 latency by &gt;75\%. For the inference stage, our speedup is on average 7.8$\times$ compared to the traditional GNN.  
### Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v3 [cs.CV] UPDATED)
- Authors : Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, Baining Guo
- Link : [http://arxiv.org/abs/2205.14141](http://arxiv.org/abs/2205.14141)
> ABSTRACT  :  Masked image modeling (MIM) learns representations with remarkably good fine-tuning performances, overshadowing previous prevalent pre-training approaches such as image classification, instance contrastive learning, and image-text alignment. In this paper, we show that the inferior fine-tuning performance of these pre-training approaches can be significantly improved by a simple post-processing in the form of feature distillation (FD). The feature distillation converts the old representations to new representations that have a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by a set of attention- and optimization-related diagnosis tools. With these properties, the new representations show strong fine-tuning performance. Specifically, the contrastive self-supervised learning methods are made as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is also significantly improved, with a CLIP ViT-L model reaching 89.0% top-1 accuracy on ImageNet-1K classification. On the 3-billion-parameter **Swin**V2-G model, the fine-tuning accuracy is improved by +1.5 mIoU / +1.1 mAP to 61.4 mIoU / 64.2 mAP on ADE20K semantic segmentation and COCO object detection, respectively, creating new records on both benchmarks. More importantly, our work provides a way for the future research to focus more effort on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness since it can be enhanced rather easily. The code will be available at https://github.com/**Swin**Transformer/Feature-Distillation.  
## cs.AI
---
### AGO-Net: Association-Guided 3D Point Cloud Object Detection Network. (arXiv:2208.11658v1 [cs.CV])
- Authors : Liang Du, Xiaoqing Ye, Xiao Tan, Edward Johns, Bo Chen, Errui Ding, Xiangyang Xue, Jianfeng Feng
- Link : [http://arxiv.org/abs/2208.11658](http://arxiv.org/abs/2208.11658)
> ABSTRACT  :  The human brain can effortlessly recognize and localize objects, whereas current 3D object detection methods based on LiDAR point clouds still report inferior performance for detecting occluded and distant objects: the point cloud appearance varies greatly due to occlusion, and has inherent variance in point densities along the distance to sensors. Therefore, designing feature representations robust to such point clouds is critical. Inspired by human associative recognition, we propose a novel 3D detection framework that associates intact features for objects via domain adaptation. We bridge the gap between the perceptual domain, where features are derived from real scenes with sub-optimal representations, and the conceptual domain, where features are extracted from augmented scenes that consist of non-occlusion objects with rich detailed information. A feasible method is investigated to construct conceptual scenes without external datasets. We further introduce an attention-based re-weighting module that adaptively strengthens the feature adaptation of more informative regions. The network's feature **enhancement** ability is exploited without introducing extra cost during inference, which is plug-and-play in various 3D detection frameworks. We achieve new state-of-the-art performance on the KITTI 3D detection benchmark in both accuracy and speed. Experiments on nuScenes and Waymo datasets also validate the versatility of our method.  
### BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection. (arXiv:2205.13084v2 [cs.LG] UPDATED)
- Authors : Mingxuan Lu, Zhichao Han, Susie Xi, Zitao Zhang, Yang Zhao, Yinan Shan, Ramesh Raghunathan, Ce Zhang, Jiawei Jiang
- Link : [http://arxiv.org/abs/2205.13084](http://arxiv.org/abs/2205.13084)
> ABSTRACT  :  Detecting fraudulent transactions is an essential component to control risk in e-commerce marketplaces. Apart from rule-based and machine learning filters that are already deployed in production, we want to enable efficient real-time inference with graph neural networks (GNNs), which is useful to catch multihop risk propagation in a transaction graph. However, two challenges arise in the implementation of GNNs in production. First, future information in a dynamic graph should not be considered in message passing to predict the past. Second, the latency of graph query and GNN model inference is usually up to hundreds of milliseconds, which is costly for some critical online services. To tackle these challenges, we propose a Batch and **Real-time** Inception GrapH Topology (BRIGHT) framework to conduct an end-to-end GNN learning that allows efficient online real-time inference. BRIGHT framework consists of a graph transformation module (Two-Stage Directed Graph) and a corresponding GNN architecture (Lambda Neural Network). The Two-Stage Directed Graph guarantees that the information passed through neighbors is only from the historical payment transactions. It consists of two subgraphs representing historical relationships and real-time links, respectively. The Lambda Neural Network decouples inference into two stages: batch inference of entity embeddings and real-time inference of transaction prediction. Our experiments show that BRIGHT outperforms the baseline models by &gt;2\% in average w.r.t.~precision. Furthermore, BRIGHT is computationally efficient for real-time fraud detection. Regarding end-to-end performance (including neighbor query and inference), BRIGHT can reduce the P99 latency by &gt;75\%. For the inference stage, our speedup is on average 7.8$\times$ compared to the traditional GNN.  
# Paper List
---
## cs.CV
---
**108** new papers in cs.CV:-) 
1. Multi-domain Learning for Updating Face Anti-spoofing Models. (arXiv:2208.11148v1 [cs.CV])
2. Doc2Graph: a Task Agnostic Document Understanding Framework based on Graph Neural Networks. (arXiv:2208.11168v1 [cs.CV])
3. A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels. (arXiv:2208.11176v1 [cs.CV])
4. AIM 2022 Challenge on Super-Resolution of Compressed Image and Video: Dataset, Methods and Results. (arXiv:2208.11184v1 [eess.IV])
5. Achieving Fairness in Dermatological Disease Diagnosis through Automatic Weight Adjusting Federated Learning and Personalization. (arXiv:2208.11187v1 [cs.CV])
6. Towards cumulative race time regression in sports: I3D ConvNet transfer learning in ultra-distance running events. (arXiv:2208.11191v1 [cs.CV])
7. GAN Inversion for Consistent Video Interpolation and Manipulation. (arXiv:2208.11197v1 [cs.CV])
8. Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents. (arXiv:2208.11203v1 [cs.CV])
9. Data augmentation on graphs for table type classification. (arXiv:2208.11210v1 [cs.CV])
10. A new explainable DTM generation algorithm with airborne LIDAR data: grounds are smoothly connected eventually. (arXiv:2208.11243v1 [cs.CV])
11. **Swin**FIR: Revisiting the **Swin**IR with Fast Fourier Convolution and Improved Training for Image Super-Resolution. (arXiv:2208.11247v1 [cs.CV])
12. Learnable human mesh triangulation for 3D human pose and shape estimation. (arXiv:2208.11251v1 [cs.CV])
13. FashionVQA: A Domain-Specific Visual Question Answering System. (arXiv:2208.11253v1 [cs.CV])
14. 3D-FM GAN: Towards 3D-Controllable Face Manipulation. (arXiv:2208.11257v1 [cs.CV])
15. Applying Eigencontours to PolarMask-Based Instance Segmentation. (arXiv:2208.11258v1 [cs.CV])
16. AT-DDPM: Restoring Faces degraded by Atmospheric Turbulence using Denoising Diffusion Probabilistic Models. (arXiv:2208.11284v1 [cs.CV])
17. Semi-Supervised and Unsupervised Deep Visual Learning: A Survey. (arXiv:2208.11296v1 [cs.CV])
18. E-**NeRF**: Neural Radiance Fields from a Moving Event Camera. (arXiv:2208.11300v1 [cs.CV])
19. Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization. (arXiv:2208.11303v1 [cs.CL])
20. Visual Subtitle Feature Enhanced Video Outline Generation. (arXiv:2208.11307v1 [cs.CV])
21. Deep model with built-in self-attention alignment for acoustic echo cancellation. (arXiv:2208.11308v1 [cs.SD])
22. RZSR: Reference-based Zero-Shot Super-Resolution with Depth Guided Self-Exemplars. (arXiv:2208.11313v1 [cs.CV])
23. Modality Mixer for Multi-modal Action Recognition. (arXiv:2208.11314v1 [cs.CV])
24. Comparison of Object Detection Algorithms for Street-level Objects. (arXiv:2208.11315v1 [cs.CV])
25. Robust Motion Averaging for Multi-view Registration of Point Sets Based Maximum Correntropy Criterion. (arXiv:2208.11327v1 [cs.CV])
26. K-Order Graph-oriented Transformer with GraAttention for 3D Pose and Shape Estimation. (arXiv:2208.11328v1 [cs.CV])
27. Monetisation of and Access to in-Vehicle data and resources: the 5GMETA approach. (arXiv:2208.11335v1 [cs.CY])
28. A Spatio-Temporal Attentive Network for Video-Based Crowd Counting. (arXiv:2208.11339v1 [cs.CV])
29. Discovering Transferable Forensic Features for CNN-generated Images Detection. (arXiv:2208.11342v1 [cs.CV])
30. ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data. (arXiv:2208.11346v1 [cs.CV])
31. Self-Filtering: A Noise-Aware Sample Selection for Label Noise with Confidence Penalization. (arXiv:2208.11351v1 [cs.CV])
32. Research on Mask Wearing Detection of Natural Population Based on Improved YOLOv4. (arXiv:2208.11353v1 [cs.CV])
33. Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors. (arXiv:2208.11356v1 [cs.CV])
34. On the Design of Privacy-Aware Cameras: a Study on Deep Neural Networks. (arXiv:2208.11372v1 [cs.CV])
35. WiCV 2022: The Tenth Women In Computer Vision Workshop. (arXiv:2208.11388v1 [cs.CV])
36. Event-based Image Deblurring with Dynamic Motion Awareness. (arXiv:2208.11398v1 [cs.CV])
37. Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability. (arXiv:2208.11401v1 [cs.CV])
38. Adaptive QoS of WebRTC for Vehicular Media Communications. (arXiv:2208.11405v1 [cs.NI])
39. Self-Supervised Endoscopic Image Key-Points Matching. (arXiv:2208.11424v1 [cs.CV])
40. YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception. (arXiv:2208.11434v1 [cs.CV])
41. UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering. (arXiv:2208.11435v1 [cs.CV])
42. Trace and Detect Adversarial Attacks on CNNs using Feature Response Maps. (arXiv:2208.11436v1 [cs.CV])
43. Dynamic Template Initialization for Part-Aware Person Re-ID. (arXiv:2208.11440v1 [cs.CV])
44. Hybrid Fusion Based Interpretable Multimodal Emotion Recognition with Insufficient Labelled Data. (arXiv:2208.11450v1 [cs.CV])
45. Q-Net: Query-Informed Few-Shot Medical Image Segmentation. (arXiv:2208.11451v1 [cs.CV])
46. Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages. (arXiv:2208.11467v1 [cs.CV])
47. Weakly Supervised Airway Orifice Segmentation in Video Bronchoscopy. (arXiv:2208.11468v1 [cs.CV])
48. A Deep Learning Approach Using Masked Image Modeling for Reconstruction of Undersampled K-spaces. (arXiv:2208.11472v1 [eess.IV])
49. SubFace: Learning with Softmax Approximation for Face Recognition. (arXiv:2208.11483v1 [cs.CV])
50. An End-to-End OCR Framework for Robust Arabic-Handwriting Recognition using a Novel Transformers-based Model and an Innovative 270 Million-Words Multi-Font Corpus of Classical Arabic with Diacritics. (arXiv:2208.11484v1 [cs.CV])
51. Semi-supervised Semantic Segmentation with Mutual Knowledge Distillation. (arXiv:2208.11499v1 [cs.CV])
52. Prostate Lesion Detection and Salient Feature Assessment Using Zone-Based Classifiers. (arXiv:2208.11522v1 [eess.IV])
53. Fast and Precise Binary Instance Segmentation of 2D Objects for Automotive Applications. (arXiv:2208.11527v1 [cs.CV])
54. A novel method for data augmentation: Nine Dot Moving Least Square (ND-MLS). (arXiv:2208.11532v1 [cs.CV])
55. ssFPN: Scale Sequence (S^2) Feature Based Feature Pyramid Network for Object Detection. (arXiv:2208.11533v1 [cs.CV])
56. PeRFception: Perception using Radiance Fields. (arXiv:2208.11537v1 [cs.CV])
57. Unsupervised Structure-Consistent Image-to-Image Translation. (arXiv:2208.11546v1 [cs.CV])
58. ZoomNAS: Searching for Whole-body Human Pose Estimation in the Wild. (arXiv:2208.11547v1 [cs.CV])
59. Improving video retrieval using multilingual knowledge transfer. (arXiv:2208.11553v1 [cs.CV])
60. Contrastive learning-based pretraining improves representation and transferability of diabetic retinopathy classification models. (arXiv:2208.11563v1 [eess.IV])
61. Apple Counting using Convolutional Neural Networks. (arXiv:2208.11566v1 [cs.CV])
62. Cats: Complementary CNN and Transformer Encoders for Segmentation. (arXiv:2208.11572v1 [eess.IV])
63. Active Gaze Control for Foveal Scene Exploration. (arXiv:2208.11594v1 [cs.CV])
64. Motion Robust High-Speed Light-weighted Object Detection with Event Camera. (arXiv:2208.11602v1 [cs.CV])
65. Learning crop type mapping from regional label proportions in large-scale SAR and optical imagery. (arXiv:2208.11607v1 [cs.CV])
66. Sliding Window Recurrent Network for Efficient Video Super-Resolution. (arXiv:2208.11608v1 [eess.IV])
67. Fast Nearest Convolution for Real-Time Efficient Image Super-Resolution. (arXiv:2208.11609v1 [eess.IV])
68. Unrestricted Black-box Adversarial Attack Using GAN with Limited Queries. (arXiv:2208.11613v1 [cs.CV])
69. Generative Adversarial Network (GAN) based Image-Deblurring. (arXiv:2208.11622v1 [cs.CV])
70. Detecting the unknown in Object Detection. (arXiv:2208.11641v1 [cs.CV])
71. Lane Change Classification and Prediction with Action Recognition Networks. (arXiv:2208.11650v1 [cs.CV])
72. AGO-Net: Association-Guided 3D Point Cloud Object Detection Network. (arXiv:2208.11658v1 [cs.CV])
73. Cross-Camera View-Overlap Recognition. (arXiv:2208.11661v1 [cs.CV])
74. Efficient Heterogeneous Video Segmentation at the Edge. (arXiv:2208.11666v1 [cs.CV])
75. Learned Lossless JPEG Transcoding via Joint Lossy and Residual Compression. (arXiv:2208.11673v1 [cs.CV])
76. ForestEyes Project: Conception, **Enhancement**s, and Challenges. (arXiv:2208.11687v1 [cs.CV])
77. Bugs in the Data: How ImageNet Misrepresents Biodiversity. (arXiv:2208.11695v1 [cs.CV])
78. Correctness Verification of Neural Networks. (arXiv:1906.01030v3 [cs.LG] UPDATED)
79. Adversarial Driving: Attacking End-to-End Autonomous Driving. (arXiv:2103.09151v3 [cs.CV] UPDATED)
80. Less is More: Accelerating Faster Neural Networks Straight from JPEG. (arXiv:2104.00185v2 [cs.CV] UPDATED)
81. Improving Transferability of Domain Adaptation Networks Through Domain Alignment Layers. (arXiv:2109.02693v2 [cs.CV] UPDATED)
82. AirDOS: Dynamic SLAM benefits from Articulated Objects. (arXiv:2109.09903v3 [cs.RO] UPDATED)
83. Unsupervised Cross-Modality Domain Adaptation for Segmenting Vestibular Schwannoma and Cochlea with Data Augmentation and Model Ensemble. (arXiv:2109.12169v4 [eess.IV] UPDATED)
84. **Real-time**, low-cost multi-person 3D pose estimation. (arXiv:2110.11414v3 [cs.CV] UPDATED)
85. A Novel Image Denoising Algorithm Using Concepts of Quantum Many-Body Theory. (arXiv:2112.09254v3 [eess.IV] UPDATED)
86. Learned Half-Quadratic Splitting Network for MR Image Reconstruction. (arXiv:2112.09760v3 [eess.IV] UPDATED)
87. Region-Based Semantic Factorization in GANs. (arXiv:2202.09649v2 [cs.CV] UPDATED)
88. LPF-Defense: 3D Adversarial Defense based on Frequency Analysis. (arXiv:2202.11287v2 [cs.CV] UPDATED)
89. Surgical Workflow Recognition: from Analysis of Challenges to Architectural Study. (arXiv:2203.09230v2 [cs.CV] UPDATED)
90. EVOPS Benchmark: Evaluation of Plane Segmentation from RGBD and LiDAR Data. (arXiv:2204.05799v2 [cs.CV] UPDATED)
91. Label Distribution Learning for Generalizable Multi-source Person Re-identification. (arXiv:2204.05903v2 [cs.CV] UPDATED)
92. Learn from Unpaired Data for Image **Restoration**: A Variational Bayes Approach. (arXiv:2204.10090v2 [cs.CV] UPDATED)
93. Masked Image Modeling Advances 3D Medical Image Analysis. (arXiv:2204.11716v2 [cs.CV] UPDATED)
94. Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v3 [cs.CV] UPDATED)
95. Two Decades of Bengali Handwritten Digit Recognition: A Survey. (arXiv:2206.02234v2 [cs.CV] UPDATED)
96. Safe Output Feedback Motion Planning from Images via Learned Perception Modules and Contraction Theory. (arXiv:2206.06553v2 [cs.RO] UPDATED)
97. Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment. (arXiv:2206.08614v2 [cs.CV] UPDATED)
98. L2E: Lasers to Events for 6-DoF Extrinsic Calibration of Lidars and Event Cameras. (arXiv:2207.01009v2 [cs.CV] UPDATED)
99. Combining Self-Training and Hybrid Architecture for Semi-supervised Abdominal Organ Segmentation. (arXiv:2207.11512v3 [cs.CV] UPDATED)
100. Deconstructing Self-Supervised Monocular Reconstruction: The Design Decisions that Matter. (arXiv:2208.01489v2 [cs.CV] UPDATED)
101. Multi-Task Transformer with uncertainty modelling for Face Based Affective Computing. (arXiv:2208.03506v2 [cs.CV] UPDATED)
102. Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets. (arXiv:2208.07463v3 [cs.CV] UPDATED)
103. Individual Tree Detection in Large-Scale Urban Environments using High-Resolution Multispectral Imagery. (arXiv:2208.10607v2 [cs.CV] UPDATED)
104. Object Detection in Aerial Images with Uncertainty-Aware Graph Network. (arXiv:2208.10781v2 [cs.CV] UPDATED)
105. FS-BAN: Born-Again Networks for Domain Generalization Few-Shot Classification. (arXiv:2208.10930v2 [cs.CV] UPDATED)
106. AniWho : A Quick and Accurate Way to Classify Anime Character Faces in Images. (arXiv:2208.11012v2 [cs.CV] UPDATED)
107. Robot Active Neural Sensing and Planning in Unknown Cluttered Environments. (arXiv:2208.11079v2 [cs.RO] UPDATED)
108. DeepInteraction: 3D Object Detection via Modality Interaction. (arXiv:2208.11112v2 [cs.CV] UPDATED)
## eess.IV
---
**21** new papers in eess.IV:-) 
1. Direct Optimisation of $\boldsymbol\lambda$ for **HDR** Content Adaptive Transcoding in AV1. (arXiv:2208.11150v1 [eess.IV])
2. AIM 2022 Challenge on Super-Resolution of Compressed Image and Video: Dataset, Methods and Results. (arXiv:2208.11184v1 [eess.IV])
3. A new explainable DTM generation algorithm with airborne LIDAR data: grounds are smoothly connected eventually. (arXiv:2208.11243v1 [cs.CV])
4. Federated Self-Supervised Contrastive Learning and Masked Autoencoder for Dermatological Disease Diagnosis. (arXiv:2208.11278v1 [cs.LG])
5. Appraisal of a Random Bit Generator Utilizing Smartphone Sensors as Entropy Source. (arXiv:2208.11370v1 [cs.CR])
6. Deep Joint Source-Channel Coding Based on Semantics of Pixels. (arXiv:2208.11375v1 [eess.IV])
7. Deep Hyperspectral and Multispectral Image Fusion with Inter-image Variability. (arXiv:2208.11376v1 [eess.IV])
8. A Deep Learning Approach Using Masked Image Modeling for Reconstruction of Undersampled K-spaces. (arXiv:2208.11472v1 [eess.IV])
9. Prostate Lesion Detection and Salient Feature Assessment Using Zone-Based Classifiers. (arXiv:2208.11522v1 [eess.IV])
10. Hierarchical Reinforcement Learning Based Video Semantic Coding for Segmentation. (arXiv:2208.11529v1 [eess.IV])
11. Contrastive learning-based pretraining improves representation and transferability of diabetic retinopathy classification models. (arXiv:2208.11563v1 [eess.IV])
12. Cats: Complementary CNN and Transformer Encoders for Segmentation. (arXiv:2208.11572v1 [eess.IV])
13. Sliding Window Recurrent Network for Efficient Video Super-Resolution. (arXiv:2208.11608v1 [eess.IV])
14. Fast Nearest Convolution for Real-Time Efficient Image Super-Resolution. (arXiv:2208.11609v1 [eess.IV])
15. Generative Adversarial Network (GAN) based Image-Deblurring. (arXiv:2208.11622v1 [cs.CV])
16. Towards Sparsified Federated Neuroimaging Models via Weight Pruning. (arXiv:2208.11669v1 [cs.LG])
17. Unsupervised Cross-Modality Domain Adaptation for Segmenting Vestibular Schwannoma and Cochlea with Data Augmentation and Model Ensemble. (arXiv:2109.12169v4 [eess.IV] UPDATED)
18. A Novel Image Denoising Algorithm Using Concepts of Quantum Many-Body Theory. (arXiv:2112.09254v3 [eess.IV] UPDATED)
19. Learned Half-Quadratic Splitting Network for MR Image Reconstruction. (arXiv:2112.09760v3 [eess.IV] UPDATED)
20. Learn from Unpaired Data for Image **Restoration**: A Variational Bayes Approach. (arXiv:2204.10090v2 [cs.CV] UPDATED)
21. A New Scheme for Image Compression and Encryption Using ECIES, Henon Map, and AEGAN. (arXiv:2208.07635v2 [cs.MM] UPDATED)
## cs.LG
---
**126** new papers in cs.LG:-) 
1. Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding. (arXiv:2208.11125v1 [cs.LG])
2. Retrieval-based Controllable Molecule Generation. (arXiv:2208.11126v1 [q-bio.QM])
3. Sparse Polynomial Optimization: Theory and Practice. (arXiv:2208.11158v1 [math.OC])
4. The Alberta Plan for AI Research. (arXiv:2208.11173v1 [cs.AI])
5. Auditing Membership Leakages of Multi-Exit Networks. (arXiv:2208.11180v1 [cs.CR])
6. Robustness to Unbounded Smoothness of Generalized SignSGD. (arXiv:2208.11195v1 [cs.LG])
7. Transfer Learning-based State of Health Estimation for Lithium-ion Battery with Cycle Synchronization. (arXiv:2208.11204v1 [cs.LG])
8. DeepPicarMicro: Applying TinyML to Autonomous Cyber Physical Systems. (arXiv:2208.11212v1 [cs.LG])
9. Why Deep Learning's Performance Data Are Misleading. (arXiv:2208.11228v1 [cs.LG])
10. Exact Penalty Method for Federated Learning. (arXiv:2208.11231v1 [cs.LG])
11. Preprocessing Source Code Comments for Linguistic Models. (arXiv:2208.11235v1 [cs.SE])
12. Psychophysical Machine Learning. (arXiv:2208.11236v1 [cs.LG])
13. Accelerating SGD for Highly Ill-Conditioned Huge-Scale Online Matrix Completion. (arXiv:2208.11246v1 [cs.LG])
14. Secondary Protein Structure Prediction Using Neural Networks. (arXiv:2208.11248v1 [cs.LG])
15. FashionVQA: A Domain-Specific Visual Question Answering System. (arXiv:2208.11253v1 [cs.CV])
16. Towards an Awareness of Time Series Anomaly Detection Models' Adversarial Vulnerability. (arXiv:2208.11264v1 [cs.LG])
17. SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge. (arXiv:2208.11266v1 [cs.LG])
18. Molecular Substructure-Aware Network for Drug-Drug Interaction Prediction. (arXiv:2208.11267v1 [cs.AI])
19. Federated Self-Supervised Contrastive Learning and Masked Autoencoder for Dermatological Disease Diagnosis. (arXiv:2208.11278v1 [cs.LG])
20. Robot Motion Planning as Video Prediction: A Spatio-Temporal Neural Network-based Motion Planner. (arXiv:2208.11287v1 [cs.RO])
21. ADMoE: Anomaly Detection with Mixture-of-Experts from Noisy Labels. (arXiv:2208.11290v1 [cs.LG])
22. Semi-Supervised and Unsupervised Deep Visual Learning: A Survey. (arXiv:2208.11296v1 [cs.CV])
23. Multi-objective optimization of actuation waveform for high-precision drop-on-demand inkjet printing. (arXiv:2208.11301v1 [physics.flu-dyn])
24. Fast emulation of density functional theory simulations using approximate Gaussian processes. (arXiv:2208.11302v1 [stat.ML])
25. Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments. (arXiv:2208.11311v1 [cs.LG])
26. Comparison of Object Detection Algorithms for Street-level Objects. (arXiv:2208.11315v1 [cs.CV])
27. TESTSGD: Interpretable Testing of Neural Networks Against Subtle Group Discrimination. (arXiv:2208.11321v1 [cs.LG])
28. A Bayesian Variational principle for dynamic Self Organizing Maps. (arXiv:2208.11337v1 [cs.LG])
29. Discovering Transferable Forensic Features for CNN-generated Images Detection. (arXiv:2208.11342v1 [cs.CV])
30. Time-to-Green predictions for fully-actuated signal control systems with supervised learning. (arXiv:2208.11344v1 [cs.LG])
31. Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors. (arXiv:2208.11356v1 [cs.CV])
32. Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning. (arXiv:2208.11361v1 [cs.LG])
33. A novel approach for Fair Principal Component Analysis based on eigendecomposition. (arXiv:2208.11362v1 [cs.LG])
34. Transformer-Boosted Anomaly Detection with Fuzzy Hashes. (arXiv:2208.11367v1 [cs.CR])
35. DCSF: Deep Convolutional Set Functions for Classification of Asynchronous Time Series. (arXiv:2208.11374v1 [cs.LG])
36. The premise of approximate MCMC in Bayesian deep learning. (arXiv:2208.11389v1 [stat.ML])
37. Improved Zero-Shot Audio Tagging & Classification with Patchout Spectrogram Transformers. (arXiv:2208.11402v1 [cs.SD])
38. Augmented cross-selling through explainable AI -- a case from energy retailing. (arXiv:2208.11404v1 [cs.LG])
39. Explainable AI for tailored electricity consumption feedback -- an experimental evaluation of visualizations. (arXiv:2208.11408v1 [cs.HC])
40. Automatic music mixing with deep learning and out-of-domain data. (arXiv:2208.11428v1 [eess.AS])
41. UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering. (arXiv:2208.11435v1 [cs.CV])
42. Scenario-Adaptive and Self-Supervised Model for Multi-Scenario Personalized Recommendation. (arXiv:2208.11457v1 [cs.IR])
43. Improving Natural-Language-based Audio Retrieval with Transfer Learning and Audio & Text Augmentations. (arXiv:2208.11460v1 [cs.SD])
44. Adverse Childhood Experiences Identification from Clinical Notes with Ontologies and NLP. (arXiv:2208.11466v1 [cs.CL])
45. Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages. (arXiv:2208.11467v1 [cs.CV])
46. Weakly Supervised Airway Orifice Segmentation in Video Bronchoscopy. (arXiv:2208.11468v1 [cs.CV])
47. Using Conservation Laws to Infer Deep Learning Model Accuracy of Richtmyer-meshkov Instabilities. (arXiv:2208.11477v1 [physics.flu-dyn])
48. An End-to-End OCR Framework for Robust Arabic-Handwriting Recognition using a Novel Transformers-based Model and an Innovative 270 Million-Words Multi-Font Corpus of Classical Arabic with Diacritics. (arXiv:2208.11484v1 [cs.CV])
49. Quantum Multi-Agent Meta Reinforcement Learning. (arXiv:2208.11510v1 [quant-ph])
50. A Graph Convolution for Signed Directed Graphs. (arXiv:2208.11511v1 [cs.LG])
51. FedOS: using open-set learning to stabilize training in federated learning. (arXiv:2208.11512v1 [stat.ML])
52. Inter- and Intra-Series Embeddings Fusion Network for Epidemiological Forecasting. (arXiv:2208.11515v1 [cs.LG])
53. EpiGNN: Exploring Spatial Transmission with Graph Neural Network for Regional Epidemic Forecasting. (arXiv:2208.11517v1 [q-bio.QM])
54. Collaborative Algorithms for Online Personalized Mean Estimation. (arXiv:2208.11530v1 [cs.LG])
55. A model-based approach to meta-Reinforcement Learning: Transformers and tree search. (arXiv:2208.11535v1 [cs.LG])
56. Metric Effects based on Fluctuations in values of k in Nearest Neighbor Regressor. (arXiv:2208.11540v1 [cs.LG])
57. A methodology for identifying resiliency in renewable electrical distribution system using complex network. (arXiv:2208.11543v1 [eess.SY])
58. Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions. (arXiv:2208.11561v1 [cs.LG])
59. Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning. (arXiv:2208.11580v1 [cs.LG])
60. A Low-Complexity Approach to Rate-Distortion Optimized Variable Bit-Rate Compression for Split DNN Computing. (arXiv:2208.11596v1 [cs.LG])
61. Calibrated and Enhanced NRLMSIS 2.0 Model with Uncertainty Quantification. (arXiv:2208.11619v1 [physics.space-ph])
62. PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models -- Federated Learning in Age of Foundation Model. (arXiv:2208.11625v1 [cs.LG])
63. Debias the Black-box: A Fair Ranking Framework via Knowledge Distillation. (arXiv:2208.11628v1 [cs.IR])
64. On a Built-in Conflict between Deep Learning and Systematic Generalization. (arXiv:2208.11633v1 [cs.LG])
65. ImitAL: Learned Active Learning Strategy on Synthetic Data. (arXiv:2208.11636v1 [cs.LG])
66. Oracle-free Reinforcement Learning in Mean-Field Games along a Single Sample Path. (arXiv:2208.11639v1 [cs.LG])
67. Constraint-driven multi-task learning. (arXiv:2208.11656v1 [cs.LG])
68. Efficient Heterogeneous Video Segmentation at the Edge. (arXiv:2208.11666v1 [cs.CV])
69. Towards Sparsified Federated Neuroimaging Models via Weight Pruning. (arXiv:2208.11669v1 [cs.LG])
70. Bugs in the Data: How ImageNet Misrepresents Biodiversity. (arXiv:2208.11695v1 [cs.CV])
71. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. (arXiv:1707.04926v3 [cs.LG] UPDATED)
72. Correctness Verification of Neural Networks. (arXiv:1906.01030v3 [cs.LG] UPDATED)
73. Dual Extrapolation for Sparse Generalized Linear Models. (arXiv:1907.05830v3 [stat.ML] UPDATED)
74. Weakly Supervised Disentangled Generative Causal Representation Learning. (arXiv:2010.02637v3 [cs.LG] UPDATED)
75. Learning to predict synchronization of coupled oscillators on randomly generated graphs. (arXiv:2012.14048v3 [math.DS] UPDATED)
76. PAC-learning gains of Turing machines over circuits and neural networks. (arXiv:2103.12686v2 [cs.LG] UPDATED)
77. A Riemannian Newton Trust-Region Method for Fitting Gaussian Mixture Models. (arXiv:2104.14957v2 [stat.ML] UPDATED)
78. FedIPR: Ownership Verification for Federated Deep Neural Network Models. (arXiv:2109.13236v3 [cs.LG] UPDATED)
79. Probabilistic Robust Autoencoders for Outlier Detection. (arXiv:2110.00494v3 [cs.LG] UPDATED)
80. Synergy: Resource Sensitive DNN Scheduling in Multi-Tenant Clusters. (arXiv:2110.06073v2 [cs.DC] UPDATED)
81. Error-Correcting Neural Networks for Semi-Lagrangian Advection in the Level-Set Method. (arXiv:2110.11611v2 [cs.LG] UPDATED)
82. Physics informed machine learning with Smoothed particle hydrodynamics: Hierarchy of reduced Lagrangian models of turbulence. (arXiv:2110.13311v4 [physics.flu-dyn] UPDATED)
83. Fairness for AUC via Feature Augmentation. (arXiv:2111.12823v2 [cs.LG] UPDATED)
84. Synthetic ECG Signal Generation Using Generative Neural Networks. (arXiv:2112.03268v2 [cs.LG] UPDATED)
85. A Novel Deep Parallel Time-series Relation Network for Fault Diagnosis. (arXiv:2112.03405v3 [cs.LG] UPDATED)
86. Fractional SDE-Net: Generation of Time Series Data with Long-term Memory. (arXiv:2201.05974v2 [cs.LG] UPDATED)
87. SPDY: Accurate Pruning with Speedup Guarantees. (arXiv:2201.13096v2 [cs.LG] UPDATED)
88. Calibration of P-values for calibration and for deviation of a subpopulation from the full population. (arXiv:2202.00100v6 [stat.ME] UPDATED)
89. LPF-Defense: 3D Adversarial Defense based on Frequency Analysis. (arXiv:2202.11287v2 [cs.CV] UPDATED)
90. DISCO: Comprehensive and Explainable Disinformation Detection. (arXiv:2203.04928v3 [cs.LG] UPDATED)
91. Plasticity Neural Network Based on Astrocytic Influence at Critical Period, Synaptic Competition and Compensation by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v4 [cs.NE] UPDATED)
92. SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis. (arXiv:2204.03040v2 [cs.SD] UPDATED)
93. Masked Image Modeling Advances 3D Medical Image Analysis. (arXiv:2204.11716v2 [cs.CV] UPDATED)
94. AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time. (arXiv:2204.13307v2 [cs.LG] UPDATED)
95. BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection. (arXiv:2205.13084v2 [cs.LG] UPDATED)
96. Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v3 [cs.CV] UPDATED)
97. GIN: Graph-based Interaction-aware Constraint Policy Optimization for Autonomous Driving. (arXiv:2206.01488v3 [cs.RO] UPDATED)
98. Probability flow solution of the Fokker-Planck equation. (arXiv:2206.04642v2 [cs.LG] UPDATED)
99. Safe Output Feedback Motion Planning from Images via Learned Perception Modules and Contraction Theory. (arXiv:2206.06553v2 [cs.RO] UPDATED)
100. Concentration inequalities and optimal number of layers for stochastic deep neural networks. (arXiv:2206.11241v2 [cs.LG] UPDATED)
101. Benchmark Dataset for Precipitation Forecasting by Post-Processing the Numerical Weather Prediction. (arXiv:2206.15241v2 [cs.LG] UPDATED)
102. Signed Link Representation in Continuous-Time Dynamic Signed Networks. (arXiv:2207.03408v2 [cs.SI] UPDATED)
103. UM4: Unified Multilingual Multiple Teacher-Student Model for Zero-Resource Neural Machine Translation. (arXiv:2207.04900v2 [cs.CL] UPDATED)
104. AutoML-Based Drought Forecast with Meteorological Variables. (arXiv:2207.07012v2 [cs.LG] UPDATED)
105. Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey. (arXiv:2207.07068v3 [cs.LG] UPDATED)
106. High-Order Conditional Mutual Information Maximization for dealing with High-Order Dependencies in Feature Selection. (arXiv:2207.08476v2 [cs.LG] UPDATED)
107. A coherence parameter characterizing generative compressed sensing with Fourier measurements. (arXiv:2207.09340v2 [cs.IT] UPDATED)
108. Deconstructing Self-Supervised Monocular Reconstruction: The Design Decisions that Matter. (arXiv:2208.01489v2 [cs.CV] UPDATED)
109. Learning the Trading Algorithm in Simulated Markets with Non-stationary Continuum Bandits. (arXiv:2208.02901v2 [cs.MA] UPDATED)
110. Semi-Supervised Junction Tree Variational Autoencoder for Molecular Property Prediction. (arXiv:2208.05119v3 [cs.LG] UPDATED)
111. Hybrid Approach to Identify Druglikeness Leading Compounds against COVID-19 3CL Protease. (arXiv:2208.06362v3 [q-bio.BM] UPDATED)
112. Making Reinforcement Learning Work on Swimmer. (arXiv:2208.07587v2 [cs.LG] UPDATED)
113. Shallow neural network representation of polynomials. (arXiv:2208.08138v4 [stat.ML] UPDATED)
114. Discovering Agents. (arXiv:2208.08345v2 [cs.AI] UPDATED)
115. ObfuNAS: A Neural Architecture Search-based DNN Obfuscation Approach. (arXiv:2208.08569v2 [cs.CR] UPDATED)
116. An ensemble meta-estimator to predict source code testability. (arXiv:2208.09614v2 [cs.SE] UPDATED)
117. Transfer Ranking in Finance: Applications to Cross-Sectional Momentum with Data Scarcity. (arXiv:2208.09968v2 [q-fin.TR] UPDATED)
118. Decentralized Collaborative Learning with Probabilistic Data Protection. (arXiv:2208.10674v2 [cs.LG] UPDATED)
119. AniWho : A Quick and Accurate Way to Classify Anime Character Faces in Images. (arXiv:2208.11012v2 [cs.CV] UPDATED)
120. Grad-Align+: Empowering Gradual Network Alignment Using Attribute Augmentation. (arXiv:2208.11025v2 [cs.SI] UPDATED)
121. Multi-Modal Representation Learning with Self-Adaptive Thresholds for Commodity Verification. (arXiv:2208.11064v2 [cs.LG] UPDATED)
122. Categoroids: Universal Conditional Independence. (arXiv:2208.11077v2 [cs.AI] UPDATED)
123. Robot Active Neural Sensing and Planning in Unknown Cluttered Environments. (arXiv:2208.11079v2 [cs.RO] UPDATED)
124. Learn Basic Skills and Reuse: Modularized Adaptive Neural Architecture Search (MANAS). (arXiv:2208.11083v2 [cs.LG] UPDATED)
125. Knowledge Graph Fact Prediction via Knowledge-Enriched Tensor Factorization. (arXiv:1902.03077v1 [cs.LG] CROSS LISTED)
126. Are disentangled representations all you need to build speaker anonymization systems?. (arXiv:2208.10497v2 [cs.SD] CROSS LISTED)
## cs.AI
---
**67** new papers in cs.AI:-) 
1. Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding. (arXiv:2208.11125v1 [cs.LG])
2. The Alberta Plan for AI Research. (arXiv:2208.11173v1 [cs.AI])
3. On Fitness Landscape Analysis of Permutation Problems: From Distance Metrics to Mutation Operator Selection. (arXiv:2208.11188v1 [cs.NE])
4. POPDx: An Automated Framework for Patient Phenotyping across 392,246 Individuals in the UK Biobank Study. (arXiv:2208.11223v1 [q-bio.QM])
5. Why Deep Learning's Performance Data Are Misleading. (arXiv:2208.11228v1 [cs.LG])
6. Psychophysical Machine Learning. (arXiv:2208.11236v1 [cs.LG])
7. FashionVQA: A Domain-Specific Visual Question Answering System. (arXiv:2208.11253v1 [cs.CV])
8. SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge. (arXiv:2208.11266v1 [cs.LG])
9. Molecular Substructure-Aware Network for Drug-Drug Interaction Prediction. (arXiv:2208.11267v1 [cs.AI])
10. Multi-AI Complex Systems in Humanitarian Response. (arXiv:2208.11282v1 [cs.CY])
11. Semi-Supervised and Unsupervised Deep Visual Learning: A Survey. (arXiv:2208.11296v1 [cs.CV])
12. Visual Subtitle Feature Enhanced Video Outline Generation. (arXiv:2208.11307v1 [cs.CV])
13. Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments. (arXiv:2208.11311v1 [cs.LG])
14. Enhancing Deep Learning Performance of Massive MIMO CSI Feedback. (arXiv:2208.11333v1 [cs.IT])
15. Advanced Tools and Methods for Treewidth-Based Problem Solving -- Extended Abstract. (arXiv:2208.11340v1 [cs.AI])
16. Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration. (arXiv:2208.11349v1 [cs.AI])
17. Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors. (arXiv:2208.11356v1 [cs.CV])
18. Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning. (arXiv:2208.11361v1 [cs.LG])
19. Augmented cross-selling through explainable AI -- a case from energy retailing. (arXiv:2208.11404v1 [cs.LG])
20. Explainable AI for tailored electricity consumption feedback -- an experimental evaluation of visualizations. (arXiv:2208.11408v1 [cs.HC])
21. Dynamic Template Initialization for Part-Aware Person Re-ID. (arXiv:2208.11440v1 [cs.CV])
22. Scenario-Adaptive and Self-Supervised Model for Multi-Scenario Personalized Recommendation. (arXiv:2208.11457v1 [cs.IR])
23. A Generalization of the Shortest Path Problem to Graphs with Multiple Edge-Cost Estimates. (arXiv:2208.11489v1 [cs.DS])
24. Quantum Multi-Agent Meta Reinforcement Learning. (arXiv:2208.11510v1 [quant-ph])
25. A Graph Convolution for Signed Directed Graphs. (arXiv:2208.11511v1 [cs.LG])
26. Inter- and Intra-Series Embeddings Fusion Network for Epidemiological Forecasting. (arXiv:2208.11515v1 [cs.LG])
27. EpiGNN: Exploring Spatial Transmission with Graph Neural Network for Regional Epidemic Forecasting. (arXiv:2208.11517v1 [q-bio.QM])
28. A model-based approach to meta-Reinforcement Learning: Transformers and tree search. (arXiv:2208.11535v1 [cs.LG])
29. CheapET-3: Cost-Efficient Use of Remote DNN Models. (arXiv:2208.11552v1 [cs.SE])
30. Toward a Reasoning and Learning Architecture for Ad Hoc Teamwork. (arXiv:2208.11556v1 [cs.AI])
31. Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions. (arXiv:2208.11561v1 [cs.LG])
32. Graphical Models of False Information and Fact Checking Ecosystems. (arXiv:2208.11582v1 [cs.SI])
33. On a Built-in Conflict between Deep Learning and Systematic Generalization. (arXiv:2208.11633v1 [cs.LG])
34. ImitAL: Learned Active Learning Strategy on Synthetic Data. (arXiv:2208.11636v1 [cs.LG])
35. Repair Is Nearly Generation: Multilingual Program Repair with LLMs. (arXiv:2208.11640v1 [cs.SE])
36. A Review of Knowledge Graph Completion. (arXiv:2208.11652v1 [cs.AI])
37. Constraint-driven multi-task learning. (arXiv:2208.11656v1 [cs.LG])
38. AGO-Net: Association-Guided 3D Point Cloud Object Detection Network. (arXiv:2208.11658v1 [cs.CV])
39. Mech-Elites: Illuminating the Mechanic Space of GVGAI. (arXiv:2002.04733v2 [cs.AI] UPDATED)
40. Conjunctive Queries: Unique Characterizations and Exact Learnability. (arXiv:2008.06824v4 [cs.LO] UPDATED)
41. Weakly Supervised Disentangled Generative Causal Representation Learning. (arXiv:2010.02637v3 [cs.LG] UPDATED)
42. FedIPR: Ownership Verification for Federated Deep Neural Network Models. (arXiv:2109.13236v3 [cs.LG] UPDATED)
43. Fairness for AUC via Feature Augmentation. (arXiv:2111.12823v2 [cs.LG] UPDATED)
44. Synthetic ECG Signal Generation Using Generative Neural Networks. (arXiv:2112.03268v2 [cs.LG] UPDATED)
45. A Novel Deep Parallel Time-series Relation Network for Fault Diagnosis. (arXiv:2112.03405v3 [cs.LG] UPDATED)
46. Cryogenic Neuromorphic Hardware. (arXiv:2204.07503v2 [cs.ET] UPDATED)
47. Masked Image Modeling Advances 3D Medical Image Analysis. (arXiv:2204.11716v2 [cs.CV] UPDATED)
48. AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time. (arXiv:2204.13307v2 [cs.LG] UPDATED)
49. BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection. (arXiv:2205.13084v2 [cs.LG] UPDATED)
50. GIN: Graph-based Interaction-aware Constraint Policy Optimization for Autonomous Driving. (arXiv:2206.01488v3 [cs.RO] UPDATED)
51. Understanding Machine Learning Practitioners' Data Documentation Perceptions, Needs, Challenges, and Desiderata. (arXiv:2206.02923v2 [cs.HC] UPDATED)
52. Safe Output Feedback Motion Planning from Images via Learned Perception Modules and Contraction Theory. (arXiv:2206.06553v2 [cs.RO] UPDATED)
53. Neural Network Normal Estimation and Bathymetry Reconstruction from Sidescan Sonar. (arXiv:2206.07819v2 [cs.RO] UPDATED)
54. UM4: Unified Multilingual Multiple Teacher-Student Model for Zero-Resource Neural Machine Translation. (arXiv:2207.04900v2 [cs.CL] UPDATED)
55. Learning to Coordinate for a Worker-Station Multi-robot System in Planar Coverage Tasks. (arXiv:2208.02993v2 [cs.RO] UPDATED)
56. Aesthetic Bot: Interactively Evolving Game Maps on Twitter. (arXiv:2208.05017v2 [cs.AI] UPDATED)
57. SignalKG: Towards Reasoning about the Underlying Causes of Sensor Observations. (arXiv:2208.05627v2 [cs.AI] UPDATED)
58. Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets. (arXiv:2208.07463v3 [cs.CV] UPDATED)
59. Discovering Agents. (arXiv:2208.08345v2 [cs.AI] UPDATED)
60. Mapping Husserlian phenomenology onto active inference. (arXiv:2208.09058v2 [q-bio.NC] UPDATED)
61. Information-Theoretic Equivalence of Entropic Multi-Marginal Optimal Transport: A Theory for Multi-Agent Communication. (arXiv:2208.10256v2 [cs.IT] UPDATED)
62. Grad-Align+: Empowering Gradual Network Alignment Using Attribute Augmentation. (arXiv:2208.11025v2 [cs.SI] UPDATED)
63. Categoroids: Universal Conditional Independence. (arXiv:2208.11077v2 [cs.AI] UPDATED)
64. Robot Active Neural Sensing and Planning in Unknown Cluttered Environments. (arXiv:2208.11079v2 [cs.RO] UPDATED)
65. Learn Basic Skills and Reuse: Modularized Adaptive Neural Architecture Search (MANAS). (arXiv:2208.11083v2 [cs.LG] UPDATED)
66. Knowledge Graph Fact Prediction via Knowledge-Enriched Tensor Factorization. (arXiv:1902.03077v1 [cs.LG] CROSS LISTED)
67. Are disentangled representations all you need to build speaker anonymization systems?. (arXiv:2208.10497v2 [cs.SD] CROSS LISTED)

