# Your interest papers
---
## cs.CV
---
### Dual-Stage Deeply Supervised Attention-based Convolutional Neural Networks for Mandibular Canal Segmentation in CBCT Scans. (arXiv:2210.03739v1 [eess.IV])
- Authors : Azka Rehman, Muhammad Usman, Rabeea Jawaid, Shi Sub, Sung Hyun, Byoung Dai, Byung il, Yeong Gil
- Link : [http://arxiv.org/abs/2210.03739](http://arxiv.org/abs/2210.03739)
> ABSTRACT  :  Accurate segmentation of mandibular canals in lower jaws is important in dental implantology, in which the implant position and dimensions are currently determined manually from 3D CT images by medical experts to avoid damaging the mandibular nerve inside the canal. In this paper, we propose a novel dual-stage deep learning based scheme for automatic detection of mandibular canal. Particularly, we first we enhance the CBCT scans by employing the novel histogram-based dynamic windowing scheme which improves the visibility of mandibular canals. After **enhancement**, we design 3D deeply supervised attention U-Net architecture for localize the volume of interest (VOI) which contains the mandibular canals (i.e., left and right canals). Finally, we employed the multi-scale input residual U-Net architecture (MS-R-UNet) to accurately segment the mandibular canals. The proposed method has been rigorously evaluated on 500 scans and results demonstrate that our technique out performs the existing state-of-the-art methods in term of segmentation performance as well as robustness.  
### Single Image Super-Resolution Based on Capsule Neural Networks. (arXiv:2210.03743v1 [eess.IV])
- Authors : George Corr, de Ara, Helio Pedrini
- Link : [http://arxiv.org/abs/2210.03743](http://arxiv.org/abs/2210.03743)
> ABSTRACT  :  Single image super-resolution (SISR) is the process of obtaining one high-resolution version of a low-resolution image by increasing the number of pixels per unit area. This method has been actively investigated by the research community, due to the wide variety of real-world problems where it can be applied, from aerial and satellite imaging to compressed image and video **enhancement**. Despite the improvements achieved by deep learning in the field, the vast majority of the used networks are based on traditional convolutions, with the solutions focusing on going deeper and/or wider, and innovations coming from jointly employing successful concepts from other fields. In this work, we decided to step up from the traditional convolutions and adopt the concept of capsules. Since their overwhelming results both in image classification and segmentation problems, we question how suitable they are for SISR. We also verify that different solutions share most of their configurations, and argue that this trend leads to fewer explorations of network varieties. During our experiments, we check various strategies to improve results, ranging from new and different loss functions to changes in the capsule layers. Our network achieved good results with fewer convolutional-based layers, showing that capsules might be a concept worth applying in the image super-resolution problem.  
### Self-Aligned Concave Curve: Illumination **Enhancement** for Unsupervised Adaptation. (arXiv:2210.03792v1 [cs.CV])
- Authors : Wenjing Wang, Zhengbo Xu, Haofeng Huang, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2210.03792](http://arxiv.org/abs/2210.03792)
> ABSTRACT  :  **Low light** conditions not only degrade human visual experience, but also reduce the performance of downstream machine analytics. Although many works have been designed for **low-light** **enhancement** or domain adaptive machine analytics, the former considers less on high-level vision, while the latter neglects the potential of image-level signal adjustment. How to restore underexposed images/videos from the perspective of machine vision has long been overlooked. In this paper, we are the first to propose a learnable illumination **enhancement** model for high-level vision. Inspired by real camera response functions, we assume that the illumination **enhancement** function should be a concave curve, and propose to satisfy this concavity through discrete integral. With the intention of adapting illumination from the perspective of machine vision without task-specific annotated data, we design an asymmetric cross-domain self-supervised training strategy. Our model architecture and training designs mutually benefit each other, forming a powerful unsupervised normal-to-**low light** adaptation framework. Comprehensive experiments demonstrate that our method surpasses existing **low-light** **enhancement** and adaptation methods and shows superior generalization on various **low-light** vision tasks, including classification, detection, action recognition, and optical flow estimation. Project website: https://daooshee.github.io/SACC-Website/  
### LW-**ISP**: A Lightweight Model with **ISP** and Deep Learning. (arXiv:2210.03904v1 [cs.CV])
- Authors : Hongyang Chen, Kaisheng Ma
- Link : [http://arxiv.org/abs/2210.03904](http://arxiv.org/abs/2210.03904)
> ABSTRACT  :  The deep learning (DL)-based methods of low-level tasks have many advantages over the traditional camera in terms of hardware prospects, error accumulation and imaging effects. Recently, the application of deep learning to replace the image signal processing (**ISP**) pipeline has appeared one after another; however, there is still a long way to go towards real landing. In this paper, we show the possibility of learning-based method to achieve real-time high-performance processing in the **ISP** pipeline. We propose LW-**ISP**, a novel architecture designed to implicitly learn the image mapping from RAW data to RGB image. Based on U-Net architecture, we propose the fine-grained attention module and a plug-and-play upsampling block suitable for low-level tasks. In particular, we design a heterogeneous distillation algorithm to distill the implicit features and reconstruction information of the clean image, so as to guide the learning of the student model. Our experiments demonstrate that LW-**ISP** has achieved a 0.38 dB improvement in PSNR compared to the previous best method, while the model parameters and calculation have been reduced by 23 times and 81 times. The inference efficiency has been accelerated by at least 15 times. Without bells and whistles, LW-**ISP** has achieved quite competitive results in **ISP** subtasks including image denoising and **enhancement**.  
### A GAN-Based Input-Size Flexibility Model for Single Image Dehazing. (arXiv:2102.09796v2 [eess.IV] UPDATED)
- Authors : Shichao Kan, Yue Zhang, Fanghui Zhang, Yigang Cen
- Link : [http://arxiv.org/abs/2102.09796](http://arxiv.org/abs/2102.09796)
> ABSTRACT  :  Image-to-image translation based on generative adversarial network (GAN) has achieved state-of-the-art performance in various image **restoration** applications. Single image dehazing is a typical example, which aims to obtain the haze-free image of a haze one. This paper concentrates on the challenging task of single image dehazing. Based on the atmospheric scattering model, a novel model is designed to directly generate the haze-free image. The main challenge of image dehazing is that the atmospheric scattering model has two parameters, i.e., transmission map and atmospheric light. When they are estimated respectively, the errors will be accumulated to compromise the dehazing quality. Considering this reason and various image sizes, a novel input-size flexibility conditional generative adversarial network (cGAN) is proposed for single image dehazing, which is input-size flexibility at both training and test stages for image-to-image translation with cGAN framework. A simple and effective U-connection residual network (UR-Net) is proposed to combine the generator and adopt the spatial pyramid pooling (SPP) to design the discriminator. Moreover, the model is trained with multi-loss function, in which the consistency loss is a novel designed loss in this paper. Finally, a multi-scale cGAN fusion model is built to realize state-of-the-art single image dehazing performance. The proposed models receive a haze image as input and directly output a haze-free one. Experimental results demonstrate the effectiveness and efficiency of the proposed models.  
### PCNet: A Structure Similarity **Enhancement** Method for Multispectral and Multimodal Image Registration. (arXiv:2106.05124v3 [cs.CV] UPDATED)
- Authors : Yuan Cao, Beinan Yu, Lun Luo, Jie Chen, Chunguang Li, Liang Shen
- Link : [http://arxiv.org/abs/2106.05124](http://arxiv.org/abs/2106.05124)
> ABSTRACT  :  Multispectral and multimodal images are of important usage in the field of multi-source visual information fusion. Due to the alternation or movement of image devices, the acquired multispectral and multimodal images are usually misaligned, and hence image registration is pre-requisite. Different from the registration of common images, the registration of multispectral or multimodal images is a challenging problem due to the nonlinear variation of intensity and gradient. To cope with this challenge, we propose the phase congruency network (PCNet) to enhance the structure similarity of multispectral or multimodal images. The images can then be aligned using the similarity-enhanced feature maps produced by the network. PCNet is constructed under the inspiration of the well-known phase congruency. The network embeds the phase congruency prior into two simple trainable layers and series of modified learnable Gabor kernels. Thanks to the prior knowledge, once trained, PCNet is applicable on a variety of multispectral and multimodal data such as flash/no-flash and RGB/NIR images without additional further tuning. The prior also makes the network lightweight. The trainable parameters of PCNet are 2400 times less than the deep-learning registration method DHN, while its registration performance surpasses DHN. Experimental results validate that PCNet outperforms current state-of-the-art conventional multimodal registration algorithms. Besides, PCNet can act as a complementary part of the deep-learning registration methods, which significantly boosts their registration accuracy. The percentage of the number of images under 1 pixel average corner error (ACE) of UDHN is raised from 0.2% to 89.9% after the processing of PCNet.  
### You Only Need 90K Parameters to Adapt Light: A Light Weight Transformer for Image **Enhancement** and **Exposure** Correction. (arXiv:2205.14871v4 [cs.CV] UPDATED)
- Authors : Ziteng Cui, Kunchang Li, Lin Gu, Shenghan Su, Peng Gao, Zhengkai Jiang, Yu Qiao, Tatsuya Harada
- Link : [http://arxiv.org/abs/2205.14871](http://arxiv.org/abs/2205.14871)
> ABSTRACT  :  Challenging illumination conditions (**low-light**, under-**exposure** and over-**exposure**) in the real world not only cast an unpleasant visual appearance but also taint the computer vision tasks. After camera captures the raw-RGB data, it renders standard sRGB images with image signal processor (**ISP**). By decomposing **ISP** pipeline into local and global image components, we propose a lightweight fast Illumination Adaptive Transformer (IAT) to restore the normal lit sRGB image from either **low-light** or under/over-**exposure** conditions. Specifically, IAT uses attention queries to represent and adjust the **ISP**-related parameters such as colour correction, gamma correction. With only ~90k parameters and ~0.004s processing speed, our IAT consistently achieves superior performance over SOTA on the current benchmark **low-light** **enhancement** and **exposure** correction datasets. Competitive experimental performance also demonstrates that our IAT significantly enhances object detection and semantic segmentation tasks under various light conditions. Training code and pretrained model is available at https://github.com/cuiziteng/Illumination-Adaptive-Transformer.  
### On a Mechanism Framework of Autoencoders. (arXiv:2208.06995v2 [cs.LG] UPDATED)
- Authors : Changcun Huang
- Link : [http://arxiv.org/abs/2208.06995](http://arxiv.org/abs/2208.06995)
> ABSTRACT  :  This paper proposes a theoretical framework on the mechanism of autoencoders. To the encoder part, under the main use of dimensionality reduction, we investigate its two fundamental properties: bijective maps and data disentangling. The general construction methods of an encoder that satisfies either or both of the above two properties are given. To the decoder part, as a consequence of the encoder constructions, we present a new basic principle of the solution, without using affine transforms. The generalization mechanism of autoencoders is modeled. The results of ReLU autoencoders are generalized to some non-ReLU cases, particularly for the sigmoid-unit autoencoder. Based on the theoretical framework above, we explain some experimental results of variational autoencoders, denoising autoencoders, and linear-unit autoencoders, with emphasis on the interpretation of the lower-dimensional representation of data via encoders; and the mechanism of image **restoration** through autoencoders is natural to be understood by those explanations. Compared to PCA and decision trees, the advantages of (generalized) autoencoders on dimensionality reduction and classification are demonstrated, respectively. Convolutional neural networks and randomly weighted neural networks are also interpreted by this framework.  
## eess.IV
---
### Dual-Stage Deeply Supervised Attention-based Convolutional Neural Networks for Mandibular Canal Segmentation in CBCT Scans. (arXiv:2210.03739v1 [eess.IV])
- Authors : Azka Rehman, Muhammad Usman, Rabeea Jawaid, Shi Sub, Sung Hyun, Byoung Dai, Byung il, Yeong Gil
- Link : [http://arxiv.org/abs/2210.03739](http://arxiv.org/abs/2210.03739)
> ABSTRACT  :  Accurate segmentation of mandibular canals in lower jaws is important in dental implantology, in which the implant position and dimensions are currently determined manually from 3D CT images by medical experts to avoid damaging the mandibular nerve inside the canal. In this paper, we propose a novel dual-stage deep learning based scheme for automatic detection of mandibular canal. Particularly, we first we enhance the CBCT scans by employing the novel histogram-based dynamic windowing scheme which improves the visibility of mandibular canals. After **enhancement**, we design 3D deeply supervised attention U-Net architecture for localize the volume of interest (VOI) which contains the mandibular canals (i.e., left and right canals). Finally, we employed the multi-scale input residual U-Net architecture (MS-R-UNet) to accurately segment the mandibular canals. The proposed method has been rigorously evaluated on 500 scans and results demonstrate that our technique out performs the existing state-of-the-art methods in term of segmentation performance as well as robustness.  
### Single Image Super-Resolution Based on Capsule Neural Networks. (arXiv:2210.03743v1 [eess.IV])
- Authors : George Corr, de Ara, Helio Pedrini
- Link : [http://arxiv.org/abs/2210.03743](http://arxiv.org/abs/2210.03743)
> ABSTRACT  :  Single image super-resolution (SISR) is the process of obtaining one high-resolution version of a low-resolution image by increasing the number of pixels per unit area. This method has been actively investigated by the research community, due to the wide variety of real-world problems where it can be applied, from aerial and satellite imaging to compressed image and video **enhancement**. Despite the improvements achieved by deep learning in the field, the vast majority of the used networks are based on traditional convolutions, with the solutions focusing on going deeper and/or wider, and innovations coming from jointly employing successful concepts from other fields. In this work, we decided to step up from the traditional convolutions and adopt the concept of capsules. Since their overwhelming results both in image classification and segmentation problems, we question how suitable they are for SISR. We also verify that different solutions share most of their configurations, and argue that this trend leads to fewer explorations of network varieties. During our experiments, we check various strategies to improve results, ranging from new and different loss functions to changes in the capsule layers. Our network achieved good results with fewer convolutional-based layers, showing that capsules might be a concept worth applying in the image super-resolution problem.  
### LW-**ISP**: A Lightweight Model with **ISP** and Deep Learning. (arXiv:2210.03904v1 [cs.CV])
- Authors : Hongyang Chen, Kaisheng Ma
- Link : [http://arxiv.org/abs/2210.03904](http://arxiv.org/abs/2210.03904)
> ABSTRACT  :  The deep learning (DL)-based methods of low-level tasks have many advantages over the traditional camera in terms of hardware prospects, error accumulation and imaging effects. Recently, the application of deep learning to replace the image signal processing (**ISP**) pipeline has appeared one after another; however, there is still a long way to go towards real landing. In this paper, we show the possibility of learning-based method to achieve real-time high-performance processing in the **ISP** pipeline. We propose LW-**ISP**, a novel architecture designed to implicitly learn the image mapping from RAW data to RGB image. Based on U-Net architecture, we propose the fine-grained attention module and a plug-and-play upsampling block suitable for low-level tasks. In particular, we design a heterogeneous distillation algorithm to distill the implicit features and reconstruction information of the clean image, so as to guide the learning of the student model. Our experiments demonstrate that LW-**ISP** has achieved a 0.38 dB improvement in PSNR compared to the previous best method, while the model parameters and calculation have been reduced by 23 times and 81 times. The inference efficiency has been accelerated by at least 15 times. Without bells and whistles, LW-**ISP** has achieved quite competitive results in **ISP** subtasks including image denoising and **enhancement**.  
### Guided Nonlocal Patch Regularization and Efficient Filtering-Based Inversion for Multiband Fusion. (arXiv:2210.04184v1 [eess.IV])
- Authors : Pravin Nair
- Link : [http://arxiv.org/abs/2210.04184](http://arxiv.org/abs/2210.04184)
> ABSTRACT  :  In multiband fusion, an image with a high spatial and low spectral resolution is combined with an image with a low spatial but high spectral resolution to produce a single multiband image having high spatial and spectral resolutions. This comes up in remote sensing applications such as pansharpening~(MS+PAN), hyperspectral sharpening~(HS+PAN), and HS-MS fusion~(HS+MS). Remote sensing images are textured and have repetitive structures. Motivated by nonlocal patch-based methods for image **restoration**, we propose a convex regularizer that (i) takes into account long-distance correlations, (ii) penalizes patch variation, which is more effective than pixel variation for capturing texture information, and (iii) uses the higher spatial resolution image as a guide image for weight computation. We come up with an efficient ADMM algorithm for optimizing the regularizer along with a standard least-squares loss function derived from the imaging model. The novelty of our algorithm is that by expressing patch variation as filtering operations and by judiciously splitting the original variables and introducing latent variables, we are able to solve the ADMM subproblems efficiently using FFT-based convolution and soft-thresholding. As far as the reconstruction quality is concerned, our method is shown to outperform state-of-the-art variational and deep learning techniques.  
### Invertible Rescaling Network and Its Extensions. (arXiv:2210.04188v1 [eess.IV])
- Authors : Mingqing Xiao, Shuxin Zheng, Chang Liu, Zhouchen Lin, Yan Liu
- Link : [http://arxiv.org/abs/2210.04188](http://arxiv.org/abs/2210.04188)
> ABSTRACT  :  Image rescaling is a commonly used bidirectional operation, which first downscales high-resolution images to fit various display screens or to be storage- and bandwidth-friendly, and afterward upscales the corresponding low-resolution images to recover the original resolution or the details in the zoom-in images. However, the non-injective downscaling mapping discards high-frequency contents, leading to the ill-posed problem for the inverse **restoration** task. This can be abstracted as a general image degradation-**restoration** problem with information loss. In this work, we propose a novel invertible framework to handle this general problem, which models the bidirectional degradation and **restoration** from a new perspective, i.e. invertible bijective transformation. The invertibility enables the framework to model the information loss of pre-degradation in the form of distribution, which could mitigate the ill-posed problem during post-**restoration**. To be specific, we develop invertible models to generate valid degraded images and meanwhile transform the distribution of lost contents to the fixed distribution of a latent variable during the forward degradation. Then **restoration** is made tractable by applying the inverse transformation on the generated degraded image together with a randomly-drawn latent variable. We start from image rescaling and instantiate the model as Invertible Rescaling Network (IRN), which can be easily extended to the similar decolorization-colorization task. We further propose to combine the invertible framework with existing degradation methods such as image compression for wider applications. Experimental results demonstrate the significant improvement of our model over existing methods in terms of both quantitative and qualitative evaluations of upscaling and colorizing reconstruction from downscaled and decolorized images, and rate-distortion of image compression.  
### DeepHS-**HDR**Video: Deep High Speed **High Dynamic Range** Video Reconstruction. (arXiv:2210.04429v1 [eess.IV])
- Authors : Zeeshan Khan, Parth Shettiwar, Mukul Khanna, Shanmuganathan Raman
- Link : [http://arxiv.org/abs/2210.04429](http://arxiv.org/abs/2210.04429)
> ABSTRACT  :  Due to hardware constraints, standard off-the-shelf digital cameras suffers from low dynamic range (LDR) and low frame per second (FPS) outputs. Previous works in **high dynamic range** (**HDR**) video reconstruction uses sequence of alternating **exposure** LDR frames as input, and align the neighbouring frames using optical flow based networks. However, these methods often result in motion artifacts in challenging situations. This is because, the alternate **exposure** frames have to be **exposure** matched in order to apply alignment using optical flow. Hence, over-saturation and noise in the LDR frames results in inaccurate alignment. To this end, we propose to align the input LDR frames using a pre-trained video frame interpolation network. This results in better alignment of LDR frames, since we circumvent the error-prone **exposure** matching step, and directly generate intermediate missing frames from the same **exposure** inputs. Furthermore, it allows us to generate high FPS **HDR** videos by recursively interpolating the intermediate frames. Through this work, we propose to use video frame interpolation for **HDR** video reconstruction, and present the first method to generate high FPS **HDR** videos. Experimental results demonstrate the efficacy of the proposed framework against optical flow based alignment methods, with an absolute improvement of 2.4 PSNR value on standard **HDR** video datasets [1], [2] and further benchmark our method for high FPS **HDR** video generation.  
### A GAN-Based Input-Size Flexibility Model for Single Image Dehazing. (arXiv:2102.09796v2 [eess.IV] UPDATED)
- Authors : Shichao Kan, Yue Zhang, Fanghui Zhang, Yigang Cen
- Link : [http://arxiv.org/abs/2102.09796](http://arxiv.org/abs/2102.09796)
> ABSTRACT  :  Image-to-image translation based on generative adversarial network (GAN) has achieved state-of-the-art performance in various image **restoration** applications. Single image dehazing is a typical example, which aims to obtain the haze-free image of a haze one. This paper concentrates on the challenging task of single image dehazing. Based on the atmospheric scattering model, a novel model is designed to directly generate the haze-free image. The main challenge of image dehazing is that the atmospheric scattering model has two parameters, i.e., transmission map and atmospheric light. When they are estimated respectively, the errors will be accumulated to compromise the dehazing quality. Considering this reason and various image sizes, a novel input-size flexibility conditional generative adversarial network (cGAN) is proposed for single image dehazing, which is input-size flexibility at both training and test stages for image-to-image translation with cGAN framework. A simple and effective U-connection residual network (UR-Net) is proposed to combine the generator and adopt the spatial pyramid pooling (SPP) to design the discriminator. Moreover, the model is trained with multi-loss function, in which the consistency loss is a novel designed loss in this paper. Finally, a multi-scale cGAN fusion model is built to realize state-of-the-art single image dehazing performance. The proposed models receive a haze image as input and directly output a haze-free one. Experimental results demonstrate the effectiveness and efficiency of the proposed models.  
### Direct Optimisation of $\boldsymbol\lambda$ for **HDR** Content Adaptive Transcoding in AV1. (arXiv:2208.11150v2 [eess.IV] UPDATED)
- Authors : ois Piti, Angeliki Katsenou, Daniel Joseph, Yeping Su, Neil Birkbeck, Jessie Lin, Balu Adsumilli, Anil Kokaram
- Link : [http://arxiv.org/abs/2208.11150](http://arxiv.org/abs/2208.11150)
> ABSTRACT  :  Since the adoption of VP9 by Netflix in 2016, royalty-free coding standards continued to gain prominence through the activities of the AOMedia consortium. AV1, the latest open source standard, is now widely supported. In the early years after standardisation, **HDR** video tends to be under served in open source encoders for a variety of reasons including the relatively small amount of true **HDR** content being broadcast and the challenges in RD optimisation with that material. AV1 codec optimisation has been ongoing since 2020 including consideration of the computational load. In this paper, we explore the idea of direct optimisation of the Lagrangian $\lambda$ parameter used in the rate control of the encoders to estimate the optimal Rate-Distortion trade-off achievable for a **High Dynamic Range** signalled video clip. We show that by adjusting the Lagrange multiplier in the RD optimisation process on a frame-hierarchy basis, we are able to increase the Bjontegaard difference rate gains by more than 3.98$\times$ on average without visually affecting the quality.  
## cs.LG
---
### Single Image Super-Resolution Based on Capsule Neural Networks. (arXiv:2210.03743v1 [eess.IV])
- Authors : George Corr, de Ara, Helio Pedrini
- Link : [http://arxiv.org/abs/2210.03743](http://arxiv.org/abs/2210.03743)
> ABSTRACT  :  Single image super-resolution (SISR) is the process of obtaining one high-resolution version of a low-resolution image by increasing the number of pixels per unit area. This method has been actively investigated by the research community, due to the wide variety of real-world problems where it can be applied, from aerial and satellite imaging to compressed image and video **enhancement**. Despite the improvements achieved by deep learning in the field, the vast majority of the used networks are based on traditional convolutions, with the solutions focusing on going deeper and/or wider, and innovations coming from jointly employing successful concepts from other fields. In this work, we decided to step up from the traditional convolutions and adopt the concept of capsules. Since their overwhelming results both in image classification and segmentation problems, we question how suitable they are for SISR. We also verify that different solutions share most of their configurations, and argue that this trend leads to fewer explorations of network varieties. During our experiments, we check various strategies to improve results, ranging from new and different loss functions to changes in the capsule layers. Our network achieved good results with fewer convolutional-based layers, showing that capsules might be a concept worth applying in the image super-resolution problem.  
### Varying Coefficient Linear Discriminant Analysis for Dynamic Data. (arXiv:2203.06371v3 [stat.ME] UPDATED)
- Authors : Yajie Bao, Yuyang Liu
- Link : [http://arxiv.org/abs/2203.06371](http://arxiv.org/abs/2203.06371)
> ABSTRACT  :  Linear discriminant analysis (LDA) is an important classification tool in statistics and machine learning. This paper investigates the varying coefficient LDA model for dynamic data, with Bayes' discriminant direction being a function of some **exposure** variable to address the heterogeneity. We propose a new least-square estimation method based on the B-spline approximation. The data-driven discriminant procedure is more computationally efficient than the dynamic linear programming rule \citep{jiang2020dynamic}. We also establish the convergence rates for the corresponding estimation error bound and the excess misclassification risk. The estimation error in $L_2$ distance is optimal for the low-dimensional regime and is near optimal for the high-dimensional regime. Numerical experiments on synthetic data and real data both corroborate the superiority of our proposed classification method.  
### On a Mechanism Framework of Autoencoders. (arXiv:2208.06995v2 [cs.LG] UPDATED)
- Authors : Changcun Huang
- Link : [http://arxiv.org/abs/2208.06995](http://arxiv.org/abs/2208.06995)
> ABSTRACT  :  This paper proposes a theoretical framework on the mechanism of autoencoders. To the encoder part, under the main use of dimensionality reduction, we investigate its two fundamental properties: bijective maps and data disentangling. The general construction methods of an encoder that satisfies either or both of the above two properties are given. To the decoder part, as a consequence of the encoder constructions, we present a new basic principle of the solution, without using affine transforms. The generalization mechanism of autoencoders is modeled. The results of ReLU autoencoders are generalized to some non-ReLU cases, particularly for the sigmoid-unit autoencoder. Based on the theoretical framework above, we explain some experimental results of variational autoencoders, denoising autoencoders, and linear-unit autoencoders, with emphasis on the interpretation of the lower-dimensional representation of data via encoders; and the mechanism of image **restoration** through autoencoders is natural to be understood by those explanations. Compared to PCA and decision trees, the advantages of (generalized) autoencoders on dimensionality reduction and classification are demonstrated, respectively. Convolutional neural networks and randomly weighted neural networks are also interpreted by this framework.  
### Deep Baseline Network for Time Series Modeling and Anomaly Detection. (arXiv:2209.04561v2 [cs.LG] UPDATED)
- Authors : Cheng Ge, Xi Chen, Ming Wang, Jin Wang
- Link : [http://arxiv.org/abs/2209.04561](http://arxiv.org/abs/2209.04561)
> ABSTRACT  :  Deep learning has seen increasing applications in time series in recent years. For time series anomaly detection scenarios, such as in finance, Internet of Things, data center operations, etc., time series usually show very flexible baselines depending on various external factors. Anomalies unveil themselves by lying far away from the baseline. However, the detection is not always easy due to some challenges including baseline shifting, lacking of labels, noise interference, **real time** detection in streaming data, result interpretability, etc. In this paper, we develop a novel deep architecture to properly extract the baseline from time series, namely Deep Baseline Network (DBLN). By using this deep network, we can easily locate the baseline position and then provide reliable and interpretable anomaly detection result. Empirical evaluation on both synthetic and public real-world datasets shows that our purely unsupervised algorithm achieves superior performance compared with state-of-art methods and has good practical applications.  
## cs.AI
---
### Dual-Stage Deeply Supervised Attention-based Convolutional Neural Networks for Mandibular Canal Segmentation in CBCT Scans. (arXiv:2210.03739v1 [eess.IV])
- Authors : Azka Rehman, Muhammad Usman, Rabeea Jawaid, Shi Sub, Sung Hyun, Byoung Dai, Byung il, Yeong Gil
- Link : [http://arxiv.org/abs/2210.03739](http://arxiv.org/abs/2210.03739)
> ABSTRACT  :  Accurate segmentation of mandibular canals in lower jaws is important in dental implantology, in which the implant position and dimensions are currently determined manually from 3D CT images by medical experts to avoid damaging the mandibular nerve inside the canal. In this paper, we propose a novel dual-stage deep learning based scheme for automatic detection of mandibular canal. Particularly, we first we enhance the CBCT scans by employing the novel histogram-based dynamic windowing scheme which improves the visibility of mandibular canals. After **enhancement**, we design 3D deeply supervised attention U-Net architecture for localize the volume of interest (VOI) which contains the mandibular canals (i.e., left and right canals). Finally, we employed the multi-scale input residual U-Net architecture (MS-R-UNet) to accurately segment the mandibular canals. The proposed method has been rigorously evaluated on 500 scans and results demonstrate that our technique out performs the existing state-of-the-art methods in term of segmentation performance as well as robustness.  
### Single Image Super-Resolution Based on Capsule Neural Networks. (arXiv:2210.03743v1 [eess.IV])
- Authors : George Corr, de Ara, Helio Pedrini
- Link : [http://arxiv.org/abs/2210.03743](http://arxiv.org/abs/2210.03743)
> ABSTRACT  :  Single image super-resolution (SISR) is the process of obtaining one high-resolution version of a low-resolution image by increasing the number of pixels per unit area. This method has been actively investigated by the research community, due to the wide variety of real-world problems where it can be applied, from aerial and satellite imaging to compressed image and video **enhancement**. Despite the improvements achieved by deep learning in the field, the vast majority of the used networks are based on traditional convolutions, with the solutions focusing on going deeper and/or wider, and innovations coming from jointly employing successful concepts from other fields. In this work, we decided to step up from the traditional convolutions and adopt the concept of capsules. Since their overwhelming results both in image classification and segmentation problems, we question how suitable they are for SISR. We also verify that different solutions share most of their configurations, and argue that this trend leads to fewer explorations of network varieties. During our experiments, we check various strategies to improve results, ranging from new and different loss functions to changes in the capsule layers. Our network achieved good results with fewer convolutional-based layers, showing that capsules might be a concept worth applying in the image super-resolution problem.  
# Paper List
---
## cs.CV
---
**130** new papers in cs.CV:-) 
1. T2CI-GAN: Text to Compressed Image generation using Generative Adversarial Network. (arXiv:2210.03734v1 [cs.CV])
2. "Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction. (arXiv:2210.03735v1 [cs.HC])
3. Trustworthy clinical AI solutions: a unified review of uncertainty quantification in deep learning models for medical image analysis. (arXiv:2210.03736v1 [eess.IV])
4. Dual-Stage Deeply Supervised Attention-based Convolutional Neural Networks for Mandibular Canal Segmentation in CBCT Scans. (arXiv:2210.03739v1 [eess.IV])
5. Single Image Super-Resolution Based on Capsule Neural Networks. (arXiv:2210.03743v1 [eess.IV])
6. In What Ways Are Deep Neural Networks Invariant and How Should We Measure This?. (arXiv:2210.03773v1 [cs.LG])
7. MRI-based classification of IDH mutation and 1p/19q codeletion status of gliomas using a 2.5D hybrid multi-task convolutional neural network. (arXiv:2210.03779v1 [eess.IV])
8. LOCL: Learning Object-Attribute Composition using Localization. (arXiv:2210.03780v1 [cs.CV])
9. Evaluating the Performance of StyleGAN2-ADA on Medical Images. (arXiv:2210.03786v1 [cs.CV])
10. Learning a Visually Grounded Memory Assistant. (arXiv:2210.03787v1 [cs.CV])
11. Self-Aligned Concave Curve: Illumination **Enhancement** for Unsupervised Adaptation. (arXiv:2210.03792v1 [cs.CV])
12. SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models. (arXiv:2210.03794v1 [cs.CV])
13. Scene-level Tracking and Reconstruction without Object Priors. (arXiv:2210.03815v1 [cs.RO])
14. Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v1 [cs.LG])
15. Self-Supervised Deep Equilibrium Models for Inverse Problems with Theoretical Guarantees. (arXiv:2210.03837v1 [eess.IV])
16. Learning to embed semantic similarity for joint image-text retrieval. (arXiv:2210.03838v1 [cs.CV])
17. Toward an Over-parameterized Direct-Fit Model of Visual Perception. (arXiv:2210.03850v1 [cs.CV])
18. Revisiting Self-Supervised Contrastive Learning for Facial Expression Recognition. (arXiv:2210.03853v1 [cs.CV])
19. Towards Light Weight Object Detection System. (arXiv:2210.03861v1 [cs.CV])
20. Improving Fine-Grain Segmentation via Interpretable Modifications: A Case Study in Fossil Segmentation. (arXiv:2210.03879v1 [cs.CV])
21. Rethinking the Detection Head Configuration for Traffic Object Detection. (arXiv:2210.03883v1 [cs.CV])
22. Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts. (arXiv:2210.03885v1 [cs.LG])
23. ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints. (arXiv:2210.03895v1 [cs.CV])
24. Multi-Scale Wavelet Transformer for Face Forgery Detection. (arXiv:2210.03899v1 [cs.CV])
25. LW-**ISP**: A Lightweight Model with **ISP** and Deep Learning. (arXiv:2210.03904v1 [cs.CV])
26. A Higher Purpose: Measuring Electricity Access Using High-Resolution Daytime Satellite Imagery. (arXiv:2210.03909v1 [cs.CV])
27. CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Image Manipulation. (arXiv:2210.03919v1 [cs.CV])
28. Contextual Modeling for 3D Dense Captioning on Point Clouds. (arXiv:2210.03925v1 [cs.CV])
29. EgoTaskQA: Understanding Human Tasks in Egocentric Videos. (arXiv:2210.03929v1 [cs.CV])
30. Cloud Native Robotic Applications with GPU Sharing on Kubernetes. (arXiv:2210.03936v1 [cs.RO])
31. Hierarchical Few-Shot Object Detection: Problem, Benchmark and Method. (arXiv:2210.03940v1 [cs.CV])
32. Learning Fine-Grained Visual Understanding for Video Question Answering via Decoupling Spatial-Temporal Modeling. (arXiv:2210.03941v1 [cs.CV])
33. Point Cloud Upsampling via Cascaded Refinement Network. (arXiv:2210.03942v1 [cs.CV])
34. ArabSign: A Multi-modality Dataset and Benchmark for Continuous Arabic Sign Language Recognition. (arXiv:2210.03951v1 [cs.CV])
35. Detaching and Boosting: Dual Engine for Scale-Invariant Self-Supervised Monocular Depth Estimation. (arXiv:2210.03952v1 [cs.CV])
36. Contact-aware Human Motion Forecasting. (arXiv:2210.03954v1 [cs.CV])
37. Robust Graph Structure Learning over Images via Multiple Statistical Tests. (arXiv:2210.03956v1 [cs.CV])
38. FBNet: Feedback Network for Point Cloud Completion. (arXiv:2210.03974v1 [cs.CV])
39. (Fusionformer):Exploiting the Joint Motion Synergy with Fusion Network Based On Transformer for 3D Human Pose Estimation. (arXiv:2210.04006v1 [cs.CV])
40. AdaptivePose++: A Powerful Single-Stage Network for Multi-Person Pose Regression. (arXiv:2210.04014v1 [cs.CV])
41. Fast-ParC: Position Aware Global Kernel for ConvNets and ViTs. (arXiv:2210.04020v1 [cs.CV])
42. Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing. (arXiv:2210.04026v1 [cs.CV])
43. Multi-Modal Human Authentication Using Silhouettes, Gait and RGB. (arXiv:2210.04050v1 [cs.CV])
44. Flow-based GAN for 3D Point Cloud Generation from a Single Image. (arXiv:2210.04072v1 [cs.CV])
45. Dual Pyramid Generative Adversarial Networks for Semantic Image Synthesis. (arXiv:2210.04085v1 [cs.CV])
46. Symmetry Subgroup Defense Against Adversarial Attacks. (arXiv:2210.04087v1 [cs.LG])
47. Training Deep Learning Algorithms on Synthetic Forest Images for Tree Detection. (arXiv:2210.04104v1 [cs.CV])
48. The effect of variable labels on deep learning models trained to predict breast density. (arXiv:2210.04106v1 [cs.CV])
49. Region Refinement Network for Salient Object Detection. (arXiv:1906.11443v2 [cs.CV] UPDATED)
50. Cali-Sketch: Stroke Calibration and Completion for High-Quality Face Image Generation from Human-Like Sketches. (arXiv:1911.00426v2 [cs.CV] UPDATED)
51. Improving Few-shot Learning by Spatially-aware Matching and CrossTransformer. (arXiv:2001.01600v2 [cs.CV] UPDATED)
52. A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v6 [cs.LG] UPDATED)
53. A Unified Mixture-View Framework for Unsupervised Representation Learning. (arXiv:2011.13356v2 [cs.CV] UPDATED)
54. Periocular Embedding Learning with Consistent Knowledge Distillation from Face. (arXiv:2012.06746v2 [cs.CV] UPDATED)
55. A GAN-Based Input-Size Flexibility Model for Single Image Dehazing. (arXiv:2102.09796v2 [eess.IV] UPDATED)
56. Deep Image Harmonization by Bridging the Reality Gap. (arXiv:2103.17104v2 [cs.CV] UPDATED)
57. Preservation of the Global Knowledge by Not-True Distillation in Federated Learning. (arXiv:2106.03097v4 [cs.LG] UPDATED)
58. PCNet: A Structure Similarity **Enhancement** Method for Multispectral and Multimodal Image Registration. (arXiv:2106.05124v3 [cs.CV] UPDATED)
59. Deep Learning for Micro-expression Recognition: A Survey. (arXiv:2107.02823v5 [cs.CV] UPDATED)
60. Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability. (arXiv:2108.01335v2 [cs.CV] UPDATED)
61. DocTr: Document Image Transformer for Geometric Unwarping and Illumination Correction. (arXiv:2110.12942v2 [cs.CV] UPDATED)
62. Towards Domain-Independent and Real-Time Gesture Recognition Using mmWave Signal. (arXiv:2111.06195v3 [cs.CV] UPDATED)
63. Why Do Self-Supervised Models Transfer? Investigating the Impact of Invariance on Downstream Tasks. (arXiv:2111.11398v2 [cs.CV] UPDATED)
64. Confounder Identification-free Causal Visual Feature Learning. (arXiv:2111.13420v3 [cs.LG] UPDATED)
65. AssistSR: Task-oriented Video Segment Retrieval for Personal AI Assistant. (arXiv:2111.15050v4 [cs.CV] UPDATED)
66. GANSeg: Learning to Segment by Unsupervised Hierarchical Image Generation. (arXiv:2112.01036v3 [cs.CV] UPDATED)
67. Learn2Reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning. (arXiv:2112.04489v3 [eess.IV] UPDATED)
68. Short and Long Range Relation Based Spatio-Temporal Transformer for Micro-Expression Recognition. (arXiv:2112.05851v3 [cs.CV] UPDATED)
69. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v5 [cs.CV] UPDATED)
70. GeoFill: Reference-Based Image Inpainting with Better Geometric Understanding. (arXiv:2201.08131v2 [cs.CV] UPDATED)
71. Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition. (arXiv:2201.10394v3 [cs.CV] UPDATED)
72. Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space. (arXiv:2202.03800v3 [cs.CV] UPDATED)
73. Adaptive Graph Convolutional Networks for Weakly Supervised Anomaly Detection in Videos. (arXiv:2202.06503v3 [cs.CV] UPDATED)
74. A multi-reconstruction study of breast density estimation using Deep Learning. (arXiv:2202.08238v3 [eess.IV] UPDATED)
75. GaitStrip: Gait Recognition via Effective Strip-based Feature Representations and Multi-Level Framework. (arXiv:2203.03966v2 [cs.CV] UPDATED)
76. Can I see an Example? Active Learning the Long Tail of Attributes and Relations. (arXiv:2203.06215v2 [cs.CV] UPDATED)
77. Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v4 [eess.IV] UPDATED)
78. Nonnegative-Constrained Joint Collaborative Representation with Union Dictionary for Hyperspectral Anomaly Detection. (arXiv:2203.10030v2 [cs.CV] UPDATED)
79. Exemplar Learning for Medical Image Segmentation. (arXiv:2204.01713v2 [eess.IV] UPDATED)
80. A Comprehensive Survey on Data-Efficient GANs in Image Generation. (arXiv:2204.08329v2 [cs.CV] UPDATED)
81. Self-paced Multi-grained Cross-modal Interaction Modeling for Referring Expression Comprehension. (arXiv:2204.09957v2 [cs.CV] UPDATED)
82. Elucidating Meta-Structures of Noisy Labels in Semantic Segmentation by Deep Neural Networks. (arXiv:2205.00160v3 [cs.CV] UPDATED)
83. BiOcularGAN: Bimodal Synthesis and Annotation of Ocular Images. (arXiv:2205.01536v2 [cs.CV] UPDATED)
84. Scene Clustering Based Pseudo-labeling Strategy for Multi-modal Aerial View Object Classification. (arXiv:2205.01920v5 [cs.CV] UPDATED)
85. AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints. (arXiv:2205.10636v5 [cs.CV] UPDATED)
86. Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy. (arXiv:2205.10683v3 [cs.LG] UPDATED)
87. Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. (arXiv:2205.12522v2 [cs.CV] UPDATED)
88. VectorAdam for Rotation Equivariant Geometry Optimization. (arXiv:2205.13599v2 [cs.LG] UPDATED)
89. You Only Need 90K Parameters to Adapt Light: A Light Weight Transformer for Image **Enhancement** and **Exposure** Correction. (arXiv:2205.14871v4 [cs.CV] UPDATED)
90. Self-Supervised Visual Representation Learning with Semantic Grouping. (arXiv:2205.15288v2 [cs.CV] UPDATED)
91. REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering. (arXiv:2206.01201v2 [cs.CV] UPDATED)
92. NeMF: Neural Motion Fields for Kinematic Animation. (arXiv:2206.03287v3 [cs.CV] UPDATED)
93. Deep Learning based Direct Segmentation Assisted by Deformable Image Registration for Cone-Beam CT based Auto-Segmentation for Adaptive Radiotherapy. (arXiv:2206.03413v2 [physics.med-ph] UPDATED)
94. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v4 [cs.AI] UPDATED)
95. Satellite-based high-resolution maps of cocoa for C\^ote d'Ivoire and Ghana. (arXiv:2206.06119v4 [cs.CV] UPDATED)
96. Singular Value Fine-tuning: Few-shot Segmentation requires Few-parameters Fine-tuning. (arXiv:2206.06122v2 [cs.CV] UPDATED)
97. Rethinking Generalization in Few-Shot Classification. (arXiv:2206.07267v2 [cs.CV] UPDATED)
98. Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. (arXiv:2206.08155v2 [cs.CV] UPDATED)
99. 3D-Aware Video Generation. (arXiv:2206.14797v2 [cs.CV] UPDATED)
100. Factorizing Knowledge in Neural Networks. (arXiv:2207.03337v2 [cs.CV] UPDATED)
101. PR-DARTS: Pruning-Based Differentiable Architecture Search. (arXiv:2207.06968v3 [cs.CV] UPDATED)
102. Automatic universal taxonomies for multi-domain semantic segmentation. (arXiv:2207.08445v2 [cs.CV] UPDATED)
103. Rethinking Alignment in Video Super-Resolution Transformers. (arXiv:2207.08494v2 [cs.CV] UPDATED)
104. Correspondences between word learning in children and captioning models. (arXiv:2207.09847v2 [cs.CL] UPDATED)
105. Is an Object-Centric Video Representation Beneficial for Transfer?. (arXiv:2207.10075v2 [cs.CV] UPDATED)
106. Simplex Clustering via sBeta with Applications to Online Adjustment of Black-Box Predictions. (arXiv:2208.00287v3 [cs.CV] UPDATED)
107. TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation. (arXiv:2208.01813v3 [cs.CV] UPDATED)
108. Constructing Balance from Imbalance for Long-tailed Image Recognition. (arXiv:2208.02567v2 [cs.CV] UPDATED)
109. Trustworthy Visual Analytics in Clinical Gait Analysis: A Case Study for Patients with Cerebral Palsy. (arXiv:2208.05232v2 [cs.HC] UPDATED)
110. On a Mechanism Framework of Autoencoders. (arXiv:2208.06995v2 [cs.LG] UPDATED)
111. Neuromorphic Visual Scene Understanding with Resonator Networks. (arXiv:2208.12880v2 [cs.CV] UPDATED)
112. Generative Modelling of the Ageing Heart with Cross-Sectional Imaging and Clinical Data. (arXiv:2208.13146v2 [eess.IV] UPDATED)
113. CounTR: Transformer-based Generalised Visual Counting. (arXiv:2208.13721v2 [cs.CV] UPDATED)
114. Neuromorphic Visual Odometry with Resonator Networks. (arXiv:2209.02000v2 [cs.RO] UPDATED)
115. Privacy-Preserving Deep Learning Model for Covid-19 Disease Detection. (arXiv:2209.04445v2 [eess.IV] UPDATED)
116. Label Refinement Network from Synthetic Error Augmentation for Medical Image Segmentation. (arXiv:2209.06353v2 [eess.IV] UPDATED)
117. Deliberated Domain Bridging for Domain Adaptive Semantic Segmentation. (arXiv:2209.07695v2 [cs.CV] UPDATED)
118. Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution. (arXiv:2209.08503v2 [cs.CV] UPDATED)
119. Self-adversarial Multi-scale Contrastive Learning for Semantic Segmentation of Thermal Facial Images. (arXiv:2209.10700v2 [cs.CV] UPDATED)
120. Correcting the Sub-optimal Bit Allocation. (arXiv:2209.14575v2 [cs.CV] UPDATED)
121. A Closer Look at Temporal Ordering in the Segmentation of Instructional Videos. (arXiv:2209.15501v2 [cs.CV] UPDATED)
122. MaskTune: Mitigating Spurious Correlations by Forcing to Explore. (arXiv:2210.00055v2 [cs.LG] UPDATED)
123. SinGRAV: Learning a Generative Radiance Volume from a Single Natural Scene. (arXiv:2210.01202v2 [cs.CV] UPDATED)
124. COARSE3D: Class-Prototypes for Contrastive Learning in Weakly-Supervised 3D Point Cloud Segmentation. (arXiv:2210.01784v2 [cs.CV] UPDATED)
125. Grounding Language with Visual Affordances over Unstructured Data. (arXiv:2210.01911v2 [cs.RO] UPDATED)
126. InterFace:Adjustable Angular Margin Inter-class Loss for Deep Face Recognition. (arXiv:2210.02018v2 [cs.CV] UPDATED)
127. Learning Consistency-Aware Unsigned Distance Functions Progressively from Raw Point Clouds. (arXiv:2210.02757v2 [cs.CV] UPDATED)
128. Synthetic Dataset Generation for Privacy-Preserving Machine Learning. (arXiv:2210.03205v2 [cs.CR] UPDATED)
129. Spatio-temporal Tendency Reasoning for Human Body Pose and Shape Estimation from Videos. (arXiv:2210.03659v2 [cs.CV] UPDATED)
130. Bi-directional Weakly Supervised Knowledge Distillation for Whole Slide Image Classification. (arXiv:2210.03664v2 [cs.CV] UPDATED)
## eess.IV
---
**37** new papers in eess.IV:-) 
1. T2CI-GAN: Text to Compressed Image generation using Generative Adversarial Network. (arXiv:2210.03734v1 [cs.CV])
2. Trustworthy clinical AI solutions: a unified review of uncertainty quantification in deep learning models for medical image analysis. (arXiv:2210.03736v1 [eess.IV])
3. Dual-Stage Deeply Supervised Attention-based Convolutional Neural Networks for Mandibular Canal Segmentation in CBCT Scans. (arXiv:2210.03739v1 [eess.IV])
4. Single Image Super-Resolution Based on Capsule Neural Networks. (arXiv:2210.03743v1 [eess.IV])
5. MRI-based classification of IDH mutation and 1p/19q codeletion status of gliomas using a 2.5D hybrid multi-task convolutional neural network. (arXiv:2210.03779v1 [eess.IV])
6. Self-Supervised Deep Equilibrium Models for Inverse Problems with Theoretical Guarantees. (arXiv:2210.03837v1 [eess.IV])
7. Rethinking the Detection Head Configuration for Traffic Object Detection. (arXiv:2210.03883v1 [cs.CV])
8. LW-**ISP**: A Lightweight Model with **ISP** and Deep Learning. (arXiv:2210.03904v1 [cs.CV])
9. Visual Looming from Motion Field and Surface Normals. (arXiv:2210.04108v1 [eess.IV])
10. Leveraging progressive model and overfitting for efficient learned image compression. (arXiv:2210.04112v1 [cs.CV])
11. HVS Revisited: A Comprehensive Video Quality Assessment Framework. (arXiv:2210.04158v1 [eess.IV])
12. Guided Nonlocal Patch Regularization and Efficient Filtering-Based Inversion for Multiband Fusion. (arXiv:2210.04184v1 [eess.IV])
13. Invertible Rescaling Network and Its Extensions. (arXiv:2210.04188v1 [eess.IV])
14. Super-Resolution by Predicting Offsets: An Ultra-Efficient Super-Resolution Network for Rasterized Images. (arXiv:2210.04198v1 [eess.IV])
15. Improved Abdominal Multi-Organ Segmentation via 3D Boundary-Constrained Deep Neural Networks. (arXiv:2210.04285v1 [eess.IV])
16. Quantification of Pollen Viability in Lantana camara By Digital Holographic Microscopy. (arXiv:2210.04421v1 [eess.IV])
17. DeepHS-**HDR**Video: Deep High Speed **High Dynamic Range** Video Reconstruction. (arXiv:2210.04429v1 [eess.IV])
18. Non-invasive color imaging through scattering medium under broadband illumination. (arXiv:2210.04630v1 [physics.optics])
19. Deep Learning Mixture-of-Experts Approach for Cytotoxic Edema Assessment in Infants and Children. (arXiv:2210.04767v1 [eess.IV])
20. Contrastive Learning Approach for Semi-Supervised Seismic Facies Identification Using High-Confidence Representations. (arXiv:2210.04776v1 [cs.CV])
21. Using Whole Slide Image Representations from Self-Supervised Contrastive Learning for Melanoma Concordance Regression. (arXiv:2210.04803v1 [cs.CV])
22. Rhombic Grids Reduce the Number of Voxels in Fast Pulse-Echo Ultrasound Imaging. (arXiv:2210.04818v1 [physics.med-ph])
23. PoGaIN: Poisson-Gaussian Image Noise Modeling from Paired Samples. (arXiv:2210.04866v1 [cs.CV])
24. A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v6 [cs.LG] UPDATED)
25. A GAN-Based Input-Size Flexibility Model for Single Image Dehazing. (arXiv:2102.09796v2 [eess.IV] UPDATED)
26. Machine Learning based Efficient QT-MTT Partitioning Scheme for VVC Intra Encoders. (arXiv:2103.05319v3 [eess.IV] UPDATED)
27. HyperPCA: a Powerful Tool to Extract Elemental Maps from Noisy Data Obtained in LIBS Mapping of Materials. (arXiv:2111.15187v3 [physics.app-ph] UPDATED)
28. Learn2Reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning. (arXiv:2112.04489v3 [eess.IV] UPDATED)
29. A multi-reconstruction study of breast density estimation using Deep Learning. (arXiv:2202.08238v3 [eess.IV] UPDATED)
30. Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v4 [eess.IV] UPDATED)
31. Exemplar Learning for Medical Image Segmentation. (arXiv:2204.01713v2 [eess.IV] UPDATED)
32. Highly accurate quantum optimization algorithm for CT image reconstructions based on sinogram patterns. (arXiv:2207.02448v2 [quant-ph] UPDATED)
33. Direct Optimisation of $\boldsymbol\lambda$ for **HDR** Content Adaptive Transcoding in AV1. (arXiv:2208.11150v2 [eess.IV] UPDATED)
34. Neuromorphic Visual Scene Understanding with Resonator Networks. (arXiv:2208.12880v2 [cs.CV] UPDATED)
35. Generative Modelling of the Ageing Heart with Cross-Sectional Imaging and Clinical Data. (arXiv:2208.13146v2 [eess.IV] UPDATED)
36. Privacy-Preserving Deep Learning Model for Covid-19 Disease Detection. (arXiv:2209.04445v2 [eess.IV] UPDATED)
37. Label Refinement Network from Synthetic Error Augmentation for Medical Image Segmentation. (arXiv:2209.06353v2 [eess.IV] UPDATED)
## cs.LG
---
**243** new papers in cs.LG:-) 
1. T2CI-GAN: Text to Compressed Image generation using Generative Adversarial Network. (arXiv:2210.03734v1 [cs.CV])
2. Trustworthy clinical AI solutions: a unified review of uncertainty quantification in deep learning models for medical image analysis. (arXiv:2210.03736v1 [eess.IV])
3. Single Image Super-Resolution Based on Capsule Neural Networks. (arXiv:2210.03743v1 [eess.IV])
4. ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction. (arXiv:2210.03745v1 [q-bio.QM])
5. A deep learning approach to solve forward differential problems on graphs. (arXiv:2210.03746v1 [cs.LG])
6. Trustworthiness of Laser-Induced Breakdown Spectroscopy Predictions via Simulation-based Synthetic Data Augmentation and Multitask Learning. (arXiv:2210.03762v1 [physics.app-ph])
7. FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings. (arXiv:2210.03766v1 [cs.CL])
8. In What Ways Are Deep Neural Networks Invariant and How Should We Measure This?. (arXiv:2210.03773v1 [cs.LG])
9. LOCL: Learning Object-Attribute Composition using Localization. (arXiv:2210.03780v1 [cs.CV])
10. Evaluating the Performance of StyleGAN2-ADA on Medical Images. (arXiv:2210.03786v1 [cs.CV])
11. Supervised and Unsupervised Learning of Audio Representations for Music Understanding. (arXiv:2210.03799v1 [cs.SD])
12. Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative. (arXiv:2210.03801v1 [cs.LG])
13. Conservative Bayesian Model-Based Value Expansion for Offline Policy Optimization. (arXiv:2210.03802v1 [cs.LG])
14. The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks. (arXiv:2210.03820v1 [cs.LG])
15. In-Context Policy Iteration. (arXiv:2210.03821v1 [cs.LG])
16. Is margin all you need? An extensive empirical study of active learning on tabular data. (arXiv:2210.03822v1 [cs.LG])
17. An Analysis of the Effects of Decoding Algorithms on Fairness in Open-Ended Language Generation. (arXiv:2210.03826v1 [cs.CL])
18. Sampling-Based Decomposition Algorithms for Arbitrary Tensor Networks. (arXiv:2210.03828v1 [math.NA])
19. Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v1 [cs.LG])
20. Differentially Private Deep Learning with ModelMix. (arXiv:2210.03843v1 [cs.LG])
21. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models. (arXiv:2210.03858v1 [cs.LG])
22. Spectrally-Corrected and Regularized Linear Discriminant Analysis for Spiked Covariance Model. (arXiv:2210.03859v1 [stat.ML])
23. TAME: Task Agnostic Continual Learning using Multiple Experts. (arXiv:2210.03869v1 [cs.LG])
24. Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts. (arXiv:2210.03885v1 [cs.LG])
25. GRANITE: A Graph Neural Network Model for Basic Block Throughput Estimation. (arXiv:2210.03894v1 [cs.LG])
26. ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints. (arXiv:2210.03895v1 [cs.CV])
27. Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU. (arXiv:2210.03900v1 [cs.AR])
28. Learning the Network of Graphs for Graph Neural Networks. (arXiv:2210.03907v1 [cs.LG])
29. Signal Detection in MIMO Systems with Hardware Imperfections: Message Passing on Neural Networks. (arXiv:2210.03911v1 [eess.SP])
30. Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding. (arXiv:2210.03915v1 [cs.CL])
31. Low Error-Rate Approximate Multiplier Design for DNNs with Hardware-Driven Co-Optimization. (arXiv:2210.03916v1 [cs.AR])
32. CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Image Manipulation. (arXiv:2210.03919v1 [cs.CV])
33. Accurate Small Models using Adaptive Sampling. (arXiv:2210.03921v1 [cs.LG])
34. Sparse Teachers Can Be Dense with Knowledge. (arXiv:2210.03923v1 [cs.CL])
35. APE: Aligning Pretrained Encoders to Quickly Learn Aligned Multimodal Representations. (arXiv:2210.03927v1 [cs.LG])
36. EgoTaskQA: Understanding Human Tasks in Egocentric Videos. (arXiv:2210.03929v1 [cs.CV])
37. Hierarchical Graph Transformer with Adaptive Node Sampling. (arXiv:2210.03930v1 [cs.LG])
38. Understanding HTML with Large Language Models. (arXiv:2210.03945v1 [cs.LG])
39. Dynamic Tensor Product Regression. (arXiv:2210.03961v1 [cs.DS])
40. Asymptotically Unbiased Instance-wise Regularized Partial AUC Optimization: Theory and Algorithm. (arXiv:2210.03967v1 [cs.LG])
41. A Survey on Extreme Multi-label Learning. (arXiv:2210.03968v1 [cs.LG])
42. Kernel-based Substructure Exploration for Next POI Recommendation. (arXiv:2210.03969v1 [cs.LG])
43. KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification. (arXiv:2210.03970v1 [cs.CL])
44. An Ordinal Latent Variable Model of Conflict Intensity. (arXiv:2210.03971v1 [cs.LG])
45. Ball-and-socket joint pose estimation using magnetic field. (arXiv:2210.03984v1 [cs.RO])
46. Weisfeiler--Lehman goes Dynamic: An Analysis of the Expressive Power of Graph Neural Networks for Attributed and Dynamic Graphs. (arXiv:2210.03990v1 [cs.LG])
47. Don't Waste Data: Transfer Learning to Leverage All Data for Machine-Learnt Climate Model Emulation. (arXiv:2210.04001v1 [cs.LG])
48. Dynamically meeting performance objectives for multiple services on a service mesh. (arXiv:2210.04002v1 [cs.LG])
49. Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model. (arXiv:2210.04017v1 [cs.LG])
50. STaSy: Score-based Tabular data Synthesis. (arXiv:2210.04018v1 [cs.LG])
51. Multi-Task Dynamical Systems. (arXiv:2210.04023v1 [cs.LG])
52. Demand Layering for Real-Time DNN Inference with Minimized Memory Usage. (arXiv:2210.04024v1 [cs.LG])
53. Almost-lossless compression of a low-rank random tensor. (arXiv:2210.04041v1 [cs.IT])
54. FedDef: Robust Federated Learning-based Network Intrusion Detection Systems Against Gradient Leakage. (arXiv:2210.04052v1 [cs.CR])
55. Robustness of Unsupervised Representation Learning without Labels. (arXiv:2210.04076v1 [cs.LG])
56. SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success. (arXiv:2210.04081v1 [cs.LG])
57. Unified Probabilistic Neural Architecture and Weight Ensembling Improves Model Robustness. (arXiv:2210.04083v1 [cs.LG])
58. Symmetry Subgroup Defense Against Adversarial Attacks. (arXiv:2210.04087v1 [cs.LG])
59. Collaborative Domain Blocking: Using federated NLP To Detect Malicious Domains. (arXiv:2210.04088v1 [cs.CR])
60. Advancing Model Pruning via Bi-level Optimization. (arXiv:2210.04092v1 [cs.LG])
61. How do you go where? Improving next location prediction by learning travel mode information using transformers. (arXiv:2210.04095v1 [cs.LG])
62. PropertyDAG: Multi-objective Bayesian optimization of partially ordered, mixed-variable properties for biological sequence design. (arXiv:2210.04096v1 [cs.LG])
63. The effect of variable labels on deep learning models trained to predict breast density. (arXiv:2210.04106v1 [cs.CV])
64. Privacy-Preserving Gradient Boosting Decision Trees. (arXiv:1911.04209v5 [cs.LG] UPDATED)
65. A Finite Time Analysis of Two Time-Scale Actor Critic Methods. (arXiv:2005.01350v3 [cs.LG] UPDATED)
66. CoinPress: Practical Private Mean and Covariance Estimation. (arXiv:2006.06618v2 [stat.ML] UPDATED)
67. A Functional Perspective on Learning Symmetric Functions with Neural Networks. (arXiv:2008.06952v4 [cs.LG] UPDATED)
68. A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v6 [cs.LG] UPDATED)
69. Adaptive KL-UCB based Bandit Algorithms for Markovian and i.i.d. Settings. (arXiv:2009.06606v4 [cs.LG] UPDATED)
70. DIFER: Differentiable Automated Feature Engineering. (arXiv:2010.08784v3 [cs.LG] UPDATED)
71. Maximum sampled conditional likelihood for informative subsampling. (arXiv:2011.05988v4 [math.ST] UPDATED)
72. A Unified Mixture-View Framework for Unsupervised Representation Learning. (arXiv:2011.13356v2 [cs.CV] UPDATED)
73. Characterization of Excess Risk for Locally Strongly Convex Population Risk. (arXiv:2012.02456v4 [cs.LG] UPDATED)
74. Towards Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework. (arXiv:2012.13567v7 [cs.LG] UPDATED)
75. Robust normalizing flows using Bernstein-type polynomials. (arXiv:2102.03509v4 [cs.LG] UPDATED)
76. Spherical Message Passing for 3D Molecular Graphs. (arXiv:2102.05013v4 [cs.LG] UPDATED)
77. Is Simple Uniform Sampling Effective for Center-Based Clustering with Outliers: When and Why?. (arXiv:2103.00558v3 [cs.LG] UPDATED)
78. A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions. (arXiv:2103.12711v4 [stat.ML] UPDATED)
79. Causal Inference Under Unmeasured Confounding With Negative Controls: A Minimax Learning Approach. (arXiv:2103.14029v4 [stat.ML] UPDATED)
80. Preservation of the Global Knowledge by Not-True Distillation in Federated Learning. (arXiv:2106.03097v4 [cs.LG] UPDATED)
81. Deep Learning Statistical Arbitrage. (arXiv:2106.04028v2 [cs.LG] UPDATED)
82. Causal Inference with Corrupted Data: Measurement Error, Missing Values, Discretization, and Differential Privacy. (arXiv:2107.02780v4 [econ.EM] UPDATED)
83. Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness. (arXiv:2107.04642v8 [cs.CY] UPDATED)
84. A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v2 [cs.LG] UPDATED)
85. Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability. (arXiv:2108.01335v2 [cs.CV] UPDATED)
86. When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?. (arXiv:2109.09444v6 [cs.LG] UPDATED)
87. The Fragility of Optimized Bandit Algorithms. (arXiv:2109.13595v4 [cs.LG] UPDATED)
88. Efficient Identification of Butterfly Sparse Matrix Factorizations. (arXiv:2110.01230v4 [cs.LG] UPDATED)
89. Global Convergence and Stability of Stochastic Gradient Descent. (arXiv:2110.01663v3 [cs.LG] UPDATED)
90. Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning. (arXiv:2110.05286v3 [cs.LG] UPDATED)
91. Empirical analysis of representation learning and exploration in neural kernel bandits. (arXiv:2111.03543v2 [cs.LG] UPDATED)
92. Distributed stochastic proximal algorithm with random reshuffling for non-smooth finite-sum optimization. (arXiv:2111.03820v2 [math.OC] UPDATED)
93. Towards Domain-Independent and Real-Time Gesture Recognition Using mmWave Signal. (arXiv:2111.06195v3 [cs.CV] UPDATED)
94. Distribution-Free Model for Community Detection. (arXiv:2111.07495v2 [cs.SI] UPDATED)
95. ELBD: Efficient score algorithm for feature selection on latent variables of VAE. (arXiv:2111.08493v3 [stat.ML] UPDATED)
96. ESCADA: Efficient Safety and Context Aware Dose Allocation for Precision Medicine. (arXiv:2111.13415v3 [cs.LG] UPDATED)
97. Confounder Identification-free Causal Visual Feature Learning. (arXiv:2111.13420v3 [cs.LG] UPDATED)
98. HyperPCA: a Powerful Tool to Extract Elemental Maps from Noisy Data Obtained in LIBS Mapping of Materials. (arXiv:2111.15187v3 [physics.app-ph] UPDATED)
99. A Novel Sequential Coreset Method for Gradient Descent Algorithms. (arXiv:2112.02504v3 [cs.LG] UPDATED)
100. Mixed Membership Distribution-Free Model. (arXiv:2112.04389v3 [cs.SI] UPDATED)
101. Self-attention Does Not Need $O(n^2)$ Memory. (arXiv:2112.05682v3 [cs.LG] UPDATED)
102. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v5 [cs.CV] UPDATED)
103. How Much Space Has Been Explored? Measuring the Chemical Space Covered by Databases and Machine-Generated Molecules. (arXiv:2112.12542v4 [cs.CE] UPDATED)
104. EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate. (arXiv:2112.14397v2 [cs.LG] UPDATED)
105. AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees. (arXiv:2201.07984v3 [cs.AI] UPDATED)
106. Communication-Efficient Stochastic Zeroth-Order Optimization for Federated Learning. (arXiv:2201.09531v2 [cs.LG] UPDATED)
107. Mask-based Latent Reconstruction for Reinforcement Learning. (arXiv:2201.12096v3 [cs.LG] UPDATED)
108. BEER: Fast $O(1/T)$ Rate for Decentralized Nonconvex Optimization with Communication Compression. (arXiv:2201.13320v2 [cs.LG] UPDATED)
109. Can Adversarial Training Be Manipulated By Non-Robust Features?. (arXiv:2201.13329v4 [cs.LG] UPDATED)
110. Accelerated Quality-Diversity through Massive Parallelism. (arXiv:2202.01258v3 [cs.NE] UPDATED)
111. Theoretical Exploration of Solutions of Feedforward ReLU Networks. (arXiv:2202.01919v8 [cs.LG] UPDATED)
112. Unsupervised Behaviour Analysis of News Consumption in Turkish Media. (arXiv:2202.02056v2 [cs.SI] UPDATED)
113. Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v2 [cs.CL] UPDATED)
114. Measuring and Reducing Model Update Regression in Structured Prediction for NLP. (arXiv:2202.02976v2 [cs.CL] UPDATED)
115. Graph Self-supervised Learning with Accurate Discrepancy Learning. (arXiv:2202.02989v5 [cs.LG] UPDATED)
116. A multi-reconstruction study of breast density estimation using Deep Learning. (arXiv:2202.08238v3 [eess.IV] UPDATED)
117. A Behavior Regularized Implicit Policy for Offline Reinforcement Learning. (arXiv:2202.09673v2 [stat.ML] UPDATED)
118. From Optimization Dynamics to Generalization Bounds via {\L}ojasiewicz Gradient Inequality. (arXiv:2202.10670v2 [stat.ML] UPDATED)
119. Single-Leg Revenue Management with Advice. (arXiv:2202.10939v2 [cs.GT] UPDATED)
120. Restless Multi-Armed Bandits under Exogenous Global Markov Process. (arXiv:2202.13665v2 [cs.LG] UPDATED)
121. Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space. (arXiv:2203.00614v4 [cs.LG] UPDATED)
122. Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models. (arXiv:2203.01104v4 [cs.CL] UPDATED)
123. Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering. (arXiv:2203.01294v2 [cs.LG] UPDATED)
124. Unfolding-Aided Bootstrapped Phase Retrieval in Optical Imaging. (arXiv:2203.01695v2 [physics.optics] UPDATED)
125. Learning to Bound: A Generative Cram\'er-Rao Bound. (arXiv:2203.03695v2 [cs.LG] UPDATED)
126. Varying Coefficient Linear Discriminant Analysis for Dynamic Data. (arXiv:2203.06371v3 [stat.ME] UPDATED)
127. Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed. (arXiv:2203.09179v2 [math.ST] UPDATED)
128. Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v4 [eess.IV] UPDATED)
129. Inspection-L: Self-Supervised GNN Node Embeddings for Money Laundering Detection in Bitcoin. (arXiv:2203.10465v4 [cs.CR] UPDATED)
130. Policy Gradients using Variational Quantum Circuits. (arXiv:2203.10591v2 [quant-ph] UPDATED)
131. Minimax Regret for Cascading Bandits. (arXiv:2203.12577v3 [cs.LG] UPDATED)
132. PerfectDou: Dominating DouDizhu with Perfect Information Distillation. (arXiv:2203.16406v5 [cs.AI] UPDATED)
133. Exemplar Learning for Medical Image Segmentation. (arXiv:2204.01713v2 [eess.IV] UPDATED)
134. Can language models learn from explanations in context?. (arXiv:2204.02329v4 [cs.CL] UPDATED)
135. Accelerating Backward Aggregation in GCN Training with Execution Path Preparing on GPUs. (arXiv:2204.02662v2 [cs.LG] UPDATED)
136. Learning to Induce Causal Structure. (arXiv:2204.04875v2 [stat.ML] UPDATED)
137. Dynamic Dialogue Policy for Continual Reinforcement Learning. (arXiv:2204.05928v2 [cs.CL] UPDATED)
138. Efficient Architecture Search for Diverse Tasks. (arXiv:2204.07554v3 [cs.LG] UPDATED)
139. BiOcularGAN: Bimodal Synthesis and Annotation of Ocular Images. (arXiv:2205.01536v2 [cs.CV] UPDATED)
140. ConfLab: A Data Collection Concept, Dataset, and Benchmark for Machine Analysis of Free-Standing Social Interactions in the Wild. (arXiv:2205.05177v3 [cs.MM] UPDATED)
141. Making Pretrained Language Models Good Long-tailed Learners. (arXiv:2205.05461v2 [cs.CL] UPDATED)
142. NN-EUCLID: deep-learning hyperelasticity without stress data. (arXiv:2205.06664v2 [cs.LG] UPDATED)
143. RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL. (arXiv:2205.06983v2 [cs.CL] UPDATED)
144. Multi-scale Attention Flow for Probabilistic Time Series Forecasting. (arXiv:2205.07493v2 [cs.LG] UPDATED)
145. Accelerated Training of Physics-Informed Neural Networks (PINNs) using Meshless Discretizations. (arXiv:2205.09332v4 [cs.LG] UPDATED)
146. Closing the gap: Exact maximum likelihood training of generative autoencoders using invertible layers. (arXiv:2205.09546v3 [stat.ML] UPDATED)
147. Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy. (arXiv:2205.10683v3 [cs.LG] UPDATED)
148. Policy-based Primal-Dual Methods for Convex Constrained Markov Decision Processes. (arXiv:2205.10715v2 [cs.LG] UPDATED)
149. Test-Time Robust Personalization for Federated Learning. (arXiv:2205.10920v2 [cs.LG] UPDATED)
150. GBA: A Tuning-free Approach to Switch between Synchronous and Asynchronous Training for Recommendation Model. (arXiv:2205.11048v2 [cs.LG] UPDATED)
151. Generic bounds on the approximation error for physics-informed (and) operator learning. (arXiv:2205.11393v2 [cs.LG] UPDATED)
152. Not too little, not too much: a theoretical analysis of graph (over)smoothing. (arXiv:2205.12156v2 [stat.ML] UPDATED)
153. A Universal Error Measure for Input Predictions Applied to Online Graph Problems. (arXiv:2205.12850v2 [cs.DS] UPDATED)
154. Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse Observations. (arXiv:2205.13479v2 [cs.LG] UPDATED)
155. VectorAdam for Rotation Equivariant Geometry Optimization. (arXiv:2205.13599v2 [cs.LG] UPDATED)
156. Tensor Program Optimization with Probabilistic Programs. (arXiv:2205.13603v2 [cs.LG] UPDATED)
157. SeedGNN: Graph Neural Networks for Supervised Seeded Graph Matching. (arXiv:2205.13679v2 [cs.LG] UPDATED)
158. Momentum Stiefel Optimizer, with Applications to Suitably-Orthogonal Attention, and Optimal Transport. (arXiv:2205.14173v2 [cs.LG] UPDATED)
159. On the Symmetries of Deep Learning Models and their Internal Representations. (arXiv:2205.14258v3 [cs.LG] UPDATED)
160. Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning. (arXiv:2205.14410v2 [cs.LG] UPDATED)
161. Happenstance: Utilizing Semantic Search to Track Russian State Media Narratives about the Russo-Ukrainian War On Reddit. (arXiv:2205.14484v2 [cs.SI] UPDATED)
162. OOD Link Prediction Generalization Capabilities of Message-Passing GNNs in Larger Test Graphs. (arXiv:2205.15117v5 [cs.LG] UPDATED)
163. Self-Supervised Visual Representation Learning with Semantic Grouping. (arXiv:2205.15288v2 [cs.CV] UPDATED)
164. Non-Markovian Reward Modelling from Trajectory Labels via Interpretable Multiple Instance Learning. (arXiv:2205.15367v2 [cs.LG] UPDATED)
165. A Reduction to Binary Approach for Debiasing Multiclass Datasets. (arXiv:2205.15860v2 [cs.LG] UPDATED)
166. In the Eye of the Beholder: Robust Prediction with Causal User Modeling. (arXiv:2206.00416v2 [cs.LG] UPDATED)
167. Is $L^2$ Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Network?. (arXiv:2206.02016v2 [cs.LG] UPDATED)
168. Diffusion-GAN: Training GANs with Diffusion. (arXiv:2206.02262v3 [cs.LG] UPDATED)
169. Sampling without Replacement Leads to Faster Rates in Finite-Sum Minimax Optimization. (arXiv:2206.02953v2 [math.OC] UPDATED)
170. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v4 [cs.AI] UPDATED)
171. Mildly Conservative Q-Learning for Offline Reinforcement Learning. (arXiv:2206.04745v2 [cs.LG] UPDATED)
172. Universally Expressive Communication in Multi-Agent Reinforcement Learning. (arXiv:2206.06758v2 [cs.MA] UPDATED)
173. Robust and Sparse Estimation of Linear Regression Coefficients with Heavy-tailed Noises and Covariates. (arXiv:2206.07594v3 [stat.ML] UPDATED)
174. Large-Scale Differentiable Causal Discovery of Factor Graphs. (arXiv:2206.07824v2 [stat.ML] UPDATED)
175. Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization. (arXiv:2206.07837v2 [cs.LG] UPDATED)
176. Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination. (arXiv:2206.07989v2 [cs.LG] UPDATED)
177. Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. (arXiv:2206.08155v2 [cs.CV] UPDATED)
178. Graph Neural Networks as Gradient Flows: understanding graph convolutions via energy. (arXiv:2206.10991v3 [cs.LG] UPDATED)
179. Repository-Level Prompt Generation for Large Language Models of Code. (arXiv:2206.12839v2 [cs.LG] UPDATED)
180. GERNERMED++: Transfer Learning in German Medical NLP. (arXiv:2206.14504v2 [cs.CL] UPDATED)
181. Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision. (arXiv:2206.14719v2 [cs.CL] UPDATED)
182. 3D-Aware Video Generation. (arXiv:2206.14797v2 [cs.CV] UPDATED)
183. Verification and search algorithms for causal DAGs. (arXiv:2206.15374v2 [cs.LG] UPDATED)
184. Forecasting Future World Events with Neural Networks. (arXiv:2206.15474v2 [cs.LG] UPDATED)
185. Integral Probability Metrics PAC-Bayes Bounds. (arXiv:2207.00614v5 [stat.ML] UPDATED)
186. Tree ensemble kernels for Bayesian optimization with known constraints over mixed-feature spaces. (arXiv:2207.00879v2 [stat.ML] UPDATED)
187. Deep Contrastive One-Class Time Series Anomaly Detection. (arXiv:2207.01472v2 [cs.LG] UPDATED)
188. Factorizing Knowledge in Neural Networks. (arXiv:2207.03337v2 [cs.CV] UPDATED)
189. Neural Topological Ordering for Computation Graphs. (arXiv:2207.05899v2 [cs.LG] UPDATED)
190. Temporal Forward-Backward Consistency, Not Residual Error, Measures the Prediction Accuracy of Extended Dynamic Mode Decomposition. (arXiv:2207.07719v2 [eess.SY] UPDATED)
191. Is Integer Arithmetic Enough for Deep Learning Training?. (arXiv:2207.08822v2 [cs.LG] UPDATED)
192. Federated Learning on Adaptively Weighted Nodes by Bilevel Optimization. (arXiv:2207.10751v2 [cs.LG] UPDATED)
193. Leveraging Explanations in Interactive Machine Learning: An Overview. (arXiv:2207.14526v2 [cs.LG] UPDATED)
194. Best-of-Both-Worlds Algorithms for Partial Monitoring. (arXiv:2207.14550v3 [cs.LG] UPDATED)
195. Simplex Clustering via sBeta with Applications to Online Adjustment of Black-Box Predictions. (arXiv:2208.00287v3 [cs.CV] UPDATED)
196. Predicting Future Mosquito Larval Habitats Using Time Series Climate Forecasting and Deep Learning. (arXiv:2208.01436v2 [cs.LG] UPDATED)
197. Constructing Balance from Imbalance for Long-tailed Image Recognition. (arXiv:2208.02567v2 [cs.CV] UPDATED)
198. Trustworthy Visual Analytics in Clinical Gait Analysis: A Case Study for Patients with Cerebral Palsy. (arXiv:2208.05232v2 [cs.HC] UPDATED)
199. Distributionally Robust Model-Based Offline Reinforcement Learning with Near-Optimal Sample Complexity. (arXiv:2208.05767v2 [cs.LG] UPDATED)
200. On a Mechanism Framework of Autoencoders. (arXiv:2208.06995v2 [cs.LG] UPDATED)
201. Universal Solutions of Feedforward ReLU Networks for Interpolations. (arXiv:2208.07498v3 [cs.LG] UPDATED)
202. Multi-Point Integrated Sensing and Communication: Fusion Model and Functionality Selection. (arXiv:2208.07592v2 [cs.IT] UPDATED)
203. Targeted Advertising on Social Networks Using Online Variational Tensor Regression. (arXiv:2208.10627v3 [cs.SI] UPDATED)
204. Hyperparameter Optimization for Unsupervised Outlier Detection. (arXiv:2208.11727v2 [cs.LG] UPDATED)
205. Learning to Prune Instances of Steiner Tree Problem in Graphs. (arXiv:2208.11985v2 [cs.DS] UPDATED)
206. Generative Modelling of the Ageing Heart with Cross-Sectional Imaging and Clinical Data. (arXiv:2208.13146v2 [eess.IV] UPDATED)
207. Graph Neural Networks for Low-Energy Event Classification & Reconstruction in IceCube. (arXiv:2209.03042v2 [hep-ex] UPDATED)
208. Kernel-Segregated Transpose Convolution Operation. (arXiv:2209.03704v2 [cs.LG] UPDATED)
209. Deep Baseline Network for Time Series Modeling and Anomaly Detection. (arXiv:2209.04561v2 [cs.LG] UPDATED)
210. Modeling Dependent Structure for Utterances in ASR Evaluation. (arXiv:2209.05281v2 [eess.AS] UPDATED)
211. Pre-training Transformers on Indian Legal Text. (arXiv:2209.06049v3 [cs.CL] UPDATED)
212. GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks. (arXiv:2209.07924v2 [cs.LG] UPDATED)
213. Honor of Kings Arena: an Environment for Generalization in Competitive Reinforcement Learning. (arXiv:2209.08483v2 [cs.LG] UPDATED)
214. UMIX: Improving Importance Weighting for Subpopulation Shift via Uncertainty-Aware Mixup. (arXiv:2209.08928v2 [cs.LG] UPDATED)
215. Whodunit? Learning to Contrast for Authorship Attribution. (arXiv:2209.11887v2 [cs.CL] UPDATED)
216. Concordance based Survival Cobra with regression type weak learners. (arXiv:2209.11919v2 [stat.ML] UPDATED)
217. Interventional Causal Representation Learning. (arXiv:2209.11924v2 [stat.ML] UPDATED)
218. Optimization of Annealed Importance Sampling Hyperparameters. (arXiv:2209.13226v3 [stat.ML] UPDATED)
219. TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method. (arXiv:2209.13791v2 [cs.LG] UPDATED)
220. LL-GNN: Low Latency Graph Neural Networks on FPGAs for Particle Detectors. (arXiv:2209.14065v2 [cs.AR] UPDATED)
221. Optimal Stopping with Gaussian Processes. (arXiv:2209.14738v2 [stat.ML] UPDATED)
222. Towards Lightweight Black-Box Attacks against Deep Neural Networks. (arXiv:2209.14826v2 [cs.LG] UPDATED)
223. How to tackle an emerging topic? Combining strong and weak labels for Covid news NER. (arXiv:2209.15108v2 [cs.CL] UPDATED)
224. MaskTune: Mitigating Spurious Correlations by Forcing to Explore. (arXiv:2210.00055v2 [cs.LG] UPDATED)
225. Neural Graphical Models. (arXiv:2210.00453v2 [cs.LG] UPDATED)
226. OCD: Learning to Overfit with Conditional Diffusion Models. (arXiv:2210.00471v2 [cs.LG] UPDATED)
227. Heterogeneous Graph Neural Network for Privacy-Preserving Recommendation. (arXiv:2210.00538v2 [cs.LG] UPDATED)
228. DDoS: A Graph Neural Network based Drug Synergy Prediction Algorithm. (arXiv:2210.00802v2 [q-bio.QM] UPDATED)
229. Automatic Generation of Product Concepts from Positive Examples, with an Application to Music Streaming. (arXiv:2210.01515v2 [cs.LG] UPDATED)
230. ProtoBandit: Efficient Prototype Selection via Multi-Armed Bandits. (arXiv:2210.01860v2 [cs.LG] UPDATED)
231. Grounding Language with Visual Affordances over Unstructured Data. (arXiv:2210.01911v2 [cs.RO] UPDATED)
232. GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks. (arXiv:2210.02040v2 [cs.LG] UPDATED)
233. Are All Losses Created Equal: A Neural Collapse Perspective. (arXiv:2210.02192v2 [cs.LG] UPDATED)
234. Leveraging Instance Features for Label Aggregation in Programmatic Weak Supervision. (arXiv:2210.02724v2 [cs.LG] UPDATED)
235. Why Should I Choose You? AutoXAI: A Framework for Selecting and Tuning eXplainable AI Solutions. (arXiv:2210.02795v2 [cs.LG] UPDATED)
236. Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2210.03022v2 [cs.AI] UPDATED)
237. Edge-Varying Fourier Graph Networks for Multivariate Time Series Forecasting. (arXiv:2210.03093v2 [cs.LG] UPDATED)
238. Towards Out-of-Distribution Adversarial Robustness. (arXiv:2210.03150v2 [cs.LG] UPDATED)
239. Comparison of Missing Data Imputation Methods using the Framingham Heart study dataset. (arXiv:2210.03154v2 [cs.LG] UPDATED)
240. Synthetic Dataset Generation for Privacy-Preserving Machine Learning. (arXiv:2210.03205v2 [cs.CR] UPDATED)
241. Event Extraction: A Survey. (arXiv:2210.03419v2 [cs.CL] UPDATED)
242. A Unified Hard-Constraint Framework for Solving Geometrically Complex PDEs. (arXiv:2210.03526v2 [cs.LG] UPDATED)
243. Koopman Neural Forecaster for Time Series with Temporal Distribution Shifts. (arXiv:2210.03675v2 [cs.LG] UPDATED)
## cs.AI
---
**121** new papers in cs.AI:-) 
1. T2CI-GAN: Text to Compressed Image generation using Generative Adversarial Network. (arXiv:2210.03734v1 [cs.CV])
2. "Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction. (arXiv:2210.03735v1 [cs.HC])
3. Trustworthy clinical AI solutions: a unified review of uncertainty quantification in deep learning models for medical image analysis. (arXiv:2210.03736v1 [eess.IV])
4. Exploring Effectiveness of Explanations for Appropriate Trust: Lessons from Cognitive Psychology. (arXiv:2210.03737v1 [cs.HC])
5. Dual-Stage Deeply Supervised Attention-based Convolutional Neural Networks for Mandibular Canal Segmentation in CBCT Scans. (arXiv:2210.03739v1 [eess.IV])
6. Single Image Super-Resolution Based on Capsule Neural Networks. (arXiv:2210.03743v1 [eess.IV])
7. ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction. (arXiv:2210.03745v1 [q-bio.QM])
8. A deep learning approach to solve forward differential problems on graphs. (arXiv:2210.03746v1 [cs.LG])
9. Trustworthiness of Laser-Induced Breakdown Spectroscopy Predictions via Simulation-based Synthetic Data Augmentation and Multitask Learning. (arXiv:2210.03762v1 [physics.app-ph])
10. Visualize Before You Write: Imagination-Guided Open-Ended Text Generation. (arXiv:2210.03765v1 [cs.CL])
11. xDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph. (arXiv:2210.03768v1 [cs.DB])
12. Supervised and Unsupervised Learning of Audio Representations for Music Understanding. (arXiv:2210.03799v1 [cs.SD])
13. Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative. (arXiv:2210.03801v1 [cs.LG])
14. Is margin all you need? An extensive empirical study of active learning on tabular data. (arXiv:2210.03822v1 [cs.LG])
15. See, Plan, Predict: Language-guided Cognitive Planning with Video Prediction. (arXiv:2210.03825v1 [cs.AI])
16. An Analysis of the Effects of Decoding Algorithms on Fairness in Open-Ended Language Generation. (arXiv:2210.03826v1 [cs.CL])
17. Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v1 [cs.LG])
18. Mutual Theory of Mind for Human-AI Communication. (arXiv:2210.03842v1 [cs.HC])
19. ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints. (arXiv:2210.03895v1 [cs.CV])
20. Learning the Network of Graphs for Graph Neural Networks. (arXiv:2210.03907v1 [cs.LG])
21. A Higher Purpose: Measuring Electricity Access Using High-Resolution Daytime Satellite Imagery. (arXiv:2210.03909v1 [cs.CV])
22. Finding and Exploring Promising Search Space for the 0-1 Multidimensional Knapsack Problem. (arXiv:2210.03918v1 [cs.AI])
23. CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Image Manipulation. (arXiv:2210.03919v1 [cs.CV])
24. EgoTaskQA: Understanding Human Tasks in Egocentric Videos. (arXiv:2210.03929v1 [cs.CV])
25. Hierarchical Graph Transformer with Adaptive Node Sampling. (arXiv:2210.03930v1 [cs.LG])
26. Cloud Native Robotic Applications with GPU Sharing on Kubernetes. (arXiv:2210.03936v1 [cs.RO])
27. Understanding HTML with Large Language Models. (arXiv:2210.03945v1 [cs.LG])
28. Kernel-based Substructure Exploration for Next POI Recommendation. (arXiv:2210.03969v1 [cs.LG])
29. KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification. (arXiv:2210.03970v1 [cs.CL])
30. Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition. (arXiv:2210.03980v1 [cs.CL])
31. Weisfeiler--Lehman goes Dynamic: An Analysis of the Expressive Power of Graph Neural Networks for Attributed and Dynamic Graphs. (arXiv:2210.03990v1 [cs.LG])
32. Relational Message Passing for Fully Inductive Knowledge Graph Completion. (arXiv:2210.03994v1 [cs.AI])
33. (Fusionformer):Exploiting the Joint Motion Synergy with Fusion Network Based On Transformer for 3D Human Pose Estimation. (arXiv:2210.04006v1 [cs.CV])
34. STaSy: Score-based Tabular data Synthesis. (arXiv:2210.04018v1 [cs.LG])
35. Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v1 [cs.CL])
36. Robustness of Unsupervised Representation Learning without Labels. (arXiv:2210.04076v1 [cs.LG])
37. Unified Probabilistic Neural Architecture and Weight Ensembling Improves Model Robustness. (arXiv:2210.04083v1 [cs.LG])
38. Training Deep Learning Algorithms on Synthetic Forest Images for Tree Detection. (arXiv:2210.04104v1 [cs.CV])
39. Knowledge representation and diagnostic inference using Bayesian networks in the medical discourse. (arXiv:1909.08549v2 [cs.AI] UPDATED)
40. Towards Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework. (arXiv:2012.13567v7 [cs.LG] UPDATED)
41. Preservation of the Global Knowledge by Not-True Distillation in Federated Learning. (arXiv:2106.03097v4 [cs.LG] UPDATED)
42. Digging Errors in NMT: Evaluating and Understanding Model Errors from Partial Hypothesis Space. (arXiv:2106.15217v2 [cs.CL] UPDATED)
43. A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v2 [cs.LG] UPDATED)
44. Human Perception of Audio Deepfakes. (arXiv:2107.09667v6 [cs.HC] UPDATED)
45. Gradient-Based Mixed Planning with Symbolic and Numeric Action Parameters. (arXiv:2110.10007v2 [cs.AI] UPDATED)
46. Cross-Domain Reasoning via Template Filling. (arXiv:2111.00539v2 [cs.CL] UPDATED)
47. Strategyproof and Proportionally Fair Facility Location. (arXiv:2111.01566v2 [cs.GT] UPDATED)
48. Empirical analysis of representation learning and exploration in neural kernel bandits. (arXiv:2111.03543v2 [cs.LG] UPDATED)
49. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v5 [cs.CV] UPDATED)
50. Adversarial Attacks against Windows PE Malware Detection: A Survey of the State-of-the-Art. (arXiv:2112.12310v2 [cs.CR] UPDATED)
51. EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate. (arXiv:2112.14397v2 [cs.LG] UPDATED)
52. Interactive Contrastive Learning for Self-supervised Entity Alignment. (arXiv:2201.06225v2 [cs.CL] UPDATED)
53. AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees. (arXiv:2201.07984v3 [cs.AI] UPDATED)
54. Communication-Efficient Stochastic Zeroth-Order Optimization for Federated Learning. (arXiv:2201.09531v2 [cs.LG] UPDATED)
55. Accelerated Quality-Diversity through Massive Parallelism. (arXiv:2202.01258v3 [cs.NE] UPDATED)
56. Theoretical Exploration of Solutions of Feedforward ReLU Networks. (arXiv:2202.01919v8 [cs.LG] UPDATED)
57. Measuring and Reducing Model Update Regression in Structured Prediction for NLP. (arXiv:2202.02976v2 [cs.CL] UPDATED)
58. Graph Self-supervised Learning with Accurate Discrepancy Learning. (arXiv:2202.02989v5 [cs.LG] UPDATED)
59. On the Evaluation Metrics for Paraphrase Generation. (arXiv:2202.08479v2 [cs.CL] UPDATED)
60. Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models. (arXiv:2203.01104v4 [cs.CL] UPDATED)
61. Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering. (arXiv:2203.01294v2 [cs.LG] UPDATED)
62. Better Approximation Guarantees for the NSGA-II by Using the Current Crowding Distance. (arXiv:2203.02693v2 [cs.NE] UPDATED)
63. Can I see an Example? Active Learning the Long Tail of Attributes and Relations. (arXiv:2203.06215v2 [cs.CV] UPDATED)
64. Automated Clinical Coding: What, Why, and Where We Are?. (arXiv:2203.11092v3 [cs.CL] UPDATED)
65. PerfectDou: Dominating DouDizhu with Perfect Information Distillation. (arXiv:2203.16406v5 [cs.AI] UPDATED)
66. Frequency and Multi-Scale Selective Kernel Attention for Speaker Verification. (arXiv:2204.01005v3 [eess.AS] UPDATED)
67. Can language models learn from explanations in context?. (arXiv:2204.02329v4 [cs.CL] UPDATED)
68. Efficient Architecture Search for Diverse Tasks. (arXiv:2204.07554v3 [cs.LG] UPDATED)
69. A Comprehensive Survey on Data-Efficient GANs in Image Generation. (arXiv:2204.08329v2 [cs.CV] UPDATED)
70. RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL. (arXiv:2205.06983v2 [cs.CL] UPDATED)
71. Unsupervised Tokenization Learning. (arXiv:2205.11443v2 [cs.CL] UPDATED)
72. Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse Observations. (arXiv:2205.13479v2 [cs.LG] UPDATED)
73. On the Symmetries of Deep Learning Models and their Internal Representations. (arXiv:2205.14258v3 [cs.LG] UPDATED)
74. Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning. (arXiv:2205.14410v2 [cs.LG] UPDATED)
75. OOD Link Prediction Generalization Capabilities of Message-Passing GNNs in Larger Test Graphs. (arXiv:2205.15117v5 [cs.LG] UPDATED)
76. Non-Markovian Reward Modelling from Trajectory Labels via Interpretable Multiple Instance Learning. (arXiv:2205.15367v2 [cs.LG] UPDATED)
77. HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System. (arXiv:2206.02628v2 [cs.IR] UPDATED)
78. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v4 [cs.AI] UPDATED)
79. Mildly Conservative Q-Learning for Offline Reinforcement Learning. (arXiv:2206.04745v2 [cs.LG] UPDATED)
80. Satellite-based high-resolution maps of cocoa for C\^ote d'Ivoire and Ghana. (arXiv:2206.06119v4 [cs.CV] UPDATED)
81. Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks. (arXiv:2206.06282v2 [cs.RO] UPDATED)
82. Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization. (arXiv:2206.07837v2 [cs.LG] UPDATED)
83. Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination. (arXiv:2206.07989v2 [cs.LG] UPDATED)
84. GERNERMED++: Transfer Learning in German Medical NLP. (arXiv:2206.14504v2 [cs.CL] UPDATED)
85. Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision. (arXiv:2206.14719v2 [cs.CL] UPDATED)
86. Tree ensemble kernels for Bayesian optimization with known constraints over mixed-feature spaces. (arXiv:2207.00879v2 [stat.ML] UPDATED)
87. Deep Contrastive One-Class Time Series Anomaly Detection. (arXiv:2207.01472v2 [cs.LG] UPDATED)
88. Factorizing Knowledge in Neural Networks. (arXiv:2207.03337v2 [cs.CV] UPDATED)
89. Diversity-aware social robots meet people: beyond context-aware embodied AI. (arXiv:2207.05372v2 [cs.RO] UPDATED)
90. Correspondences between word learning in children and captioning models. (arXiv:2207.09847v2 [cs.CL] UPDATED)
91. Simplex Clustering via sBeta with Applications to Online Adjustment of Black-Box Predictions. (arXiv:2208.00287v3 [cs.CV] UPDATED)
92. Semi-supervised Learning with Deterministic Labeling and Large Margin Projection. (arXiv:2208.08058v2 [cs.AI] UPDATED)
93. Learning to Prune Instances of Steiner Tree Problem in Graphs. (arXiv:2208.11985v2 [cs.DS] UPDATED)
94. Neuromorphic Visual Scene Understanding with Resonator Networks. (arXiv:2208.12880v2 [cs.CV] UPDATED)
95. Neuromorphic Visual Odometry with Resonator Networks. (arXiv:2209.02000v2 [cs.RO] UPDATED)
96. A Survey on Generative Diffusion Model. (arXiv:2209.02646v6 [cs.AI] UPDATED)
97. Kernel-Segregated Transpose Convolution Operation. (arXiv:2209.03704v2 [cs.LG] UPDATED)
98. SUPER-Rec: SUrrounding Position-Enhanced Representation for Recommendation. (arXiv:2209.04154v2 [cs.IR] UPDATED)
99. Modeling Dependent Structure for Utterances in ASR Evaluation. (arXiv:2209.05281v2 [eess.AS] UPDATED)
100. Pre-training Transformers on Indian Legal Text. (arXiv:2209.06049v3 [cs.CL] UPDATED)
101. Sketch of a novel approach to a neural model. (arXiv:2209.06865v2 [q-bio.NC] UPDATED)
102. GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks. (arXiv:2209.07924v2 [cs.LG] UPDATED)
103. Honor of Kings Arena: an Environment for Generalization in Competitive Reinforcement Learning. (arXiv:2209.08483v2 [cs.LG] UPDATED)
104. Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer. (arXiv:2209.08902v2 [cs.CL] UPDATED)
105. Whodunit? Learning to Contrast for Authorship Attribution. (arXiv:2209.11887v2 [cs.CL] UPDATED)
106. Concordance based Survival Cobra with regression type weak learners. (arXiv:2209.11919v2 [stat.ML] UPDATED)
107. How to tackle an emerging topic? Combining strong and weak labels for Covid news NER. (arXiv:2209.15108v2 [cs.CL] UPDATED)
108. Neural Graphical Models. (arXiv:2210.00453v2 [cs.LG] UPDATED)
109. Estimating productivity gains in digital automation. (arXiv:2210.01252v2 [cs.AI] UPDATED)
110. Automatic Generation of Product Concepts from Positive Examples, with an Application to Music Streaming. (arXiv:2210.01515v2 [cs.LG] UPDATED)
111. ProtoBandit: Efficient Prototype Selection via Multi-Armed Bandits. (arXiv:2210.01860v2 [cs.LG] UPDATED)
112. Grounding Language with Visual Affordances over Unstructured Data. (arXiv:2210.01911v2 [cs.RO] UPDATED)
113. GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks. (arXiv:2210.02040v2 [cs.LG] UPDATED)
114. Are All Losses Created Equal: A Neural Collapse Perspective. (arXiv:2210.02192v2 [cs.LG] UPDATED)
115. Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering. (arXiv:2210.02933v2 [cs.CL] UPDATED)
116. Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2210.03022v2 [cs.AI] UPDATED)
117. State-of-the-art generalisation research in NLP: a taxonomy and review. (arXiv:2210.03050v2 [cs.CL] UPDATED)
118. Edge-Varying Fourier Graph Networks for Multivariate Time Series Forecasting. (arXiv:2210.03093v2 [cs.LG] UPDATED)
119. Towards Out-of-Distribution Adversarial Robustness. (arXiv:2210.03150v2 [cs.LG] UPDATED)
120. Synthetic Dataset Generation for Privacy-Preserving Machine Learning. (arXiv:2210.03205v2 [cs.CR] UPDATED)
121. Spatio-temporal Tendency Reasoning for Human Body Pose and Shape Estimation from Videos. (arXiv:2210.03659v2 [cs.CV] UPDATED)

