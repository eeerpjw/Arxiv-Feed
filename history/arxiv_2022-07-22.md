# Your interest papers
---
## cs.CV
---
### Injecting 3D Perception of Controllable **NeRF**-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v1 [cs.CV])
- Authors : gi Kwak, Yuanming Li, Dongsik Yoon, Donghyeon Kim, David Han, Hanseok Ko
- Link : [http://arxiv.org/abs/2207.10257](http://arxiv.org/abs/2207.10257)
> ABSTRACT  :  Over the years, 2D GANs have achieved great successes in photorealistic portrait generation. However, they lack 3D understanding in the generation process, thus they suffer from multi-view inconsistency problem. To alleviate the issue, many 3D-aware GANs have been proposed and shown notable results, but 3D GANs struggle with editing semantic attributes. The controllability and interpretability of 3D GANs have not been much explored. In this work, we propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of discovering semantic attributes during training and controlling them in an unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN to obtain a high-fidelity 3D-controllable generator. Unlike existing latent-based methods allowing implicit pose control, the proposed 3D-controllable StyleGAN enables explicit pose control over portrait generation. This distillation allows direct compatibility between 3D control and many StyleGAN-based techniques (e.g., inversion and stylization), and also brings an advantage in terms of computational resources. Our codes are available at https://github.com/jgkwak95/SURF-GAN.  
### Don't Forget Me: Accurate Background Recovery for Text Removal via Modeling Local-Global Context. (arXiv:2207.10273v1 [cs.CV])
- Authors : Chongyu Liu, Lianwen Jin, Yuliang Liu, Canjie Luo, Bangdong Chen, Fengjun Guo, Kai Ding
- Link : [http://arxiv.org/abs/2207.10273](http://arxiv.org/abs/2207.10273)
> ABSTRACT  :  Text removal has attracted increasingly attention due to its various applications on privacy protection, document **restoration**, and text editing. It has shown significant progress with deep neural network. However, most of the existing methods often generate inconsistent results for complex background. To address this issue, we propose a Contextual-guided Text Removal Network, termed as CTRNet. CTRNet explores both low-level structure and high-level discriminative context feature as prior knowledge to guide the process of background **restoration**. We further propose a Local-global Content Modeling (LGCM) block with CNNs and Transformer-Encoder to capture local features and establish the long-term relationship among pixels globally. Finally, we incorporate LGCM with context guidance for feature modeling and decoding. Experiments on benchmark datasets, SCUT-EnsText and SCUT-Syn show that CTRNet significantly outperforms the existing state-of-the-art methods. Furthermore, a qualitative experiment on examination papers also demonstrates the generalization ability of our method. The codes and supplement materials are available at https://github.com/lcy0604/CTRNet.  
### A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and **Restoration**. (arXiv:2207.10309v1 [cs.CV])
- Authors : Ming Liu, Yuxiang Wei, Xiaohe Wu, Wangmeng Zuo, **Lei Zhang**
- Link : [http://arxiv.org/abs/2207.10309](http://arxiv.org/abs/2207.10309)
> ABSTRACT  :  Generative adversarial networks (GANs) have drawn enormous attention due to the simple yet effective training mechanism and superior image generation quality. With the ability to generate photo-realistic high-resolution (e.g., $1024\times1024$) images, recent GAN models have greatly narrowed the gaps between the generated images and the real ones. Therefore, many recent works show emerging interest to take advantage of pre-trained GAN models by exploiting the well-disentangled latent space and the learned GAN priors. In this paper, we briefly review recent progress on leveraging pre-trained large-scale GAN models from three aspects, i.e., 1) the training of large-scale generative adversarial networks, 2) exploring and understanding the pre-trained GAN models, and 3) leveraging these models for subsequent tasks like image **restoration** and editing. More information about relevant methods and repositories can be found at https://github.com/csmliu/pretrained-GANs.  
### Ada**NeRF**: Adaptive Sampling for **Real-time** Rendering of Neural Radiance Fields. (arXiv:2207.10312v1 [cs.CV])
- Authors : Andreas Kurz, Thomas Neff, Zhaoyang Lv, Michael Zollh, Markus Steinberger
- Link : [http://arxiv.org/abs/2207.10312](http://arxiv.org/abs/2207.10312)
> ABSTRACT  :  Novel view synthesis has recently been revolutionized by learning neural radiance fields directly from sparse observations. However, rendering images with this new paradigm is slow due to the fact that an accurate quadrature of the volume rendering equation requires a large number of samples for each ray. Previous work has mainly focused on speeding up the network evaluations that are associated with each sample point, e.g., via caching of radiance values into explicit spatial data structures, but this comes at the expense of model compactness. In this paper, we propose a novel dual-network architecture that takes an orthogonal direction by learning how to best reduce the number of required sample points. To this end, we split our network into a sampling and shading network that are jointly trained. Our training scheme employs fixed sample positions along each ray, and incrementally introduces sparsity throughout training to achieve high quality even at low sample counts. After fine-tuning with the target number of samples, the resulting compact neural representation can be rendered in real-time. Our experiments demonstrate that our approach outperforms concurrent compact neural representations in terms of quality and frame rate and performs on par with highly efficient hybrid representations. Code and supplementary material is available at https://thomasneff.github.io/adanerf.  
### Improved Generative Model for Weakly Supervised Chest Anomaly Localization via Pseudo-paired Registration with **Bilateral**ly Symmetrical Data Augmentation. (arXiv:2207.10324v1 [eess.IV])
- Authors : Su Kim, Seong Je, Tae Uk, Myung Jin
- Link : [http://arxiv.org/abs/2207.10324](http://arxiv.org/abs/2207.10324)
> ABSTRACT  :  Image translation based on a generative adversarial network (GAN-IT) is a promising method for precise localization of abnormal regions in chest X-ray images (AL-CXR). However, heterogeneous unpaired datasets undermine existing methods to extract key features and distinguish normal from abnormal cases, resulting in inaccurate and unstable AL-CXR. To address this problem, we propose an improved two-stage GAN-IT involving registration and data augmentation. For the first stage, we introduce an invertible deep-learning-based registration technique that virtually and reasonably converts unpaired data into paired data for learning registration maps. This novel approach achieves high registration performance. For the second stage, we apply data augmentation to diversify anomaly locations by swapping the left and right lung regions on the uniform registered frames, further improving the performance by alleviating imbalance in data distribution showing left and right lung lesions. Our method is intended for application to existing GAN-IT models, allowing existing architecture to benefit from key features for translation. By showing that the AL-CXR performance is uniformly improved when applying the proposed method, we believe that GAN-IT for AL-CXR can be deployed in clinical environments, even if learning data are scarce.  
### CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution. (arXiv:2207.10345v1 [cs.CV])
- Authors : Cheeun Hong, Sungyong Baik, Heewon Kim, Seungjun Nah, Kyoung Mu
- Link : [http://arxiv.org/abs/2207.10345](http://arxiv.org/abs/2207.10345)
> ABSTRACT  :  Despite breakthrough advances in image super-resolution (SR) with convolutional neural networks (CNNs), SR has yet to enjoy ubiquitous applications due to the high computational complexity of SR networks. Quantization is one of the promising approaches to solve this problem. However, existing methods fail to quantize SR models with a bit-width lower than 8 bits, suffering from severe accuracy loss due to fixed bit-width quantization applied everywhere. In this work, to achieve high average bit-reduction with less accuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ) method for SR networks that allocates optimal bits to local regions and layers adaptively based on the local contents of an input image. To this end, a trainable bit selector module is introduced to determine the proper bit-width and quantization level for each layer and a given local image patch. This module is governed by the quantization sensitivity that is estimated by using both the average magnitude of image gradient of the patch and the standard deviation of the input feature of the layer. The proposed quantization pipeline has been tested on various SR networks and evaluated on several standard benchmarks extensively. Significant reduction in computational complexity and the elevated **restoration** accuracy clearly demonstrate the effectiveness of the proposed CADyQ framework for SR. Codes are available at https://github.com/Cheeun/CADyQ.  
### Auto Machine Learning for Medical Image Analysis by Unifying the Search on Data Augmentation and Neural Architecture. (arXiv:2207.10351v1 [cs.CV])
- Authors : Jianwei Zhang, Dong Li, Lituan Wang, **Lei Zhang**
- Link : [http://arxiv.org/abs/2207.10351](http://arxiv.org/abs/2207.10351)
> ABSTRACT  :  Automated data augmentation, which aims at engineering augmentation policy automatically, recently draw a growing research interest. Many previous auto-augmentation methods utilized a Density Matching strategy by evaluating policies in terms of the test-time augmentation performance. In this paper, we theoretically and empirically demonstrated the inconsistency between the train and validation set of small-scale medical image datasets, referred to as in-domain sampling bias. Next, we demonstrated that the in-domain sampling bias might cause the inefficiency of Density Matching. To address the problem, an improved augmentation search strategy, named Augmented Density Matching, was proposed by randomly sampling policies from a prior distribution for training. Moreover, an efficient automatical machine learning(AutoML) algorithm was proposed by unifying the search on data augmentation and neural architecture. Experimental results indicated that the proposed methods outperformed state-of-the-art approaches on MedMNIST, a pioneering benchmark designed for AutoML in medical image analysis.  
### Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption. (arXiv:2207.10402v1 [cs.CV])
- Authors : Jiazhi Guan, Hang Zhou, Mingming Gong, Youjian Zhao, Errui Ding, Jingdong Wang
- Link : [http://arxiv.org/abs/2207.10402](http://arxiv.org/abs/2207.10402)
> ABSTRACT  :  Despite encouraging progress in deepfake detection, generalization to unseen forgery types remains a significant challenge due to the limited forgery clues explored during training. In contrast, we notice a common phenomenon in deepfake: fake video creation inevitably disrupts the statistical regularity in original videos. Inspired by this observation, we propose to boost the generalization of deepfake detection by distinguishing the "regularity disruption" that does not appear in real videos. Specifically, by carefully examining the spatial and temporal properties, we propose to disrupt a real video through a Pseudo-fake Generator and create a wide range of pseudo-fake videos for training. Such practice allows us to achieve deepfake detection without using fake videos and improves the generalization ability in a simple and efficient manner. To jointly capture the spatial and temporal disruptions, we propose a Spatio-Temporal **Enhancement** block to learn the regularity disruption across space and time on our self-created videos. Through comprehensive experiments, our method exhibits excellent performance on several datasets.  
### StreamYOLO: **Real-time** Object Detection for Streaming Perception. (arXiv:2207.10433v1 [cs.CV])
- Authors : Jinrong Yang, Songtao Liu, Zeming Li, Xiaoping Li, Jian Sun
- Link : [http://arxiv.org/abs/2207.10433](http://arxiv.org/abs/2207.10433)
> ABSTRACT  :  The perceptive models of autonomous driving require fast inference within a low latency for safety. While existing works ignore the inevitable environmental changes after processing, streaming perception jointly evaluates the latency and accuracy into a single metric for video online perception, guiding the previous works to search trade-offs between accuracy and speed. In this paper, we explore the performance of **real time** models on this metric and endow the models with the capacity of predicting the future, significantly improving the results for streaming perception. Specifically, we build a simple framework with two effective modules. One is a Dual Flow Perception module (DFP). It consists of dynamic flow and static flow in parallel to capture moving tendency and basic detection feature, respectively. Trend Aware Loss (TAL) is the other module which adaptively generates loss weight for each object with its moving speed. Realistically, we consider multiple velocities driving scene and further propose Velocity-awared streaming AP (VsAP) to jointly evaluate the accuracy. In this realistic setting, we design a efficient mix-velocity training strategy to guide detector perceive any velocities. Our simple method achieves the state-of-the-art performance on Argoverse-HD dataset and improves the sAP and VsAP by 4.7% and 8.2% respectively compared to the strong baseline, validating its effectiveness.  
### Online Localisation and Colored Mesh Reconstruction Architecture for 3D Visual Feedback in Robotic Exploration Missions. (arXiv:2207.10489v1 [cs.RO])
- Authors : Quentin Serdel, Christophe Grand, Julien Marzat, Julien Moras
- Link : [http://arxiv.org/abs/2207.10489](http://arxiv.org/abs/2207.10489)
> ABSTRACT  :  This paper introduces an Online Localisation and Colored Mesh Reconstruction (OLCMR) ROS perception architecture for ground exploration robots aiming to perform robust Simultaneous Localisation And Mapping (SLAM) in challenging unknown environments and provide an associated colored 3D mesh representation in **real time**. It is intended to be used by a remote human operator to easily visualise the mapped environment during or after the mission or as a development base for further researches in the field of exploration robotics. The architecture is mainly composed of carefully-selected open-source ROS implementations of a LiDAR-based SLAM algorithm alongside a colored surface reconstruction procedure using a point cloud and RGB camera images projected into the 3D space. The overall performances are evaluated on the Newer College handheld LiDAR-Vision reference dataset and on two experimental trajectories gathered on board of representative wheeled robots in respectively urban and countryside outdoor environments. Index Terms: Field Robots, Mapping, SLAM, Colored Surface Reconstruction  
### Real-Time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition. (arXiv:2207.10519v1 [cs.CV])
- Authors : Han Sun, Yu Chen
- Link : [http://arxiv.org/abs/2207.10519](http://arxiv.org/abs/2207.10519)
> ABSTRACT  :  With an increasing number of elders living alone, care-giving from a distance becomes a compelling need, particularly for safety. **Real-time** monitoring and action recognition are essential to raise an alert timely when abnormal behaviors or unusual activities occur. While wearable sensors are widely recognized as a promising solution, highly depending on user's ability and willingness makes them inefficient. In contrast, video streams collected through non-contact optical cameras provide richer information and release the burden on elders. In this paper, leveraging the Independently-Recurrent neural Network (IndRNN) we propose a novel **Real-time** Elderly Monitoring for senior Safety (REMS) based on lightweight human action recognition (HAR) technology. Using captured skeleton images, the REMS scheme is able to recognize abnormal behaviors or actions and preserve the user's privacy. To achieve high accuracy, the HAR module is trained and fine-tuned using multiple databases. An extensive experimental study verified that REMS system performs action recognition accurately and timely. REMS meets the design goals as a privacy-preserving elderly safety monitoring system and possesses the potential to be adopted in various smart monitoring systems.  
### Unsupervised **Night** Image **Enhancement**: When Layer Decomposition Meets Light-Effects Suppression. (arXiv:2207.10564v1 [cs.CV])
- Authors : Yeying Jin, **Wenhan Yang**
- Link : [http://arxiv.org/abs/2207.10564](http://arxiv.org/abs/2207.10564)
> ABSTRACT  :  **Night** images suffer not only from **low light**, but also from uneven distributions of light. Most existing **night** visibility **enhancement** methods focus mainly on enhancing **low-light** regions. This inevitably leads to over **enhancement** and saturation in bright regions, such as those regions affected by light effects (glare, floodlight, etc). To address this problem, we need to suppress the light effects in bright regions while, at the same time, boosting the intensity of **dark** regions. With this idea in mind, we introduce an unsupervised method that integrates a layer decomposition network and a light-effects suppression network. Given a single **night** image as input, our decomposition network learns to decompose shading, reflectance and light-effects layers, guided by unsupervised layer-specific prior losses. Our light-effects suppression network further suppresses the light effects and, at the same time, enhances the illumination in **dark** regions. This light-effects suppression network exploits the estimated light-effects layer as the guidance to focus on the light-effects regions. To recover the background details and reduce hallucination/artefacts, we propose structure and high-frequency consistency losses. Our quantitative and qualitative evaluations on real images show that our method outperforms state-of-the-art methods in suppressing **night** light effects and boosting the intensity of **dark** regions.  
### Generalizable Patch-Based Neural Rendering. (arXiv:2207.10662v1 [cs.CV])
- Authors : Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia
- Link : [http://arxiv.org/abs/2207.10662](http://arxiv.org/abs/2207.10662)
> ABSTRACT  :  Neural rendering has received tremendous attention since the advent of Neural Radiance Fields (**NeRF**), and has pushed the state-of-the-art on novel-view synthesis considerably. The recent focus has been on models that overfit to a single scene, and the few attempts to learn models that can synthesize novel views of unseen scenes mostly consist of combining deep convolutional features with a **NeRF**-like model. We propose a different paradigm, where no deep features and no **NeRF**-like volume rendering are needed. Our method is capable of predicting the color of a target ray in a novel scene directly, just from a collection of patches sampled from the scene. We first leverage epipolar geometry to extract patches along the epipolar lines of each reference view. Each patch is linearly projected into a 1D feature vector and a sequence of transformers process the collection. For positional encoding, we parameterize rays as in a light field representation, with the crucial difference that the coordinates are canonicalized with respect to the target ray, which makes our method independent of the reference frame and improves generalization. We show that our approach outperforms the state-of-the-art on novel view synthesis of unseen scenes even when being trained with considerably less data than prior work.  
### TinyViT: Fast Pretraining Distillation for Small Vision Transformers. (arXiv:2207.10666v1 [cs.CV])
- Authors : Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, Lu Yuan
- Link : [http://arxiv.org/abs/2207.10666](http://arxiv.org/abs/2207.10666)
> ABSTRACT  :  Vision transformer (ViT) recently has drawn great attention in computer vision due to its remarkable model capability. However, most prevailing ViT models suffer from huge number of parameters, restricting their applicability on devices with limited resources. To alleviate this issue, we propose TinyViT, a new family of tiny and efficient small vision transformers pretrained on large-scale datasets with our proposed fast distillation framework. The central idea is to transfer knowledge from large pretrained models to small ones, while enabling small models to get the dividends of massive pretraining data. More specifically, we apply distillation during pretraining for knowledge transfer. The logits of large teacher models are sparsified and stored in disk in advance to save the memory cost and computation overheads. The tiny student transformers are automatically scaled down from a large pretrained model with computation and parameter constraints. Comprehensive experiments demonstrate the efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k with only 21M parameters, being comparable to **Swin**-B pretrained on ImageNet-21k while using 4.2 times fewer parameters. Moreover, increasing image resolutions, TinyViT can reach 86.5% accuracy, being slightly better than **Swin**-L while using only 11% parameters. Last but not the least, we demonstrate a good transfer ability of TinyViT on various downstream tasks. Code and models are available at https://github.com/microsoft/Cream/tree/main/TinyViT.  
### Neural Image Representations for Multi-Image Fusion and Layer Separation. (arXiv:2108.01199v4 [cs.CV] UPDATED)
- Authors : Seonghyeon Nam
- Link : [http://arxiv.org/abs/2108.01199](http://arxiv.org/abs/2108.01199)
> ABSTRACT  :  We propose a framework for aligning and fusing multiple images into a single view using neural image representations (NIRs), also known as implicit or coordinate-based neural representations. Our framework targets burst images that exhibit camera ego motion and potential changes in the scene. We describe different strategies for alignment depending on the nature of the scene motion -- namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. With the neural image representation, our framework effectively combines multiple inputs into a single canonical view without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks. The code and results are available at https://shnnam.github.io/research/nir.  
### **NeRF**-SR: High-Quality Neural Radiance Fields using Supersampling. (arXiv:2112.01759v3 [cs.CV] UPDATED)
- Authors : Chen Wang, Xian Wu, Chen Guo, Hai Zhang, Wing Tai, Min Hu
- Link : [http://arxiv.org/abs/2112.01759](http://arxiv.org/abs/2112.01759)
> ABSTRACT  :  We present **NeRF**-SR, a solution for high-resolution (HR) novel view synthesis with mostly low-resolution (LR) inputs. Our method is built upon Neural Radiance Fields (**NeRF**) that predicts per-point density and color with a multi-layer perceptron. While producing images at arbitrary scales, **NeRF** struggles with resolutions that go beyond observed images. Our key insight is that **NeRF** benefits from 3D consistency, which means an observed pixel absorbs information from nearby views. We first exploit it by a supersampling strategy that shoots multiple rays at each image pixel, which further enforces multi-view constraint at a sub-pixel level. Then, we show that **NeRF**-SR can further boost the performance of supersampling by a refinement network that leverages the estimated depth at hand to hallucinate details from related patches on only one HR reference image. Experiment results demonstrate that **NeRF**-SR generates high-quality results for novel view synthesis at HR on both synthetic and real-world datasets without any external information.  
### **NeRF** for Outdoor Scene Relighting. (arXiv:2112.05140v2 [cs.CV] UPDATED)
- Authors : Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik, Christian Theobalt
- Link : [http://arxiv.org/abs/2112.05140](http://arxiv.org/abs/2112.05140)
> ABSTRACT  :  Photorealistic editing of outdoor scenes from photographs requires a profound understanding of the image formation process and an accurate estimation of the scene geometry, reflectance and illumination. A delicate manipulation of the lighting can then be performed while keeping the scene albedo and geometry unaltered. We present **NeRF**-OSR, i.e., the first approach for outdoor scene relighting based on neural radiance fields. In contrast to the prior art, our technique allows simultaneous editing of both scene illumination and camera viewpoint using only a collection of outdoor photos shot in uncontrolled settings. Moreover, it enables direct control over the scene illumination, as defined through a spherical harmonics model. For evaluation, we collect a new benchmark dataset of several outdoor sites photographed from multiple viewpoints and at different times. For each time, a 360 degree environment map is captured together with a colour-calibration chequerboard to allow accurate numerical evaluations on real data against ground truth. Comparisons against SoTA show that **NeRF**-OSR enables controllable lighting and viewpoint editing at higher quality and with realistic self-shadowing reproduction. Our method and the dataset are publicly available at https://4dqv.mpi-inf.mpg.de/**NeRF**-OSR/.  
### SeqFormer: Sequential Transformer for Video Instance Segmentation. (arXiv:2112.08275v2 [cs.CV] UPDATED)
- Authors : Junfeng Wu, Yi Jiang, Song Bai, Wenqing Zhang, Xiang Bai
- Link : [http://arxiv.org/abs/2112.08275](http://arxiv.org/abs/2112.08275)
> ABSTRACT  :  In this work, we present SeqFormer for video instance segmentation. SeqFormer follows the principle of vision transformer that models instance relationships among video frames. Nevertheless, we observe that a stand-alone instance query suffices for capturing a time sequence of instances in a video, but attention mechanisms shall be done with each frame independently. To achieve this, SeqFormer locates an instance in each frame and aggregates temporal information to learn a powerful representation of a video-level instance, which is used to predict the mask sequences on each frame dynamically. Instance tracking is achieved naturally without tracking branches or post-processing. On YouTube-VIS, SeqFormer achieves 47.4 AP with a ResNet-50 backbone and 49.0 AP with a ResNet-101 backbone without bells and whistles. Such achievement significantly exceeds the previous state-of-the-art performance by 4.6 and 4.4, respectively. In addition, integrated with the recently-proposed **Swin** transformer, SeqFormer achieves a much higher AP of 59.3. We hope SeqFormer could be a strong baseline that fosters future research in video instance segmentation, and in the meantime, advances this field with a more robust, accurate, neat model. The code is available at https://github.com/wjf5203/SeqFormer.  
### Style**Swin**: Transformer-based GAN for High-resolution Image Generation. (arXiv:2112.10762v2 [cs.CV] UPDATED)
- Authors : Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, Baining Guo
- Link : [http://arxiv.org/abs/2112.10762](http://arxiv.org/abs/2112.10762)
> ABSTRACT  :  Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts **Swin** transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed Style**Swin** is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., 1024x1024. The Style**Swin**, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and models will be available at https://github.com/microsoft/Style**Swin**.  
### PC-**Swin**Morph: Patch Representation for Unsupervised Medical Image Registration and Segmentation. (arXiv:2203.05684v2 [cs.CV] UPDATED)
- Authors : Lihao Liu, Zhening Huang, Pietro Li, Bibiane Sch
- Link : [http://arxiv.org/abs/2203.05684](http://arxiv.org/abs/2203.05684)
> ABSTRACT  :  Medical image registration and segmentation are critical tasks for several clinical procedures. Manual realisation of those tasks is time-consuming and the quality is highly dependent on the level of expertise of the physician. To mitigate that laborious task, automatic tools have been developed where the majority of solutions are supervised techniques. However, in medical domain, the strong assumption of having a well-representative ground truth is far from being realistic. To overcome this challenge, unsupervised techniques have been investigated. However, they are still limited in performance and they fail to produce plausible results. In this work, we propose a novel unified unsupervised framework for image registration and segmentation that we called PC-**Swin**Morph. The core of our framework is two patch-based strategies, where we demonstrate that patch representation is key for performance gain. We first introduce a patch-based contrastive strategy that enforces locality conditions and richer feature representation. Secondly, we utilise a 3D window/shifted-window multi-head self-attention module as a patch stitching strategy to eliminate artifacts from the patch splitting. We demonstrate, through a set of numerical and visual results, that our technique outperforms current state-of-the-art unsupervised techniques.  
### ViewFormer: **NeRF**-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v2 [cs.CV] UPDATED)
- Authors : Erik Derner, Torsten Sattler, Robert Babu
- Link : [http://arxiv.org/abs/2203.10157](http://arxiv.org/abs/2203.10157)
> ABSTRACT  :  Novel view synthesis is a long-standing problem. In this work, we consider a variant of the problem where we are given only a few context views sparsely covering a scene or an object. The goal is to predict novel viewpoints in the scene, which requires learning priors. The current state of the art is based on Neural Radiance Field (**NeRF**), and while achieving impressive results, the methods suffer from long training times as they require evaluating millions of 3D point samples via a neural network for each image. We propose a 2D-only method that maps multiple context views and a query pose to a new image in a single pass of a neural network. Our model uses a two-stage architecture consisting of a codebook and a transformer model. The codebook is used to embed individual images into a smaller latent space, and the transformer solves the view synthesis task in this more compact space. To train our model efficiently, we introduce a novel branching attention mechanism that allows us to use the same model not only for neural rendering but also for camera pose estimation. Experimental results on real-world scenes show that our approach is competitive compared to **NeRF**-based methods while not reasoning explicitly in 3D, and it is faster to train.  
### Sem2**NeRF**: Converting Single-View Semantic Masks to Neural Radiance Fields. (arXiv:2203.10821v2 [cs.CV] UPDATED)
- Authors : Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Jen Cham, Jianfei Cai
- Link : [http://arxiv.org/abs/2203.10821](http://arxiv.org/abs/2203.10821)
> ABSTRACT  :  Image translation and manipulation have gain increasing attention along with the rapid development of deep generative models. Although existing approaches have brought impressive results, they mainly operated in 2D space. In light of recent advances in **NeRF**-based 3D-aware generative models, we introduce a new task, Semantic-to-**NeRF** translation, that aims to reconstruct a 3D scene modelled by **NeRF**, conditioned on one single-view semantic mask as input. To kick-off this novel task, we propose the Sem2**NeRF** framework. In particular, Sem2**NeRF** addresses the highly challenging task by encoding the semantic mask into the latent code that controls the 3D scene representation of a pre-trained decoder. To further improve the accuracy of the mapping, we integrate a new region-aware learning strategy into the design of both the encoder and the decoder. We verify the efficacy of the proposed Sem2**NeRF** and demonstrate that it outperforms several strong baselines on two benchmark datasets. Code and video are available at https://donydchen.github.io/sem2nerf/  
### Adaptive Patch Exiting for Scalable Single Image Super-Resolution. (arXiv:2203.11589v2 [cs.CV] UPDATED)
- Authors : Shizun Wang, Jiaming Liu, Kaixin Chen, Xiaoqi Li, Ming Lu, Yandong Guo
- Link : [http://arxiv.org/abs/2203.11589](http://arxiv.org/abs/2203.11589)
> ABSTRACT  :  Since the future of computing is heterogeneous, scalability is a crucial problem for single image super-resolution. Recent works try to train one network, which can be deployed on platforms with different capacities. However, they rely on the pixel-wise sparse convolution, which is not hardware-friendly and achieves limited practical speedup. As image can be divided into patches, which have various **restoration** difficulties, we present a scalable method based on Adaptive Patch Exiting (APE) to achieve more practical speedup. Specifically, we propose to train a regressor to predict the incremental capacity of each layer for the patch. Once the incremental capacity is below the threshold, the patch can exit at the specific layer. Our method can easily adjust the trade-off between performance and efficiency by changing the threshold of incremental capacity. Furthermore, we propose a novel strategy to enable the network training of our method. We conduct extensive experiments across various backbones, datasets and scaling factors to demonstrate the advantages of our method. Code is available at https://github.com/littlepure2333/APE  
### Keypoint**NeRF**: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints. (arXiv:2205.04992v2 [cs.CV] UPDATED)
- Authors : Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, Shunsuke Saito
- Link : [http://arxiv.org/abs/2205.04992](http://arxiv.org/abs/2205.04992)
> ABSTRACT  :  Image-based volumetric humans using pixel-aligned features promise generalization to unseen poses and identities. Prior work leverages global spatial encodings and multi-view geometric consistency to reduce spatial ambiguity. However, global encodings often suffer from overfitting to the distribution of the training data, and it is difficult to learn multi-view consistent reconstruction from sparse views. In this work, we investigate common issues with existing spatial encodings and propose a simple yet highly effective approach to modeling high-fidelity volumetric humans from sparse views. One of the key ideas is to encode relative spatial 3D information via sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and cross-dataset domain gap. Our approach outperforms state-of-the-art methods for head reconstruction. On human body reconstruction for unseen subjects, we also achieve performance comparable to prior work that uses a parametric human body model and temporal feature aggregation. Our experiments show that a majority of errors in prior work stem from an inappropriate choice of spatial encoding and thus we suggest a new direction for high-fidelity image-based human modeling. https://markomih.github.io/Keypoint**NeRF**  
### Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video. (arXiv:2207.08920v2 [cs.CV] UPDATED)
- Authors : Fen Tsai
- Link : [http://arxiv.org/abs/2207.08920](http://arxiv.org/abs/2207.08920)
> ABSTRACT  :  Introduction: Hand function is a central determinant of independence after stroke. Measuring hand use in the home environment is necessary to evaluate the impact of new interventions, and calls for novel wearable technologies. Egocentric video can capture hand-object interactions in context, as well as show how more-affected hands are used during **bilateral** tasks (for stabilization or manipulation). Automated methods are required to extract this information. Objective: To use artificial intelligence-based computer vision to classify hand use and hand role from egocentric videos recorded at home after stroke. Methods: Twenty-one stroke survivors participated in the study. A random forest classifier, a SlowFast neural network, and the Hand Object Detector neural network were applied to identify hand use and hand role at home. Leave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the performance of the three models. Between-group differences of the models were calculated based on the Mathews correlation coefficient (MCC). Results: For hand use detection, the Hand Object Detector had significantly higher performance than the other models. The macro average MCCs using this model in the LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for the less-affected hands. Hand role classification had macro average MCCs in the LOSOCV that were close to zero for all models. Conclusion: Using egocentric video to capture the hand use of stroke survivors at home is feasible. Pose estimation to track finger movements may be beneficial to classifying hand roles in the future.  
## eess.IV
---
### A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and **Restoration**. (arXiv:2207.10309v1 [cs.CV])
- Authors : Ming Liu, Yuxiang Wei, Xiaohe Wu, Wangmeng Zuo, **Lei Zhang**
- Link : [http://arxiv.org/abs/2207.10309](http://arxiv.org/abs/2207.10309)
> ABSTRACT  :  Generative adversarial networks (GANs) have drawn enormous attention due to the simple yet effective training mechanism and superior image generation quality. With the ability to generate photo-realistic high-resolution (e.g., $1024\times1024$) images, recent GAN models have greatly narrowed the gaps between the generated images and the real ones. Therefore, many recent works show emerging interest to take advantage of pre-trained GAN models by exploiting the well-disentangled latent space and the learned GAN priors. In this paper, we briefly review recent progress on leveraging pre-trained large-scale GAN models from three aspects, i.e., 1) the training of large-scale generative adversarial networks, 2) exploring and understanding the pre-trained GAN models, and 3) leveraging these models for subsequent tasks like image **restoration** and editing. More information about relevant methods and repositories can be found at https://github.com/csmliu/pretrained-GANs.  
### Improved Generative Model for Weakly Supervised Chest Anomaly Localization via Pseudo-paired Registration with **Bilateral**ly Symmetrical Data Augmentation. (arXiv:2207.10324v1 [eess.IV])
- Authors : Su Kim, Seong Je, Tae Uk, Myung Jin
- Link : [http://arxiv.org/abs/2207.10324](http://arxiv.org/abs/2207.10324)
> ABSTRACT  :  Image translation based on a generative adversarial network (GAN-IT) is a promising method for precise localization of abnormal regions in chest X-ray images (AL-CXR). However, heterogeneous unpaired datasets undermine existing methods to extract key features and distinguish normal from abnormal cases, resulting in inaccurate and unstable AL-CXR. To address this problem, we propose an improved two-stage GAN-IT involving registration and data augmentation. For the first stage, we introduce an invertible deep-learning-based registration technique that virtually and reasonably converts unpaired data into paired data for learning registration maps. This novel approach achieves high registration performance. For the second stage, we apply data augmentation to diversify anomaly locations by swapping the left and right lung regions on the uniform registered frames, further improving the performance by alleviating imbalance in data distribution showing left and right lung lesions. Our method is intended for application to existing GAN-IT models, allowing existing architecture to benefit from key features for translation. By showing that the AL-CXR performance is uniformly improved when applying the proposed method, we believe that GAN-IT for AL-CXR can be deployed in clinical environments, even if learning data are scarce.  
### CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution. (arXiv:2207.10345v1 [cs.CV])
- Authors : Cheeun Hong, Sungyong Baik, Heewon Kim, Seungjun Nah, Kyoung Mu
- Link : [http://arxiv.org/abs/2207.10345](http://arxiv.org/abs/2207.10345)
> ABSTRACT  :  Despite breakthrough advances in image super-resolution (SR) with convolutional neural networks (CNNs), SR has yet to enjoy ubiquitous applications due to the high computational complexity of SR networks. Quantization is one of the promising approaches to solve this problem. However, existing methods fail to quantize SR models with a bit-width lower than 8 bits, suffering from severe accuracy loss due to fixed bit-width quantization applied everywhere. In this work, to achieve high average bit-reduction with less accuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ) method for SR networks that allocates optimal bits to local regions and layers adaptively based on the local contents of an input image. To this end, a trainable bit selector module is introduced to determine the proper bit-width and quantization level for each layer and a given local image patch. This module is governed by the quantization sensitivity that is estimated by using both the average magnitude of image gradient of the patch and the standard deviation of the input feature of the layer. The proposed quantization pipeline has been tested on various SR networks and evaluated on several standard benchmarks extensively. Significant reduction in computational complexity and the elevated **restoration** accuracy clearly demonstrate the effectiveness of the proposed CADyQ framework for SR. Codes are available at https://github.com/Cheeun/CADyQ.  
## cs.LG
---
### Digraphwave: Scalable Extraction of Structural Node Embeddings via Diffusion on Directed Graphs. (arXiv:2207.10149v1 [cs.SI])
- Authors : Ciwan Ceylan, Kambiz Ghoorchian, Danica Kragic
- Link : [http://arxiv.org/abs/2207.10149](http://arxiv.org/abs/2207.10149)
> ABSTRACT  :  Structural node embeddings, vectors capturing local connectivity information for each node in a graph, have many applications in data mining and machine learning, e.g., network alignment and node classification, clustering and anomaly detection. For the analysis of directed graphs, e.g., transactions graphs, communication networks and social networks, the capability to capture directional information in the structural node embeddings is highly desirable, as is scalability of the embedding extraction method. Most existing methods are nevertheless only designed for undirected graph. Therefore, we present Digraphwave -- a scalable algorithm for extracting structural node embeddings on directed graphs. The Digraphwave embeddings consist of compressed diffusion pattern signatures, which are twice enhanced to increase their discriminate capacity. By proving a lower bound on the heat contained in the local vicinity of a diffusion initialization node, theoretically justified diffusion timescale values are established, and Digraphwave is left with only two easy-to-interpret hyperparameters: the embedding dimension and a neighbourhood resolution specifier. In our experiments, the two embedding **enhancement**s, named transposition and aggregation, are shown to lead to a significant increase in macro F1 score for classifying automorphic identities, with Digraphwave outperforming all other structural embedding baselines. Moreover, Digraphwave either outperforms or matches the performance of all baselines on real graph datasets, displaying a particularly large performance gain in a network alignment task, while also being scalable to graphs with millions of nodes and edges, running up to 30x faster than a previous diffusion pattern based method and with a fraction of the memory consumption.  
### Improved Generative Model for Weakly Supervised Chest Anomaly Localization via Pseudo-paired Registration with **Bilateral**ly Symmetrical Data Augmentation. (arXiv:2207.10324v1 [eess.IV])
- Authors : Su Kim, Seong Je, Tae Uk, Myung Jin
- Link : [http://arxiv.org/abs/2207.10324](http://arxiv.org/abs/2207.10324)
> ABSTRACT  :  Image translation based on a generative adversarial network (GAN-IT) is a promising method for precise localization of abnormal regions in chest X-ray images (AL-CXR). However, heterogeneous unpaired datasets undermine existing methods to extract key features and distinguish normal from abnormal cases, resulting in inaccurate and unstable AL-CXR. To address this problem, we propose an improved two-stage GAN-IT involving registration and data augmentation. For the first stage, we introduce an invertible deep-learning-based registration technique that virtually and reasonably converts unpaired data into paired data for learning registration maps. This novel approach achieves high registration performance. For the second stage, we apply data augmentation to diversify anomaly locations by swapping the left and right lung regions on the uniform registered frames, further improving the performance by alleviating imbalance in data distribution showing left and right lung lesions. Our method is intended for application to existing GAN-IT models, allowing existing architecture to benefit from key features for translation. By showing that the AL-CXR performance is uniformly improved when applying the proposed method, we believe that GAN-IT for AL-CXR can be deployed in clinical environments, even if learning data are scarce.  
### Deep Audio Waveform Prior. (arXiv:2207.10441v1 [cs.SD])
- Authors : Arnon Turetzky, Tzvi Michelson, Yossi Adi, Shmuel Peleg
- Link : [http://arxiv.org/abs/2207.10441](http://arxiv.org/abs/2207.10441)
> ABSTRACT  :  Convolutional neural networks contain strong priors for generating natural looking images [1]. These priors enable image denoising, super resolution, and inpainting in an unsupervised manner. Previous attempts to demonstrate similar ideas in audio, namely deep audio priors, (i) use hand picked architectures such as harmonic convolutions, (ii) only work with spectrogram input, and (iii) have been used mostly for eliminating Gaussian noise [2]. In this work we show that existing SOTA architectures for audio source separation contain deep priors even when working with the raw waveform. Deep priors can be discovered by training a neural network to generate a single corrupted signal when given white noise as input. A network with relevant deep priors is likely to generate a cleaner version of the signal before converging on the corrupted signal. We demonstrate this **restoration** effect with several corruptions: background noise, reverberations, and a gap in the signal (audio inpainting).  
### ViewFormer: **NeRF**-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v2 [cs.CV] UPDATED)
- Authors : Erik Derner, Torsten Sattler, Robert Babu
- Link : [http://arxiv.org/abs/2203.10157](http://arxiv.org/abs/2203.10157)
> ABSTRACT  :  Novel view synthesis is a long-standing problem. In this work, we consider a variant of the problem where we are given only a few context views sparsely covering a scene or an object. The goal is to predict novel viewpoints in the scene, which requires learning priors. The current state of the art is based on Neural Radiance Field (**NeRF**), and while achieving impressive results, the methods suffer from long training times as they require evaluating millions of 3D point samples via a neural network for each image. We propose a 2D-only method that maps multiple context views and a query pose to a new image in a single pass of a neural network. Our model uses a two-stage architecture consisting of a codebook and a transformer model. The codebook is used to embed individual images into a smaller latent space, and the transformer solves the view synthesis task in this more compact space. To train our model efficiently, we introduce a novel branching attention mechanism that allows us to use the same model not only for neural rendering but also for camera pose estimation. Experimental results on real-world scenes show that our approach is competitive compared to **NeRF**-based methods while not reasoning explicitly in 3D, and it is faster to train.  
## cs.AI
---
### **NeRF**-SR: High-Quality Neural Radiance Fields using Supersampling. (arXiv:2112.01759v3 [cs.CV] UPDATED)
- Authors : Chen Wang, Xian Wu, Chen Guo, Hai Zhang, Wing Tai, Min Hu
- Link : [http://arxiv.org/abs/2112.01759](http://arxiv.org/abs/2112.01759)
> ABSTRACT  :  We present **NeRF**-SR, a solution for high-resolution (HR) novel view synthesis with mostly low-resolution (LR) inputs. Our method is built upon Neural Radiance Fields (**NeRF**) that predicts per-point density and color with a multi-layer perceptron. While producing images at arbitrary scales, **NeRF** struggles with resolutions that go beyond observed images. Our key insight is that **NeRF** benefits from 3D consistency, which means an observed pixel absorbs information from nearby views. We first exploit it by a supersampling strategy that shoots multiple rays at each image pixel, which further enforces multi-view constraint at a sub-pixel level. Then, we show that **NeRF**-SR can further boost the performance of supersampling by a refinement network that leverages the estimated depth at hand to hallucinate details from related patches on only one HR reference image. Experiment results demonstrate that **NeRF**-SR generates high-quality results for novel view synthesis at HR on both synthetic and real-world datasets without any external information.  
### CausalMTA: Eliminating the User Confounding Bias for Causal Multi-touch Attribution. (arXiv:2201.00689v2 [cs.IR] UPDATED)
- Authors : Di Yao, Chang Gong, **Lei Zhang**, Sheng Chen, Jingping Bi
- Link : [http://arxiv.org/abs/2201.00689](http://arxiv.org/abs/2201.00689)
> ABSTRACT  :  Multi-touch attribution (MTA), aiming to estimate the contribution of each advertisement touchpoint in conversion journeys, is essential for budget allocation and automatically advertising. Existing methods first train a model to predict the conversion probability of the advertisement journeys with historical data and calculate the attribution of each touchpoint using counterfactual predictions. An assumption of these works is the conversion prediction model is unbiased, i.e., it can give accurate predictions on any randomly assigned journey, including both the factual and counterfactual ones. Nevertheless, this assumption does not always hold as the exposed advertisements are recommended according to user preferences. This confounding bias of users would lead to an out-of-distribution (OOD) problem in the counterfactual prediction and cause concept drift in attribution. In this paper, we define the causal MTA task and propose CausalMTA to eliminate the influence of user preferences. It systemically eliminates the confounding bias from both static and dynamic preferences to learn the conversion prediction model using historical data. We also provide a theoretical analysis to prove CausalMTA can learn an unbiased prediction model with sufficient data. Extensive experiments on both public datasets and the impression data in an e-commerce company show that CausalMTA not only achieves better prediction performance than the state-of-the-art method but also generates meaningful attribution credits across different advertising channels.  
### Sem2**NeRF**: Converting Single-View Semantic Masks to Neural Radiance Fields. (arXiv:2203.10821v2 [cs.CV] UPDATED)
- Authors : Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Jen Cham, Jianfei Cai
- Link : [http://arxiv.org/abs/2203.10821](http://arxiv.org/abs/2203.10821)
> ABSTRACT  :  Image translation and manipulation have gain increasing attention along with the rapid development of deep generative models. Although existing approaches have brought impressive results, they mainly operated in 2D space. In light of recent advances in **NeRF**-based 3D-aware generative models, we introduce a new task, Semantic-to-**NeRF** translation, that aims to reconstruct a 3D scene modelled by **NeRF**, conditioned on one single-view semantic mask as input. To kick-off this novel task, we propose the Sem2**NeRF** framework. In particular, Sem2**NeRF** addresses the highly challenging task by encoding the semantic mask into the latent code that controls the 3D scene representation of a pre-trained decoder. To further improve the accuracy of the mapping, we integrate a new region-aware learning strategy into the design of both the encoder and the decoder. We verify the efficacy of the proposed Sem2**NeRF** and demonstrate that it outperforms several strong baselines on two benchmark datasets. Code and video are available at https://donydchen.github.io/sem2nerf/  
### Embracing AWKWARD! **Real-time** Adjustment of Reactive Plans Using Social Norms. (arXiv:2204.10740v3 [cs.MA] UPDATED)
- Authors : Leila Methnani, Andreas Antoniades, Andreas Theodorou
- Link : [http://arxiv.org/abs/2204.10740](http://arxiv.org/abs/2204.10740)
> ABSTRACT  :  This paper presents the AWKWARD architecture for the development of hybrid agents in Multi-Agent Systems. AWKWARD agents can have their plans re-configured in **real time** to align with social role requirements under changing environmental and social circumstances. The proposed hybrid architecture makes use of Behaviour Oriented Design (BOD) to develop agents with reactive planning and of the well-established OperA framework to provide organisational, social, and interaction definitions in order to validate and adjust agents' behaviours. Together, OperA and BOD can achieve real-time adjustment of agent plans for evolving social roles, while providing the additional benefit of transparency into the interactions that drive this behavioural change in individual agents. We present this architecture to motivate the bridging between traditional symbolic- and behaviour-based AI communities, where such combined solutions can help MAS researchers in their pursuit of building stronger, more robust intelligent agent teams. We use DOTA2, a game where success is heavily dependent on social interactions, as a medium to demonstrate a sample implementation of our proposed hybrid architecture.  
# Paper List
---
## cs.CV
---
**213** new papers in cs.CV:-) 
1. Model Compression for Resource-Constrained Mobile Robots. (arXiv:2207.10082v1 [cs.LG])
2. World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v1 [cs.RO])
3. BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis. (arXiv:2207.10120v1 [cs.CV])
4. Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance. (arXiv:2207.10123v1 [cs.CV])
5. Latent Discriminant deterministic Uncertainty. (arXiv:2207.10130v1 [cs.CV])
6. Continual Variational Autoencoder Learning via Online Cooperative Memorization. (arXiv:2207.10131v1 [cs.LG])
7. A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation. (arXiv:2207.10137v1 [cs.CV])
8. AudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation. (arXiv:2207.10141v1 [cs.SD])
9. Tackling Long-Tailed Category Distribution Under Domain Shifts. (arXiv:2207.10150v1 [cs.CV])
10. Analysis of the Effect of Low-Overhead Lossy Image Compression on the Performance of Visual Crowd Counting for Smart City Applications. (arXiv:2207.10155v1 [cs.CV])
11. Structural Causal 3D Reconstruction. (arXiv:2207.10156v1 [cs.LG])
12. Visual Knowledge Tracing. (arXiv:2207.10157v1 [cs.CV])
13. GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning. (arXiv:2207.10158v1 [cs.CV])
14. Liver Segmentation using Turbolift Learning for CT and Cone-beam C-arm Perfusion Imaging. (arXiv:2207.10167v1 [eess.IV])
15. Pediatric Bone Age Assessment using Deep Learning Models. (arXiv:2207.10169v1 [cs.CV])
16. Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles. (arXiv:2207.10172v1 [cs.CV])
17. Scene Recognition with Objectness, Attribute and Category Learning. (arXiv:2207.10174v1 [cs.CV])
18. Controllable and Guided Face Synthesis for Unconstrained Face Recognition. (arXiv:2207.10180v1 [cs.CV])
19. Flow-based Visual Quality Enhancer for Super-resolution Magnetic Resonance Spectroscopic Imaging. (arXiv:2207.10181v1 [eess.IV])
20. 2D GANs Meet Unsupervised Single-view 3D Reconstruction. (arXiv:2207.10183v1 [cs.CV])
21. Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach. (arXiv:2207.10188v1 [cs.LG])
22. Revisiting Hotels-50K and Hotel-ID. (arXiv:2207.10200v1 [cs.CV])
23. Hybrid CNN-Transformer Model For Facial Affect Recognition In the ABAW4 Challenge. (arXiv:2207.10201v1 [cs.CV])
24. On the Robustness of 3D Object Detectors. (arXiv:2207.10205v1 [cs.CV])
25. Spotting Temporally Precise, Fine-Grained Events in Video. (arXiv:2207.10213v1 [cs.CV])
26. On Label Granularity and Object Localization. (arXiv:2207.10225v1 [cs.CV])
27. MeshMAE: Masked Autoencoders for 3D Mesh Data Analysis. (arXiv:2207.10228v1 [cs.CV])
28. SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks. (arXiv:2207.10237v1 [cs.CV])
29. GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection. (arXiv:2207.10246v1 [cs.CV])
30. SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v1 [cs.CV])
31. SGBANet: Semantic GAN and Balanced Attention Network for Arbitrarily Oriented Scene Text Recognition. (arXiv:2207.10256v1 [cs.CV])
32. Injecting 3D Perception of Controllable **NeRF**-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v1 [cs.CV])
33. Region Aware Video Object Segmentation with Deep Motion Modeling. (arXiv:2207.10258v1 [cs.CV])
34. Human-centric Image Cropping with Partition-aware and Content-preserving Features. (arXiv:2207.10269v1 [cs.CV])
35. DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2207.10271v1 [cs.CV])
36. Don't Forget Me: Accurate Background Recovery for Text Removal via Modeling Local-Global Context. (arXiv:2207.10273v1 [cs.CV])
37. Beyond single receptive field: A receptive field fusion-and-stratification network for airborne laser scanning point cloud classification. (arXiv:2207.10278v1 [cs.CV])
38. Gradient-based Point Cloud Denoising with Uniformity. (arXiv:2207.10279v1 [cs.CV])
39. Grounding Visual Representations with Texts for Domain Generalization. (arXiv:2207.10285v1 [cs.CV])
40. Towards Accurate Open-Set Recognition via Background-Class Regularization. (arXiv:2207.10287v1 [cs.CV])
41. AugRmixAT: A Data Processing and Training Method for Improving Multiple Robustness and Generalization Performance. (arXiv:2207.10290v1 [cs.CV])
42. Image Generation Network for Covert Transmission in Online Social Network. (arXiv:2207.10292v1 [cs.CV])
43. Multi-task Cross Attention Network in Facial Behavior Analysis. (arXiv:2207.10293v1 [cs.CV])
44. Deep Learning for Unsupervised Anomaly Localization in Industrial Images: A Survey. (arXiv:2207.10298v1 [cs.CV])
45. Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition. (arXiv:2207.10299v1 [cs.CV])
46. On an Edge-Preserving Variational Model for Optical Flow Estimation. (arXiv:2207.10302v1 [cs.CV])
47. A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and **Restoration**. (arXiv:2207.10309v1 [cs.CV])
48. Ada**NeRF**: Adaptive Sampling for **Real-time** Rendering of Neural Radiance Fields. (arXiv:2207.10312v1 [cs.CV])
49. Semi-Supervised Learning of Optical Flow by Flow Supervisor. (arXiv:2207.10314v1 [cs.CV])
50. SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer. (arXiv:2207.10315v1 [cs.CV])
51. AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection. (arXiv:2207.10316v1 [cs.CV])
52. Efficient CNN Architecture Design Guided by Visualization. (arXiv:2207.10318v1 [cs.CV])
53. OIMNet++: Prototypical Normalization and Localization-aware Learning for Person Search. (arXiv:2207.10320v1 [cs.CV])
54. Improved Generative Model for Weakly Supervised Chest Anomaly Localization via Pseudo-paired Registration with **Bilateral**ly Symmetrical Data Augmentation. (arXiv:2207.10324v1 [eess.IV])
55. UFO: Unified Feature Optimization. (arXiv:2207.10341v1 [cs.CV])
56. CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution. (arXiv:2207.10345v1 [cs.CV])
57. Auto Machine Learning for Medical Image Analysis by Unifying the Search on Data Augmentation and Neural Architecture. (arXiv:2207.10351v1 [cs.CV])
58. Learning from Data with Noisy Labels Using Temporal Self-Ensemble. (arXiv:2207.10354v1 [cs.CV])
59. LocVTP: Video-Text Pre-training for Temporal Localization. (arXiv:2207.10362v1 [cs.CV])
60. Land Classification in Satellite Images by Injecting Traditional Features to CNN Models. (arXiv:2207.10368v1 [cs.CV])
61. Temporal Saliency Query Network for Efficient Video Recognition. (arXiv:2207.10379v1 [cs.CV])
62. Pose for Everything: Towards Category-Agnostic Pose Estimation. (arXiv:2207.10387v1 [cs.CV])
63. NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition. (arXiv:2207.10388v1 [cs.CV])
64. Error Compensation Framework for Flow-Guided Video Inpainting. (arXiv:2207.10391v1 [cs.CV])
65. FADE: Fusing the Assets of Decoder and Encoder for Task-Agnostic Upsampling. (arXiv:2207.10392v1 [cs.CV])
66. Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives. (arXiv:2207.10395v1 [cs.CV])
67. D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights. (arXiv:2207.10398v1 [cs.CV])
68. Correspondence Matters for Video Referring Expression Comprehension. (arXiv:2207.10400v1 [cs.CV])
69. Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption. (arXiv:2207.10402v1 [cs.CV])
70. Semantic-aware Modular Capsule Routing for Visual Question Answering. (arXiv:2207.10404v1 [cs.CV])
71. Sequence Models for Drone vs Bird Classification. (arXiv:2207.10409v1 [cs.CV])
72. KD-MVS: Knowledge Distillation Based Self-supervised Learning for MVS. (arXiv:2207.10425v1 [cs.CV])
73. StreamYOLO: **Real-time** Object Detection for Streaming Perception. (arXiv:2207.10433v1 [cs.CV])
74. DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network. (arXiv:2207.10434v1 [cs.CV])
75. Human Trajectory Prediction via Neural Social Physics. (arXiv:2207.10435v1 [cs.CV])
76. Mining Relations among Cross-Frame Affinities for Video Semantic Segmentation. (arXiv:2207.10436v1 [cs.CV])
77. COBRA: Cpu-Only aBdominal oRgan segmentAtion. (arXiv:2207.10446v1 [eess.IV])
78. Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration. (arXiv:2207.10447v1 [cs.CV])
79. An Efficient Spatio-Temporal Pyramid Transformer for Action Detection. (arXiv:2207.10448v1 [cs.CV])
80. Magic ELF: Image Deraining Meets Association Learning and Transformer. (arXiv:2207.10455v1 [cs.CV])
81. Semantic-Aware Fine-Grained Correspondence. (arXiv:2207.10456v1 [cs.CV])
82. Fast Data Driven Estimation of Cluster Number in Multiplex Images using Embedded Density Outliers. (arXiv:2207.10469v1 [cs.LG])
83. LPYOLO: Low Precision YOLO for Face Detection on FPGA. (arXiv:2207.10482v1 [cs.CV])
84. Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound. (arXiv:2207.10485v1 [eess.IV])
85. Online Localisation and Colored Mesh Reconstruction Architecture for 3D Visual Feedback in Robotic Exploration Missions. (arXiv:2207.10489v1 [cs.RO])
86. Multi-Event-Camera Depth Estimation and Outlier Rejection by Refocused Events Fusion. (arXiv:2207.10494v1 [cs.CV])
87. Towards Efficient Adversarial Training on Vision Transformers. (arXiv:2207.10498v1 [cs.CV])
88. Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel Structure Aligning Network. (arXiv:2207.10506v1 [cs.CV])
89. Real-Time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition. (arXiv:2207.10519v1 [cs.CV])
90. Neural Network Learning of Chemical Bond Representations in Spectral Indices and Features. (arXiv:2207.10530v1 [cs.CV])
91. A Primer on Topological Data Analysis to Support Image Analysis Tasks in Environmental Science. (arXiv:2207.10552v1 [cs.LG])
92. The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior. (arXiv:2207.10553v1 [cs.LG])
93. Unsupervised **Night** Image **Enhancement**: When Layer Decomposition Meets Light-Effects Suppression. (arXiv:2207.10564v1 [cs.CV])
94. Face-to-Face Co-Located Human-Human Social Interaction Analysis using Nonverbal Cues: A Survey. (arXiv:2207.10574v1 [cs.HC])
95. Designing An Illumination-Aware Network for Deep Image Relighting. (arXiv:2207.10582v1 [cs.CV])
96. Boosting 3D Object Detection via Object-Focused Image Fusion. (arXiv:2207.10589v1 [cs.CV])
97. Approximate Differentiable Rendering with Algebraic Surfaces. (arXiv:2207.10606v1 [cs.CV])
98. Deep Statistic Shape Model for Myocardium Segmentation. (arXiv:2207.10607v1 [cs.CV])
99. A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing. (arXiv:2207.10614v1 [cs.CV])
100. MetaComp: Learning to Adapt for Online Depth Completion. (arXiv:2207.10623v1 [cs.CV])
101. A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery. (arXiv:2207.10625v1 [cs.CV])
102. Generative Multiplane Images: Making a 2D GAN 3D-Aware. (arXiv:2207.10642v1 [cs.CV])
103. Novel Class Discovery without Forgetting. (arXiv:2207.10659v1 [cs.CV])
104. Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild. (arXiv:2207.10660v1 [cs.CV])
105. In Defense of Online Models for Video Instance Segmentation. (arXiv:2207.10661v1 [cs.CV])
106. Generalizable Patch-Based Neural Rendering. (arXiv:2207.10662v1 [cs.CV])
107. Neural Pixel Composition: 3D-4D View Synthesis from Multi-Views. (arXiv:2207.10663v1 [cs.CV])
108. Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset. (arXiv:2207.10664v1 [cs.CV])
109. TinyViT: Fast Pretraining Distillation for Small Vision Transformers. (arXiv:2207.10666v1 [cs.CV])
110. Online Domain Adaptation for Semantic Segmentation in Ever-Changing Conditions. (arXiv:2207.10667v1 [cs.CV])
111. Zero-Shot Imitating Collaborative Manipulation Plans from YouTube Cooking Videos. (arXiv:1911.10686v4 [cs.RO] UPDATED)
112. Photorealism in Driving Simulations: Blending Generative Adversarial Image Synthesis with Rendering. (arXiv:2007.15820v2 [cs.CV] UPDATED)
113. Contrastive Registration for Unsupervised Medical Image Segmentation. (arXiv:2011.08894v3 [cs.CV] UPDATED)
114. AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification. (arXiv:2101.08238v3 [cs.CV] UPDATED)
115. Comprehensive Multi-Modal Interactions for Referring Image Segmentation. (arXiv:2104.10412v3 [cs.CV] UPDATED)
116. How Well Does Self-Supervised Pre-Training Perform with Streaming Data?. (arXiv:2104.12081v3 [cs.LG] UPDATED)
117. Towards Unsupervised Sketch-based Image Retrieval. (arXiv:2105.08237v2 [cs.CV] UPDATED)
118. Contrastive Learning with Complex Heterogeneity. (arXiv:2105.09401v2 [cs.LG] UPDATED)
119. Transforming the Latent Space of StyleGAN for Real Face Editing. (arXiv:2105.14230v2 [cs.CV] UPDATED)
120. Unsupervised Knowledge-Transfer for Learned Image Reconstruction. (arXiv:2107.02572v2 [eess.IV] UPDATED)
121. DISP6D: Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose Estimation. (arXiv:2107.12549v2 [cs.CV] UPDATED)
122. Neural Image Representations for Multi-Image Fusion and Layer Separation. (arXiv:2108.01199v4 [cs.CV] UPDATED)
123. Automatic Gaze Analysis: A Survey of Deep Learning based Approaches. (arXiv:2108.05479v3 [cs.CV] UPDATED)
124. Identifying partial mouse brain microscopy images from Allen reference atlas using a contrastively learned semantic space. (arXiv:2109.06662v3 [cs.CV] UPDATED)
125. Towards Autonomous Visual Navigation in Arable Fields. (arXiv:2109.11936v3 [cs.RO] UPDATED)
126. Robust Pedestrian Attribute Recognition Using Group Sparsity for Occlusion Videos. (arXiv:2110.08708v4 [cs.CV] UPDATED)
127. Algorithmic encoding of protected characteristics in image-based models for disease detection. (arXiv:2110.14755v4 [cs.LG] UPDATED)
128. CLIP2TV: Align, Match and Distill for Video-Text Retrieval. (arXiv:2111.05610v2 [cs.CV] UPDATED)
129. AnimeCeleb: Large-Scale Animation CelebHeads Dataset for Head Reenactment. (arXiv:2111.07640v2 [cs.AI] UPDATED)
130. Layered Controllable Video Generation. (arXiv:2111.12747v2 [cs.CV] UPDATED)
131. Data Invariants to Understand Unsupervised Out-of-Distribution Detection. (arXiv:2111.13362v2 [cs.CV] UPDATED)
132. Learning to Fit Morphable Models. (arXiv:2111.14824v2 [cs.CV] UPDATED)
133. EdiBERT, a generative model for image editing. (arXiv:2111.15264v3 [cs.CV] UPDATED)
134. **NeRF**-SR: High-Quality Neural Radiance Fields using Supersampling. (arXiv:2112.01759v3 [cs.CV] UPDATED)
135. HIVE: Evaluating the Human Interpretability of Visual Explanations. (arXiv:2112.03184v4 [cs.CV] UPDATED)
136. Joint Global and Local Hierarchical Priors for Learned Image Compression. (arXiv:2112.04487v2 [eess.IV] UPDATED)
137. **NeRF** for Outdoor Scene Relighting. (arXiv:2112.05140v2 [cs.CV] UPDATED)
138. Triangle Attack: A Query-efficient Decision-based Adversarial Attack. (arXiv:2112.06569v3 [cs.CV] UPDATED)
139. SeqFormer: Sequential Transformer for Video Instance Segmentation. (arXiv:2112.08275v2 [cs.CV] UPDATED)
140. Mimic Embedding via Adaptive Aggregation: Learning Generalizable Person Re-identification. (arXiv:2112.08684v3 [cs.CV] UPDATED)
141. DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation. (arXiv:2112.08775v2 [cs.CV] UPDATED)
142. Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds. (arXiv:2112.08879v5 [cs.CV] UPDATED)
143. Style**Swin**: Transformer-based GAN for High-resolution Image Generation. (arXiv:2112.10762v2 [cs.CV] UPDATED)
144. Scaling Open-Vocabulary Image Segmentation with Image-Level Labels. (arXiv:2112.12143v2 [cs.CV] UPDATED)
145. SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos. (arXiv:2112.13715v2 [cs.CV] UPDATED)
146. Multi-Query Video Retrieval. (arXiv:2201.03639v2 [cs.CV] UPDATED)
147. Improving Robustness by Enhancing Weak Subnets. (arXiv:2201.12765v2 [cs.CV] UPDATED)
148. Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v2 [cs.CV] UPDATED)
149. Realistic Blur Synthesis for Learning Image Deblurring. (arXiv:2202.08771v3 [cs.CV] UPDATED)
150. FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context. (arXiv:2203.02113v3 [cs.CV] UPDATED)
151. Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective. (arXiv:2203.03871v3 [cs.CV] UPDATED)
152. ClearPose: Large-scale Transparent Object Dataset and Benchmark. (arXiv:2203.03890v2 [cs.CV] UPDATED)
153. Deep Multimodal Guidance for Medical Image Classification. (arXiv:2203.05683v2 [cs.CV] UPDATED)
154. PC-**Swin**Morph: Patch Representation for Unsupervised Medical Image Registration and Segmentation. (arXiv:2203.05684v2 [cs.CV] UPDATED)
155. Implicit field supervision for robust non-rigid shape matching. (arXiv:2203.07694v3 [cs.CV] UPDATED)
156. DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation. (arXiv:2203.08713v2 [cs.CV] UPDATED)
157. Knee arthritis severity measurement using deep learning: a publicly available algorithm with a multi-institutional validation showing radiologist-level performance. (arXiv:2203.08914v2 [eess.IV] UPDATED)
158. ViewFormer: **NeRF**-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v2 [cs.CV] UPDATED)
159. Sem2**NeRF**: Converting Single-View Semantic Masks to Neural Radiance Fields. (arXiv:2203.10821v2 [cs.CV] UPDATED)
160. Generative Adversarial Network for Future Hand Segmentation from Egocentric Video. (arXiv:2203.11305v2 [cs.CV] UPDATED)
161. Adaptive Patch Exiting for Scalable Single Image Super-Resolution. (arXiv:2203.11589v2 [cs.CV] UPDATED)
162. AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography. (arXiv:2203.11726v2 [physics.med-ph] UPDATED)
163. A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v3 [cs.CV] UPDATED)
164. Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v3 [cs.LG] UPDATED)
165. CM-GAN: Image Inpainting with Cascaded Modulation GAN and Object-Aware Training. (arXiv:2203.11947v3 [cs.CV] UPDATED)
166. R-DFCIL: Relation-Guided Representation Learning for Data-Free Class Incremental Learning. (arXiv:2203.13104v2 [cs.CV] UPDATED)
167. FlowFormer: A Transformer Architecture for Optical Flow. (arXiv:2203.16194v2 [cs.CV] UPDATED)
168. DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation. (arXiv:2204.01599v2 [cs.CV] UPDATED)
169. Long Movie Clip Classification with State-Space Video Models. (arXiv:2204.01692v2 [cs.CV] UPDATED)
170. Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v3 [eess.IV] UPDATED)
171. 3D face reconstruction with dense landmarks. (arXiv:2204.02776v2 [cs.CV] UPDATED)
172. ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v2 [cs.CV] UPDATED)
173. DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors. (arXiv:2204.03039v3 [cs.CV] UPDATED)
174. Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking. (arXiv:2204.07049v2 [cs.RO] UPDATED)
175. A Level Set Theory for Neural Implicit Evolution under Explicit Flows. (arXiv:2204.07159v2 [cs.CV] UPDATED)
176. GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation. (arXiv:2204.07733v2 [cs.CV] UPDATED)
177. Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v4 [cs.CV] UPDATED)
178. Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v2 [cs.LG] UPDATED)
179. BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking. (arXiv:2205.02301v2 [cs.CV] UPDATED)
180. Keypoint**NeRF**: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints. (arXiv:2205.04992v2 [cs.CV] UPDATED)
181. TOCH: Spatio-Temporal Object-to-Hand Correspondence for Motion Refinement. (arXiv:2205.07982v2 [cs.CV] UPDATED)
182. Contrasting quadratic assignments for set-based representation learning. (arXiv:2205.15814v2 [cs.CV] UPDATED)
183. CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. (arXiv:2206.04382v2 [cs.CV] UPDATED)
184. Self-Supervised Implicit Attention: Guided Attention by The Model Itself. (arXiv:2206.07434v2 [cs.CV] UPDATED)
185. Online Segmentation of LiDAR Sequences: Dataset and Algorithm. (arXiv:2206.08194v2 [cs.CV] UPDATED)
186. VectorMapNet: End-to-end Vectorized HD Map Learning. (arXiv:2206.08920v2 [cs.CV] UPDATED)
187. GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles. (arXiv:2206.10255v4 [eess.SY] UPDATED)
188. Automated GI tract segmentation using deep learning. (arXiv:2206.11048v3 [eess.IV] UPDATED)
189. Video Activity Localisation with Uncertainties in Temporal Boundary. (arXiv:2206.12923v2 [cs.CV] UPDATED)
190. Exploring Lottery Ticket Hypothesis in Spiking Neural Networks. (arXiv:2207.01382v2 [cs.AI] UPDATED)
191. Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease. (arXiv:2207.04574v2 [cs.CV] UPDATED)
192. Towards Scale-Aware, Robust, and Generalizable Unsupervised Monocular Depth Estimation by Integrating IMU Motion Dynamics. (arXiv:2207.04680v3 [cs.CV] UPDATED)
193. Video Graph Transformer for Video Question Answering. (arXiv:2207.05342v3 [cs.CV] UPDATED)
194. Image and Model Transformation with Secret Key for Vision Transformer. (arXiv:2207.05366v2 [cs.CV] UPDATED)
195. Towards Lightweight Super-Resolution with Dual Regression Learning. (arXiv:2207.07929v2 [cs.CV] UPDATED)
196. Towards Understanding The Semidefinite Relaxations of Truncated Least-Squares in Robust Rotation Search. (arXiv:2207.08350v2 [math.OC] UPDATED)
197. Latency-Aware Collaborative Perception. (arXiv:2207.08560v2 [cs.CV] UPDATED)
198. Geometry-Aware Reference Synthesis for Multi-View Image Super-Resolution. (arXiv:2207.08601v2 [cs.CV] UPDATED)
199. Latent Partition Implicit with Surface Codes for 3D Representation. (arXiv:2207.08631v2 [cs.CV] UPDATED)
200. Leveraging Action Affinity and Continuity for Semi-supervised Temporal Action Segmentation. (arXiv:2207.08653v2 [cs.CV] UPDATED)
201. Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video. (arXiv:2207.08920v2 [cs.CV] UPDATED)
202. Self-Supervised Interactive Object Segmentation Through a Singulation-and-Grasping Approach. (arXiv:2207.09314v2 [cs.RO] UPDATED)
203. PoserNet: Refining Relative Camera Poses Exploiting Object Detections. (arXiv:2207.09445v2 [cs.CV] UPDATED)
204. HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images. (arXiv:2207.09508v2 [cs.CV] UPDATED)
205. Segmentation of 3D Dental Images Using Deep Learning. (arXiv:2207.09582v2 [eess.IV] UPDATED)
206. Perspective Phase Angle Model for Polarimetric 3D Reconstruction. (arXiv:2207.09629v2 [cs.CV] UPDATED)
207. HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers. (arXiv:2207.09662v2 [cs.CV] UPDATED)
208. ERA: Expert Retrieval and Assembly for Early Action Prediction. (arXiv:2207.09675v2 [cs.CV] UPDATED)
209. The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing. (arXiv:2207.09812v2 [cs.CV] UPDATED)
210. Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v2 [cs.CV] UPDATED)
211. Secrets of Event-Based Optical Flow. (arXiv:2207.10022v2 [cs.CV] UPDATED)
212. Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks. (arXiv:2207.10025v2 [cs.CV] UPDATED)
213. Densely Constrained Depth Estimator for Monocular 3D Object Detection. (arXiv:2207.10047v2 [cs.CV] UPDATED)
## eess.IV
---
**20** new papers in eess.IV:-) 
1. Liver Segmentation using Turbolift Learning for CT and Cone-beam C-arm Perfusion Imaging. (arXiv:2207.10167v1 [eess.IV])
2. Flow-based Visual Quality Enhancer for Super-resolution Magnetic Resonance Spectroscopic Imaging. (arXiv:2207.10181v1 [eess.IV])
3. A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and **Restoration**. (arXiv:2207.10309v1 [cs.CV])
4. Ensemble Learning for Efficient VVC Bitrate Ladder Prediction. (arXiv:2207.10317v1 [eess.IV])
5. Improved Generative Model for Weakly Supervised Chest Anomaly Localization via Pseudo-paired Registration with **Bilateral**ly Symmetrical Data Augmentation. (arXiv:2207.10324v1 [eess.IV])
6. CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution. (arXiv:2207.10345v1 [cs.CV])
7. Synthesizing Light Field Video from Monocular Video. (arXiv:2207.10357v1 [eess.IV])
8. COBRA: Cpu-Only aBdominal oRgan segmentAtion. (arXiv:2207.10446v1 [eess.IV])
9. Fast Data Driven Estimation of Cluster Number in Multiplex Images using Embedded Density Outliers. (arXiv:2207.10469v1 [cs.LG])
10. Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound. (arXiv:2207.10485v1 [eess.IV])
11. Neural Network Learning of Chemical Bond Representations in Spectral Indices and Features. (arXiv:2207.10530v1 [cs.CV])
12. A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery. (arXiv:2207.10625v1 [cs.CV])
13. Unsupervised Knowledge-Transfer for Learned Image Reconstruction. (arXiv:2107.02572v2 [eess.IV] UPDATED)
14. Joint Global and Local Hierarchical Priors for Learned Image Compression. (arXiv:2112.04487v2 [eess.IV] UPDATED)
15. Knee arthritis severity measurement using deep learning: a publicly available algorithm with a multi-institutional validation showing radiologist-level performance. (arXiv:2203.08914v2 [eess.IV] UPDATED)
16. AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography. (arXiv:2203.11726v2 [physics.med-ph] UPDATED)
17. Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v3 [eess.IV] UPDATED)
18. Automated GI tract segmentation using deep learning. (arXiv:2206.11048v3 [eess.IV] UPDATED)
19. Visual-Assisted Sound Source Depth Estimation in the Wild. (arXiv:2207.03074v2 [cs.SD] UPDATED)
20. Segmentation of 3D Dental Images Using Deep Learning. (arXiv:2207.09582v2 [eess.IV] UPDATED)
## cs.LG
---
**146** new papers in cs.LG:-) 
1. What Do We Maximize in Self-Supervised Learning?. (arXiv:2207.10081v1 [cs.LG])
2. Model Compression for Resource-Constrained Mobile Robots. (arXiv:2207.10082v1 [cs.LG])
3. Mixed-Precision Inference Quantization: Radically Towards Faster inference speed, Lower Storage requirement, and Lower Loss. (arXiv:2207.10083v1 [cs.LG])
4. World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v1 [cs.RO])
5. Towards Better Evaluation for Dynamic Link Prediction. (arXiv:2207.10128v1 [cs.LG])
6. Latent Discriminant deterministic Uncertainty. (arXiv:2207.10130v1 [cs.CV])
7. Continual Variational Autoencoder Learning via Online Cooperative Memorization. (arXiv:2207.10131v1 [cs.LG])
8. Learning Underspecified Models. (arXiv:2207.10140v1 [econ.TH])
9. Learning Deformable Object Manipulation from Expert Demonstrations. (arXiv:2207.10148v1 [cs.RO])
10. Digraphwave: Scalable Extraction of Structural Node Embeddings via Diffusion on Directed Graphs. (arXiv:2207.10149v1 [cs.SI])
11. Structural Causal 3D Reconstruction. (arXiv:2207.10156v1 [cs.LG])
12. Constrained Prescriptive Trees via Column Generation. (arXiv:2207.10163v1 [math.OC])
13. Liver Segmentation using Turbolift Learning for CT and Cone-beam C-arm Perfusion Imaging. (arXiv:2207.10167v1 [eess.IV])
14. Pediatric Bone Age Assessment using Deep Learning Models. (arXiv:2207.10169v1 [cs.CV])
15. Flow-based Visual Quality Enhancer for Super-resolution Magnetic Resonance Spectroscopic Imaging. (arXiv:2207.10181v1 [eess.IV])
16. An Introduction to Modern Statistical Learning. (arXiv:2207.10185v1 [cs.LG])
17. Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach. (arXiv:2207.10188v1 [cs.LG])
18. Provably tuning the ElasticNet across instances. (arXiv:2207.10199v1 [cs.LG])
19. On the Robustness of 3D Object Detectors. (arXiv:2207.10205v1 [cs.CV])
20. Hydra: Hybrid Server Power Model. (arXiv:2207.10217v1 [cs.DC])
21. The Game of Hidden Rules: A New Kind of Benchmark Challenge for Machine Learning. (arXiv:2207.10218v1 [cs.LG])
22. Slimmable Quantum Federated Learning. (arXiv:2207.10221v1 [cs.LG])
23. Direct Localization in Underwater Acoustics via Convolutional Neural Networks: A Data-Driven Approach. (arXiv:2207.10222v1 [cs.LG])
24. On Label Granularity and Object Localization. (arXiv:2207.10225v1 [cs.CV])
25. Improving Privacy-Preserving Vertical Federated Learning by Efficient Communication with ADMM. (arXiv:2207.10226v1 [cs.LG])
26. Unsupervised Legendre-Galerkin Neural Network for Stiff Partial Differential Equations. (arXiv:2207.10241v1 [cs.LG])
27. GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection. (arXiv:2207.10246v1 [cs.CV])
28. SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v1 [cs.CV])
29. FOCUS: Fairness via Agent-Awareness for Federated Learning on Heterogeneous Data. (arXiv:2207.10265v1 [cs.LG])
30. ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v1 [cs.LG])
31. Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for Adversarial Robustness. (arXiv:2207.10283v1 [cs.LG])
32. Multi Resolution Analysis (MRA) for Approximate Self-Attention. (arXiv:2207.10284v1 [cs.LG])
33. Grounding Visual Representations with Texts for Domain Generalization. (arXiv:2207.10285v1 [cs.CV])
34. Comparative Study on Supervised versus Semi-supervised Machine Learning for Anomaly Detection of In-vehicle CAN Network. (arXiv:2207.10286v1 [cs.LG])
35. A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks. (arXiv:2207.10289v1 [physics.comp-ph])
36. Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning. (arXiv:2207.10295v1 [cs.LG])
37. Action2Score: An Embedding Approach To Score Player Action. (arXiv:2207.10297v1 [cs.LG])
38. Subgraph Matching via Query-Conditioned Subgraph Matching Neural Networks and Bi-Level Tree Search. (arXiv:2207.10305v1 [cs.LG])
39. Knowledge-enhanced Black-box Attacks for Recommendations. (arXiv:2207.10307v1 [cs.LG])
40. UniFed: A Benchmark for Federated Learning Frameworks. (arXiv:2207.10308v1 [cs.LG])
41. Improved Generative Model for Weakly Supervised Chest Anomaly Localization via Pseudo-paired Registration with **Bilateral**ly Symmetrical Data Augmentation. (arXiv:2207.10324v1 [eess.IV])
42. Efficient Search of Multiple Neural Architectures with Different Complexities via Importance Sampling. (arXiv:2207.10334v1 [cs.NE])
43. Unimodal vs. Multimodal Siamese Networks for Outfit Completion. (arXiv:2207.10355v1 [cs.IR])
44. Multi-Asset Closed-Loop Reservoir Management Using Deep Reinforcement Learning. (arXiv:2207.10376v1 [cs.LG])
45. Detecting and Preventing Shortcut Learning for Fair Medical AI using Shortcut Testing (ShorT). (arXiv:2207.10384v1 [cs.LG])
46. On the Implementation of a Reinforcement Learning-based Capacity Sharing Algorithm in O-RAN. (arXiv:2207.10390v1 [cs.NI])
47. Error Compensation Framework for Flow-Guided Video Inpainting. (arXiv:2207.10391v1 [cs.CV])
48. Sequence Models for Drone vs Bird Classification. (arXiv:2207.10409v1 [cs.CV])
49. Log Barriers for Safe Black-box Optimization with Application to Safe Reinforcement Learning. (arXiv:2207.10415v1 [math.OC])
50. The Neural Race Reduction: Dynamics of Abstraction in Gated Networks. (arXiv:2207.10430v1 [cs.LG])
51. Deep Audio Waveform Prior. (arXiv:2207.10441v1 [cs.SD])
52. Estimation of Non-Crossing Quantile Regression Process with Deep ReQU Neural Networks. (arXiv:2207.10442v1 [stat.ML])
53. Fast Data Driven Estimation of Cluster Number in Multiplex Images using Embedded Density Outliers. (arXiv:2207.10469v1 [cs.LG])
54. LPYOLO: Low Precision YOLO for Face Detection on FPGA. (arXiv:2207.10482v1 [cs.CV])
55. Bayesian Recurrent Units and the Forward-Backward Algorithm. (arXiv:2207.10486v1 [stat.ML])
56. Metropolis Monte Carlo sampling: convergence, localization transition and optimality. (arXiv:2207.10488v1 [cond-mat.stat-mech])
57. A Forgotten Danger in DNN Supervision Testing: Generating and Detecting True Ambiguity. (arXiv:2207.10495v1 [cs.SE])
58. MQRetNN: Multi-Horizon Time Series Forecasting with Retrieval Augmentation. (arXiv:2207.10517v1 [cs.LG])
59. Neural Network Learning of Chemical Bond Representations in Spectral Indices and Features. (arXiv:2207.10530v1 [cs.CV])
60. Optimal precision for GANs. (arXiv:2207.10541v1 [cs.LG])
61. Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?. (arXiv:2207.10551v1 [cs.LG])
62. A Primer on Topological Data Analysis to Support Image Analysis Tasks in Environmental Science. (arXiv:2207.10552v1 [cs.LG])
63. The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior. (arXiv:2207.10553v1 [cs.LG])
64. Careful What You Wish For: on the Extraction of Adversarially Trained Models. (arXiv:2207.10561v1 [cs.LG])
65. Face-to-Face Co-Located Human-Human Social Interaction Analysis using Nonverbal Cues: A Survey. (arXiv:2207.10574v1 [cs.HC])
66. Unsupervised pre-training of graph transformers on patient population graphs. (arXiv:2207.10603v1 [cs.LG])
67. A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery. (arXiv:2207.10625v1 [cs.CV])
68. Deep Learning Reveals Patterns of Diverse and Changing Sentiments Towards COVID-19 Vaccines Based on 11 Million Tweets. (arXiv:2207.10641v1 [cs.CL])
69. CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for the Single-Corpus and Cross-Corpus Speech Emotion Recognition. (arXiv:2207.10644v1 [cs.CL])
70. Multilingual Disinformation Detection for Digital Advertising. (arXiv:2207.10649v1 [cs.CL])
71. Deep Learning of Radiative Atmospheric Transfer with an Autoencoder. (arXiv:2207.10650v1 [physics.comp-ph])
72. RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping. (arXiv:2207.10653v1 [cs.LG])
73. Novel Class Discovery without Forgetting. (arXiv:2207.10659v1 [cs.CV])
74. Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset. (arXiv:2207.10664v1 [cs.CV])
75. High-Dimensional $L_2$Boosting: Rate of Convergence. (arXiv:1602.08927v3 [stat.ML] UPDATED)
76. Federated Learning with Non-IID Data. (arXiv:1806.00582v2 [cs.LG] UPDATED)
77. Classification of Macromolecule Type Based on Sequences of Amino Acids Using Deep Learning. (arXiv:1907.03532v2 [q-bio.BM] UPDATED)
78. Conditional Hierarchical Bayesian Tucker Decomposition for Genetic Data Analysis. (arXiv:1911.12426v5 [cs.LG] UPDATED)
79. Distribution Approximation and Statistical Estimation Guarantees of Generative Adversarial Networks. (arXiv:2002.03938v3 [cs.LG] UPDATED)
80. A Survey of Deep Learning Architectures for Intelligent Reflecting Surfaces. (arXiv:2009.02540v5 [eess.SP] UPDATED)
81. Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition. (arXiv:2010.10504v2 [eess.AS] UPDATED)
82. AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification. (arXiv:2101.08238v3 [cs.CV] UPDATED)
83. Encrypted Internet traffic classification using a supervised Spiking Neural Network. (arXiv:2101.09818v2 [cs.LG] UPDATED)
84. The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games. (arXiv:2103.01955v3 [cs.LG] UPDATED)
85. How Well Does Self-Supervised Pre-Training Perform with Streaming Data?. (arXiv:2104.12081v3 [cs.LG] UPDATED)
86. Contrastive Learning with Complex Heterogeneity. (arXiv:2105.09401v2 [cs.LG] UPDATED)
87. On learning parametric distributions from quantized samples. (arXiv:2105.12019v2 [cs.IT] UPDATED)
88. Heterogeneous Graph Neural Network with Multi-view Representation Learning. (arXiv:2108.13650v2 [cs.LG] UPDATED)
89. Gaussian Process Uniform Error Bounds with Unknown Hyperparameters for Safety-Critical Applications. (arXiv:2109.02606v2 [cs.LG] UPDATED)
90. Analysis of Regularized Learning for Generalized Data in Banach Spaces. (arXiv:2109.03159v5 [cs.LG] UPDATED)
91. Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles. (arXiv:2109.06126v4 [cs.SE] UPDATED)
92. Identifying partial mouse brain microscopy images from Allen reference atlas using a contrastively learned semantic space. (arXiv:2109.06662v3 [cs.CV] UPDATED)
93. On Learning the Transformer Kernel. (arXiv:2110.08323v2 [cs.LG] UPDATED)
94. Variational quantum algorithm for Gaussian discrete solitons and their boson sampling. (arXiv:2110.12379v4 [quant-ph] UPDATED)
95. APPTeK: Agent-Based Predicate Prediction in Temporal Knowledge Graphs. (arXiv:2110.14284v2 [cs.AI] UPDATED)
96. Algorithmic encoding of protected characteristics in image-based models for disease detection. (arXiv:2110.14755v4 [cs.LG] UPDATED)
97. An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v6 [cs.CL] UPDATED)
98. RADAMS: Resilient and Adaptive Alert and Attention Management Strategy against Informational Denial-of-Service (IDoS) Attacks. (arXiv:2111.03463v2 [cs.CR] UPDATED)
99. EdiBERT, a generative model for image editing. (arXiv:2111.15264v3 [cs.CV] UPDATED)
100. OCR-free Document Understanding Transformer. (arXiv:2111.15664v2 [cs.LG] UPDATED)
101. Inducing Causal Structure for Interpretable Neural Networks. (arXiv:2112.00826v2 [cs.LG] UPDATED)
102. Generative Adversarial Networks for Labeled Acceleration Data Augmentation for Structural Damage Detection. (arXiv:2112.03478v5 [cs.LG] UPDATED)
103. High-Dimensional Inference in Bayesian Networks. (arXiv:2112.09217v2 [stat.ML] UPDATED)
104. An Efficient and Adaptive Granular-ball Generation Method in Classification Problem. (arXiv:2201.04343v2 [cs.LG] UPDATED)
105. Neural Architecture Search for Spiking Neural Networks. (arXiv:2201.10355v3 [cs.NE] UPDATED)
106. Neural Tangent Kernel Beyond the Infinite-Width Limit: Effects of Depth and Initialization. (arXiv:2202.00553v2 [cs.LG] UPDATED)
107. Geometric Multimodal Contrastive Representation Learning. (arXiv:2202.03390v3 [cs.LG] UPDATED)
108. Locally Random P-adic Alloy Codes with Channel Coding Theorems for Distributed Coded Tensors. (arXiv:2202.03469v4 [cs.IT] UPDATED)
109. Combining Intra-Risk and Contagion Risk for Enterprise Bankruptcy Prediction Using Graph Neural Networks. (arXiv:2202.03874v4 [q-fin.RM] UPDATED)
110. An Equivalence Between Data Poisoning and Byzantine Gradient Attacks. (arXiv:2202.08578v2 [cs.LG] UPDATED)
111. AutoIP: A United Framework to Integrate Physics into Gaussian Processes. (arXiv:2202.12316v2 [cs.LG] UPDATED)
112. TANDEM: Learning Joint Exploration and Decision Making with Tactile Sensors. (arXiv:2203.00798v3 [cs.RO] UPDATED)
113. Knee arthritis severity measurement using deep learning: a publicly available algorithm with a multi-institutional validation showing radiologist-level performance. (arXiv:2203.08914v2 [eess.IV] UPDATED)
114. ViewFormer: **NeRF**-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v2 [cs.CV] UPDATED)
115. Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v3 [cs.LG] UPDATED)
116. Provable concept learning for interpretable predictions using variational autoencoders. (arXiv:2204.00492v2 [cs.LG] UPDATED)
117. DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation. (arXiv:2204.01599v2 [cs.CV] UPDATED)
118. Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v3 [eess.IV] UPDATED)
119. Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking. (arXiv:2204.07049v2 [cs.RO] UPDATED)
120. A Level Set Theory for Neural Implicit Evolution under Explicit Flows. (arXiv:2204.07159v2 [cs.CV] UPDATED)
121. Long-term Spatio-temporal Forecasting via Dynamic Multiple-Graph Attention. (arXiv:2204.11008v4 [cs.LG] UPDATED)
122. Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v2 [cs.LG] UPDATED)
123. Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v2 [cs.AI] UPDATED)
124. FELARE: Fair Scheduling of Machine Learning Tasks on Heterogeneous Edge Systems. (arXiv:2206.00065v3 [cs.DC] UPDATED)
125. Clustering with Queries under Semi-Random Noise. (arXiv:2206.04583v3 [cs.LG] UPDATED)
126. Symbolic Regression in Materials Science: Discovering Interatomic Potentials from Data. (arXiv:2206.06422v2 [cond-mat.mtrl-sci] UPDATED)
127. Continual-Learning-as-a-Service (CLaaS): On-Demand Efficient Adaptation of Predictive Models. (arXiv:2206.06957v2 [cs.LG] UPDATED)
128. Automated GI tract segmentation using deep learning. (arXiv:2206.11048v3 [eess.IV] UPDATED)
129. Attack Agnostic Dataset: Towards Generalization and Stabilization of Audio DeepFake Detection. (arXiv:2206.13979v2 [cs.SD] UPDATED)
130. Causal Machine Learning: A Survey and Open Problems. (arXiv:2206.15475v2 [cs.LG] UPDATED)
131. Denoised MDPs: Learning World Models Better Than the World Itself. (arXiv:2206.15477v4 [cs.LG] UPDATED)
132. CPrune: Compiler-Informed Model Pruning for Efficient Target-Aware DNN Execution. (arXiv:2207.01260v2 [cs.LG] UPDATED)
133. Storehouse: a Reinforcement Learning Environment for Optimizing Warehouse Management. (arXiv:2207.03851v2 [cs.LG] UPDATED)
134. Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease. (arXiv:2207.04574v2 [cs.CV] UPDATED)
135. Bootstrapping a User-Centered Task-Oriented Dialogue System. (arXiv:2207.05223v2 [cs.CL] UPDATED)
136. Every Preference Changes Differently: Neural Multi-Interest Preference Model with Temporal Dynamics for Recommendation. (arXiv:2207.06652v2 [cs.IR] UPDATED)
137. Anomal-E: A Self-Supervised Network Intrusion Detection System based on Graph Neural Networks. (arXiv:2207.06819v3 [cs.LG] UPDATED)
138. FLOWGEN: Fast and slow graph generation. (arXiv:2207.07656v2 [cs.LG] UPDATED)
139. Multigraph Topology Design for Cross-Silo Federated Learning. (arXiv:2207.09657v2 [cs.LG] UPDATED)
140. Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation. (arXiv:2207.09860v2 [cs.AI] UPDATED)
141. Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v2 [cs.CV] UPDATED)
142. NeuralNEB -- Neural Networks can find Reaction Paths Fast. (arXiv:2207.09971v2 [physics.comp-ph] UPDATED)
143. ReFactorGNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective. (arXiv:2207.09980v2 [cs.LG] UPDATED)
144. Deep Reinforcement Learning for Field Development Optimization. (arXiv:2008.12627v1 [eess.SP] CROSS LISTED)
145. Deep Reinforcement Learning for Constrained Field Development Optimization in Subsurface Two-phase Flow. (arXiv:2104.00527v1 [cs.LG] CROSS LISTED)
146. Deep reinforcement learning for optimal well control in subsurface systems with uncertain geology. (arXiv:2203.13375v1 [physics.comp-ph] CROSS LISTED)
## cs.AI
---
**87** new papers in cs.AI:-) 
1. What Do We Maximize in Self-Supervised Learning?. (arXiv:2207.10081v1 [cs.LG])
2. Model Compression for Resource-Constrained Mobile Robots. (arXiv:2207.10082v1 [cs.LG])
3. World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v1 [cs.RO])
4. Continual Variational Autoencoder Learning via Online Cooperative Memorization. (arXiv:2207.10131v1 [cs.LG])
5. Learning Deformable Object Manipulation from Expert Demonstrations. (arXiv:2207.10148v1 [cs.RO])
6. Automated Kantian Ethics: A Faithful Implementation. (arXiv:2207.10152v1 [cs.AI])
7. Structural Causal 3D Reconstruction. (arXiv:2207.10156v1 [cs.LG])
8. Illusionary Attacks on Sequential Decision Makers and Countermeasures. (arXiv:2207.10170v1 [cs.AI])
9. Scene Recognition with Objectness, Attribute and Category Learning. (arXiv:2207.10174v1 [cs.CV])
10. Differentially Private Partial Set Cover with Applications to Facility Location. (arXiv:2207.10240v1 [cs.DS])
11. The Birth of Bias: A case study on the evolution of gender bias in an English language model. (arXiv:2207.10245v1 [cs.CL])
12. GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection. (arXiv:2207.10246v1 [cs.CV])
13. SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v1 [cs.CV])
14. An Evolutionary Game based Secure Clustering Protocol with Fuzzy Trust Evaluation and Outlier Detection for Wireless Sensor Networks. (arXiv:2207.10282v1 [cs.NI])
15. Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for Adversarial Robustness. (arXiv:2207.10283v1 [cs.LG])
16. Comparative Study on Supervised versus Semi-supervised Machine Learning for Anomaly Detection of In-vehicle CAN Network. (arXiv:2207.10286v1 [cs.LG])
17. Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning. (arXiv:2207.10295v1 [cs.LG])
18. Knowledge-enhanced Black-box Attacks for Recommendations. (arXiv:2207.10307v1 [cs.LG])
19. Reinforcement learning for Energies of the future and carbon neutrality: a Challenge Design. (arXiv:2207.10330v1 [cs.AI])
20. Language Model Cascades. (arXiv:2207.10342v1 [cs.CL])
21. Unimodal vs. Multimodal Siamese Networks for Outfit Completion. (arXiv:2207.10355v1 [cs.IR])
22. EC-KitY: Evolutionary Computation Tool Kit in Python with Seamless Machine Learning Integration. (arXiv:2207.10367v1 [cs.NE])
23. Land Classification in Satellite Images by Injecting Traditional Features to CNN Models. (arXiv:2207.10368v1 [cs.CV])
24. Multi-Asset Closed-Loop Reservoir Management Using Deep Reinforcement Learning. (arXiv:2207.10376v1 [cs.LG])
25. CodeT: Code Generation with Generated Tests. (arXiv:2207.10397v1 [cs.CL])
26. The Neural Race Reduction: Dynamics of Abstraction in Gated Networks. (arXiv:2207.10430v1 [cs.LG])
27. Wer ist schuld, wenn Algorithmen irren? Entscheidungsautomatisierung, Organisationen und Verantwortung. (arXiv:2207.10479v1 [cs.CY])
28. A cost effective eye movement tracker based wheel chair control algorithm for people with paraplegia. (arXiv:2207.10511v1 [cs.HC])
29. MQRetNN: Multi-Horizon Time Series Forecasting with Retrieval Augmentation. (arXiv:2207.10517v1 [cs.LG])
30. NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian Languages. (arXiv:2207.10524v1 [cs.CL])
31. Optimal precision for GANs. (arXiv:2207.10541v1 [cs.LG])
32. The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior. (arXiv:2207.10553v1 [cs.LG])
33. CheckINN: Wide Range Neural Network Verification in Imandra. (arXiv:2207.10562v1 [cs.LO])
34. Face-to-Face Co-Located Human-Human Social Interaction Analysis using Nonverbal Cues: A Survey. (arXiv:2207.10574v1 [cs.HC])
35. Approximate Differentiable Rendering with Algebraic Surfaces. (arXiv:2207.10606v1 [cs.CV])
36. Generative Multiplane Images: Making a 2D GAN 3D-Aware. (arXiv:2207.10642v1 [cs.CV])
37. STOP: A dataset for Spoken Task Oriented Semantic Parsing. (arXiv:2207.10643v1 [cs.CL])
38. CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for the Single-Corpus and Cross-Corpus Speech Emotion Recognition. (arXiv:2207.10644v1 [cs.CL])
39. Wide & Deep Learning for Judging Student Performance in Online One-on-one Math Classes. (arXiv:2207.10645v1 [cs.CL])
40. A No-Code Low-Code Paradigm for Authoring Business Automations Using Natural Language. (arXiv:2207.10648v1 [cs.CL])
41. RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping. (arXiv:2207.10653v1 [cs.LG])
42. Emotion detection of social data: APIs comparative study. (arXiv:2207.10654v1 [cs.CL])
43. Novel Class Discovery without Forgetting. (arXiv:2207.10659v1 [cs.CV])
44. Achilles Heels for AGI/ASI via Decision Theoretic Adversaries. (arXiv:2010.05418v6 [cs.AI] UPDATED)
45. The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games. (arXiv:2103.01955v3 [cs.LG] UPDATED)
46. Heterogeneous Graph Neural Network with Multi-view Representation Learning. (arXiv:2108.13650v2 [cs.LG] UPDATED)
47. APPTeK: Agent-Based Predicate Prediction in Temporal Knowledge Graphs. (arXiv:2110.14284v2 [cs.AI] UPDATED)
48. Algorithmic encoding of protected characteristics in image-based models for disease detection. (arXiv:2110.14755v4 [cs.LG] UPDATED)
49. RADAMS: Resilient and Adaptive Alert and Attention Management Strategy against Informational Denial-of-Service (IDoS) Attacks. (arXiv:2111.03463v2 [cs.CR] UPDATED)
50. AnimeCeleb: Large-Scale Animation CelebHeads Dataset for Head Reenactment. (arXiv:2111.07640v2 [cs.AI] UPDATED)
51. OCR-free Document Understanding Transformer. (arXiv:2111.15664v2 [cs.LG] UPDATED)
52. Digital Twinning Remote Laboratories for Online Practical Learning. (arXiv:2112.00649v3 [cs.HC] UPDATED)
53. **NeRF**-SR: High-Quality Neural Radiance Fields using Supersampling. (arXiv:2112.01759v3 [cs.CV] UPDATED)
54. Self-supervised Graph Learning for Occasional Group Recommendation. (arXiv:2112.02274v4 [cs.IR] UPDATED)
55. Generative Adversarial Networks for Labeled Acceleration Data Augmentation for Structural Damage Detection. (arXiv:2112.03478v5 [cs.LG] UPDATED)
56. Mimic Embedding via Adaptive Aggregation: Learning Generalizable Person Re-identification. (arXiv:2112.08684v3 [cs.CV] UPDATED)
57. CausalMTA: Eliminating the User Confounding Bias for Causal Multi-touch Attribution. (arXiv:2201.00689v2 [cs.IR] UPDATED)
58. Neural Architecture Search for Spiking Neural Networks. (arXiv:2201.10355v3 [cs.NE] UPDATED)
59. Neural Tangent Kernel Beyond the Infinite-Width Limit: Effects of Depth and Initialization. (arXiv:2202.00553v2 [cs.LG] UPDATED)
60. Geometric Multimodal Contrastive Representation Learning. (arXiv:2202.03390v3 [cs.LG] UPDATED)
61. Strategy Synthesis for Zero-sum Neuro-symbolic Concurrent Stochastic Games (Extended Version). (arXiv:2202.06255v4 [cs.AI] UPDATED)
62. TANDEM: Learning Joint Exploration and Decision Making with Tactile Sensors. (arXiv:2203.00798v3 [cs.RO] UPDATED)
63. Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective. (arXiv:2203.03871v3 [cs.CV] UPDATED)
64. Sem2**NeRF**: Converting Single-View Semantic Masks to Neural Radiance Fields. (arXiv:2203.10821v2 [cs.CV] UPDATED)
65. ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v2 [cs.CV] UPDATED)
66. Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking. (arXiv:2204.07049v2 [cs.RO] UPDATED)
67. GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation. (arXiv:2204.07733v2 [cs.CV] UPDATED)
68. Embracing AWKWARD! **Real-time** Adjustment of Reactive Plans Using Social Norms. (arXiv:2204.10740v3 [cs.MA] UPDATED)
69. Long-term Spatio-temporal Forecasting via Dynamic Multiple-Graph Attention. (arXiv:2204.11008v4 [cs.LG] UPDATED)
70. Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v2 [cs.LG] UPDATED)
71. A First Runtime Analysis of the NSGA-II on a Multimodal Problem. (arXiv:2204.13750v2 [cs.NE] UPDATED)
72. Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v2 [cs.AI] UPDATED)
73. CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. (arXiv:2206.04382v2 [cs.CV] UPDATED)
74. Continual-Learning-as-a-Service (CLaaS): On-Demand Efficient Adaptation of Predictive Models. (arXiv:2206.06957v2 [cs.LG] UPDATED)
75. Exploring Lottery Ticket Hypothesis in Spiking Neural Networks. (arXiv:2207.01382v2 [cs.AI] UPDATED)
76. Storehouse: a Reinforcement Learning Environment for Optimizing Warehouse Management. (arXiv:2207.03851v2 [cs.LG] UPDATED)
77. Bootstrapping a User-Centered Task-Oriented Dialogue System. (arXiv:2207.05223v2 [cs.CL] UPDATED)
78. Every Preference Changes Differently: Neural Multi-Interest Preference Model with Temporal Dynamics for Recommendation. (arXiv:2207.06652v2 [cs.IR] UPDATED)
79. Anomal-E: A Self-Supervised Network Intrusion Detection System based on Graph Neural Networks. (arXiv:2207.06819v3 [cs.LG] UPDATED)
80. FLOWGEN: Fast and slow graph generation. (arXiv:2207.07656v2 [cs.LG] UPDATED)
81. Teaching Qubits to Sing: Mission Impossible?. (arXiv:2207.08225v2 [quant-ph] UPDATED)
82. Leveraging Action Affinity and Continuity for Semi-supervised Temporal Action Segmentation. (arXiv:2207.08653v2 [cs.CV] UPDATED)
83. Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation. (arXiv:2207.09860v2 [cs.AI] UPDATED)
84. ReFactorGNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective. (arXiv:2207.09980v2 [cs.LG] UPDATED)
85. Deep Reinforcement Learning for Field Development Optimization. (arXiv:2008.12627v1 [eess.SP] CROSS LISTED)
86. Deep Reinforcement Learning for Constrained Field Development Optimization in Subsurface Two-phase Flow. (arXiv:2104.00527v1 [cs.LG] CROSS LISTED)
87. Deep reinforcement learning for optimal well control in subsurface systems with uncertain geology. (arXiv:2203.13375v1 [physics.comp-ph] CROSS LISTED)

