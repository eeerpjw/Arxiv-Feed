# Your interest papers
---
## cs.CV
---
### Modeling sRGB Camera Noise with Normalizing Flows. (arXiv:2206.00812v1 [cs.CV])
- Authors : Shayan Kousha, Ali Maleky
- Link : [http://arxiv.org/abs/2206.00812](http://arxiv.org/abs/2206.00812)
> ABSTRACT  :  Noise modeling and reduction are fundamental tasks in low-level computer vision. They are particularly important for smartphone cameras relying on small sensors that exhibit visually noticeable noise. There has recently been renewed interest in using data-driven approaches to improve camera noise models via neural networks. These data-driven approaches target noise present in the raw-sensor image before it has been processed by the camera's image signal processor (ISP). Modeling noise in the RAW-rgb domain is useful for improving and testing the in-camera denoising algorithm; however, there are situations where the camera's ISP does not apply denoising or additional denoising is desired when the RAW-rgb domain image is no longer available. In such cases, the sensor noise propagates through the ISP to the final rendered image encoded in standard RGB (sRGB). The nonlinear steps on the ISP culminate in a significantly more complex noise distribution in the sRGB domain and existing raw-domain noise models are unable to capture the sRGB noise distribution. We propose a new sRGB-domain noise model based on normalizing flows that is capable of learning the complex noise distribution found in sRGB images under various ISO levels. Our normalizing flows-based approach outperforms other models by a large margin in noise modeling and synthesis tasks. We also show that image denoisers trained on noisy images synthesized with our noise model outperforms those trained with noise from baselines models.  
### Efficient**NeRF**: Efficient Neural Radiance Fields. (arXiv:2206.00878v1 [cs.CV])
- Authors : Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, Jiaya Jia
- Link : [http://arxiv.org/abs/2206.00878](http://arxiv.org/abs/2206.00878)
> ABSTRACT  :  Neural Radiance Fields (**NeRF**) has been wildly applied to various tasks for its high-quality representation of 3D scenes. It takes long per-scene training time and per-image testing time. In this paper, we present Efficient**NeRF** as an efficient **NeRF**-based method to represent 3D scene and synthesize novel-view images. Although several ways exist to accelerate the training or testing process, it is still difficult to much reduce time for both phases simultaneously. We analyze the density and weight distribution of the sampled points then propose valid and pivotal sampling at the coarse and fine stage, respectively, to significantly improve sampling efficiency. In addition, we design a novel data structure to cache the whole scene during testing to accelerate the rendering speed. Overall, our method can reduce over 88\% of training time, reach rendering speed of over 200 FPS, while still achieving competitive accuracy. Experiments prove that our method promotes the practicality of **NeRF** in the real world and enables many applications.  
### xView3-SAR: Detecting **Dark** Fishing Activity Using Synthetic Aperture Imagery. (arXiv:2206.00897v1 [cs.CV])
- Authors : Fernando Paolo, ting Tim, Ritwik Gupta, Bryce Goodman, Nirav Patel, Daniel Kuster, David Kroodsma, Jared Dunnmon
- Link : [http://arxiv.org/abs/2206.00897](http://arxiv.org/abs/2206.00897)
> ABSTRACT  :  Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that evade monitoring systems -- known as "**dark** vessels" -- is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of **dark** vessels day or **night**, under all-weather conditions. SAR images, however, require domain-specific treatment and is not widely accessible to the ML community. Moreover, the objects (vessels) are small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels from SAR. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We provide an overview of the results from the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data (https://iuu.xview.us/) and code (https://github.com/DIUx-xView) to support ongoing development and evaluation of ML approaches for this important application.  
### Noise2NoiseFlow: Realistic Camera Noise Modeling without Clean Images. (arXiv:2206.01103v1 [eess.IV])
- Authors : Ali Maleky, Shayan Kousha
- Link : [http://arxiv.org/abs/2206.01103](http://arxiv.org/abs/2206.01103)
> ABSTRACT  :  Image noise modeling is a long-standing problem with many applications in computer vision. Early attempts that propose simple models, such as signal-independent additive white Gaussian noise or the heteroscedastic Gaussian noise model (a.k.a., camera noise level function) are not sufficient to learn the complex behavior of the camera sensor noise. Recently, more complex learning-based models have been proposed that yield better results in noise synthesis and downstream tasks, such as denoising. However, their dependence on supervised data (i.e., paired clean images) is a limiting factor given the challenges in producing ground-truth images. This paper proposes a framework for training a noise model and a denoiser simultaneously while relying only on pairs of noisy images rather than noisy/clean paired image data. We apply this framework to the training of the Noise Flow architecture. The noise synthesis and density estimation results show that our framework outperforms previous signal-processing-based noise models and is on par with its supervised counterpart. The trained denoiser is also shown to significantly improve upon both supervised and weakly supervised baseline denoising approaches. The results indicate that the joint training of a denoiser and a noise model yields significant improvements in the denoiser.  
### Comparing Conventional and Deep Feature Models for Classifying Fundus Photography of Hemorrhages. (arXiv:2206.01118v1 [eess.IV])
- Authors : Tamoor Aziz, Chalie Charoenlarpnopparut, Srijidtra Mahapakulchai
- Link : [http://arxiv.org/abs/2206.01118](http://arxiv.org/abs/2206.01118)
> ABSTRACT  :  Diabetic retinopathy is an eye-related pathology creating abnormalities and causing visual impairment, proper treatment of which requires identifying irregularities. This research uses a hemorrhage detection method and compares classification of conventional and deep features. Especially, method identifies hemorrhage connected with blood vessels or reside at retinal border and reported challenging. Initially, adaptive brightness adjustment and contrast **enhancement** rectify degraded images. Prospective locations of hemorrhages are estimated by a Gaussian matched filter, entropy thresholding, and morphological operation. Hemorrhages are segmented by a novel technique based on regional variance of intensities. Features are then extracted by conventional methods and deep models for training support vector machines, and results evaluated. Evaluation metrics for each model are promising, but findings suggest that comparatively, deep models are more effective than conventional features.  
### Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives. (arXiv:2206.01136v1 [cs.CV])
- Authors : Jun Li, Junyu Chen, Yucheng Tang, Kevin Zhou
- Link : [http://arxiv.org/abs/2206.01136](http://arxiv.org/abs/2206.01136)
> ABSTRACT  :  Transformer, the latest technological advance of deep learning, has gained prevalence in natural language processing or computer vision. Since medical imaging bear some resemblance to computer vision, it is natural to inquire about the status quo of Transformers in medical imaging and ask the question: can the Transformer models transform medical imaging? In this paper, we attempt to make a response to the inquiry. After a brief introduction of the fundamentals of Transformers, especially in comparison with convolutional neural networks (CNNs), and highlighting key defining properties that characterize the Transformers, we offer a comprehensive review of the state-of-the-art Transformer-based approaches for medical imaging and exhibit current research progresses made in the areas of medical image segmentation, recognition, detection, registration, reconstruction, **enhancement**, etc. In particular, what distinguishes our review lies in its organization based on the Transformer's key defining properties, which are mostly derived from comparing the Transformer and CNN, and its type of architecture, which specifies the manner in which the Transformer and CNN are combined, all helping the readers to best understand the rationale behind the reviewed approaches. We conclude with discussions of future perspectives.  
### Combining machine learning with physics: A framework for tracking and sorting multiple **dark** solitons. (arXiv:2111.04881v2 [cond-mat.quant-gas] UPDATED)
- Authors : Shangjie Guo
- Link : [http://arxiv.org/abs/2111.04881](http://arxiv.org/abs/2111.04881)
> ABSTRACT  :  In ultracold-atom experiments, data often comes in the form of images which suffer information loss inherent in the techniques used to prepare and measure the system. This is particularly problematic when the processes of interest are complicated, such as interactions among excitations in Bose-Einstein condensates (BECs). In this paper, we describe a framework combining machine learning (ML) models with physics-based traditional analyses to identify and track multiple solitonic excitations in images of BECs. We use an ML-based object detector to locate the solitonic excitations and develop a physics-informed classifier to sort solitonic excitations into physically motivated subcategories. Lastly, we introduce a quality metric quantifying the likelihood that a specific feature is a longitudinal soliton. Our trained implementation of this framework, SolDet, is publicly available as an open-source python package. SolDet is broadly applicable to feature identification in cold-atom images when trained on a suitable user-provided dataset.  
## eess.IV
---
### Modeling sRGB Camera Noise with Normalizing Flows. (arXiv:2206.00812v1 [cs.CV])
- Authors : Shayan Kousha, Ali Maleky
- Link : [http://arxiv.org/abs/2206.00812](http://arxiv.org/abs/2206.00812)
> ABSTRACT  :  Noise modeling and reduction are fundamental tasks in low-level computer vision. They are particularly important for smartphone cameras relying on small sensors that exhibit visually noticeable noise. There has recently been renewed interest in using data-driven approaches to improve camera noise models via neural networks. These data-driven approaches target noise present in the raw-sensor image before it has been processed by the camera's image signal processor (ISP). Modeling noise in the RAW-rgb domain is useful for improving and testing the in-camera denoising algorithm; however, there are situations where the camera's ISP does not apply denoising or additional denoising is desired when the RAW-rgb domain image is no longer available. In such cases, the sensor noise propagates through the ISP to the final rendered image encoded in standard RGB (sRGB). The nonlinear steps on the ISP culminate in a significantly more complex noise distribution in the sRGB domain and existing raw-domain noise models are unable to capture the sRGB noise distribution. We propose a new sRGB-domain noise model based on normalizing flows that is capable of learning the complex noise distribution found in sRGB images under various ISO levels. Our normalizing flows-based approach outperforms other models by a large margin in noise modeling and synthesis tasks. We also show that image denoisers trained on noisy images synthesized with our noise model outperforms those trained with noise from baselines models.  
### Noise2NoiseFlow: Realistic Camera Noise Modeling without Clean Images. (arXiv:2206.01103v1 [eess.IV])
- Authors : Ali Maleky, Shayan Kousha
- Link : [http://arxiv.org/abs/2206.01103](http://arxiv.org/abs/2206.01103)
> ABSTRACT  :  Image noise modeling is a long-standing problem with many applications in computer vision. Early attempts that propose simple models, such as signal-independent additive white Gaussian noise or the heteroscedastic Gaussian noise model (a.k.a., camera noise level function) are not sufficient to learn the complex behavior of the camera sensor noise. Recently, more complex learning-based models have been proposed that yield better results in noise synthesis and downstream tasks, such as denoising. However, their dependence on supervised data (i.e., paired clean images) is a limiting factor given the challenges in producing ground-truth images. This paper proposes a framework for training a noise model and a denoiser simultaneously while relying only on pairs of noisy images rather than noisy/clean paired image data. We apply this framework to the training of the Noise Flow architecture. The noise synthesis and density estimation results show that our framework outperforms previous signal-processing-based noise models and is on par with its supervised counterpart. The trained denoiser is also shown to significantly improve upon both supervised and weakly supervised baseline denoising approaches. The results indicate that the joint training of a denoiser and a noise model yields significant improvements in the denoiser.  
### Comparing Conventional and Deep Feature Models for Classifying Fundus Photography of Hemorrhages. (arXiv:2206.01118v1 [eess.IV])
- Authors : Tamoor Aziz, Chalie Charoenlarpnopparut, Srijidtra Mahapakulchai
- Link : [http://arxiv.org/abs/2206.01118](http://arxiv.org/abs/2206.01118)
> ABSTRACT  :  Diabetic retinopathy is an eye-related pathology creating abnormalities and causing visual impairment, proper treatment of which requires identifying irregularities. This research uses a hemorrhage detection method and compares classification of conventional and deep features. Especially, method identifies hemorrhage connected with blood vessels or reside at retinal border and reported challenging. Initially, adaptive brightness adjustment and contrast **enhancement** rectify degraded images. Prospective locations of hemorrhages are estimated by a Gaussian matched filter, entropy thresholding, and morphological operation. Hemorrhages are segmented by a novel technique based on regional variance of intensities. Features are then extracted by conventional methods and deep models for training support vector machines, and results evaluated. Evaluation metrics for each model are promising, but findings suggest that comparatively, deep models are more effective than conventional features.  
## cs.LG
---
### Comparing Conventional and Deep Feature Models for Classifying Fundus Photography of Hemorrhages. (arXiv:2206.01118v1 [eess.IV])
- Authors : Tamoor Aziz, Chalie Charoenlarpnopparut, Srijidtra Mahapakulchai
- Link : [http://arxiv.org/abs/2206.01118](http://arxiv.org/abs/2206.01118)
> ABSTRACT  :  Diabetic retinopathy is an eye-related pathology creating abnormalities and causing visual impairment, proper treatment of which requires identifying irregularities. This research uses a hemorrhage detection method and compares classification of conventional and deep features. Especially, method identifies hemorrhage connected with blood vessels or reside at retinal border and reported challenging. Initially, adaptive brightness adjustment and contrast **enhancement** rectify degraded images. Prospective locations of hemorrhages are estimated by a Gaussian matched filter, entropy thresholding, and morphological operation. Hemorrhages are segmented by a novel technique based on regional variance of intensities. Features are then extracted by conventional methods and deep models for training support vector machines, and results evaluated. Evaluation metrics for each model are promising, but findings suggest that comparatively, deep models are more effective than conventional features.  
### Graph Signal **Restoration** Using Nested Deep Algorithm Unrolling. (arXiv:2106.15910v3 [eess.SP] UPDATED)
- Authors : Masatoshi Nagahama, Koki Yamada, Yuichi Tanaka
- Link : [http://arxiv.org/abs/2106.15910](http://arxiv.org/abs/2106.15910)
> ABSTRACT  :  Graph signal processing is a ubiquitous task in many applications such as sensor, social, transportation and brain networks, point cloud processing, and graph neural networks. Often, graph signals are corrupted in the sensing process, thus requiring **restoration**. In this paper, we propose two graph signal **restoration** methods based on deep algorithm unrolling (DAU). First, we present a graph signal denoiser by unrolling iterations of the alternating direction method of multiplier (ADMM). We then suggest a general **restoration** method for linear degradation by unrolling iterations of Plug-and-Play ADMM (PnP-ADMM). In the second approach, the unrolled ADMM-based denoiser is incorporated as a submodule, leading to a nested DAU structure. The parameters in the proposed denoising/**restoration** methods are trainable in an end-to-end manner. Our approach is interpretable and keeps the number of parameters small since we only tune graph-independent regularization parameters. We overcome two main challenges in existing graph signal **restoration** methods: 1) limited performance of convex optimization algorithms due to fixed parameters which are often determined manually. 2) large number of parameters of graph neural networks that result in difficulty of training. Several experiments for graph signal denoising and interpolation are performed on synthetic and real-world data. The proposed methods show performance improvements over several existing techniques in terms of root mean squared error in both tasks.  
### Combining machine learning with physics: A framework for tracking and sorting multiple **dark** solitons. (arXiv:2111.04881v2 [cond-mat.quant-gas] UPDATED)
- Authors : Shangjie Guo
- Link : [http://arxiv.org/abs/2111.04881](http://arxiv.org/abs/2111.04881)
> ABSTRACT  :  In ultracold-atom experiments, data often comes in the form of images which suffer information loss inherent in the techniques used to prepare and measure the system. This is particularly problematic when the processes of interest are complicated, such as interactions among excitations in Bose-Einstein condensates (BECs). In this paper, we describe a framework combining machine learning (ML) models with physics-based traditional analyses to identify and track multiple solitonic excitations in images of BECs. We use an ML-based object detector to locate the solitonic excitations and develop a physics-informed classifier to sort solitonic excitations into physically motivated subcategories. Lastly, we introduce a quality metric quantifying the likelihood that a specific feature is a longitudinal soliton. Our trained implementation of this framework, SolDet, is publicly available as an open-source python package. SolDet is broadly applicable to feature identification in cold-atom images when trained on a suitable user-provided dataset.  
### On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (arXiv:2205.07890v2 [cs.LG] UPDATED)
- Authors : Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad, Jonas Guan, Nicolas Papernot
- Link : [http://arxiv.org/abs/2205.07890](http://arxiv.org/abs/2205.07890)
> ABSTRACT  :  Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their **exposure** over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.  
## cs.AI
---
### On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (arXiv:2205.07890v2 [cs.LG] UPDATED)
- Authors : Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad, Jonas Guan, Nicolas Papernot
- Link : [http://arxiv.org/abs/2205.07890](http://arxiv.org/abs/2205.07890)
> ABSTRACT  :  Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their **exposure** over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.  
# Paper List
---
## cs.CV
---
**94** new papers in cs.CV:-) 
1. Context-Driven Detection of Invertebrate Species in Deep-Sea Video. (arXiv:2206.00718v1 [cs.CV])
2. Dataset Distillation using Neural Feature Regression. (arXiv:2206.00719v1 [cs.LG])
3. Cascaded Video Generation for Videos In-the-Wild. (arXiv:2206.00735v1 [cs.CV])
4. Residual Multiplicative Filter Networks for Multiscale Reconstruction. (arXiv:2206.00746v1 [cs.CV])
5. Dynamic Linear Transformer for 3D Biomedical Image Segmentation. (arXiv:2206.00771v1 [cs.CV])
6. Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness. (arXiv:2206.00785v1 [cs.DL])
7. Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction. (arXiv:2206.00790v1 [cs.CV])
8. Multi-scale frequency separation network for image deblurring. (arXiv:2206.00798v1 [cs.CV])
9. CcHarmony: Color-checker based Image Harmonization Dataset. (arXiv:2206.00800v1 [cs.CV])
10. XBound-Former: Toward Cross-scale Boundary Modeling in Transformers. (arXiv:2206.00806v1 [cs.CV])
11. Distilling Knowledge from Object Classification to Aesthetics Assessment. (arXiv:2206.00809v1 [cs.MM])
12. Modeling sRGB Camera Noise with Normalizing Flows. (arXiv:2206.00812v1 [cs.CV])
13. Dynamic Cardiac MRI Reconstruction Using Combined Tensor Nuclear Norm and Casorati Matrix Nuclear Norm Regularizations. (arXiv:2206.00831v1 [eess.IV])
14. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. (arXiv:2206.00843v1 [cs.LG])
15. Hyperspherical Consistency Regularization. (arXiv:2206.00845v1 [cs.LG])
16. Dynamic MRI using Learned Transform-based Deep Tensor Low-Rank Network (DTLR-Net). (arXiv:2206.00850v1 [eess.IV])
17. Disentangled Generation Network for Enlarged License Plate Recognition and A Unified Dataset. (arXiv:2206.00859v1 [cs.CV])
18. Efficient**NeRF**: Efficient Neural Radiance Fields. (arXiv:2206.00878v1 [cs.CV])
19. Leveraging Systematic Knowledge of 2D Transformations. (arXiv:2206.00893v1 [cs.CV])
20. xView3-SAR: Detecting **Dark** Fishing Activity Using Synthetic Aperture Imagery. (arXiv:2206.00897v1 [cs.CV])
21. MISSU: 3D Medical Image Segmentation via Self-distilling TransUNet. (arXiv:2206.00902v1 [cs.CV])
22. Mask-Guided Divergence Loss Improves the Generalization and Robustness of Deep Neural Network. (arXiv:2206.00913v1 [cs.LG])
23. Modeling Image Composition for Complex Scene Generation. (arXiv:2206.00923v1 [cs.CV])
24. FACM: Correct the Output of Deep Neural Network with Middle Layers Features against Adversarial Samples. (arXiv:2206.00924v1 [cs.CV])
25. Predicting Physical Object Properties from Video. (arXiv:2206.00930v1 [cs.CV])
26. Improving Diffusion Models for Inverse Problems using Manifold Constraints. (arXiv:2206.00941v1 [cs.LG])
27. Feature Space Particle Inference for Neural Network Ensembles. (arXiv:2206.00944v1 [cs.LG])
28. A Bhattacharyya Coefficient-Based Framework for Noise Model-Aware Random Walker Image Segmentation. (arXiv:2206.00947v1 [cs.CV])
29. SparseDet: Towards End-to-End 3D Object Detection. (arXiv:2206.00960v1 [cs.CV])
30. CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron. (arXiv:2206.00971v1 [cs.CV])
31. StopNet: Scalable Trajectory and Occupancy Prediction for Urban Autonomous Driving. (arXiv:2206.00991v1 [cs.RO])
32. Is Mapping Necessary for Realistic PointGoal Navigation?. (arXiv:2206.00997v1 [cs.CV])
33. Introducing One Sided Margin Loss for Solving Classification Problems in Deep Networks. (arXiv:2206.01002v1 [cs.LG])
34. Unified Recurrence Modeling for Video Action Anticipation. (arXiv:2206.01009v1 [cs.CV])
35. Long-tailed Recognition by Learning from Latent Categories. (arXiv:2206.01010v1 [cs.CV])
36. Suggestive Annotation of Brain MR Images with Gradient-guided Sampling. (arXiv:2206.01014v1 [cs.CV])
37. Structured Two-stream Attention Network for Video Question Answering. (arXiv:2206.01017v1 [cs.CV])
38. Adversarial Laser Spot: Robust and Covert Physical Adversarial Attack to DNNs. (arXiv:2206.01034v1 [cs.CV])
39. A Survey on Video Action Recognition in Sports: Datasets, Methods and Applications. (arXiv:2206.01038v1 [cs.CV])
40. FV-UPatches: Enhancing Universality in Finger Vein Recognition. (arXiv:2206.01061v1 [cs.CV])
41. DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis. (arXiv:2206.01062v1 [cs.CV])
42. Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning. (arXiv:2206.01088v1 [eess.IV])
43. A DTCWT-SVD Based Video Watermarking resistant to frame rate conversion. (arXiv:2206.01094v1 [cs.MM])
44. A Dual-fusion Semantic Segmentation Framework With GAN For SAR Images. (arXiv:2206.01096v1 [eess.IV])
45. A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection. (arXiv:2206.01102v1 [cs.CV])
46. Noise2NoiseFlow: Realistic Camera Noise Modeling without Clean Images. (arXiv:2206.01103v1 [eess.IV])
47. Comparing Conventional and Deep Feature Models for Classifying Fundus Photography of Hemorrhages. (arXiv:2206.01118v1 [eess.IV])
48. Prefix Conditioning Unifies Language and Label Supervision. (arXiv:2206.01125v1 [cs.CV])
49. VL-BEiT: Generative Vision-Language Pretraining. (arXiv:2206.01127v1 [cs.CV])
50. Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives. (arXiv:2206.01136v1 [cs.CV])
51. Multi-View Active Fine-Grained Recognition. (arXiv:2206.01153v1 [cs.CV])
52. DE-Net: Dynamic Text-guided Image Editing Adversarial Networks. (arXiv:2206.01160v1 [cs.CV])
53. Optimizing Relevance Maps of Vision Transformers Improves Robustness. (arXiv:2206.01161v1 [cs.CV])
54. Deep Learning on Implicit Neural Datasets. (arXiv:2206.01178v1 [cs.LG])
55. EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v1 [cs.CV])
56. Hard Negative Sampling Strategies for Contrastive Representation Learning. (arXiv:2206.01197v1 [cs.LG])
57. Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization. (arXiv:2206.01198v1 [cs.CV])
58. REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering. (arXiv:2206.01201v1 [cs.CV])
59. Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features. (arXiv:2206.01202v1 [cs.CV])
60. Semantic Instance Segmentation of 3D Scenes Through Weak Bounding Box Supervision. (arXiv:2206.01203v1 [cs.CV])
61. Siamese Image Modeling for Self-Supervised Vision Representation Learning. (arXiv:2206.01204v1 [cs.CV])
62. Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v4 [cs.CV] UPDATED)
63. DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning. (arXiv:2106.05682v2 [cs.CV] UPDATED)
64. Dynamic Knowledge Distillation With Noise Elimination for RGB-D Salient Object Detection. (arXiv:2106.09517v3 [cs.CV] UPDATED)
65. Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v4 [cs.CV] UPDATED)
66. Towards real-world navigation with deep differentiable planners. (arXiv:2108.05713v2 [cs.RO] UPDATED)
67. Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v4 [cs.CV] UPDATED)
68. DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v5 [cs.CV] UPDATED)
69. Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v4 [cs.LG] UPDATED)
70. A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v4 [cs.CV] UPDATED)
71. Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v4 [cs.CV] UPDATED)
72. Physically Explainable CNN for SAR Image Classification. (arXiv:2110.14144v2 [eess.IV] UPDATED)
73. Combining machine learning with physics: A framework for tracking and sorting multiple **dark** solitons. (arXiv:2111.04881v2 [cond-mat.quant-gas] UPDATED)
74. Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework. (arXiv:2112.05141v2 [cs.CV] UPDATED)
75. LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation. (arXiv:2112.13985v2 [cs.CV] UPDATED)
76. SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v2 [eess.IV] UPDATED)
77. Auto-Lambda: Disentangling Dynamic Task Relationships. (arXiv:2202.03091v2 [cs.LG] UPDATED)
78. Uncalibrated Models Can Improve Human-AI Collaboration. (arXiv:2202.05983v2 [cs.AI] UPDATED)
79. Rare Gems: Finding Lottery Tickets at Initialization. (arXiv:2202.12002v2 [cs.LG] UPDATED)
80. Training privacy-preserving video analytics pipelines by suppressing features that reveal information about private attributes. (arXiv:2203.02635v2 [cs.CV] UPDATED)
81. AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v3 [cs.CV] UPDATED)
82. Annotation Efficient Person Re-Identification with Diverse Cluster-Based Pair Selection. (arXiv:2203.05395v2 [cs.CV] UPDATED)
83. Font Generation with Missing Impression Labels. (arXiv:2203.10348v2 [cs.CV] UPDATED)
84. APP: Anytime Progressive Pruning. (arXiv:2204.01640v2 [cs.LG] UPDATED)
85. Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v4 [cs.CV] UPDATED)
86. Penalizing Proposals using Classifiers for Semi-Supervised Object Detection. (arXiv:2205.13219v2 [cs.CV] UPDATED)
87. Efficient textual explanations for complex road and traffic scenarios based on semantic segmentation. (arXiv:2205.14118v2 [cs.CV] UPDATED)
88. Is Lip Region-of-Interest Sufficient for Lipreading?. (arXiv:2205.14295v2 [cs.CV] UPDATED)
89. DeepRM: Deep Recurrent Matching for 6D Pose Refinement. (arXiv:2205.14474v2 [cs.CV] UPDATED)
90. Batch Normalization Is Blind to the First and Second Derivatives of the Loss. (arXiv:2205.15146v2 [cs.LG] UPDATED)
91. Adversarial synthesis based data-augmentation for code-switched spoken language identification. (arXiv:2205.15747v2 [eess.AS] UPDATED)
92. Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images. (arXiv:2206.00252v2 [cs.CV] UPDATED)
93. Learning Invariant Visual Representations for Compositional Zero-Shot Learning. (arXiv:2206.00415v2 [cs.CV] UPDATED)
94. Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines. (arXiv:2206.00535v2 [cs.CV] UPDATED)
## eess.IV
---
**15** new papers in eess.IV:-) 
1. Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data. (arXiv:2206.00775v1 [eess.IV])
2. Modeling sRGB Camera Noise with Normalizing Flows. (arXiv:2206.00812v1 [cs.CV])
3. Dynamic Cardiac MRI Reconstruction Using Combined Tensor Nuclear Norm and Casorati Matrix Nuclear Norm Regularizations. (arXiv:2206.00831v1 [eess.IV])
4. Dynamic MRI using Learned Transform-based Deep Tensor Low-Rank Network (DTLR-Net). (arXiv:2206.00850v1 [eess.IV])
5. A Bhattacharyya Coefficient-Based Framework for Noise Model-Aware Random Walker Image Segmentation. (arXiv:2206.00947v1 [cs.CV])
6. 2D-MRI of the Central Nervous System: The effect of a deep learning-based reconstruction pipeline on the overall image quality. (arXiv:2206.01082v1 [physics.med-ph])
7. Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning. (arXiv:2206.01088v1 [eess.IV])
8. A Dual-fusion Semantic Segmentation Framework With GAN For SAR Images. (arXiv:2206.01096v1 [eess.IV])
9. Noise2NoiseFlow: Realistic Camera Noise Modeling without Clean Images. (arXiv:2206.01103v1 [eess.IV])
10. Comparing Conventional and Deep Feature Models for Classifying Fundus Photography of Hemorrhages. (arXiv:2206.01118v1 [eess.IV])
11. Block-Parallel Systolic-Array Architecture for 2-D NTT-based Fragile Watermark Embedding. (arXiv:2206.01146v1 [cs.MM])
12. Principles of perceptual grouping: implications for image-guided surgery. (arXiv:1808.01640v2 [cs.HC] UPDATED)
13. A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v4 [cs.CV] UPDATED)
14. Physically Explainable CNN for SAR Image Classification. (arXiv:2110.14144v2 [eess.IV] UPDATED)
15. SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v2 [eess.IV] UPDATED)
## cs.LG
---
**179** new papers in cs.LG:-) 
1. How Biased is Your Feature?: Computing Fairness Influence Functions with Global Sensitivity Analysis. (arXiv:2206.00667v1 [cs.LG])
2. Learning to Untangle Genome Assembly with Graph Convolutional Networks. (arXiv:2206.00668v1 [q-bio.GN])
3. Bayesian Learning to Discover Mathematical Operations in Governing Equations of Dynamic Systems. (arXiv:2206.00669v1 [cs.LG])
4. Why Did This Model Forecast This Future? Closed-Form Temporal Saliency Towards Causal Explanations of Probabilistic Forecasts. (arXiv:2206.00679v1 [cs.LG])
5. Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data. (arXiv:2206.00686v1 [cs.LG])
6. Meta-SysId: A Meta-Learning Approach for Simultaneous Identification and Prediction. (arXiv:2206.00694v1 [cs.LG])
7. Know Your Boundaries: The Necessity of Explicit Behavioral Cloning in Offline RL. (arXiv:2206.00695v1 [cs.LG])
8. RoCourseNet: Distributionally Robust Training of a Prediction Aware Recourse Model. (arXiv:2206.00700v1 [cs.LG])
9. Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search. (arXiv:2206.00702v1 [cs.AI])
10. Split-kl and PAC-Bayes-split-kl Inequalities. (arXiv:2206.00706v1 [stat.ML])
11. Collaborative Learning of Distributions under Heterogeneity and Communication Constraints. (arXiv:2206.00707v1 [stat.ML])
12. Learning to Solve PDE-constrained Inverse Problems with Graph Networks. (arXiv:2206.00711v1 [cs.LG])
13. Dataset Distillation using Neural Feature Regression. (arXiv:2206.00719v1 [cs.LG])
14. (Machine) Learning What Policies Value. (arXiv:2206.00727v1 [econ.GN])
15. The Phenomenon of Policy Churn. (arXiv:2206.00730v1 [cs.LG])
16. Cascaded Video Generation for Videos In-the-Wild. (arXiv:2206.00735v1 [cs.CV])
17. Walk for Learning: A Random Walk Approach for Federated Learning from Heterogeneous Data. (arXiv:2206.00737v1 [cs.LG])
18. Composition of Relational Features with an Application to Explaining Black-Box Predictors. (arXiv:2206.00738v1 [cs.LG])
19. Nest Your Adaptive Algorithm for Parameter-Agnostic Nonconvex Minimax Optimization. (arXiv:2206.00743v1 [math.OC])
20. A Log-Linear Time Sequential Optimal Calibration Algorithm for Quantized Isotonic L2 Regression. (arXiv:2206.00744v1 [cs.LG])
21. Residual Multiplicative Filter Networks for Multiscale Reconstruction. (arXiv:2206.00746v1 [cs.CV])
22. SolarGAN: Synthetic Annual Solar Irradiance Time Series on Urban Building Facades via Deep Generative Networks. (arXiv:2206.00747v1 [cs.LG])
23. Merlin-Arthur Classifiers: Formal Interpretability with Interactive Black Boxes. (arXiv:2206.00759v1 [cs.LG])
24. On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting. (arXiv:2206.00761v1 [cs.LG])
25. Defense Against Gradient Leakage Attacks via Learning to Obscure Data. (arXiv:2206.00769v1 [cs.LG])
26. On the reversibility of adversarial attacks. (arXiv:2206.00772v1 [cs.LG])
27. Assessing the trade-off between prediction accuracy and interpretability for topic modeling on energetic materials corpora. (arXiv:2206.00773v1 [cs.CL])
28. Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data. (arXiv:2206.00775v1 [eess.IV])
29. Core-periphery Models for Hypergraphs. (arXiv:2206.00783v1 [cs.SI])
30. Neural Decoding with Optimization of Node Activations. (arXiv:2206.00786v1 [cs.IT])
31. On the Generalization of Neural Combinatorial Optimization Heuristics. (arXiv:2206.00787v1 [cs.LG])
32. Sequential Bayesian Neural Subnetwork Ensembles. (arXiv:2206.00794v1 [stat.ML])
33. Stabilizing Q-learning with Linear Architectures for Provably Efficient Learning. (arXiv:2206.00796v1 [cs.LG])
34. Federated Learning under Distributed Concept Drift. (arXiv:2206.00799v1 [cs.LG])
35. Indeterminacy in Latent Variable Models: Characterization and Strong Identifiability. (arXiv:2206.00801v1 [stat.ML])
36. Learning code summarization from a small and local dataset. (arXiv:2206.00804v1 [cs.SE])
37. Applied Federated Learning: Architectural Design for Robust and Efficient Learning in Privacy Aware Settings. (arXiv:2206.00807v1 [cs.LG])
38. Offline Reinforcement Learning with Differential Privacy. (arXiv:2206.00810v1 [cs.LG])
39. NIPQ: Noise Injection Pseudo Quantization for Automated DNN Optimization. (arXiv:2206.00820v1 [cs.LG])
40. BayesFormer: Transformer with Uncertainty Estimation. (arXiv:2206.00826v1 [cs.CL])
41. Progressive Purification for Instance-Dependent Partial Label Learning. (arXiv:2206.00830v1 [cs.LG])
42. Dynamic Cardiac MRI Reconstruction Using Combined Tensor Nuclear Norm and Casorati Matrix Nuclear Norm Regularizations. (arXiv:2206.00831v1 [eess.IV])
43. Fast Benchmarking of Accuracy vs. Training Time with Cyclic Learning Rates. (arXiv:2206.00832v1 [cs.LG])
44. Finite-Time Analysis of Entropy-Regularized Neural Natural Actor-Critic Algorithm. (arXiv:2206.00833v1 [cs.LG])
45. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. (arXiv:2206.00843v1 [cs.LG])
46. Hyperspherical Consistency Regularization. (arXiv:2206.00845v1 [cs.LG])
47. Faster Rates of Convergence to Stationary Points in Differentially Private Optimization. (arXiv:2206.00846v1 [cs.LG])
48. Dynamic MRI using Learned Transform-based Deep Tensor Low-Rank Network (DTLR-Net). (arXiv:2206.00850v1 [eess.IV])
49. Masked Bayesian Neural Networks : Computation and Optimality. (arXiv:2206.00853v1 [stat.ML])
50. Bayesian Inference of Stochastic Dynamical Networks. (arXiv:2206.00858v1 [stat.ML])
51. Self-Consistency of the Fokker-Planck Equation. (arXiv:2206.00860v1 [cs.LG])
52. Dynamic Structure Estimation from Bandit Feedback. (arXiv:2206.00861v1 [cs.DM])
53. Nearly Optimal Best-of-Both-Worlds Algorithms for Online Learning with Feedback Graphs. (arXiv:2206.00873v1 [cs.LG])
54. Coordinated Double Machine Learning. (arXiv:2206.00885v1 [stat.ML])
55. Watch Out for the Safety-Threatening Actors: Proactively Mitigating Safety Hazards. (arXiv:2206.00886v1 [cs.RO])
56. Leveraging Systematic Knowledge of 2D Transformations. (arXiv:2206.00893v1 [cs.CV])
57. Mask-Guided Divergence Loss Improves the Generalization and Robustness of Deep Neural Network. (arXiv:2206.00913v1 [cs.LG])
58. Federated Learning with a Sampling Algorithm under Isoperimetry. (arXiv:2206.00920v1 [cs.LG])
59. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. (arXiv:2206.00927v1 [cs.LG])
60. Generating Sparse Counterfactual Explanations For Multivariate Time Series. (arXiv:2206.00931v1 [cs.LG])
61. Gradient flow dynamics of shallow ReLU networks for square loss and orthogonal inputs. (arXiv:2206.00939v1 [stat.ML])
62. Improving Diffusion Models for Inverse Problems using Manifold Constraints. (arXiv:2206.00941v1 [cs.LG])
63. Feature Space Particle Inference for Neural Network Ensembles. (arXiv:2206.00944v1 [cs.LG])
64. Primal-dual extrapolation methods for monotone inclusions under local Lipschitz continuity with applications to variational inequality, conic constrained saddle point, and convex conic optimization problems. (arXiv:2206.00973v1 [math.OC])
65. Graph Kernels Based on Multi-scale Graph Embeddings. (arXiv:2206.00979v1 [cs.LG])
66. On the Effectiveness of Knowledge Graph Embeddings: a Rule Mining Approach. (arXiv:2206.00983v1 [cs.LG])
67. Introducing One Sided Margin Loss for Solving Classification Problems in Deep Networks. (arXiv:2206.01002v1 [cs.LG])
68. Shortest Path Networks for Graph Property Prediction. (arXiv:2206.01003v1 [cs.LG])
69. Approximate Network Motif Mining Via Graph Learning. (arXiv:2206.01008v1 [cs.LG])
70. Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes. (arXiv:2206.01011v1 [cs.LG])
71. Score-Based Generative Models Detect Manifolds. (arXiv:2206.01018v1 [stat.ML])
72. Learning Disentangled Representations for Counterfactual Regression via Mutual Information Minimization. (arXiv:2206.01022v1 [cs.LG])
73. Trajectory of Mini-Batch Momentum: Batch Size Saturation and Convergence in High Dimensions. (arXiv:2206.01029v1 [math.OC])
74. DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis. (arXiv:2206.01062v1 [cs.CV])
75. Practical Adversarial Multivalid Conformal Prediction. (arXiv:2206.01067v1 [cs.LG])
76. Deep Transformer Q-Networks for Partially Observable Reinforcement Learning. (arXiv:2206.01078v1 [cs.LG])
77. When does return-conditioned supervised learning work for offline reinforcement learning?. (arXiv:2206.01079v1 [cs.LG])
78. Revisiting the General Identifiability Problem. (arXiv:2206.01081v1 [cs.LG])
79. Incorporating Explicit Uncertainty Estimates into Deep Offline Reinforcement Learning. (arXiv:2206.01085v1 [cs.LG])
80. Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning. (arXiv:2206.01088v1 [eess.IV])
81. Combining Machine Learning and Agent-Based Modeling to Study Biomedical Systems. (arXiv:2206.01092v1 [q-bio.QM])
82. Clipped Stochastic Methods for Variational Inequalities with Heavy-Tailed Noise. (arXiv:2206.01095v1 [math.OC])
83. Weakly Supervised Representation Learning with Sparse Perturbations. (arXiv:2206.01101v1 [cs.LG])
84. Robustness to Label Noise Depends on the Shape of the Noise Distribution in Feature Space. (arXiv:2206.01106v1 [cs.LG])
85. Comparing Conventional and Deep Feature Models for Classifying Fundus Photography of Hemorrhages. (arXiv:2206.01118v1 [eess.IV])
86. Super-resolving 2D stress tensor field conserving equilibrium constraints using physics informed U-Net. (arXiv:2206.01122v1 [cs.LG])
87. Predictive Multiplicity in Probabilistic Classification. (arXiv:2206.01131v1 [cs.LG])
88. A Communication-efficient Algorithm with Linear Convergence for Federated Minimax Learning. (arXiv:2206.01132v1 [cs.LG])
89. Vygotskian Autotelic Artificial Intelligence: Language and Culture Internalization for Human-Like AI. (arXiv:2206.01134v1 [cs.AI])
90. Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation. (arXiv:2206.01137v1 [cs.CL])
91. Causal Structure Learning: a Combinatorial Perspective. (arXiv:2206.01152v1 [stat.ME])
92. Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning. (arXiv:2206.01162v1 [cs.LG])
93. Invertible Neural Networks for Graph Prediction. (arXiv:2206.01163v1 [stat.ML])
94. Sparse Mixed Linear Regression with Guarantees: Taming an Intractable Problem with Invex Relaxation. (arXiv:2206.01167v1 [cs.LG])
95. Robust Longitudinal Control for Vehicular Autonomous Platoons Using Deep Reinforcement Learning. (arXiv:2206.01175v1 [eess.SY])
96. From Cities to Series: Complex Networks and Deep Learning for Improved Spatial and Temporal Analytics*. (arXiv:2206.01176v1 [cs.LG])
97. Deep Learning on Implicit Neural Datasets. (arXiv:2206.01178v1 [cs.LG])
98. ORC: Network Group-based Knowledge Distillation using Online Role Change. (arXiv:2206.01186v1 [cs.LG])
99. Uniqueness and Complexity of Inverse MDP Models. (arXiv:2206.01192v1 [cs.LG])
100. Hard Negative Sampling Strategies for Contrastive Representation Learning. (arXiv:2206.01197v1 [cs.LG])
101. Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features. (arXiv:2206.01202v1 [cs.CV])
102. ZOOpt: Toolbox for Derivative-Free Optimization. (arXiv:1801.00329v3 [cs.LG] UPDATED)
103. Compositional Coding Capsule Network with K-Means Routing for Text Classification. (arXiv:1810.09177v5 [cs.LG] UPDATED)
104. Exponential Convergence Rates of Classification Errors on Learning with SGD and Random Features. (arXiv:1911.05350v2 [stat.ML] UPDATED)
105. On the Global Convergence Rates of Softmax Policy Gradient Methods. (arXiv:2005.06392v3 [cs.LG] UPDATED)
106. Understanding Nesterov's Acceleration via Proximal Point Method. (arXiv:2005.08304v3 [math.OC] UPDATED)
107. Multi-source Domain Adaptation via Weighted Joint Distributions Optimal Transport. (arXiv:2006.12938v2 [cs.LG] UPDATED)
108. Bridging the Gap: Unifying the Training and Evaluation of Neural Network Binary Classifiers. (arXiv:2009.01367v3 [cs.LG] UPDATED)
109. Fictitious play in zero-sum stochastic games. (arXiv:2010.04223v6 [cs.GT] UPDATED)
110. Temporal Knowledge Graph Forecasting with Neural ODE. (arXiv:2101.05151v3 [cs.LG] UPDATED)
111. Dynamic Privacy Budget Allocation Improves Data Efficiency of Differentially Private Gradient Descent. (arXiv:2101.07413v2 [cs.LG] UPDATED)
112. New Riemannian preconditioned algorithms for tensor completion via polyadic decomposition. (arXiv:2101.11108v2 [math.OC] UPDATED)
113. Regularized Nonlinear Regression for Simultaneously Selecting and Estimating Key Model Parameters. (arXiv:2104.11426v2 [stat.ME] UPDATED)
114. Leveraging Non-uniformity in First-order Non-convex Optimization. (arXiv:2105.06072v3 [cs.LG] UPDATED)
115. DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning. (arXiv:2106.05682v2 [cs.CV] UPDATED)
116. Evaluating Modules in Graph Contrastive Learning. (arXiv:2106.08171v2 [cs.LG] UPDATED)
117. RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks. (arXiv:2106.08928v4 [cs.LG] UPDATED)
118. Boundary Graph Neural Networks for 3D Simulations. (arXiv:2106.11299v3 [cs.LG] UPDATED)
119. Graph Signal **Restoration** Using Nested Deep Algorithm Unrolling. (arXiv:2106.15910v3 [eess.SP] UPDATED)
120. Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification. (arXiv:2107.12838v2 [q-bio.NC] UPDATED)
121. Towards real-world navigation with deep differentiable planners. (arXiv:2108.05713v2 [cs.RO] UPDATED)
122. Collaboration Equilibrium in Federated Learning. (arXiv:2108.07926v3 [cs.LG] UPDATED)
123. Anarchic Federated Learning. (arXiv:2108.09875v2 [cs.LG] UPDATED)
124. SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural Networks. (arXiv:2109.04566v3 [cs.LG] UPDATED)
125. DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v5 [cs.CV] UPDATED)
126. DNN-assisted Particle-based Bayesian Joint Synchronization and Localization. (arXiv:2110.02771v2 [cs.IT] UPDATED)
127. Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v4 [cs.LG] UPDATED)
128. A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v4 [cs.CV] UPDATED)
129. Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v4 [cs.CV] UPDATED)
130. Availability Attacks Create Shortcuts. (arXiv:2111.00898v2 [cs.LG] UPDATED)
131. Combining machine learning with physics: A framework for tracking and sorting multiple **dark** solitons. (arXiv:2111.04881v2 [cond-mat.quant-gas] UPDATED)
132. Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data. (arXiv:2112.04351v2 [cs.CL] UPDATED)
133. Boosting Independent Component Analysis. (arXiv:2112.06920v3 [stat.ML] UPDATED)
134. Sampling Trade-Offs in Duty-Cycled Systems for Air Quality Low-Cost Sensors. (arXiv:2112.09072v2 [eess.SP] UPDATED)
135. WebGPT: Browser-assisted question-answering with human feedback. (arXiv:2112.09332v3 [cs.CL] UPDATED)
136. The effective noise of Stochastic Gradient Descent. (arXiv:2112.10852v3 [cond-mat.dis-nn] UPDATED)
137. How Infinitely Wide Neural Networks Benefit from Multi-task Learning -- an Exact Macroscopic Characterization. (arXiv:2112.15577v3 [cs.LG] UPDATED)
138. Automated Reinforcement Learning (AutoRL): A Survey and Open Problems. (arXiv:2201.03916v2 [cs.LG] UPDATED)
139. Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v2 [cs.LG] UPDATED)
140. Homotopic Policy Mirror Descent: Policy Convergence, Implicit Regularization, and Improved Sample Complexity. (arXiv:2201.09457v7 [cs.LG] UPDATED)
141. Composing a surrogate observation operator for sequential data assimilation. (arXiv:2201.12514v3 [cs.LG] UPDATED)
142. Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks. (arXiv:2202.00293v2 [stat.ML] UPDATED)
143. A Fair Comparison of Two Popular Flat Minima Optimizers: Stochastic Weight Averaging vs. Sharpness-Aware Minimization. (arXiv:2202.00661v3 [cs.LG] UPDATED)
144. Pre-Trained Language Models for Interactive Decision-Making. (arXiv:2202.01771v3 [cs.LG] UPDATED)
145. Graph Self-supervised Learning with Accurate Discrepancy Learning. (arXiv:2202.02989v4 [cs.LG] UPDATED)
146. Auto-Lambda: Disentangling Dynamic Task Relationships. (arXiv:2202.03091v2 [cs.LG] UPDATED)
147. Conditional Gradients for the Approximately Vanishing Ideal. (arXiv:2202.03349v10 [cs.LG] UPDATED)
148. Locating and Editing Factual Associations in GPT. (arXiv:2202.05262v3 [cs.CL] UPDATED)
149. Uncalibrated Models Can Improve Human-AI Collaboration. (arXiv:2202.05983v2 [cs.AI] UPDATED)
150. A Barrier Certificate-based Simplex Architecture with Application to Microgrids. (arXiv:2202.09710v2 [eess.SY] UPDATED)
151. Counterfactual Phenotyping with Censored Time-to-Events. (arXiv:2202.11089v2 [cs.LG] UPDATED)
152. Bayesian Model Selection, the Marginal Likelihood, and Generalization. (arXiv:2202.11678v2 [cs.LG] UPDATED)
153. Rare Gems: Finding Lottery Tickets at Initialization. (arXiv:2202.12002v2 [cs.LG] UPDATED)
154. Numeric Lyndon-based feature embedding of sequencing reads for machine learning approaches. (arXiv:2202.13884v2 [q-bio.GN] UPDATED)
155. Query Processing on Tensor Computation Runtimes. (arXiv:2203.01877v2 [cs.DB] UPDATED)
156. Intrinsically-Motivated Reinforcement Learning: A Brief Introduction. (arXiv:2203.02298v2 [cs.LG] UPDATED)
157. Training privacy-preserving video analytics pipelines by suppressing features that reveal information about private attributes. (arXiv:2203.02635v2 [cs.CV] UPDATED)
158. The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning. (arXiv:2203.06498v8 [cs.LG] UPDATED)
159. DPar2: Fast and Scalable PARAFAC2 Decomposition for Irregular Dense Tensors. (arXiv:2203.12798v2 [cs.LG] UPDATED)
160. APP: Anytime Progressive Pruning. (arXiv:2204.01640v2 [cs.LG] UPDATED)
161. AQuaMoHo: Localized Low-Cost Outdoor Air Quality Sensing over a Thermo-Hygrometer. (arXiv:2204.11484v2 [cs.CY] UPDATED)
162. The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations. (arXiv:2205.03295v2 [cs.LG] UPDATED)
163. $\alpha$NAS: Neural Architecture Search using Property Guided Synthesis. (arXiv:2205.03960v2 [cs.LG] UPDATED)
164. Analyzing Lottery Ticket Hypothesis from PAC-Bayesian Theory Perspective. (arXiv:2205.07320v2 [cs.LG] UPDATED)
165. On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (arXiv:2205.07890v2 [cs.LG] UPDATED)
166. Analytics of Business Time Series Using Machine Learning and Bayesian Inference. (arXiv:2205.12905v2 [cs.LG] UPDATED)
167. Penalizing Proposals using Classifiers for Semi-Supervised Object Detection. (arXiv:2205.13219v2 [cs.CV] UPDATED)
168. Kernel Ridgeless Regression is Inconsistent in Low Dimensions. (arXiv:2205.13525v2 [cs.LG] UPDATED)
169. Dataset Condensation via Efficient Synthetic-Data Parameterization. (arXiv:2205.14959v2 [cs.LG] UPDATED)
170. Metrizing Fairness. (arXiv:2205.15049v2 [cs.LG] UPDATED)
171. Batch Normalization Is Blind to the First and Second Derivatives of the Loss. (arXiv:2205.15146v2 [cs.LG] UPDATED)
172. Efficient $\Phi$-Regret Minimization in Extensive-Form Games via Online Mirror Descent. (arXiv:2205.15294v2 [cs.LG] UPDATED)
173. Minimax Optimal Online Imitation Learning via Replay Estimation. (arXiv:2205.15397v2 [cs.LG] UPDATED)
174. Fairness in the First Stage of Two-Stage Recommender Systems. (arXiv:2205.15436v2 [cs.IR] UPDATED)
175. Adversarial synthesis based data-augmentation for code-switched spoken language identification. (arXiv:2205.15747v2 [eess.AS] UPDATED)
176. Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images. (arXiv:2206.00252v2 [cs.CV] UPDATED)
177. Task-Specific Expert Pruning for Sparse Mixture-of-Experts. (arXiv:2206.00277v2 [cs.LG] UPDATED)
178. Model Generation with Provable Coverability for Offline Reinforcement Learning. (arXiv:2206.00316v2 [cs.LG] UPDATED)
179. Algorithmic Foundation of Deep X-Risk Optimization. (arXiv:2206.00439v2 [cs.LG] UPDATED)
## cs.AI
---
**77** new papers in cs.AI:-) 
1. How Biased is Your Feature?: Computing Fairness Influence Functions with Global Sensitivity Analysis. (arXiv:2206.00667v1 [cs.LG])
2. Bayesian Learning to Discover Mathematical Operations in Governing Equations of Dynamic Systems. (arXiv:2206.00669v1 [cs.LG])
3. Why Did This Model Forecast This Future? Closed-Form Temporal Saliency Towards Causal Explanations of Probabilistic Forecasts. (arXiv:2206.00679v1 [cs.LG])
4. Studying the Practices of Deploying Machine Learning Projects on Docker. (arXiv:2206.00699v1 [cs.SE])
5. What Changed? Investigating Debiasing Methods using Causal Mediation Analysis. (arXiv:2206.00701v1 [cs.CL])
6. Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search. (arXiv:2206.00702v1 [cs.AI])
7. Dense Crowd Flow-Informed Path Planning. (arXiv:2206.00705v1 [cs.RO])
8. The Phenomenon of Policy Churn. (arXiv:2206.00730v1 [cs.LG])
9. Composition of Relational Features with an Application to Explaining Black-Box Predictors. (arXiv:2206.00738v1 [cs.LG])
10. Merlin-Arthur Classifiers: Formal Interpretability with Interactive Black Boxes. (arXiv:2206.00759v1 [cs.LG])
11. On the reversibility of adversarial attacks. (arXiv:2206.00772v1 [cs.LG])
12. Neural Decoding with Optimization of Node Activations. (arXiv:2206.00786v1 [cs.IT])
13. On the Generalization of Neural Combinatorial Optimization Heuristics. (arXiv:2206.00787v1 [cs.LG])
14. XBound-Former: Toward Cross-scale Boundary Modeling in Transformers. (arXiv:2206.00806v1 [cs.CV])
15. Offline Reinforcement Learning with Differential Privacy. (arXiv:2206.00810v1 [cs.LG])
16. Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules. (arXiv:2206.00823v1 [cs.NE])
17. BayesFormer: Transformer with Uncertainty Estimation. (arXiv:2206.00826v1 [cs.CL])
18. Hyperspherical Consistency Regularization. (arXiv:2206.00845v1 [cs.LG])
19. NeuralSympCheck: A Symptom Checking and Disease Diagnostic Neural Model with Logic Regularization. (arXiv:2206.00906v1 [cs.CL])
20. Generating Sparse Counterfactual Explanations For Multivariate Time Series. (arXiv:2206.00931v1 [cs.LG])
21. Improving Diffusion Models for Inverse Problems using Manifold Constraints. (arXiv:2206.00941v1 [cs.LG])
22. SparseDet: Towards End-to-End 3D Object Detection. (arXiv:2206.00960v1 [cs.CV])
23. Graph Kernels Based on Multi-scale Graph Embeddings. (arXiv:2206.00979v1 [cs.LG])
24. On the Effectiveness of Knowledge Graph Embeddings: a Rule Mining Approach. (arXiv:2206.00983v1 [cs.LG])
25. Shortest Path Networks for Graph Property Prediction. (arXiv:2206.01003v1 [cs.LG])
26. Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes. (arXiv:2206.01011v1 [cs.LG])
27. Suggestive Annotation of Brain MR Images with Gradient-guided Sampling. (arXiv:2206.01014v1 [cs.CV])
28. Adversarial Laser Spot: Robust and Covert Physical Adversarial Attack to DNNs. (arXiv:2206.01034v1 [cs.CV])
29. A Survey on Video Action Recognition in Sports: Datasets, Methods and Applications. (arXiv:2206.01038v1 [cs.CV])
30. Artificial Open World for Evaluating AGI: a Conceptual Design. (arXiv:2206.01044v1 [cs.AI])
31. FV-UPatches: Enhancing Universality in Finger Vein Recognition. (arXiv:2206.01061v1 [cs.CV])
32. Deep Transformer Q-Networks for Partially Observable Reinforcement Learning. (arXiv:2206.01078v1 [cs.LG])
33. Revisiting the General Identifiability Problem. (arXiv:2206.01081v1 [cs.LG])
34. A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection. (arXiv:2206.01102v1 [cs.CV])
35. Robustness to Label Noise Depends on the Shape of the Noise Distribution in Feature Space. (arXiv:2206.01106v1 [cs.LG])
36. Vygotskian Autotelic Artificial Intelligence: Language and Culture Internalization for Human-Like AI. (arXiv:2206.01134v1 [cs.AI])
37. From Cities to Series: Complex Networks and Deep Learning for Improved Spatial and Temporal Analytics*. (arXiv:2206.01176v1 [cs.LG])
38. ORC: Network Group-based Knowledge Distillation using Online Role Change. (arXiv:2206.01186v1 [cs.LG])
39. Hard Negative Sampling Strategies for Contrastive Representation Learning. (arXiv:2206.01197v1 [cs.LG])
40. Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features. (arXiv:2206.01202v1 [cs.CV])
41. Meet MASKS: A novel Multi-Classifier's verification approach. (arXiv:2007.10090v3 [cs.AI] UPDATED)
42. Hindsight and Sequential Rationality of Correlated Play. (arXiv:2012.05874v5 [cs.GT] UPDATED)
43. Ludii Game Logic Guide. (arXiv:2101.02120v2 [cs.AI] UPDATED)
44. Temporal Knowledge Graph Forecasting with Neural ODE. (arXiv:2101.05151v3 [cs.LG] UPDATED)
45. Efficient Deviation Types and Learning for Hindsight Rationality in Extensive-Form Games. (arXiv:2102.06973v5 [cs.GT] UPDATED)
46. Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v4 [cs.CV] UPDATED)
47. DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning. (arXiv:2106.05682v2 [cs.CV] UPDATED)
48. Boundary Graph Neural Networks for 3D Simulations. (arXiv:2106.11299v3 [cs.LG] UPDATED)
49. Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI Interactions. (arXiv:2107.07015v2 [cs.AI] UPDATED)
50. Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification. (arXiv:2107.12838v2 [q-bio.NC] UPDATED)
51. Towards real-world navigation with deep differentiable planners. (arXiv:2108.05713v2 [cs.RO] UPDATED)
52. ProGCL: Rethinking Hard Negative Mining in Graph Contrastive Learning. (arXiv:2110.02027v2 [cs.AI] UPDATED)
53. DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v5 [cs.CV] UPDATED)
54. DNN-assisted Particle-based Bayesian Joint Synchronization and Localization. (arXiv:2110.02771v2 [cs.IT] UPDATED)
55. Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v4 [cs.LG] UPDATED)
56. A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v4 [cs.CV] UPDATED)
57. Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v4 [cs.CV] UPDATED)
58. Inherently Explainable Reinforcement Learning in Natural Language. (arXiv:2112.08907v2 [cs.HC] UPDATED)
59. WebGPT: Browser-assisted question-answering with human feedback. (arXiv:2112.09332v3 [cs.CL] UPDATED)
60. Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v2 [cs.LG] UPDATED)
61. SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v2 [eess.IV] UPDATED)
62. Homotopic Policy Mirror Descent: Policy Convergence, Implicit Regularization, and Improved Sample Complexity. (arXiv:2201.09457v7 [cs.LG] UPDATED)
63. Graph Self-supervised Learning with Accurate Discrepancy Learning. (arXiv:2202.02989v4 [cs.LG] UPDATED)
64. Auto-Lambda: Disentangling Dynamic Task Relationships. (arXiv:2202.03091v2 [cs.LG] UPDATED)
65. What are the best systems? New perspectives on NLP Benchmarking. (arXiv:2202.03799v3 [cs.CL] UPDATED)
66. Uncalibrated Models Can Improve Human-AI Collaboration. (arXiv:2202.05983v2 [cs.AI] UPDATED)
67. A Barrier Certificate-based Simplex Architecture with Application to Microgrids. (arXiv:2202.09710v2 [eess.SY] UPDATED)
68. Rare Gems: Finding Lottery Tickets at Initialization. (arXiv:2202.12002v2 [cs.LG] UPDATED)
69. Interpretability of Neural Network With Physiological Mechanisms. (arXiv:2203.13262v2 [cs.NE] UPDATED)
70. The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations. (arXiv:2205.03295v2 [cs.LG] UPDATED)
71. On the Difficulty of Defending Self-Supervised Learning against Model Extraction. (arXiv:2205.07890v2 [cs.LG] UPDATED)
72. Efficient Deviation Types and Learning for Hindsight Rationality in Extensive-Form Games: Corrections. (arXiv:2205.12031v2 [cs.GT] UPDATED)
73. Batch Normalization Is Blind to the First and Second Derivatives of the Loss. (arXiv:2205.15146v2 [cs.LG] UPDATED)
74. Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images. (arXiv:2206.00252v2 [cs.CV] UPDATED)
75. Task-Specific Expert Pruning for Sparse Mixture-of-Experts. (arXiv:2206.00277v2 [cs.LG] UPDATED)
76. Algorithmic Foundation of Deep X-Risk Optimization. (arXiv:2206.00439v2 [cs.LG] UPDATED)
77. Logic-Based Ethical Planning. (arXiv:2206.00595v2 [cs.AI] UPDATED)

