# Your interest papers
---
## cs.CV
---
### Thermal to Visible Image Synthesis under Atmospheric Turbulence. (arXiv:2204.03057v1 [cs.CV])
- Authors : Kangfu Mei, Yiqun Mei
- Link : [http://arxiv.org/abs/2204.03057](http://arxiv.org/abs/2204.03057)
> ABSTRACT  :  In many practical applications of long-range imaging such as biometrics and surveillance, thermal imagining modalities are often used to capture images in **low-light** and **night**time conditions. However, such imaging systems often suffer from atmospheric turbulence, which introduces severe blur and deformation artifacts to the captured images. Such an issue is unavoidable in long-range imaging and significantly decreases the face verification accuracy. In this paper, we first investigate the problem with a turbulence simulation method on real-world thermal images. An end-to-end reconstruction method is then proposed which can directly transform thermal images into visible-spectrum images by utilizing natural image priors based on a pre-trained StyleGAN2 network. Compared with the existing two-steps methods of consecutive turbulence mitigation and thermal to visible image translation, our method is demonstrated to be effective in terms of both the visual quality of the reconstructed results and face verification accuracy. Moreover, to the best of our knowledge, this is the first work that studies the problem of thermal to visible image translation under atmospheric turbulence.  
### HIT-UAV: A High-altitude Infrared Thermal Dataset for Unmanned Aerial Vehicles. (arXiv:2204.03245v1 [cs.CV])
- Authors : Jiashun Suo, Tianyi Wang, Xingzhou Zhang, Haiyang Chen, Wei Zhou, Weisong Shi
- Link : [http://arxiv.org/abs/2204.03245](http://arxiv.org/abs/2204.03245)
> ABSTRACT  :  This paper presents a High-altitude infrared thermal dataset, HIT-UAV, for object detection applications on Unmanned Aerial Vehicles (UAVs). HIT-UAV contains 2898 infrared thermal images extracted from 43470 frames. These images are collected by UAV from schools, parking lots, roads, playgrounds, etc. HIT-UAV provides different flight data for each place, including flight altitude (from 60 to 130 meters), camera perspective (from 30 to 90 degrees), date, and daylight intensity. For each image, the HIT-UAV manual annotates object instances with two types of the bounding box (oriented and standard) to address the challenge that object instances have a significant overlap in aerial images. To the best of our knowledge, HIT-UAV is the first publicly available high-altitude infrared thermal UAV dataset for persons and vehicles detection. Moreover, we trained and evaluated the benchmark detection algorithms (YOLOv4 and YOLOv4-tiny) on HIT-UAV. Compared to the visual light dataset, the detection algorithms have excellent performance on HIT-UAV because the infrared thermal images do not contain a significant quantity of irrelevant information with detection objects. This indicates that infrared thermal datasets can significantly promote the development of object detection applications. We hope HIT-UAV contributes to UAV applications such as traffic surveillance and city monitoring at **night**. The dataset is available at https://github.com/suojiashun/HIT-UAV-Infrared-Thermal-Dataset.  
### Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge TPU. (arXiv:2204.03296v1 [cs.CV])
- Authors : Alessandro Lotti, Dario Modenini, Paolo Tortora, Massimiliano Saponara
- Link : [http://arxiv.org/abs/2204.03296](http://arxiv.org/abs/2204.03296)
> ABSTRACT  :  Pose estimation of an uncooperative space resident object is a key asset towards autonomy in close proximity operations. In this context monocular cameras are a valuable solution because of their low system requirements. However, the associated image processing algorithms are either too computationally expensive for **real time** on-board implementation, or not enough accurate. In this paper we propose a pose estimation software exploiting neural network architectures which can be scaled to different accuracy-latency trade-offs. We designed our pipeline to be compatible with Edge Tensor Processing Units to show how low power machine learning accelerators could enable Artificial Intelligence exploitation in space. The neural networks were tested both on the benchmark Spacecraft Pose Estimation Dataset, and on the purposely developed Cosmo Photorealistic Dataset, which depicts a COSMO-SkyMed satellite in a variety of random poses and steerable solar panels orientations. The lightest version of our architecture achieves state-of-the-art accuracy on both datasets but at a fraction of networks complexity, running at 7.7 frames per second on a Coral Dev Board Mini consuming just 2.2W.  
### Learning to Sieve: Prediction of Grading Curves from Images of Concrete Aggregate. (arXiv:2204.03333v1 [cs.CV])
- Authors : Max Coenen, Dries Beyer, Christian Heipke, Michael Haist
- Link : [http://arxiv.org/abs/2204.03333](http://arxiv.org/abs/2204.03333)
> ABSTRACT  :  A large component of the building material concrete consists of aggregate with varying particle sizes between 0.125 and 32 mm. Its actual size distribution significantly affects the quality characteristics of the final concrete in both, the fresh and hardened states. The usually unknown variations in the size distribution of the aggregate particles, which can be large especially when using recycled aggregate materials, are typically compensated by an increased usage of cement which, however, has severe negative impacts on economical and ecological aspects of the concrete production. In order to allow a precise control of the target properties of the concrete, unknown variations in the size distribution have to be quantified to enable a proper adaptation of the concrete's mixture design in **real time**. To this end, this paper proposes a deep learning based method for the determination of concrete aggregate grading curves. In this context, we propose a network architecture applying multi-scale feature extraction modules in order to handle the strongly diverse object sizes of the particles. Furthermore, we propose and publish a novel dataset of concrete aggregate used for the quantitative evaluation of our method.  
### Event Transformer. A sparse-aware solution for efficient event data processing. (arXiv:2204.03355v1 [cs.CV])
- Authors : Alberto Sabater, Luis Montesano
- Link : [http://arxiv.org/abs/2204.03355](http://arxiv.org/abs/2204.03355)
> ABSTRACT  :  Event cameras are sensors of great interest for many applications that run in low-resource and challenging environments. They log sparse illumination changes with high temporal resolution and **high dynamic range**, while they present minimal power consumption. However, top-performing methods often ignore specific event-data properties, leading to the development of generic but computationally expensive algorithms. Efforts toward efficient solutions usually do not achieve top-accuracy results for complex tasks. This work proposes a novel framework, Event Transformer (EvT), that effectively takes advantage of event-data properties to be highly efficient and accurate. We introduce a new patch-based event representation and a compact transformer-like architecture to process it. EvT is evaluated on different event-based benchmarks for action and gesture recognition. Evaluation results show better or comparable accuracy to the state-of-the-art while requiring significantly less computation resources, which makes EvT able to work with minimal latency both on GPU and CPU.  
### HunYuan_tvr for Text-Video Retrivial. (arXiv:2204.03382v1 [cs.CV])
- Authors : Shaobo Min, Weijie Kong, Cheng Tu, Dihong Gong, Chengfei Cai, Wenzhe Zhao, Chenyang Liu, Sixiao Zheng, Hongfa Wang, Zhifeng Li, Wei Liu
- Link : [http://arxiv.org/abs/2204.03382](http://arxiv.org/abs/2204.03382)
> ABSTRACT  :  Text-Video Retrieval plays an important role in multi-modal understanding and has attracted increasing attention in recent years. Most existing methods focus on constructing contrastive pairs between whole videos and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short clips and phrases or single frame and word. In this paper, we propose a novel method, named HunYuan\_tvr, to explore hierarchical cross-modal interactions by simultaneously exploring video-sentence, clip-phrase, and frame-word relationships. Considering intrinsic semantic relations between frames, HunYuan\_tvr first performs self-attention to explore frame-wise correlations and adaptively clusters correlated frames into clip-level representations. Then, the clip-wise correlation is explored to aggregate clip representations into a compact one to describe the video globally. In this way, we can construct hierarchical video representations for frame-clip-video granularities, and also explore word-wise correlations to form word-phrase-sentence embeddings for the text modality. Finally, hierarchical contrastive learning is designed to explore cross-modal relationships,~\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which enables HunYuan\_tvr to achieve a comprehensive multi-modal understanding. Further boosted by adaptive label denosing and marginal sample **enhancement**, HunYuan\_tvr obtains new state-of-the-art results on various benchmarks, e.g., Rank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC, DiDemo, and ActivityNet respectively.  
### Evaluating Procedures for Establishing Generative Adversarial Network-based Stochastic Image Models in Medical Imaging. (arXiv:2204.03547v1 [eess.IV])
- Authors : Prabhat KC, Rongping Zeng
- Link : [http://arxiv.org/abs/2204.03547](http://arxiv.org/abs/2204.03547)
> ABSTRACT  :  Modern generative models, such as generative adversarial networks (GANs), hold tremendous promise for several areas of medical imaging, such as unconditional medical image synthesis, image **restoration**, reconstruction and translation, and optimization of imaging systems. However, procedures for establishing stochastic image models (SIMs) using GANs remain generic and do not address specific issues relevant to medical imaging. In this work, canonical SIMs that simulate realistic vessels in angiography images are employed to evaluate procedures for establishing SIMs using GANs. The GAN-based SIM is compared to the canonical SIM based on its ability to reproduce those statistics that are meaningful to the particular medically realistic SIM considered. It is shown that evaluating GANs using classical metrics and medically relevant metrics may lead to different conclusions about the fidelity of the trained GANs. This work highlights the need for the development of objective metrics for evaluating GANs.  
### Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])
- Authors : Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, Jianfeng Gao
- Link : [http://arxiv.org/abs/2204.03610](http://arxiv.org/abs/2204.03610)
> ABSTRACT  :  Visual recognition is recently learned via either supervised learning on human-annotated image-label data or language-image contrastive learning with webly-crawled image-text pairs. While supervised learning may result in a more discriminative representation, language-image pretraining shows unprecedented zero-shot recognition capability, largely due to the different properties of data sources and learning objectives. In this work, we introduce a new formulation by combining the two data sources into a common image-text-label space. In this space, we propose a new learning paradigm, called Unified Contrastive Learning (UniCL) with a single learning objective to seamlessly prompt the synergy of two data types. Extensive experiments show that our UniCL is an effective way of learning semantically rich yet discriminative representations, universally for image recognition in zero-shot, linear-probe, fully finetuning and transfer learning scenarios. Particularly, it attains gains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over the language-image contrastive learning and supervised learning methods, respectively. In linear probe setting, it also boosts the performance over the two methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL stand-alone is a good learner on pure image-label data, rivaling the supervised learning methods across three image classification datasets and two types of vision backbones, ResNet and **Swin** Transformer. Code is available at https://github.com/microsoft/UniCL.  
### Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)
- Authors : Denys Rozumnyi, Vittorio Ferrari, Marc Pollefeys
- Link : [http://arxiv.org/abs/2111.14465](http://arxiv.org/abs/2111.14465)
> ABSTRACT  :  We propose a method for jointly estimating the 3D motion, 3D shape, and appearance of highly motion-blurred objects from a video. To this end, we model the blurred appearance of a fast moving object in a generative fashion by parametrizing its 3D position, rotation, velocity, acceleration, bounces, shape, and texture over the duration of a predefined time window spanning multiple frames. Using differentiable rendering, we are able to estimate all parameters by minimizing the pixel-wise reprojection error to the input video via backpropagating through a rendering pipeline that accounts for motion blur by averaging the graphics output over short time intervals. For that purpose, we also estimate the camera **exposure** gap time within the same optimization. To account for abrupt motion changes like bounces, we model the motion trajectory as a piece-wise polynomial, and we are able to estimate the specific time of the bounce at sub-frame accuracy. Experiments on established benchmark datasets demonstrate that our method outperforms previous methods for fast moving object deblurring and 3D reconstruction.  
### Dense Depth Priors for Neural Radiance Fields from Sparse Input Views. (arXiv:2112.03288v2 [cs.CV] UPDATED)
- Authors : Barbara Roessle, Ben Mildenhall, Matthias Nie
- Link : [http://arxiv.org/abs/2112.03288](http://arxiv.org/abs/2112.03288)
> ABSTRACT  :  Neural radiance fields (**NeRF**) encode a scene into a neural representation that enables photo-realistic rendering of novel views. However, a successful reconstruction from RGB images requires a large number of input views taken under static conditions - typically up to a few hundred images for room-size scenes. Our method aims to synthesize novel views of whole rooms from an order of magnitude fewer images. To this end, we leverage dense depth priors in order to constrain the **NeRF** optimization. First, we take advantage of the sparse depth data that is freely available from the structure from motion (SfM) preprocessing step used to estimate camera poses. Second, we use depth completion to convert these sparse points into dense depth maps and uncertainty estimates, which are used to guide **NeRF** optimization. Our method enables data-efficient novel view synthesis on challenging indoor scenes, using as few as 18 images for an entire scene.  
### DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v3 [cs.CV] UPDATED)
- Authors : Hao Zhang, Feng Li, Shilong Liu, **Lei Zhang**, Hang Su, Jun Zhu, Yeung Shum
- Link : [http://arxiv.org/abs/2203.03605](http://arxiv.org/abs/2203.03605)
> ABSTRACT  :  We present DINO (\textbf{D}ETR with \textbf{I}mproved de\textbf{N}oising anch\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $48.3$AP in $12$ epochs and $51.0$AP in $36$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\textbf{+4.9}$\textbf{AP} and $\textbf{+2.4}$\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a **Swin**L backbone, DINO obtains the best results on both COCO \texttt{val2017} ($\textbf{63.2}$\textbf{AP}) and \texttt{test-dev} (\textbf{$\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \url{https://github.com/IDEACVR/DINO}.  
### Squeeze**NeRF**: Further factorized Fast**NeRF** for memory-efficient inference. (arXiv:2204.02585v2 [cs.CV] UPDATED)
- Authors : Krishna Wadhwani, Tamaki Kojima
- Link : [http://arxiv.org/abs/2204.02585](http://arxiv.org/abs/2204.02585)
> ABSTRACT  :  Neural Radiance Fields (**NeRF**) has emerged as the state-of-the-art method for novel view generation of complex scenes, but is very slow during inference. Recently, there have been multiple works on speeding up **NeRF** inference, but the state of the art methods for real-time **NeRF** inference rely on caching the neural network output, which occupies several giga-bytes of disk space that limits their real-world applicability. As caching the neural network of original **NeRF** network is not feasible, Garbin et al. proposed "Fast**NeRF**" which factorizes the problem into 2 sub-networks - one which depends only on the 3D coordinate of a sample point and one which depends only on the 2D camera viewing direction. Although this factorization enables them to reduce the cache size and perform inference at over 200 frames per second, the memory overhead is still substantial. In this work, we propose Squeeze**NeRF**, which is more than 60 times memory-efficient than the sparse cache of Fast**NeRF** and is still able to render at more than 190 frames per second on a high spec GPU during inference.  
### Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v2 [cs.CV] UPDATED)
- Authors : Zhiqi Kang, Mostafa Sadeghi, Radu Horaud, Xavier Alameda
- Link : [http://arxiv.org/abs/2204.02810](http://arxiv.org/abs/2204.02810)
> ABSTRACT  :  Face frontalization consists of synthesizing a frontally-viewed face from an arbitrarily-viewed one. The main contribution of this paper is a frontalization methodology that preserves non-rigid facial deformations in order to boost the performance of visually assisted speech communication. The method alternates between the estimation of (i)~the rigid transformation (scale, rotation, and translation) and (ii)~the non-rigid deformation between an arbitrarily-viewed face and a face model. The method has two important merits: it can deal with non-Gaussian errors in the data and it incorporates a dynamical face deformation model. For that purpose, we use the generalized Student t-distribution in combination with a linear dynamic system in order to account for both rigid head motions and time-varying facial deformations caused by speech production. We propose to use the zero-mean normalized cross-correlation (ZNCC) score to evaluate the ability of the method to preserve facial expressions. The method is thoroughly evaluated and compared with several state of the art methods, either based on traditional geometric models or on deep learning. Moreover, we show that the method, when incorporated into deep learning pipelines, namely lip reading and speech **enhancement**, improves word recognition and speech intelligibilty scores by a considerable margin. Supplemental material is accessible at https://team.inria.fr/robotlearn/research/facefrontalization-benchmark/  
## eess.IV
---
### Evaluating Procedures for Establishing Generative Adversarial Network-based Stochastic Image Models in Medical Imaging. (arXiv:2204.03547v1 [eess.IV])
- Authors : Prabhat KC, Rongping Zeng
- Link : [http://arxiv.org/abs/2204.03547](http://arxiv.org/abs/2204.03547)
> ABSTRACT  :  Modern generative models, such as generative adversarial networks (GANs), hold tremendous promise for several areas of medical imaging, such as unconditional medical image synthesis, image **restoration**, reconstruction and translation, and optimization of imaging systems. However, procedures for establishing stochastic image models (SIMs) using GANs remain generic and do not address specific issues relevant to medical imaging. In this work, canonical SIMs that simulate realistic vessels in angiography images are employed to evaluate procedures for establishing SIMs using GANs. The GAN-based SIM is compared to the canonical SIM based on its ability to reproduce those statistics that are meaningful to the particular medically realistic SIM considered. It is shown that evaluating GANs using classical metrics and medically relevant metrics may lead to different conclusions about the fidelity of the trained GANs. This work highlights the need for the development of objective metrics for evaluating GANs.  
## cs.LG
---
### Knowledge Infused Decoding. (arXiv:2204.03084v1 [cs.CL])
- Authors : Ruibo Liu, Guoqing Zheng, Shashank Gupta, Radhika Gaonkar, Chongyang Gao, Soroush Vosoughi, Milad Shokouhi, Ahmed Hassan
- Link : [http://arxiv.org/abs/2204.03084](http://arxiv.org/abs/2204.03084)
> ABSTRACT  :  Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence, they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. Recent remedies to this problem focus on modifying either the pre-training or task fine-tuning objectives to incorporate knowledge, which normally require additional costly training or architecture modification of LMs for practical applications. We present Knowledge Infused Decoding (KID) -- a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. On six diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART) armed with KID outperform many task-optimized state-of-the-art models, and show particularly strong performance in few-shot scenarios over seven related knowledge-infusion techniques. Human evaluation confirms KID's ability to generate more relevant and factual language for the input context when compared with multiple baselines. Finally, KID also alleviates **exposure** bias and provides stable generation quality when generating longer sequences. Code for KID is available at https://github.com/microsoft/KID.  
### **Enhancement** on Model Interpretability and Sleep Stage Scoring Performance with A Novel Pipeline Based on Deep Neural Network. (arXiv:2204.03173v1 [cs.LG])
- Authors : Zheng Chen, Ziwei Yang, Ming Huang, Toshiyo Tamura, Naoaki Ono, MD Altaf, Shigehiko Kanaya
- Link : [http://arxiv.org/abs/2204.03173](http://arxiv.org/abs/2204.03173)
> ABSTRACT  :  Considering the natural frequency characteristics in sleep medicine, this paper first proposes a time-frequency framework for the representation learning of the electroencephalogram (EEG) following the definition of the American Academy of Sleep Medicine. To meet the temporal-random and transient nature of the defining characteristics of sleep stages, we further design a context-sensitive flexible pipeline that automatically adapts to the attributes of data itself. That is, the input EEG spectrogram is partitioned into a sequence of patches in the time and frequency axes, and then input to a delicate deep learning network for further representation learning to extract the stage-dependent features, which are used in the classification step finally. The proposed pipeline is validated against a large database, i.e., the Sleep Heart Health Study (SHHS), and the results demonstrate that the competitive performance for the wake, N2, and N3 stages outperforms the state-of-art works, with the F1 scores being 0.93, 0.88, and 0.87, respectively, and the proposed method has a high inter-rater reliability of 0.80 kappa. Importantly, we visualize the stage scoring process of the model decision with the Layer-wise Relevance Propagation (LRP) method, which shows that the proposed pipeline is more sensitive and perceivable in the decision-making process than the baseline pipelines. Therefore, the pipeline together with the LRP method can provide better model interpretability, which is important for clinical support.  
### FedCos: A Scene-adaptive Federated Optimization **Enhancement** for Performance Improvement. (arXiv:2204.03174v1 [cs.LG])
- Authors : Hao Zhang, Tingting Wu, Siyao Cheng, Jie Liu
- Link : [http://arxiv.org/abs/2204.03174](http://arxiv.org/abs/2204.03174)
> ABSTRACT  :  As an emerging technology, federated learning (FL) involves training machine learning models over distributed edge devices, which attracts sustained attention and has been extensively studied. However, the heterogeneity of client data severely degrades the performance of FL compared with that in centralized training. It causes the locally trained models of clients to move in different directions. On the one hand, it slows down or even stalls the global updates, leading to inefficient communication. On the other hand, it enlarges the distances between local models, resulting in an aggregated global model with poor performance. Fortunately, these shortcomings can be mitigated by reducing the angle between the directions that local models move in. Based on this fact, we propose FedCos, which reduces the directional inconsistency of local models by introducing a cosine-similarity penalty. It promotes the local model iterations towards an auxiliary global direction. Moreover, our approach is auto-adapt to various non-IID settings without an elaborate selection of hyperparameters. The experimental results show that FedCos outperforms the well-known baselines and can enhance them under a variety of FL scenes, including varying degrees of data heterogeneity, different number of participants, and cross-silo and cross-device settings. Besides, FedCos improves communication efficiency by 2 to 5 times. With the help of FedCos, multiple FL methods require significantly fewer communication rounds than before to obtain a model with comparable performance.  
### Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])
- Authors : Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, Jianfeng Gao
- Link : [http://arxiv.org/abs/2204.03610](http://arxiv.org/abs/2204.03610)
> ABSTRACT  :  Visual recognition is recently learned via either supervised learning on human-annotated image-label data or language-image contrastive learning with webly-crawled image-text pairs. While supervised learning may result in a more discriminative representation, language-image pretraining shows unprecedented zero-shot recognition capability, largely due to the different properties of data sources and learning objectives. In this work, we introduce a new formulation by combining the two data sources into a common image-text-label space. In this space, we propose a new learning paradigm, called Unified Contrastive Learning (UniCL) with a single learning objective to seamlessly prompt the synergy of two data types. Extensive experiments show that our UniCL is an effective way of learning semantically rich yet discriminative representations, universally for image recognition in zero-shot, linear-probe, fully finetuning and transfer learning scenarios. Particularly, it attains gains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over the language-image contrastive learning and supervised learning methods, respectively. In linear probe setting, it also boosts the performance over the two methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL stand-alone is a good learner on pure image-label data, rivaling the supervised learning methods across three image classification datasets and two types of vision backbones, ResNet and **Swin** Transformer. Code is available at https://github.com/microsoft/UniCL.  
### Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)
- Authors : Denys Rozumnyi, Vittorio Ferrari, Marc Pollefeys
- Link : [http://arxiv.org/abs/2111.14465](http://arxiv.org/abs/2111.14465)
> ABSTRACT  :  We propose a method for jointly estimating the 3D motion, 3D shape, and appearance of highly motion-blurred objects from a video. To this end, we model the blurred appearance of a fast moving object in a generative fashion by parametrizing its 3D position, rotation, velocity, acceleration, bounces, shape, and texture over the duration of a predefined time window spanning multiple frames. Using differentiable rendering, we are able to estimate all parameters by minimizing the pixel-wise reprojection error to the input video via backpropagating through a rendering pipeline that accounts for motion blur by averaging the graphics output over short time intervals. For that purpose, we also estimate the camera **exposure** gap time within the same optimization. To account for abrupt motion changes like bounces, we model the motion trajectory as a piece-wise polynomial, and we are able to estimate the specific time of the bounce at sub-frame accuracy. Experiments on established benchmark datasets demonstrate that our method outperforms previous methods for fast moving object deblurring and 3D reconstruction.  
## cs.AI
---
### FFC-SE: Fast Fourier Convolution for Speech **Enhancement**. (arXiv:2204.03042v1 [cs.SD])
- Authors : Ivan Shchekotov, Pavel Andreev, Oleg Ivanov, Aibek Alanov, Dmitry Vetrov
- Link : [http://arxiv.org/abs/2204.03042](http://arxiv.org/abs/2204.03042)
> ABSTRACT  :  Fast Fourier convolution (FFC) is the recently proposed neural operator showing promising performance in several computer vision problems. The FFC operator allows employing large receptive field operations within early layers of the neural network. It was shown to be especially helpful for inpainting of periodic structures which are common in audio processing. In this work, we design neural network architectures which adapt FFC for speech **enhancement**. We hypothesize that a large receptive field allows these networks to produce more coherent phases than vanilla convolutional models, and validate this hypothesis experimentally. We found that neural networks based on Fast Fourier convolution outperform analogous convolutional models and show better or comparable results with other speech **enhancement** baselines.  
### Knowledge Infused Decoding. (arXiv:2204.03084v1 [cs.CL])
- Authors : Ruibo Liu, Guoqing Zheng, Shashank Gupta, Radhika Gaonkar, Chongyang Gao, Soroush Vosoughi, Milad Shokouhi, Ahmed Hassan
- Link : [http://arxiv.org/abs/2204.03084](http://arxiv.org/abs/2204.03084)
> ABSTRACT  :  Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence, they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. Recent remedies to this problem focus on modifying either the pre-training or task fine-tuning objectives to incorporate knowledge, which normally require additional costly training or architecture modification of LMs for practical applications. We present Knowledge Infused Decoding (KID) -- a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. On six diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART) armed with KID outperform many task-optimized state-of-the-art models, and show particularly strong performance in few-shot scenarios over seven related knowledge-infusion techniques. Human evaluation confirms KID's ability to generate more relevant and factual language for the input context when compared with multiple baselines. Finally, KID also alleviates **exposure** bias and provides stable generation quality when generating longer sequences. Code for KID is available at https://github.com/microsoft/KID.  
### **Enhancement** on Model Interpretability and Sleep Stage Scoring Performance with A Novel Pipeline Based on Deep Neural Network. (arXiv:2204.03173v1 [cs.LG])
- Authors : Zheng Chen, Ziwei Yang, Ming Huang, Toshiyo Tamura, Naoaki Ono, MD Altaf, Shigehiko Kanaya
- Link : [http://arxiv.org/abs/2204.03173](http://arxiv.org/abs/2204.03173)
> ABSTRACT  :  Considering the natural frequency characteristics in sleep medicine, this paper first proposes a time-frequency framework for the representation learning of the electroencephalogram (EEG) following the definition of the American Academy of Sleep Medicine. To meet the temporal-random and transient nature of the defining characteristics of sleep stages, we further design a context-sensitive flexible pipeline that automatically adapts to the attributes of data itself. That is, the input EEG spectrogram is partitioned into a sequence of patches in the time and frequency axes, and then input to a delicate deep learning network for further representation learning to extract the stage-dependent features, which are used in the classification step finally. The proposed pipeline is validated against a large database, i.e., the Sleep Heart Health Study (SHHS), and the results demonstrate that the competitive performance for the wake, N2, and N3 stages outperforms the state-of-art works, with the F1 scores being 0.93, 0.88, and 0.87, respectively, and the proposed method has a high inter-rater reliability of 0.80 kappa. Importantly, we visualize the stage scoring process of the model decision with the Layer-wise Relevance Propagation (LRP) method, which shows that the proposed pipeline is more sensitive and perceivable in the decision-making process than the baseline pipelines. Therefore, the pipeline together with the LRP method can provide better model interpretability, which is important for clinical support.  
### FedCos: A Scene-adaptive Federated Optimization **Enhancement** for Performance Improvement. (arXiv:2204.03174v1 [cs.LG])
- Authors : Hao Zhang, Tingting Wu, Siyao Cheng, Jie Liu
- Link : [http://arxiv.org/abs/2204.03174](http://arxiv.org/abs/2204.03174)
> ABSTRACT  :  As an emerging technology, federated learning (FL) involves training machine learning models over distributed edge devices, which attracts sustained attention and has been extensively studied. However, the heterogeneity of client data severely degrades the performance of FL compared with that in centralized training. It causes the locally trained models of clients to move in different directions. On the one hand, it slows down or even stalls the global updates, leading to inefficient communication. On the other hand, it enlarges the distances between local models, resulting in an aggregated global model with poor performance. Fortunately, these shortcomings can be mitigated by reducing the angle between the directions that local models move in. Based on this fact, we propose FedCos, which reduces the directional inconsistency of local models by introducing a cosine-similarity penalty. It promotes the local model iterations towards an auxiliary global direction. Moreover, our approach is auto-adapt to various non-IID settings without an elaborate selection of hyperparameters. The experimental results show that FedCos outperforms the well-known baselines and can enhance them under a variety of FL scenes, including varying degrees of data heterogeneity, different number of participants, and cross-silo and cross-device settings. Besides, FedCos improves communication efficiency by 2 to 5 times. With the help of FedCos, multiple FL methods require significantly fewer communication rounds than before to obtain a model with comparable performance.  
### HIT-UAV: A High-altitude Infrared Thermal Dataset for Unmanned Aerial Vehicles. (arXiv:2204.03245v1 [cs.CV])
- Authors : Jiashun Suo, Tianyi Wang, Xingzhou Zhang, Haiyang Chen, Wei Zhou, Weisong Shi
- Link : [http://arxiv.org/abs/2204.03245](http://arxiv.org/abs/2204.03245)
> ABSTRACT  :  This paper presents a High-altitude infrared thermal dataset, HIT-UAV, for object detection applications on Unmanned Aerial Vehicles (UAVs). HIT-UAV contains 2898 infrared thermal images extracted from 43470 frames. These images are collected by UAV from schools, parking lots, roads, playgrounds, etc. HIT-UAV provides different flight data for each place, including flight altitude (from 60 to 130 meters), camera perspective (from 30 to 90 degrees), date, and daylight intensity. For each image, the HIT-UAV manual annotates object instances with two types of the bounding box (oriented and standard) to address the challenge that object instances have a significant overlap in aerial images. To the best of our knowledge, HIT-UAV is the first publicly available high-altitude infrared thermal UAV dataset for persons and vehicles detection. Moreover, we trained and evaluated the benchmark detection algorithms (YOLOv4 and YOLOv4-tiny) on HIT-UAV. Compared to the visual light dataset, the detection algorithms have excellent performance on HIT-UAV because the infrared thermal images do not contain a significant quantity of irrelevant information with detection objects. This indicates that infrared thermal datasets can significantly promote the development of object detection applications. We hope HIT-UAV contributes to UAV applications such as traffic surveillance and city monitoring at **night**. The dataset is available at https://github.com/suojiashun/HIT-UAV-Infrared-Thermal-Dataset.  
### Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])
- Authors : Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, Jianfeng Gao
- Link : [http://arxiv.org/abs/2204.03610](http://arxiv.org/abs/2204.03610)
> ABSTRACT  :  Visual recognition is recently learned via either supervised learning on human-annotated image-label data or language-image contrastive learning with webly-crawled image-text pairs. While supervised learning may result in a more discriminative representation, language-image pretraining shows unprecedented zero-shot recognition capability, largely due to the different properties of data sources and learning objectives. In this work, we introduce a new formulation by combining the two data sources into a common image-text-label space. In this space, we propose a new learning paradigm, called Unified Contrastive Learning (UniCL) with a single learning objective to seamlessly prompt the synergy of two data types. Extensive experiments show that our UniCL is an effective way of learning semantically rich yet discriminative representations, universally for image recognition in zero-shot, linear-probe, fully finetuning and transfer learning scenarios. Particularly, it attains gains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over the language-image contrastive learning and supervised learning methods, respectively. In linear probe setting, it also boosts the performance over the two methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL stand-alone is a good learner on pure image-label data, rivaling the supervised learning methods across three image classification datasets and two types of vision backbones, ResNet and **Swin** Transformer. Code is available at https://github.com/microsoft/UniCL.  
### Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)
- Authors : Denys Rozumnyi, Vittorio Ferrari, Marc Pollefeys
- Link : [http://arxiv.org/abs/2111.14465](http://arxiv.org/abs/2111.14465)
> ABSTRACT  :  We propose a method for jointly estimating the 3D motion, 3D shape, and appearance of highly motion-blurred objects from a video. To this end, we model the blurred appearance of a fast moving object in a generative fashion by parametrizing its 3D position, rotation, velocity, acceleration, bounces, shape, and texture over the duration of a predefined time window spanning multiple frames. Using differentiable rendering, we are able to estimate all parameters by minimizing the pixel-wise reprojection error to the input video via backpropagating through a rendering pipeline that accounts for motion blur by averaging the graphics output over short time intervals. For that purpose, we also estimate the camera **exposure** gap time within the same optimization. To account for abrupt motion changes like bounces, we model the motion trajectory as a piece-wise polynomial, and we are able to estimate the specific time of the bounce at sub-frame accuracy. Experiments on established benchmark datasets demonstrate that our method outperforms previous methods for fast moving object deblurring and 3D reconstruction.  
# Paper List
---
## cs.CV
---
**134** new papers in cs.CV:-) 
1. Follow My Eye: Using Gaze to Supervise Computer-Aided Diagnosis. (arXiv:2204.02976v1 [eess.IV])
2. Multi-Scale Memory-Based Video Deblurring. (arXiv:2204.02977v1 [eess.IV])
3. Analysis of Different Losses for Deep Learning Image Colorization. (arXiv:2204.02980v1 [cs.CV])
4. EfficientCellSeg: Efficient Volumetric Cell Segmentation Using Context Aware Pseudocoloring. (arXiv:2204.03014v1 [eess.IV])
5. Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency. (arXiv:2204.03017v1 [cs.CV])
6. Statistical Model Criticism of Variational Auto-Encoders. (arXiv:2204.03030v1 [cs.LG])
7. DSGN++: Exploiting Visual-Spatial Relation forStereo-based 3D Detectors. (arXiv:2204.03039v1 [cs.CV])
8. Fusing finetuned models for better pretraining. (arXiv:2204.03044v1 [cs.CL])
9. Thermal to Visible Image Synthesis under Atmospheric Turbulence. (arXiv:2204.03057v1 [cs.CV])
10. Late multimodal fusion for image and audio music transcription. (arXiv:2204.03063v1 [cs.MM])
11. The Self-Optimal-Transport Feature Transform. (arXiv:2204.03065v1 [cs.CV])
12. OSCARS: An Outlier-Sensitive Content-Based Radiography Retrieval System. (arXiv:2204.03074v1 [cs.CV])
13. Instance Segmentation of Unlabeled Modalities via Cyclic Segmentation GAN. (arXiv:2204.03082v1 [cs.CV])
14. Audio-Visual Person-of-Interest DeepFake Detection. (arXiv:2204.03083v1 [cs.CV])
15. Hierarchical Self-supervised Representation Learning for Movie Understanding. (arXiv:2204.03101v1 [cs.CV])
16. AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis. (arXiv:2204.03105v1 [cs.CV])
17. UIGR: Unified Interactive Garment Retrieval. (arXiv:2204.03111v1 [cs.CV])
18. AutoCOR: Autonomous Condylar Offset Ratio Calculator on TKA-Postoperative Lateral Knee X-ray. (arXiv:2204.03120v1 [cs.CV])
19. DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects. (arXiv:2204.03139v1 [cs.RO])
20. Adversarial Machine Learning Attacks Against Video Anomaly Detection Systems. (arXiv:2204.03141v1 [cs.CV])
21. Exploring Cross-Domain Pretrained Model for Hyperspectral Image Classification. (arXiv:2204.03144v1 [cs.CV])
22. Just-Noticeable-Difference Based Edge Map Quality Measure. (arXiv:2204.03155v1 [cs.CV])
23. Flexible Sampling for Long-tailed Skin Lesion Classification. (arXiv:2204.03161v1 [cs.CV])
24. Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v1 [cs.CV])
25. Low-Dose CT Denoising via Sinogram Inner-Structure Transformer. (arXiv:2204.03163v1 [eess.IV])
26. MDA GAN: Adversarial-Learning-based 3-D Seismic Data Interpolation and Reconstruction for Complex Missing. (arXiv:2204.03197v1 [physics.geo-ph])
27. Convolutional Neural Network for Early Pulmonary Embolism Detection via Computed Tomography Pulmonary Angiography. (arXiv:2204.03204v1 [eess.IV])
28. L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation. (arXiv:2204.03206v1 [cs.CV])
29. MC-UNet Multi-module Concatenation based on U-shape Network for Retinal Blood Vessels Segmentation. (arXiv:2204.03213v1 [eess.IV])
30. What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning. (arXiv:2204.03230v1 [cs.LG])
31. HIT-UAV: A High-altitude Infrared Thermal Dataset for Unmanned Aerial Vehicles. (arXiv:2204.03245v1 [cs.CV])
32. Pan-cancer computational histopathology reveals tumor mutational burden status through weakly-supervised deep learning. (arXiv:2204.03257v1 [cs.CV])
33. Context-Sensitive Temporal Feature Learning for Gait Recognition. (arXiv:2204.03270v1 [cs.CV])
34. Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge TPU. (arXiv:2204.03296v1 [cs.CV])
35. Swarm behavior tracking based on a deep vision algorithm. (arXiv:2204.03319v1 [cs.CV])
36. Multi-Sample $\zeta$-mixup: Richer, More Realistic Synthetic Samples from a $p$-Series Interpolant. (arXiv:2204.03323v1 [cs.LG])
37. A Comprehensive Review of Sign Language Recognition: Different Types, Modalities, and Datasets. (arXiv:2204.03328v1 [cs.CV])
38. Coarse-to-Fine Feature Mining for Video Semantic Segmentation. (arXiv:2204.03330v1 [cs.CV])
39. Sparse Optical Flow-Based Line Feature Tracking. (arXiv:2204.03331v1 [cs.CV])
40. Learning to Sieve: Prediction of Grading Curves from Images of Concrete Aggregate. (arXiv:2204.03333v1 [cs.CV])
41. PSTR: End-to-End One-Step Person Search With Transformers. (arXiv:2204.03340v1 [cs.CV])
42. Implementing a Real-Time, YOLOv5 based Social Distancing Measuring System for Covid-19. (arXiv:2204.03350v1 [cs.CV])
43. Learning Online Multi-Sensor Depth Fusion. (arXiv:2204.03353v1 [cs.CV])
44. Event Transformer. A sparse-aware solution for efficient event data processing. (arXiv:2204.03355v1 [cs.CV])
45. ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v1 [cs.CV])
46. Detection of Distracted Driver using Convolution Neural Network. (arXiv:2204.03371v1 [cs.CV])
47. HunYuan_tvr for Text-Video Retrivial. (arXiv:2204.03382v1 [cs.CV])
48. Surface Vision Transformers: Flexible Attention-Based Modelling of Biomedical Surfaces. (arXiv:2204.03408v1 [eess.IV])
49. Incremental Prototype Prompt-tuning with Pre-trained Representation for Class Incremental Learning. (arXiv:2204.03410v1 [cs.CV])
50. Task-Aware Active Learning for Endoscopic Image Analysis. (arXiv:2204.03440v1 [cs.CV])
51. Deep Visual Geo-localization Benchmark. (arXiv:2204.03444v1 [cs.CV])
52. Video Diffusion Models. (arXiv:2204.03458v1 [cs.CV])
53. Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results. (arXiv:2204.03475v1 [cs.CV])
54. ProbNVS: Fast Novel View Synthesis with Learned Probability-Guided Sampling. (arXiv:2204.03476v1 [cs.CV])
55. Optimizing the Long-Term Behaviour of Deep Reinforcement Learning for Pushing and Grasping. (arXiv:2204.03487v1 [cs.LG])
56. Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation. (arXiv:2204.03500v1 [cs.LG])
57. Many-to-many Splatting for Efficient Video Frame Interpolation. (arXiv:2204.03513v1 [cs.CV])
58. Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale. (arXiv:2204.03514v1 [cs.AI])
59. Visualizing Deep Neural Networks with Topographic Activation Maps. (arXiv:2204.03528v1 [cs.LG])
60. Efficient Multiscale Object-based Superpixel Framework. (arXiv:2204.03533v1 [cs.CV])
61. End-to-End Zero-Shot HOI Detection via Vision and Language Knowledge Distillation. (arXiv:2204.03541v1 [cs.CV])
62. Evaluating Procedures for Establishing Generative Adversarial Network-based Stochastic Image Models in Medical Imaging. (arXiv:2204.03547v1 [eess.IV])
63. Practical Digital Disguises: Leveraging Face Swaps to Protect Patient Privacy. (arXiv:2204.03559v1 [cs.CV])
64. Emotional Speech Recognition with Pre-trained Deep Visual Models. (arXiv:2204.03561v1 [cs.CV])
65. Explicit and Implicit Pattern Relation Analysis for Discovering Actionable Negative Sequences. (arXiv:2204.03571v1 [cs.AI])
66. A Pathology-Based Machine Learning Method to Assist in Epithelial Dysplasia Diagnosis. (arXiv:2204.03572v1 [eess.IV])
67. Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v1 [cs.LG])
68. AutoRF: Learning 3D Object Radiance Fields from Single View Observations. (arXiv:2204.03593v1 [cs.CV])
69. Pin the Memory: Learning to Generalize Semantic Segmentation. (arXiv:2204.03609v1 [cs.CV])
70. Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])
71. Pneumonia Detection in Chest X-Rays using Neural Networks. (arXiv:2204.03618v1 [eess.IV])
72. The Effects of Regularization and Data Augmentation are Class Dependent. (arXiv:2204.03632v1 [cs.LG])
73. Class-Incremental Learning with Strong Pre-trained Models. (arXiv:2204.03634v1 [cs.CV])
74. Zero-Shot Category-Level Object Pose Estimation. (arXiv:2204.03635v1 [cs.CV])
75. SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation. (arXiv:2204.03636v1 [cs.CV])
76. Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. (arXiv:2204.03638v1 [cs.CV])
77. Equivariance Discovery by Learned Parameter-Sharing. (arXiv:2204.03640v1 [cs.LG])
78. Unsupervised Image-to-Image Translation with Generative Prior. (arXiv:2204.03641v1 [cs.CV])
79. Pre-train, Self-train, Distill: A simple recipe for Supersizing 3D Reconstruction. (arXiv:2204.03642v1 [cs.CV])
80. Total Variation Optimization Layers for Computer Vision. (arXiv:2204.03643v1 [cs.CV])
81. DaViT: Dual Attention Vision Transformers. (arXiv:2204.03645v1 [cs.CV])
82. FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment. (arXiv:2204.03646v1 [cs.CV])
83. Adapting CLIP For Phrase Localization Without Further Training. (arXiv:2204.03647v1 [cs.CV])
84. SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage. (arXiv:2204.03648v1 [cs.CV])
85. Unsupervised Prompt Learning for Vision-Language Models. (arXiv:2204.03649v1 [cs.CV])
86. Generating Hard Examples for Pixel-wise Classification. (arXiv:1812.05447v3 [cs.CV] UPDATED)
87. Semantic Driven Multi-Camera Pedestrian Detection. (arXiv:1812.10779v3 [cs.CV] UPDATED)
88. GGNN: Graph-based GPU Nearest Neighbor Search. (arXiv:1912.01059v4 [cs.CV] UPDATED)
89. DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator Search. (arXiv:2011.02166v2 [cs.CV] UPDATED)
90. Physics-informed neural networks for myocardial perfusion MRI quantification. (arXiv:2011.12844v3 [eess.IV] UPDATED)
91. Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v4 [cs.LG] UPDATED)
92. Semi-Supervised Federated Peer Learning for Skin Lesion Classification. (arXiv:2103.03703v4 [cs.CV] UPDATED)
93. Multiscale Clustering of Hyperspectral Images Through Spectral-Spatial Diffusion Geometry. (arXiv:2103.15783v2 [cs.LG] UPDATED)
94. A tutorial on $\mathbf{SE}(3)$ transformation parameterizations and on-manifold optimization. (arXiv:2103.15980v2 [cs.RO] UPDATED)
95. Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v3 [cs.LG] UPDATED)
96. Scarce Data Driven Deep Learning of Drones via Generalized Data Distribution Space. (arXiv:2108.08244v2 [cs.CV] UPDATED)
97. Unsupervised Cycle-consistent Generative Adversarial Networks for Pan-sharpening. (arXiv:2109.09395v3 [cs.CV] UPDATED)
98. Machine Learning based Medical Image Deepfake Detection: A Comparative Study. (arXiv:2109.12800v2 [cs.CV] UPDATED)
99. Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v5 [cs.CV] UPDATED)
100. ByteTrack: Multi-Object Tracking by Associating Every Detection Box. (arXiv:2110.06864v3 [cs.CV] UPDATED)
101. Unsupervised Lightweight Single Object Tracking with UHP-SOT++. (arXiv:2111.07548v2 [cs.CV] UPDATED)
102. StylePart: Image-based Shape Part Manipulation. (arXiv:2111.10520v3 [cs.CV] UPDATED)
103. Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)
104. SPIN: Simplifying Polar Invariance for Neural networks Application to vision-based irradiance forecasting. (arXiv:2111.14507v2 [cs.CV] UPDATED)
105. Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation. (arXiv:2111.14826v2 [cs.CV] UPDATED)
106. Dense Depth Priors for Neural Radiance Fields from Sparse Input Views. (arXiv:2112.03288v2 [cs.CV] UPDATED)
107. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v2 [cs.CV] UPDATED)
108. An effective coaxiality measurement for twist drill based on line structured light sensor. (arXiv:2112.09873v2 [cs.CV] UPDATED)
109. Raw High-Definition Radar for Multi-Task Learning. (arXiv:2112.10646v2 [cs.CV] UPDATED)
110. Metrics for saliency map evaluation of deep learning explanation methods. (arXiv:2201.13291v2 [cs.CV] UPDATED)
111. Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v2 [eess.IV] UPDATED)
112. Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v3 [cs.CV] UPDATED)
113. Discriminability-enforcing loss to improve representation learning. (arXiv:2202.07073v2 [cs.CV] UPDATED)
114. ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v2 [cs.CV] UPDATED)
115. Realistic Blur Synthesis for Learning Image Deblurring. (arXiv:2202.08771v2 [cs.CV] UPDATED)
116. DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v3 [cs.CV] UPDATED)
117. Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v2 [eess.IV] UPDATED)
118. CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data. (arXiv:2203.09475v2 [cs.RO] UPDATED)
119. Incremental Few-Shot Learning via Implanting and Compressing. (arXiv:2203.10297v2 [cs.CV] UPDATED)
120. DAN: a Segmentation-free Document Attention Network for Handwritten Document Recognition. (arXiv:2203.12273v2 [cs.CV] UPDATED)
121. AI-augmented histopathologic review using image analysis to optimize DNA yield and tumor purity from FFPE slides. (arXiv:2203.13948v2 [cs.CV] UPDATED)
122. SimT: Handling Open-set Noise for Domain Adaptive Semantic Segmentation. (arXiv:2203.15202v2 [cs.CV] UPDATED)
123. Pay Attention to Hidden States for Video Deblurring: Ping-Pong Recurrent Neural Networks and Selective Non-Local Attention. (arXiv:2203.16063v2 [cs.CV] UPDATED)
124. CADG: A Model Based on Cross Attention for Domain Generalization. (arXiv:2203.17067v2 [cs.CV] UPDATED)
125. Do learned representations respect causal relationships?. (arXiv:2204.00762v2 [cs.CV] UPDATED)
126. SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images. (arXiv:2204.00790v2 [cs.CV] UPDATED)
127. Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images. (arXiv:2204.00840v2 [cs.CV] UPDATED)
128. An Exploration of Active Learning for Affective Digital Phenotyping. (arXiv:2204.01915v2 [cs.LG] UPDATED)
129. PSDoodle: Searching for App Screens via Interactive Sketching. (arXiv:2204.01968v2 [cs.CV] UPDATED)
130. Rethinking Visual Geo-localization for Large-Scale Applications. (arXiv:2204.02287v2 [cs.CV] UPDATED)
131. Squeeze**NeRF**: Further factorized Fast**NeRF** for memory-efficient inference. (arXiv:2204.02585v2 [cs.CV] UPDATED)
132. Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification. (arXiv:2204.02611v2 [cs.CV] UPDATED)
133. Towards An End-to-End Framework for Flow-Guided Video Inpainting. (arXiv:2204.02663v2 [eess.IV] UPDATED)
134. Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v2 [cs.CV] UPDATED)
## eess.IV
---
**23** new papers in eess.IV:-) 
1. Holistic Fault Detection and Diagnosis System in Imbalanced, Scarce, Multi-Domain (ISMD) Data Setting for Component-Level Prognostics and Health Management (PHM). (arXiv:2204.02969v1 [eess.SP])
2. Follow My Eye: Using Gaze to Supervise Computer-Aided Diagnosis. (arXiv:2204.02976v1 [eess.IV])
3. Multi-Scale Memory-Based Video Deblurring. (arXiv:2204.02977v1 [eess.IV])
4. EfficientCellSeg: Efficient Volumetric Cell Segmentation Using Context Aware Pseudocoloring. (arXiv:2204.03014v1 [eess.IV])
5. Low-Dose CT Denoising via Sinogram Inner-Structure Transformer. (arXiv:2204.03163v1 [eess.IV])
6. Convolutional Neural Network for Early Pulmonary Embolism Detection via Computed Tomography Pulmonary Angiography. (arXiv:2204.03204v1 [eess.IV])
7. MC-UNet Multi-module Concatenation based on U-shape Network for Retinal Blood Vessels Segmentation. (arXiv:2204.03213v1 [eess.IV])
8. Texture-Dependent Frequency Selective Reconstruction of Non-Regularly Sampled Images. (arXiv:2204.03261v1 [eess.IV])
9. Dynamic Non-Regular Sampling Sensor Using Frequency Selective Reconstruction. (arXiv:2204.03268v1 [eess.IV])
10. Recursive Frequency Selective Reconstruction of Non-Regularly Sampled Video Data. (arXiv:2204.03277v1 [eess.IV])
11. Surface Vision Transformers: Flexible Attention-Based Modelling of Biomedical Surfaces. (arXiv:2204.03408v1 [eess.IV])
12. Evaluating Procedures for Establishing Generative Adversarial Network-based Stochastic Image Models in Medical Imaging. (arXiv:2204.03547v1 [eess.IV])
13. A Pathology-Based Machine Learning Method to Assist in Epithelial Dysplasia Diagnosis. (arXiv:2204.03572v1 [eess.IV])
14. Pneumonia Detection in Chest X-Rays using Neural Networks. (arXiv:2204.03618v1 [eess.IV])
15. Physics-informed neural networks for myocardial perfusion MRI quantification. (arXiv:2011.12844v3 [eess.IV] UPDATED)
16. An Overview on Artificial Intelligence Techniques for Diagnosis of Schizophrenia Based on Magnetic Resonance Imaging Modalities: Methods, Challenges, and Future Works. (arXiv:2103.03081v2 [cs.LG] UPDATED)
17. SplitAVG: A heterogeneity-aware federated deep learning method for medical imaging. (arXiv:2107.02375v4 [cs.LG] UPDATED)
18. Unsupervised Cycle-consistent Generative Adversarial Networks for Pan-sharpening. (arXiv:2109.09395v3 [cs.CV] UPDATED)
19. Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v5 [cs.CV] UPDATED)
20. Raw High-Definition Radar for Multi-Task Learning. (arXiv:2112.10646v2 [cs.CV] UPDATED)
21. Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v2 [eess.IV] UPDATED)
22. Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v2 [eess.IV] UPDATED)
23. Towards An End-to-End Framework for Flow-Guided Video Inpainting. (arXiv:2204.02663v2 [eess.IV] UPDATED)
## cs.LG
---
**151** new papers in cs.LG:-) 
1. Multi-task nonparallel support vector machine for classification. (arXiv:2204.02972v1 [cs.LG])
2. Incremental Unsupervised Feature Selection for Dynamic Incomplete Multi-view Data. (arXiv:2204.02973v1 [cs.LG])
3. End-To-End Optimization of Online Neural Network-supported Two-Stage Dereverberation for Hearing Devices. (arXiv:2204.02978v1 [eess.AS])
4. Federated Learning for Distributed Spectrum Sensing in NextG Communication Networks. (arXiv:2204.03027v1 [cs.NI])
5. Statistical Model Criticism of Variational Auto-Encoders. (arXiv:2204.03030v1 [cs.LG])
6. SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis. (arXiv:2204.03040v1 [cs.SD])
7. Fusing finetuned models for better pretraining. (arXiv:2204.03044v1 [cs.CL])
8. Perceive, Represent, Generate: Translating Multimodal Information to Robotic Motion Trajectories. (arXiv:2204.03051v1 [cs.RO])
9. Standardized feature extraction from pairwise conflicts applied to the train rescheduling problem. (arXiv:2204.03061v1 [cs.LG])
10. Graph Neural Networks Designed for Different Graph Types: A Survey. (arXiv:2204.03080v1 [cs.LG])
11. Knowledge Infused Decoding. (arXiv:2204.03084v1 [cs.CL])
12. Data Justice Stories: A Repository of Case Studies. (arXiv:2204.03100v1 [cs.CY])
13. AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis. (arXiv:2204.03105v1 [cs.CV])
14. Deep transfer learning for system identification using long short-term memory neural networks. (arXiv:2204.03125v1 [eess.SY])
15. First-Order Algorithms for Nonlinear Generalized Nash Equilibrium Problems. (arXiv:2204.03132v1 [math.OC])
16. DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects. (arXiv:2204.03139v1 [cs.RO])
17. Learning and Transferring Value Function for Robot Exploration in Subterranean Environments. (arXiv:2204.03140v1 [cs.RO])
18. DeepTensor: Low-Rank Tensor Decomposition with Deep Network Priors. (arXiv:2204.03145v1 [stat.AP])
19. Optimization Models and Interpretations for Three Types of Adversarial Perturbations against Support Vector Machines. (arXiv:2204.03154v1 [cs.LG])
20. **Enhancement** on Model Interpretability and Sleep Stage Scoring Performance with A Novel Pipeline Based on Deep Neural Network. (arXiv:2204.03173v1 [cs.LG])
21. FedCos: A Scene-adaptive Federated Optimization **Enhancement** for Performance Improvement. (arXiv:2204.03174v1 [cs.LG])
22. Distributed Statistical Min-Max Learning in the Presence of Byzantine Agents. (arXiv:2204.03187v1 [cs.LG])
23. MultiAuto-DeepONet: A Multi-resolution Autoencoder DeepONet for Nonlinear Dimension Reduction, Uncertainty Quantification and Operator Learning of Forward and Inverse Stochastic Problems. (arXiv:2204.03193v1 [stat.ML])
24. A Joint Learning Approach for Semi-supervised Neural Topic Modeling. (arXiv:2204.03208v1 [cs.IR])
25. Transformer-Based Language Models for Software Vulnerability Detection: Performance, Model's Security and Platforms. (arXiv:2204.03214v1 [cs.CR])
26. Neural Implicit Flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data. (arXiv:2204.03216v1 [cs.LG])
27. DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores. (arXiv:2204.03219v1 [eess.AS])
28. Explicit Feature Interaction-aware Graph Neural Networks. (arXiv:2204.03225v1 [cs.LG])
29. Accelerating Attention through Gradient-Based Learned Runtime Pruning. (arXiv:2204.03227v1 [cs.CL])
30. What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning. (arXiv:2204.03230v1 [cs.LG])
31. Learning to Solve Travelling Salesman Problem with Hardness-adaptive Curriculum. (arXiv:2204.03236v1 [cs.LG])
32. Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators. (arXiv:2204.03243v1 [cs.CL])
33. Composite Spatial Monte Carlo Integration Based on Generalized Least Squares. (arXiv:2204.03248v1 [stat.CO])
34. mulEEG: A Multi-View Representation Learning on EEG Signals. (arXiv:2204.03272v1 [cs.LG])
35. PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v1 [cs.LG])
36. Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation. (arXiv:2204.03293v1 [cs.SE])
37. Federated Learning from Only Unlabeled Data with Class-Conditional-Sharing Clients. (arXiv:2204.03304v1 [cs.LG])
38. MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids. (arXiv:2204.03305v1 [eess.AS])
39. MTI-Net: A Multi-Target Speech Intelligibility Prediction Model. (arXiv:2204.03310v1 [eess.AS])
40. Using Decision Tree as Local Interpretable Model in Autoencoder-based LIME. (arXiv:2204.03321v1 [cs.LG])
41. Multi-Sample $\zeta$-mixup: Richer, More Realistic Synthetic Samples from a $p$-Series Interpolant. (arXiv:2204.03323v1 [cs.LG])
42. Enabling Deep Learning for All-in EDGE paradigm. (arXiv:2204.03326v1 [cs.LG])
43. Robust and Explainable Autoencoders for Unsupervised Time Series Outlier Detection---Extended Version. (arXiv:2204.03341v1 [cs.LG])
44. Domain Adaptation for Time-Series Classification to Mitigate Covariate Shift. (arXiv:2204.03342v1 [cs.LG])
45. Inference over radiative transfer models using variational and expectation maximization methods. (arXiv:2204.03346v1 [cs.LG])
46. Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes. (arXiv:2204.03376v1 [cs.LG])
47. Correcting Misproducted Speech using Spectrogram Inpainting. (arXiv:2204.03379v1 [eess.AS])
48. Continual Inference: A Library for Efficient Online Inference with Deep Neural Networks in PyTorch. (arXiv:2204.03418v1 [cs.LG])
49. Self supervised learning for robust voice cloning. (arXiv:2204.03421v1 [cs.SD])
50. Energy-Efficient Adaptive Machine Learning on IoT End-Nodes With Class-Dependent Confidence. (arXiv:2204.03431v1 [cs.LG])
51. Machine Learning-Enabled IoT Security: Open Issues and Challenges Under Advanced Persistent Threats. (arXiv:2204.03433v1 [cs.CR])
52. Half-sibling regression meets exoplanet imaging: PSF modeling and subtraction using a flexible, domain knowledge-driven, causal framework. (arXiv:2204.03439v1 [astro-ph.IM])
53. Few-Shot Forecasting of Time-Series with Heterogeneous Channels. (arXiv:2204.03456v1 [cs.LG])
54. Video Diffusion Models. (arXiv:2204.03458v1 [cs.CV])
55. BERTuit: Understanding Spanish language in Twitter through a native transformer. (arXiv:2204.03465v1 [cs.CL])
56. Jacobian Norm for Unsupervised Source-Free Domain Adaptation. (arXiv:2204.03467v1 [cs.LG])
57. DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v1 [cs.AI])
58. Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results. (arXiv:2204.03475v1 [cs.CV])
59. Delta Keyword Transformer: Bringing Transformers to the Edge through Dynamically Pruned Multi-Head Self-Attention. (arXiv:2204.03479v1 [cs.CL])
60. Optimizing the Long-Term Behaviour of Deep Reinforcement Learning for Pushing and Grasping. (arXiv:2204.03487v1 [cs.LG])
61. Position-based Prompting for Health Outcome Generation. (arXiv:2204.03489v1 [cs.CL])
62. Covariance matrix preparation for quantum principal component analysis. (arXiv:2204.03495v1 [quant-ph])
63. Generalised Latent Assimilation in Heterogeneous Reduced Spaces with Machine Learning Surrogate Models. (arXiv:2204.03497v1 [cs.LG])
64. On the Effectiveness of Pretrained Models for API Learning. (arXiv:2204.03498v1 [cs.SE])
65. Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation. (arXiv:2204.03500v1 [cs.LG])
66. Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers. (arXiv:2204.03503v1 [cs.CL])
67. AI-aided Traffic Control Scheme for M2M Communications in the Internet of Vehicles. (arXiv:2204.03504v1 [cs.NI])
68. Interval Bound Propagation--aided Few-shot Learning. (arXiv:2204.03511v1 [cs.LG])
69. Distributed Reinforcement Learning for Robot Teams: A Review. (arXiv:2204.03516v1 [cs.RO])
70. Temporal Alignment for History Representation in Reinforcement Learning. (arXiv:2204.03525v1 [cs.LG])
71. Visualizing Deep Neural Networks with Topographic Activation Maps. (arXiv:2204.03528v1 [cs.LG])
72. FedADMM: A Robust Federated Deep Learning Framework with Adaptivity to System Heterogeneity. (arXiv:2204.03529v1 [cs.LG])
73. RF Signal Transformation and Classification using Deep Neural Networks. (arXiv:2204.03564v1 [eess.SP])
74. Adaptive Spike-Like Representation of EEG Signals for Sleep Stages Scoring. (arXiv:2204.03565v1 [eess.SP])
75. Faster algorithms for learning to link, align sequences, and price two-part tariffs. (arXiv:2204.03569v1 [cs.DS])
76. Improving Urban Mobility: using artificial intelligence and new technologies to connect supply and demand. (arXiv:2204.03570v1 [cs.CY])
77. A Pathology-Based Machine Learning Method to Assist in Epithelial Dysplasia Diagnosis. (arXiv:2204.03572v1 [eess.IV])
78. An optimized hybrid solution for IoT based lifestyle disease classification using stress data. (arXiv:2204.03573v1 [eess.SP])
79. Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v1 [cs.LG])
80. Risk-based regulation for all: The need and a method for a wide adoption solution for data-driven inspection targeting. (arXiv:2204.03583v1 [cs.LG])
81. Heterogeneous Target Speech Separation. (arXiv:2204.03594v1 [cs.SD])
82. Imitating, Fast and Slow: Robust learning from demonstrations via decision-time planning. (arXiv:2204.03597v1 [cs.LG])
83. Pin the Memory: Learning to Generalize Semantic Segmentation. (arXiv:2204.03609v1 [cs.CV])
84. Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])
85. Modeling Label Correlations for Second-Order Semantic Dependency Parsing with Mean-Field Inference. (arXiv:2204.03619v1 [cs.CL])
86. Security Aspects of Quantum Machine Learning: Opportunities, Threats and Defenses. (arXiv:2204.03625v1 [cs.CR])
87. The Effects of Regularization and Data Augmentation are Class Dependent. (arXiv:2204.03632v1 [cs.LG])
88. Class-Incremental Learning with Strong Pre-trained Models. (arXiv:2204.03634v1 [cs.CV])
89. Equivariance Discovery by Learned Parameter-Sharing. (arXiv:2204.03640v1 [cs.LG])
90. Unsupervised Image-to-Image Translation with Generative Prior. (arXiv:2204.03641v1 [cs.CV])
91. Mo\"ET: Mixture of Expert Trees and its Application to Verifiable Reinforcement Learning. (arXiv:1906.06717v4 [cs.LG] UPDATED)
92. Bidimensional linked matrix factorization for pan-omics pan-cancer analysis. (arXiv:2002.02601v2 [stat.ML] UPDATED)
93. Differentially Private Set Union. (arXiv:2002.09745v2 [cs.CR] UPDATED)
94. Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning. (arXiv:2004.10888v6 [cs.LG] UPDATED)
95. Towards Improving Selective Prediction Ability of NLP Systems. (arXiv:2008.09371v3 [cs.CL] UPDATED)
96. Joint Adaptive Graph and Structured Sparsity Regularization for Unsupervised Feature Selection. (arXiv:2010.05454v3 [cs.LG] UPDATED)
97. DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator Search. (arXiv:2011.02166v2 [cs.CV] UPDATED)
98. RL-QN: A Reinforcement Learning Framework for Optimal Control of Queueing Systems. (arXiv:2011.07401v2 [cs.PF] UPDATED)
99. GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks. (arXiv:2011.11048v6 [cs.HC] UPDATED)
100. Policy Mirror Descent for Reinforcement Learning: Linear Convergence, New Sampling Complexity, and Generalized Problem Classes. (arXiv:2102.00135v6 [cs.LG] UPDATED)
101. Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v4 [cs.LG] UPDATED)
102. An Overview on Artificial Intelligence Techniques for Diagnosis of Schizophrenia Based on Magnetic Resonance Imaging Modalities: Methods, Challenges, and Future Works. (arXiv:2103.03081v2 [cs.LG] UPDATED)
103. Covariate-assisted Sparse Tensor Completion. (arXiv:2103.06428v3 [stat.ML] UPDATED)
104. Multiscale Clustering of Hyperspectral Images Through Spectral-Spatial Diffusion Geometry. (arXiv:2103.15783v2 [cs.LG] UPDATED)
105. Fast Design Space Exploration of Nonlinear Systems: Part I. (arXiv:2104.01747v7 [cs.LG] UPDATED)
106. Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews. (arXiv:2104.05861v3 [cs.SE] UPDATED)
107. Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning. (arXiv:2104.08676v2 [cs.CL] UPDATED)
108. Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. (arXiv:2106.02190v5 [cs.LG] UPDATED)
109. Amortized Auto-Tuning: Cost-Efficient Bayesian Transfer Optimization for Hyperparameter Recommendation. (arXiv:2106.09179v2 [cs.LG] UPDATED)
110. Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v3 [cs.LG] UPDATED)
111. SplitAVG: A heterogeneity-aware federated deep learning method for medical imaging. (arXiv:2107.02375v4 [cs.LG] UPDATED)
112. Improving Cooperative Game Theory-based Data Valuation via Data Utility Learning. (arXiv:2107.06336v2 [cs.LG] UPDATED)
113. Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning. (arXiv:2108.03706v2 [stat.ML] UPDATED)
114. Self-Supervised Learning to Prove Equivalence Between Programs via Semantics-Preserving Rewrite Rules. (arXiv:2109.10476v2 [cs.LG] UPDATED)
115. Machine Learning based Medical Image Deepfake Detection: A Comparative Study. (arXiv:2109.12800v2 [cs.CV] UPDATED)
116. On the Limitations of Multimodal VAEs. (arXiv:2110.04121v2 [cs.LG] UPDATED)
117. Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings. (arXiv:2111.00185v2 [cs.LG] UPDATED)
118. A comparison of mixed-variables Bayesian optimization approaches. (arXiv:2111.01533v2 [math.OC] UPDATED)
119. Membership Inference Attacks Against Self-supervised Speech Models. (arXiv:2111.05113v2 [cs.CR] UPDATED)
120. Automated question generation and question answering from Turkish texts. (arXiv:2111.06476v4 [cs.LG] UPDATED)
121. GFlowNet Foundations. (arXiv:2111.09266v2 [cs.LG] UPDATED)
122. Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)
123. Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation. (arXiv:2111.14826v2 [cs.CV] UPDATED)
124. Deep learning method for identifying mass composition of ultra-high-energy cosmic rays. (arXiv:2112.02072v2 [astro-ph.IM] UPDATED)
125. Reinforcement Learning with Almost Sure Constraints. (arXiv:2112.05198v2 [cs.LG] UPDATED)
126. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v2 [cs.CV] UPDATED)
127. Scientific Discovery and the Cost of Measurement -- Balancing Information and Cost in Reinforcement Learning. (arXiv:2112.07535v2 [cs.LG] UPDATED)
128. Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v4 [cs.AI] UPDATED)
129. Multiplayer Performative Prediction: Learning in Decision-Dependent Games. (arXiv:2201.03398v2 [cs.GT] UPDATED)
130. Graph Neural Network-based Android Malware Classification with Jumping Knowledge. (arXiv:2201.07537v5 [cs.CR] UPDATED)
131. Variational Autoencoder based Metamodeling for Multi-Objective Topology Optimization of Electrical Machines. (arXiv:2201.08877v2 [cs.LG] UPDATED)
132. Federated Learning with Erroneous Communication Links. (arXiv:2201.12991v2 [cs.LG] UPDATED)
133. On Monte Carlo Tree Search for Weighted Vertex Coloring. (arXiv:2202.01665v2 [cs.LG] UPDATED)
134. Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v2 [eess.IV] UPDATED)
135. Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v3 [cs.CV] UPDATED)
136. Discriminability-enforcing loss to improve representation learning. (arXiv:2202.07073v2 [cs.CV] UPDATED)
137. Quantum Distributed Deep Learning Architectures: Models, Discussions, and Applications. (arXiv:2202.11200v3 [quant-ph] UPDATED)
138. ECMG: Exemplar-based Commit Message Generation. (arXiv:2203.02700v2 [cs.SE] UPDATED)
139. Reinforcement Learning for Linear Quadratic Control is Vulnerable Under Cost Manipulation. (arXiv:2203.05774v2 [eess.SY] UPDATED)
140. Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v2 [eess.IV] UPDATED)
141. Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems. (arXiv:2203.06416v2 [cs.AI] UPDATED)
142. Towards Backwards-Compatible Data with Confounded Domain Adaptation. (arXiv:2203.12720v2 [stat.ML] UPDATED)
143. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v3 [cs.IR] UPDATED)
144. Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v2 [eess.AS] UPDATED)
145. Message Passing Neural Networks for Hypergraphs. (arXiv:2203.16995v2 [cs.LG] UPDATED)
146. GraFN: Semi-Supervised Node Classification on Graph with Few Labels via Non-Parametric Distribution Assignment. (arXiv:2204.01303v2 [cs.LG] UPDATED)
147. Causality, Causal Discovery, and Causal Inference in Structural Engineering. (arXiv:2204.01543v2 [cs.LG] UPDATED)
148. An Exploration of Active Learning for Affective Digital Phenotyping. (arXiv:2204.01915v2 [cs.LG] UPDATED)
149. VNIbCReg: VICReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series. (arXiv:2204.02697v2 [cs.LG] UPDATED)
150. Data-Centric Green AI: An Exploratory Empirical Study. (arXiv:2204.02766v2 [cs.LG] UPDATED)
151. A survey on recently proposed activation functions for Deep Learning. (arXiv:2204.02921v2 [cs.LG] UPDATED)
## cs.AI
---
**88** new papers in cs.AI:-) 
1. Evolutionary Programmer: Autonomously Creating Path Planning Programs based on Evolutionary Algorithms. (arXiv:2204.02970v1 [cs.NE])
2. Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding. (arXiv:2204.03035v1 [cs.CL])
3. FFC-SE: Fast Fourier Convolution for Speech **Enhancement**. (arXiv:2204.03042v1 [cs.SD])
4. Perceive, Represent, Generate: Translating Multimodal Information to Robotic Motion Trajectories. (arXiv:2204.03051v1 [cs.RO])
5. Semantic Sensor Network Ontology based Decision Support System for Forest Fire Management. (arXiv:2204.03059v1 [cs.AI])
6. Standardized feature extraction from pairwise conflicts applied to the train rescheduling problem. (arXiv:2204.03061v1 [cs.LG])
7. Knowledge Infused Decoding. (arXiv:2204.03084v1 [cs.CL])
8. Advancing Data Justice Research and Practice: An Integrated Literature Review. (arXiv:2204.03090v1 [cs.CY])
9. BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis. (arXiv:2204.03117v1 [cs.CL])
10. Learning and Transferring Value Function for Robot Exploration in Subterranean Environments. (arXiv:2204.03140v1 [cs.RO])
11. Adversarial Machine Learning Attacks Against Video Anomaly Detection Systems. (arXiv:2204.03141v1 [cs.CV])
12. **Enhancement** on Model Interpretability and Sleep Stage Scoring Performance with A Novel Pipeline Based on Deep Neural Network. (arXiv:2204.03173v1 [cs.LG])
13. FedCos: A Scene-adaptive Federated Optimization **Enhancement** for Performance Improvement. (arXiv:2204.03174v1 [cs.LG])
14. Transformer-Based Language Models for Software Vulnerability Detection: Performance, Model's Security and Platforms. (arXiv:2204.03214v1 [cs.CR])
15. Explicit Feature Interaction-aware Graph Neural Networks. (arXiv:2204.03225v1 [cs.LG])
16. What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning. (arXiv:2204.03230v1 [cs.LG])
17. Leveraging Real Conversational Data for Multi-Channel Continuous Speech Separation. (arXiv:2204.03232v1 [eess.AS])
18. Learning to Solve Travelling Salesman Problem with Hardness-adaptive Curriculum. (arXiv:2204.03236v1 [cs.LG])
19. HIT-UAV: A High-altitude Infrared Thermal Dataset for Unmanned Aerial Vehicles. (arXiv:2204.03245v1 [cs.CV])
20. Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation. (arXiv:2204.03293v1 [cs.SE])
21. A Multi-Transformation Evolutionary Framework for Influence Maximization in Social Networks. (arXiv:2204.03297v1 [cs.NE])
22. Genre-conditioned Acoustic Models for Automatic Lyrics Transcription of Polyphonic Music. (arXiv:2204.03307v1 [cs.SD])
23. Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation. (arXiv:2204.03324v1 [cs.CL])
24. Coarse-to-Fine Feature Mining for Video Semantic Segmentation. (arXiv:2204.03330v1 [cs.CV])
25. Domain Adaptation for Time-Series Classification to Mitigate Covariate Shift. (arXiv:2204.03342v1 [cs.LG])
26. Predictive Coding and Stochastic Resonance: Towards a Unified Theory of Auditory (Phantom) Perception. (arXiv:2204.03354v1 [q-bio.NC])
27. Parameter-Efficient Abstractive Question Answering over Tables or Text. (arXiv:2204.03357v1 [cs.CL])
28. Robust Event-Driven Interactions in Cooperative Multi-Agent Learning. (arXiv:2204.03361v1 [cs.MA])
29. Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes. (arXiv:2204.03376v1 [cs.LG])
30. Continual Inference: A Library for Efficient Online Inference with Deep Neural Networks in PyTorch. (arXiv:2204.03418v1 [cs.LG])
31. Finding Counterfactual Explanations through Constraint Relaxations. (arXiv:2204.03429v1 [cs.AI])
32. Video Diffusion Models. (arXiv:2204.03458v1 [cs.CV])
33. DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v1 [cs.AI])
34. Deep Understanding based Multi-Document Machine Reading Comprehension. (arXiv:2204.03494v1 [cs.CL])
35. Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers. (arXiv:2204.03503v1 [cs.CL])
36. QCRI's COVID-19 Disinformation Detector: A System to Fight the COVID-19 Infodemic in Social Media. (arXiv:2204.03506v1 [cs.CL])
37. A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods. (arXiv:2204.03508v1 [cs.CL])
38. Many-to-many Splatting for Efficient Video Frame Interpolation. (arXiv:2204.03513v1 [cs.CV])
39. Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale. (arXiv:2204.03514v1 [cs.AI])
40. Distributed Reinforcement Learning for Robot Teams: A Review. (arXiv:2204.03516v1 [cs.RO])
41. Temporal Alignment for History Representation in Reinforcement Learning. (arXiv:2204.03525v1 [cs.LG])
42. Abstracting Noisy Robot Programs. (arXiv:2204.03536v1 [cs.AI])
43. Leveraging pre-trained language models for conversational information seeking from text. (arXiv:2204.03542v1 [cs.CL])
44. Strong Admissibility, a Tractable Algorithmic Approach (proofs). (arXiv:2204.03551v1 [cs.AI])
45. Practical Digital Disguises: Leveraging Face Swaps to Protect Patient Privacy. (arXiv:2204.03559v1 [cs.CV])
46. Transfinite Modal Logic: a Semi-quantitative Explanation for Bayesian Reasoning. (arXiv:2204.03563v1 [cs.AI])
47. Improving Urban Mobility: using artificial intelligence and new technologies to connect supply and demand. (arXiv:2204.03570v1 [cs.CY])
48. Explicit and Implicit Pattern Relation Analysis for Discovering Actionable Negative Sequences. (arXiv:2204.03571v1 [cs.AI])
49. Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v1 [cs.CL])
50. Controlling Golog Programs against MTL Constraints. (arXiv:2204.03596v1 [cs.AI])
51. Imitating, Fast and Slow: Robust learning from demonstrations via decision-time planning. (arXiv:2204.03597v1 [cs.LG])
52. Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])
53. Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. (arXiv:2204.03638v1 [cs.CV])
54. Adapting CLIP For Phrase Localization Without Further Training. (arXiv:2204.03647v1 [cs.CV])
55. On the Complexity of Robust Stable Marriage. (arXiv:1709.06172v3 [cs.CC] UPDATED)
56. Mo\"ET: Mixture of Expert Trees and its Application to Verifiable Reinforcement Learning. (arXiv:1906.06717v4 [cs.LG] UPDATED)
57. Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning. (arXiv:2004.10888v6 [cs.LG] UPDATED)
58. Policy Mirror Descent for Reinforcement Learning: Linear Convergence, New Sampling Complexity, and Generalized Problem Classes. (arXiv:2102.00135v6 [cs.LG] UPDATED)
59. Fast Design Space Exploration of Nonlinear Systems: Part I. (arXiv:2104.01747v7 [cs.LG] UPDATED)
60. Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning. (arXiv:2104.08676v2 [cs.CL] UPDATED)
61. Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. (arXiv:2106.02190v5 [cs.LG] UPDATED)
62. Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection. (arXiv:2106.04564v3 [cs.CL] UPDATED)
63. Zero-Shot Controlled Generation with Encoder-Decoder Transformers. (arXiv:2106.06411v3 [cs.CL] UPDATED)
64. Amortized Auto-Tuning: Cost-Efficient Bayesian Transfer Optimization for Hyperparameter Recommendation. (arXiv:2106.09179v2 [cs.LG] UPDATED)
65. Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning. (arXiv:2108.03706v2 [stat.ML] UPDATED)
66. Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings. (arXiv:2111.00185v2 [cs.LG] UPDATED)
67. GFlowNet Foundations. (arXiv:2111.09266v2 [cs.LG] UPDATED)
68. Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)
69. Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation. (arXiv:2111.14826v2 [cs.CV] UPDATED)
70. Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v2 [cs.CV] UPDATED)
71. Scientific Discovery and the Cost of Measurement -- Balancing Information and Cost in Reinforcement Learning. (arXiv:2112.07535v2 [cs.LG] UPDATED)
72. Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v4 [cs.AI] UPDATED)
73. Search and Score-based Waterfall Auction Optimization. (arXiv:2201.06409v2 [cs.AI] UPDATED)
74. Metrics for saliency map evaluation of deep learning explanation methods. (arXiv:2201.13291v2 [cs.CV] UPDATED)
75. Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v2 [cs.AI] UPDATED)
76. Illuminating the Space of Dungeon Maps, Locked-door Missions and Enemy Placement Through MAP-Elites. (arXiv:2202.09301v2 [cs.AI] UPDATED)
77. ECMG: Exemplar-based Commit Message Generation. (arXiv:2203.02700v2 [cs.SE] UPDATED)
78. Reinforcement Learning for Linear Quadratic Control is Vulnerable Under Cost Manipulation. (arXiv:2203.05774v2 [eess.SY] UPDATED)
79. Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems. (arXiv:2203.06416v2 [cs.AI] UPDATED)
80. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v3 [cs.IR] UPDATED)
81. CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues. (arXiv:2203.13926v3 [cs.CL] UPDATED)
82. SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images. (arXiv:2204.00790v2 [cs.CV] UPDATED)
83. Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images. (arXiv:2204.00840v2 [cs.CV] UPDATED)
84. AutoOpt: A Methodological Framework of Automatically Designing Metaheuristics for Optimization Problems. (arXiv:2204.00998v2 [cs.NE] UPDATED)
85. GraFN: Semi-Supervised Node Classification on Graph with Few Labels via Non-Parametric Distribution Assignment. (arXiv:2204.01303v2 [cs.LG] UPDATED)
86. Data-Centric Green AI: An Exploratory Empirical Study. (arXiv:2204.02766v2 [cs.LG] UPDATED)
87. A survey on recently proposed activation functions for Deep Learning. (arXiv:2204.02921v2 [cs.LG] UPDATED)
88. A Pixel-based Encryption Method for Privacy-Preserving Deep Learning Models. (arXiv:2203.16780v1 [cs.CR] CROSS LISTED)

