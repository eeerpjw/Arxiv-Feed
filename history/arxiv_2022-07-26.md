# Your interest papers
---
## cs.CV
---
### Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing. (arXiv:2207.11250v1 [cs.CV])
- Authors : Sai Mitheran, Anushri Suresh
- Link : [http://arxiv.org/abs/2207.11250](http://arxiv.org/abs/2207.11250)
> ABSTRACT  :  Single-image haze removal is a long-standing hurdle for computer vision applications. Several works have been focused on transferring advances from image classification, detection, and segmentation to the niche of image dehazing, primarily focusing on contrastive learning and knowledge distillation. However, these approaches prove computationally expensive, raising concern regarding their applicability to on-the-edge use-cases. This work introduces a simple, lightweight, and efficient framework for single-image haze removal, exploiting rich "**dark**-knowledge" information from a lightweight pre-trained super-resolution model via the notion of heterogeneous knowledge distillation. We designed a feature affinity module to maximize the flow of rich feature semantics from the super-resolution teacher to the student dehazing network. In order to evaluate the efficacy of our proposed framework, its performance as a plug-and-play setup to a baseline model is examined. Our experiments are carried out on the RESIDE-Standard dataset to demonstrate the robustness of our framework to the synthetic and real-world domains. The extensive qualitative and quantitative results provided establish the effectiveness of the framework, achieving gains of upto 15\% (PSNR) while reducing the model size by $\sim$20 times.  
### Video **Swin** Transformers for Egocentric Video Understanding @ Ego4D Challenges 2022. (arXiv:2207.11329v1 [cs.CV])
- Authors : Maria Escobar, Laura Daza, Cristina Gonz, Jordi Pont, Pablo Arbel
- Link : [http://arxiv.org/abs/2207.11329](http://arxiv.org/abs/2207.11329)
> ABSTRACT  :  We implemented Video **Swin** Transformer as a base architecture for the tasks of Point-of-No-Return temporal localization and Object State Change Classification. Our method achieved competitive performance on both challenges.  
### Neural-Sim: Learning to Generate Training Data with **NeRF**. (arXiv:2207.11368v1 [cs.CV])
- Authors : Yunhao Ge, Harkirat Behl, Jiashu Xu, Suriya Gunasekar, Neel Joshi, Yale Song, Xin Wang, Laurent Itti, Vibhav Vineet
- Link : [http://arxiv.org/abs/2207.11368](http://arxiv.org/abs/2207.11368)
> ABSTRACT  :  Training computer vision models usually requires collecting and labeling vast amounts of imagery under a diverse set of scene configurations and properties. This process is incredibly time-consuming, and it is challenging to ensure that the captured data distribution maps well to the target domain of an application scenario. Recently, synthetic data has emerged as a way to address both of these issues. However, existing approaches either require human experts to manually tune each scene property or use automatic methods that provide little to no control; this requires rendering large amounts of random data variations, which is slow and is often suboptimal for the target domain. We present the first fully differentiable synthetic data pipeline that uses Neural Radiance Fields (**NeRF**s) in a closed-loop with a target application's loss function. Our approach generates data on-demand, with no human labor, to maximize accuracy for a target task. We illustrate the effectiveness of our method on synthetic and real-world object detection tasks. We also introduce a new "YCB-in-the-Wild" dataset and benchmark that provides a test scenario for object detection with varied poses in real-world environments.  
### PS-**NeRF**: Neural Inverse Rendering for Multi-view Photometric Stereo. (arXiv:2207.11406v1 [cs.CV])
- Authors : Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang Chen
- Link : [http://arxiv.org/abs/2207.11406](http://arxiv.org/abs/2207.11406)
> ABSTRACT  :  Traditional multi-view photometric stereo (MVPS) methods are often composed of multiple disjoint stages, resulting in noticeable accumulated errors. In this paper, we present a neural inverse rendering method for MVPS based on implicit representation. Given multi-view images of a non-Lambertian object illuminated by multiple unknown directional lights, our method jointly estimates the geometry, materials, and lights. Our method first employs multi-light images to estimate per-view surface normal maps, which are used to regularize the normals derived from the neural radiance field. It then jointly optimizes the surface normals, spatially-varying BRDFs, and lights based on a shadow-aware differentiable rendering layer. After optimization, the reconstructed object can be used for novel-view rendering, relighting, and material editing. Experiments on both synthetic and real datasets demonstrate that our method achieves far more accurate shape reconstruction than existing MVPS and neural rendering methods. Our code and model can be found at https://ywq.github.io/psnerf.  
### Arbitrary Style Transfer with Structure **Enhancement** by Combining the Global and Local Loss. (arXiv:2207.11438v1 [cs.CV])
- Authors : Lizhen Long, Man Pun
- Link : [http://arxiv.org/abs/2207.11438](http://arxiv.org/abs/2207.11438)
> ABSTRACT  :  Arbitrary style transfer generates an artistic image which combines the structure of a content image and the artistic style of the artwork by using only one trained network. The image representation used in this method contains content structure representation and the style patterns representation, which is usually the features representation of high-level in the pre-trained classification networks. However, the traditional classification networks were designed for classification which usually focus on high-level features and ignore other features. As the result, the stylized images distribute style elements evenly throughout the image and make the overall image structure unrecognizable. To solve this problem, we introduce a novel arbitrary style transfer method with structure **enhancement** by combining the global and local loss. The local structure details are represented by Lapstyle and the global structure is controlled by the image depth. Experimental results demonstrate that our method can generate higher-quality images with impressive visual effects on several common datasets, comparing with other state-of-the-art methods.  
### Contrastive Monotonic Pixel-Level Modulation. (arXiv:2207.11517v1 [cs.CV])
- Authors : Kun Lu, Rongpeng Li, Honggang Zhang
- Link : [http://arxiv.org/abs/2207.11517](http://arxiv.org/abs/2207.11517)
> ABSTRACT  :  Continuous one-to-many mapping is a less investigated yet important task in both low-level visions and neural image translation. In this paper, we present a new formulation called MonoPix, an unsupervised and contrastive continuous modulation model, and take a step further to enable a pixel-level spatial control which is critical but can not be properly handled previously. The key feature of this work is to model the monotonicity between controlling signals and the domain discriminator with a novel contrastive modulation framework and corresponding monotonicity constraints. We have also introduced a selective inference strategy with logarithmic approximation complexity and support fast domain adaptations. The state-of-the-art performance is validated on a variety of continuous mapping tasks, including AFHQ cat-dog and Yosemite summer-winter translation. The introduced approach also helps to provide a new solution for many low-level tasks like **low-light** **enhancement** and natural noise generation, which is beyond the long-established practice of one-to-one training and inference. Code is available at https://github.com/lukun199/MonoPix.  
### Unstructured Road Segmentation using Hypercolumn based Random Forests of Local experts. (arXiv:2207.11523v1 [cs.CV])
- Authors : Prassanna Ganesh
- Link : [http://arxiv.org/abs/2207.11523](http://arxiv.org/abs/2207.11523)
> ABSTRACT  :  Monocular vision based road detection methods are mostly based on machine learning methods, relying on classification and feature extraction accuracy, and suffer from appearance, illumination and weather changes. Traditional methods introduce the predictions into conditional random fields or markov random fields models to improve the intermediate predictions based on structure. These methods are optimization based and therefore resource heavy and slow, making it unsuitable for **real time** applications. We propose a method to detect and segment roads with a random forest classifier of local experts with superpixel based machine-learned features. The random forest takes in machine learnt descriptors from a pre-trained convolutional neural network - VGG-16. The features are also pooled into their respective superpixels, allowing for local structure to be continuous. We compare our algorithm against Nueral Network based methods and Traditional approaches (based on Hand-crafted features), on both Structured Road (CamVid and Kitti) and Unstructured Road Datasets. Finally, we introduce a Road Scene Dataset with 1000 annotated images, and verify that our algorithm works well in non-urban and rural road scenarios.  
### High-Resolution **Swin** Transformer for Automatic Medical Image Segmentation. (arXiv:2207.11553v1 [cs.CV])
- Authors : Chen Wei, Shenghan Ren, Kaitai Guo, Haihong Hu, Jimin Liang
- Link : [http://arxiv.org/abs/2207.11553](http://arxiv.org/abs/2207.11553)
> ABSTRACT  :  The Resolution of feature maps is critical for medical image segmentation. Most of the existing Transformer-based networks for medical image segmentation are U-Net-like architecture that contains an encoder that utilizes a sequence of Transformer blocks to convert the input medical image from high-resolution representation into low-resolution feature maps and a decoder that gradually recovers the high-resolution representation from low-resolution feature maps. Unlike previous studies, in this paper, we utilize the network design style from the High-Resolution Network (HRNet), replace the convolutional layers with Transformer blocks, and continuously exchange information from the different resolution feature maps that are generated by Transformer blocks. The newly Transformer-based network presented in this paper is denoted as High-Resolution **Swin** Transformer Network (HRSTNet). Extensive experiments illustrate that HRSTNet can achieve comparable performance with the state-of-the-art Transformer-based U-Net-like architecture on Brain Tumor Segmentation(BraTS) 2021 and the liver dataset from Medical Segmentation Decathlon. The code of HRSTNet will be publicly available at https://github.com/auroua/HRSTNet.  
### Face Deblurring using Dual Camera Fusion on Mobile Phones. (arXiv:2207.11617v1 [cs.CV])
- Authors : Sheng Lai, YiChang Shih, Cheng Chu, Xiaotong Wu, Fang Tsai, Michael Krainin, Deqing Sun, Kai Liang
- Link : [http://arxiv.org/abs/2207.11617](http://arxiv.org/abs/2207.11617)
> ABSTRACT  :  Motion blur of fast-moving subjects is a longstanding problem in photography and very common on mobile phones due to limited light collection efficiency, particularly in **low-light** conditions. While we have witnessed great progress in image deblurring in recent years, most methods require significant computational power and have limitations in processing high-resolution photos with severe local motions. To this end, we develop a novel face deblurring system based on the dual camera fusion technique for mobile phones. The system detects subject motion to dynamically enable a reference camera, e.g., ultrawide angle camera commonly available on recent premium phones, and captures an auxiliary photo with faster shutter settings. While the main shot is low noise but blurry, the reference shot is sharp but noisy. We learn ML models to align and fuse these two shots and output a clear photo without motion blur. Our algorithm runs efficiently on Google Pixel 6, which takes 463 ms overhead per shot. Our experiments demonstrate the advantage and robustness of our system against alternative single-image, multi-frame, face-specific, and video deblurring algorithms as well as commercial products. To the best of our knowledge, our work is the first mobile solution for face motion deblurring that works reliably and robustly over thousands of images in diverse motion and lighting conditions.  
### Robust Scene Inference under Noise-Blur Dual Corruptions. (arXiv:2207.11643v1 [cs.CV])
- Authors : Bhavya Goyal, ois Lalonde, Yin Li, Mohit Gupta
- Link : [http://arxiv.org/abs/2207.11643](http://arxiv.org/abs/2207.11643)
> ABSTRACT  :  Scene inference under **low-light** is a challenging problem due to severe noise in the captured images. One way to reduce noise is to use longer **exposure** during the capture. However, in the presence of motion (scene or camera motion), longer **exposure**s lead to motion blur, resulting in loss of image information. This creates a trade-off between these two kinds of image degradations: motion blur (due to long **exposure**) vs. noise (due to short **exposure**), also referred as a dual image corruption pair in this paper. With the rise of cameras capable of capturing multiple **exposure**s of the same scene simultaneously, it is possible to overcome this trade-off. Our key observation is that although the amount and nature of degradation varies for these different image captures, the semantic content remains the same across all images. To this end, we propose a method to leverage these multi **exposure** captures for robust inference under **low-light** and motion. Our method builds on a feature consistency loss to encourage similar results from these individual captures, and uses the ensemble of their final predictions for robust visual recognition. We demonstrate the effectiveness of our approach on simulated images as well as real captures with multiple **exposure**s, and across the tasks of object detection and image classification.  
### Improved Regularization of Event-based Learning by Reversing and Drifting. (arXiv:2207.11659v1 [cs.CV])
- Authors : Haibo Shen, Yihao Luo, Xiang Cao, Liangqi Zhang, Juyu Xiao, Tianjiang Wang
- Link : [http://arxiv.org/abs/2207.11659](http://arxiv.org/abs/2207.11659)
> ABSTRACT  :  Event camera has an enormous potential in challenging scenes for its advantages of high temporal resolution, **high dynamic range**, low power consumption, and no motion blur. However, event-based learning is hindered by insufficient generalization ability. In this paper, we first analyze the influence of different brightness variations on event data. Then we propose two novel augmentation methods: EventReverse and EventDrift. By reversing and drifting events to their corresponding positions in the spatiotemporal or polarity domain, the proposed methods generate samples affected by different brightness variations, which improves the robustness of event-based learning and results in a better generalization. Extensive experiments on N-CARS, N-Caltech101 and CIFAR10-DVS datasets demonstrate that our method is general and remarkably effective.  
### FD-MAR: Fourier Dual-domain Network for CT Metal Artifact Reduction. (arXiv:2207.11678v1 [eess.IV])
- Authors : Zilong Li, Qi Gao, Yaping Wu, Chuang Niu, Junping Zhang, Meiyun Wang, Ge Wang, Hongming Shan
- Link : [http://arxiv.org/abs/2207.11678](http://arxiv.org/abs/2207.11678)
> ABSTRACT  :  The presence of high-density objects such as metal implants and dental fillings can introduce severely streak-like artifacts in computed tomography (CT) images, greatly limiting subsequent diagnosis. Although various deep neural networks-based methods have been proposed for metal artifact reduction (MAR), they usually suffer from poor performance due to limited exploitation of global context in the sinogram domain, secondary artifacts introduced in the image domain, and the requirement of precise metal masks. To address these issues, this paper explores fast Fourier convolution for MAR in both sinogram and image domains, and proposes a Fourier dual-domain network for MAR, termed FD-MAR. Specifically, we first propose a Fourier sinogram **restoration** network, which can leverage sinogram-wide receptive context to fill in the metal-corrupted region from uncorrupted region and, hence, is robust to the metal trace. Second, we propose a Fourier refinement network in the image domain, which can refine the reconstructed images in a local-to-global manner by exploring image-wide context information. As a result, the proposed FD-MAR can explore the sinogram- and image-wide receptive fields for MAR. By optimizing FD-MAR with a composite loss function, extensive experimental results demonstrate the superiority of the proposed FD-MAR over the state-of-the-art MAR methods in terms of quantitative metrics and visual comparison. Notably, FD-MAR does not require precise metal masks, which is of great importance in clinical routine.  
### Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (arXiv:2207.11694v1 [cs.LG])
- Authors : Quanshi Zhang, Xin Wang, Jie Ren, Xu Cheng, Shuyun Lin, Yisen Wang, Xiangming Zhu
- Link : [http://arxiv.org/abs/2207.11694](http://arxiv.org/abs/2207.11694)
> ABSTRACT  :  Although many methods have been proposed to enhance the transferability of adversarial perturbations, these methods are designed in a heuristic manner, and the essential mechanism for improving adversarial transferability is still unclear. This paper summarizes the common mechanism shared by twelve previous transferability-boosting methods in a unified view, i.e., these methods all reduce game-theoretic interactions between regional adversarial perturbations. To this end, we focus on the attacking utility of all interactions between regional adversarial perturbations, and we first discover and prove the negative correlation between the adversarial transferability and the attacking utility of interactions. Based on this discovery, we theoretically prove and empirically verify that twelve previous transferability-boosting methods all reduce interactions between regional adversarial perturbations. More crucially, we consider the reduction of interactions as the essential reason for the **enhancement** of adversarial transferability. Furthermore, we design the interaction loss to directly penalize interactions between regional adversarial perturbations during attacking. Experimental results show that the interaction loss significantly improves the transferability of adversarial perturbations.  
### Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges. (arXiv:2207.11759v1 [cs.LG])
- Authors : **Lei Zhang**, Guanyu Gao, Huaizheng Zhang
- Link : [http://arxiv.org/abs/2207.11759](http://arxiv.org/abs/2207.11759)
> ABSTRACT  :  Data drift is a thorny challenge when deploying person re-identification (ReID) models into real-world devices, where the data distribution is significantly different from that of the training environment and keeps changing. To tackle this issue, we propose a federated spatial-temporal incremental learning approach, named FedSTIL, which leverages both lifelong learning and federated learning to continuously optimize models deployed on many distributed edge clients. Unlike previous efforts, FedSTIL aims to mine spatial-temporal correlations among the knowledge learnt from different edge clients. Specifically, the edge clients first periodically extract general representations of drifted data to optimize their local models. Then, the learnt knowledge from edge clients will be aggregated by centralized parameter server, where the knowledge will be selectively and attentively distilled from spatial- and temporal-dimension with carefully designed mechanisms. Finally, the distilled informative spatial-temporal knowledge will be sent back to correlated edge clients to further improve the recognition accuracy of each edge client with a lifelong learning method. Extensive experiments on a mixture of five real-world datasets demonstrate that our method outperforms others by nearly 4% in Rank-1 accuracy, while reducing communication cost by 62%. All implementation codes are publicly available on https://github.com/MSNLAB/Federated-Lifelong-Person-ReID  
### Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis. (arXiv:2207.11770v1 [cs.CV])
- Authors : Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou, Jiwen Lu
- Link : [http://arxiv.org/abs/2207.11770](http://arxiv.org/abs/2207.11770)
> ABSTRACT  :  Talking head synthesis is an emerging technology with wide applications in film dubbing, virtual avatars and online education. Recent **NeRF**-based methods generate more natural talking videos, as they better capture the 3D structural information of faces. However, a specific model needs to be trained for each identity with a large dataset. In this paper, we propose Dynamic Facial Radiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly generalize to an unseen identity with few training data. Different from the existing **NeRF**-based methods which directly encode the 3D geometry and appearance of a specific person into the network, our DFRF conditions face radiance field on 2D appearance images to learn the face prior. Thus the facial radiance field can be flexibly adjusted to the new identity with few reference images. Additionally, for better modeling of the facial deformations, we propose a differentiable face warping module conditioned on audio signals to deform all reference images to the query space. Extensive experiments show that with only tens of seconds of training clip available, our proposed DFRF can synthesize natural and high-quality audio-driven talking head videos for novel identities with only 40k iterations. We highly recommend readers view our supplementary video for intuitive comparisons. Code is available in https://sstzal.github.io/DFRF/.  
### Enhancing Image Rescaling using Dual Latent Variables in Invertible Neural Network. (arXiv:2207.11844v1 [cs.CV])
- Authors : Min Zhang, Zhihong Pan, Xin Zhou, Jay Kuo
- Link : [http://arxiv.org/abs/2207.11844](http://arxiv.org/abs/2207.11844)
> ABSTRACT  :  Normalizing flow models have been used successfully for generative image super-resolution (SR) by approximating complex distribution of natural images to simple tractable distribution in latent space through Invertible Neural Networks (INN). These models can generate multiple realistic SR images from one low-resolution (LR) input using randomly sampled points in the latent space, simulating the ill-posed nature of image upscaling where multiple high-resolution (HR) images correspond to the same LR. Lately, the invertible process in INN has also been used successfully by bidirectional image rescaling models like IRN and HCFlow for joint optimization of downscaling and inverse upscaling, resulting in significant improvements in upscaled image quality. While they are optimized for image downscaling too, the ill-posed nature of image downscaling, where one HR image could be downsized to multiple LR images depending on different interpolation kernels and resampling methods, is not considered. A new downscaling latent variable, in addition to the original one representing uncertainties in image upscaling, is introduced to model variations in the image downscaling process. This dual latent variable **enhancement** is applicable to different image rescaling models and it is shown in extensive experiments that it can improve image upscaling accuracy consistently without sacrificing image quality in downscaled LR images. It is also shown to be effective in enhancing other INN-based models for image **restoration** applications like image hiding.  
### Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v2 [cs.CV] UPDATED)
- Authors : Chuanguang Yang, Zhulin An, Linhang Cai, Yongjun Xu
- Link : [http://arxiv.org/abs/2109.03075](http://arxiv.org/abs/2109.03075)
> ABSTRACT  :  Knowledge distillation (KD) is an effective framework that aims to transfer meaningful information from a large teacher to a smaller student. Generally, KD often involves how to define and transfer knowledge. Previous KD methods often focus on mining various forms of knowledge, for example, feature maps and refined information. However, the knowledge is derived from the primary supervised task and thus is highly task-specific. Motivated by the recent success of self-supervised representation learning, we propose an auxiliary self-supervision augmented task to guide networks to learn more meaningful features. Therefore, we can derive soft self-supervision augmented distributions as richer **dark** knowledge from this task for KD. Unlike previous knowledge, this distribution encodes joint knowledge from supervised and self-supervised feature learning. Beyond knowledge exploration, we propose to append several auxiliary branches at various hidden layers, to fully take advantage of hierarchical feature maps. Each auxiliary branch is guided to learn self-supervision augmented task and distill this distribution from teacher to student. Overall, we call our KD method as Hierarchical Self-Supervision Augmented Knowledge Distillation (HSSAKD). Experiments on standard image classification show that both offline and online HSSAKD achieves state-of-the-art performance in the field of KD. Further transfer experiments on object detection further verify that HSSAKD can guide the network to learn better features. The code is available at https://github.com/winycg/HSAKD.  
### SWAT: Spatial Structure Within and Among Tokens. (arXiv:2111.13677v2 [cs.CV] UPDATED)
- Authors : Kumara Kahatapitiya
- Link : [http://arxiv.org/abs/2111.13677](http://arxiv.org/abs/2111.13677)
> ABSTRACT  :  Modeling visual data as tokens (i.e., image patches) using attention mechanisms, feed-forward networks or convolutions has been highly effective in recent years. Such methods usually have a common pipeline: a tokenization method, followed by a set of layers/blocks for information mixing, both within and among tokens. When image patches are converted into tokens, they are often flattened, discarding the spatial structure within each patch. As a result, any processing that follows (eg: multi-head self-attention) may fail to recover and/or benefit from such information. In this paper, we argue that models can have significant gains when spatial structure is preserved during tokenization, and is explicitly used during the mixing stage. We propose two key contributions: (1) Structure-aware Tokenization and, (2) Structure-aware Mixing, both of which can be combined with existing models with minimal effort. We introduce a family of models (SWAT), showing improvements over the likes of DeiT, MLP-Mixer and **Swin** Transformer, across multiple benchmarks including ImageNet classification and ADE20K segmentation. Our code and models will be released online.  
### Event Neural Networks. (arXiv:2112.00891v2 [cs.CV] UPDATED)
- Authors : Matthew Dutson, Yin Li, Mohit Gupta
- Link : [http://arxiv.org/abs/2112.00891](http://arxiv.org/abs/2112.00891)
> ABSTRACT  :  Video data is often repetitive; for example, the contents of adjacent frames are usually strongly correlated. Such redundancy occurs at multiple levels of complexity, from low-level pixel values to textures and high-level semantics. We propose Event Neural Networks (EvNets), which leverage this redundancy to achieve considerable computation savings during video inference. A defining characteristic of EvNets is that each neuron has state variables that provide it with long-term memory, which allows low-cost, high-accuracy inference even in the presence of significant camera motion. We show that it is possible to transform a wide range of neural networks into EvNets without re-training. We demonstrate our method on state-of-the-art architectures for both high- and low-level visual processing, including pose recognition, object detection, optical flow, and image **enhancement**. We observe roughly an order-of-magnitude reduction in computational costs compared to conventional networks, with minimal reductions in model accuracy.  
### Bungee**NeRF**: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. (arXiv:2112.05504v3 [cs.CV] UPDATED)
- Authors : Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, Dahua Lin
- Link : [http://arxiv.org/abs/2112.05504](http://arxiv.org/abs/2112.05504)
> ABSTRACT  :  Neural radiance fields (**NeRF**) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce Bungee**NeRF**, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shallow base block, as training progresses, new blocks are appended to accommodate the emerging details in the increasingly closer views. The strategy progressively activates high-frequency channels in **NeRF**'s positional encoding inputs and successively unfolds more complex details as the training proceeds. We demonstrate the superiority of Bungee**NeRF** in modeling diverse multi-scale scenes with drastically varying views on multiple data sources (city models, synthetic, and drone captured data) and its support for high-quality rendering in different levels of detail.  
### Multimodal Image Synthesis and Editing: A Survey. (arXiv:2112.13592v2 [cs.CV] UPDATED)
- Authors : Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian Theobalt, Eric Xing
- Link : [http://arxiv.org/abs/2112.13592](http://arxiv.org/abs/2112.13592)
> ABSTRACT  :  As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modelling the interaction among multimodal information, multimodal image synthesis and editing has become a hot research topic in recent years. Instead of providing explicit guidance for network training, multimodal guidance offers intuitive and flexible means for image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of features with inherent modality gaps, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis and editing and formulate taxonomies according to data modality and model architectures. We start with an introduction to different types of guidance modalities in image synthesis and editing. We then describe multimodal image synthesis and editing approaches extensively with detailed frameworks including Generative Adversarial Networks (GANs), Auto-regressive models, Diffusion models, Neural Radiance Fields (**NeRF**) and other methods. This is followed by a comprehensive description of benchmark datasets and corresponding evaluation metrics as widely adopted in multimodal image synthesis and editing, as well as detailed comparisons of various synthesis methods with analysis of respective advantages and limitations. Finally, we provide insights about the current research challenges and possible directions for future research. We hope this survey could lay a sound and valuable foundation for future development of multimodal image synthesis and editing. A project associated with this survey is available at https://github.com/fnzhan/MISE.  
### PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v3 [eess.IV] UPDATED)
- Authors : Wentao Liu, Tong Tian, Weijin Xu, Huihua Yang, Xipeng Pan, Songlin Yan, Lemeng Wang
- Link : [http://arxiv.org/abs/2203.04568](http://arxiv.org/abs/2203.04568)
> ABSTRACT  :  The success of Transformer in computer vision has attracted increasing attention in the medical imaging community. Especially for medical image segmentation, many excellent hybrid architectures based on convolutional neural networks (CNNs) and Transformer have been presented and achieve impressive performance. However, most of these methods, which embed modular Transformer into CNNs, struggle to reach their full potential. In this paper, we propose a novel hybrid architecture for medical image segmentation called PHTrans, which parallelly hybridizes Transformer and CNN in main building blocks to produce hierarchical representations from global and local features and adaptively aggregate them, aiming to fully exploit their strengths to obtain better segmentation performance. Specifically, PHTrans follows the U-shaped encoder-decoder design and introduces the parallel hybird module in deep stages, where convolution blocks and the modified 3D **Swin** Transformer learn local features and global dependencies separately, then a sequence-to-volume operation unifies the dimensions of the outputs to achieve feature aggregation. Extensive experimental results on both Multi-Atlas Labeling Beyond the Cranial Vault and Automated Cardiac Diagnosis Challeng datasets corroborate its effectiveness, consistently outperforming state-of-the-art methods. The code is available at: https://github.com/lseventeen/PHTrans.  
### R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v2 [cs.CV] UPDATED)
- Authors : Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, Sergey Tulyakov
- Link : [http://arxiv.org/abs/2203.17261](http://arxiv.org/abs/2203.17261)
> ABSTRACT  :  Recent research explosion on Neural Radiance Field (**NeRF**) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of **NeRF** is its prohibitive inference time: Rendering a single pixel requires querying the **NeRF** network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over **NeRF** in novel view synthesis -- the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained **NeRF** model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average PSNR improvement) rendering quality than **NeRF** without any customized parallelism requirement.  
### Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v2 [cs.CV] UPDATED)
- Authors : Jiawang Bai, Li Yuan, Tao Xia, Shuicheng Yan, Zhifeng Li, Wei Liu
- Link : [http://arxiv.org/abs/2204.00993](http://arxiv.org/abs/2204.00993)
> ABSTRACT  :  The transformer models have shown promising effectiveness in dealing with various vision tasks. However, compared with training Convolutional Neural Network (CNN) models, training Vision Transformer (ViT) models is more difficult and relies on the large-scale training set. To explain this observation we make a hypothesis that \textit{ViT models are less effective in capturing the high-frequency components of images than CNN models}, and verify it by a frequency analysis. Inspired by this finding, we first investigate the effects of existing techniques for improving ViT models from a new frequency perspective, and find that the success of some techniques (e.g., RandAugment) can be attributed to the better usage of the high-frequency components. Then, to compensate for this insufficient ability of ViT models, we propose HAT, which directly augments high-frequency components of images via adversarial training. We show that HAT can consistently boost the performance of various ViT models (e.g., +1.2% for ViT-B, +0.5% for **Swin**-B), and especially enhance the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the superiority can also be maintained on out-of-distribution data and transferred to downstream tasks. The code is available at: https://github.com/jiawangbai/HAT.  
### MaxViT: Multi-Axis Vision Transformer. (arXiv:2204.01697v2 [cs.CV] UPDATED)
- Authors : Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, **Peyman Milanfar**, Alan Bovik, Yinxiao Li
- Link : [http://arxiv.org/abs/2204.01697](http://arxiv.org/abs/2204.01697)
> ABSTRACT  :  Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to ''see'' globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. The source code and trained models will be available at https://github.com/google-research/maxvit.  
### VQFR: Blind Face **Restoration** with Vector-Quantized Dictionary and Parallel Decoder. (arXiv:2205.06803v2 [cs.CV] UPDATED)
- Authors : Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, Ming Cheng
- Link : [http://arxiv.org/abs/2205.06803](http://arxiv.org/abs/2205.06803)
> ABSTRACT  :  Although generative facial prior and geometric prior have recently demonstrated high-quality results for blind face **restoration**, producing fine-grained facial details faithful to inputs remains a challenging problem. Motivated by the classical dictionary-based methods and the recent vector quantization (VQ) technique, we propose a VQ-based face **restoration** method - VQFR. VQFR takes advantage of high-quality low-level feature banks extracted from high-quality faces and can thus help recover realistic facial details. However, the simple application of the VQ codebook cannot achieve good results with faithful details and identity preservation. Therefore, we further introduce two special network designs. 1). We first investigate the compression patch size in the VQ codebook and find that the VQ codebook designed with a proper compression patch size is crucial to balance the quality and fidelity. 2). To further fuse low-level features from inputs while not "contaminating" the realistic details generated from the VQ codebook, we proposed a parallel decoder consisting of a texture decoder and a main decoder. Those two decoders then interact with a texture warping module with deformable convolution. Equipped with the VQ codebook as a facial detail dictionary and the parallel decoder design, the proposed VQFR can largely enhance the restored quality of facial details while keeping the fidelity to previous methods.  
### Physically-Based Editing of Indoor Scene Lighting from a Single Image. (arXiv:2205.09343v2 [cs.CV] UPDATED)
- Authors : Zhengqin Li, Jia Shi, Sai Bi, Rui Zhu, Kalyan Sunkavalli, Zexiang Xu, Ravi Ramamoorthi, Manmohan Chandraker
- Link : [http://arxiv.org/abs/2205.09343](http://arxiv.org/abs/2205.09343)
> ABSTRACT  :  We present a method to edit complex indoor lighting from a single image with its predicted depth and light source segmentation masks. This is an extremely challenging problem that requires modeling complex light transport, and disentangling **HDR** lighting from material and geometry with only a partial LDR observation of the scene. We tackle this problem using two novel components: 1) a holistic scene reconstruction method that estimates scene reflectance and parametric 3D lighting, and 2) a neural rendering framework that re-renders the scene from our predictions. We use physically-based indoor light representations that allow for intuitive editing, and infer both visible and invisible light sources. Our neural rendering framework combines physically-based direct illumination and shadow rendering with deep networks to approximate global illumination. It can capture challenging lighting effects, such as soft shadows, directional lighting, specular materials, and interreflections. Previous single image inverse rendering methods usually entangle scene lighting and geometry and only support applications like object insertion. Instead, by combining parametric 3D lighting estimation with neural scene rendering, we demonstrate the first automatic method to achieve full scene relighting, including light source insertion, removal, and replacement, from a single image. All source code and data will be publicly released.  
### Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model. (arXiv:2207.10040v2 [eess.IV] UPDATED)
- Authors : Zhiyuan Mao, Ajay Jaiswal, Zhangyang Wang
- Link : [http://arxiv.org/abs/2207.10040](http://arxiv.org/abs/2207.10040)
> ABSTRACT  :  Image **restoration** algorithms for atmospheric turbulence are known to be much more challenging to design than traditional ones such as blur or noise because the distortion caused by the turbulence is an entanglement of spatially varying blur, geometric distortion, and sensor noise. Existing CNN-based **restoration** methods built upon convolutional kernels with static weights are insufficient to handle the spatially dynamical atmospheric turbulence effect. To address this problem, in this paper, we propose a physics-inspired transformer model for imaging through atmospheric turbulence. The proposed network utilizes the power of transformer blocks to jointly extract a dynamical turbulence distortion map and restore a turbulence-free image. In addition, recognizing the lack of a comprehensive dataset, we collect and present two new real-world turbulence datasets that allow for evaluation with both classical objective metrics (e.g., PSNR and SSIM) and a new task-driven metric using text recognition accuracy. Both real testing sets and all related code will be made publicly available.  
### Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v2 [eess.IV] UPDATED)
- Authors : Emanuele Dalsasso, my Abergel, Florence Tupin
- Link : [http://arxiv.org/abs/2207.11095](http://arxiv.org/abs/2207.11095)
> ABSTRACT  :  Speckle filtering is generally a prerequisite to the analysis of synthetic aperture radar (SAR) images. Tremendous progress has been achieved in the domain of single-image despeckling. Latest techniques rely on deep neural networks to restore the various structures and textures peculiar to SAR images. The availability of time series of SAR images offers the possibility of improving speckle filtering by combining different speckle realizations over the same area. The supervised training of deep neural networks requires ground-truth speckle-free images. Such images can only be obtained indirectly through some form of averaging, by spatial or temporal integration, and are imperfect. Given the potential of very high quality **restoration** reachable by multi-temporal speckle filtering, the limitations of ground-truth images need to be circumvented. We extend a recent self-supervised training strategy for single-look complex SAR images, called MERLIN, to the case of multi-temporal filtering. This requires modeling the sources of statistical dependencies in the spatial and temporal dimensions as well as between the real and imaginary components of the complex amplitudes. Quantitative analysis on datasets with simulated speckle indicates a clear improvement of speckle reduction when additional SAR images are included. Our method is then applied to stacks of TerraSAR-X images and shown to outperform competing multi-temporal speckle filtering approaches. The code of the trained models is made freely available on the Gitlab of the IMAGES team of the LTCI Lab, T\'el\'ecom Paris Institut Polytechnique de Paris (https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/).  
### STEFANN: Scene Text Editor using Font Adaptive Neural Network. (arXiv:1903.01192v2 [cs.CV] CROSS LISTED)
- Authors : Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
- Link : [http://arxiv.org/abs/1903.01192](http://arxiv.org/abs/1903.01192)
> ABSTRACT  :  Textual information in a captured scene plays an important role in scene interpretation and decision making. Though there exist methods that can successfully detect and interpret complex text regions present in a scene, to the best of our knowledge, there is no significant prior work that aims to modify the textual information in an image. The ability to edit text directly on images has several advantages including error correction, text **restoration** and image reusability. In this paper, we propose a method to modify text in an image at character-level. We approach the problem in two stages. At first, the unobserved character (target) is generated from an observed character (source) being modified. We propose two different neural network architectures - (a) FANnet to achieve structural consistency with source font and (b) Colornet to preserve source color. Next, we replace the source character with the generated character maintaining both geometric and visual consistency with neighboring characters. Our method works as a unified platform for modifying text in images. We present the effectiveness of our method on COCO-Text and ICDAR datasets both qualitatively and quantitatively.  
## eess.IV
---
### Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing. (arXiv:2207.11250v1 [cs.CV])
- Authors : Sai Mitheran, Anushri Suresh
- Link : [http://arxiv.org/abs/2207.11250](http://arxiv.org/abs/2207.11250)
> ABSTRACT  :  Single-image haze removal is a long-standing hurdle for computer vision applications. Several works have been focused on transferring advances from image classification, detection, and segmentation to the niche of image dehazing, primarily focusing on contrastive learning and knowledge distillation. However, these approaches prove computationally expensive, raising concern regarding their applicability to on-the-edge use-cases. This work introduces a simple, lightweight, and efficient framework for single-image haze removal, exploiting rich "**dark**-knowledge" information from a lightweight pre-trained super-resolution model via the notion of heterogeneous knowledge distillation. We designed a feature affinity module to maximize the flow of rich feature semantics from the super-resolution teacher to the student dehazing network. In order to evaluate the efficacy of our proposed framework, its performance as a plug-and-play setup to a baseline model is examined. Our experiments are carried out on the RESIDE-Standard dataset to demonstrate the robustness of our framework to the synthetic and real-world domains. The extensive qualitative and quantitative results provided establish the effectiveness of the framework, achieving gains of upto 15\% (PSNR) while reducing the model size by $\sim$20 times.  
### Arbitrary Style Transfer with Structure **Enhancement** by Combining the Global and Local Loss. (arXiv:2207.11438v1 [cs.CV])
- Authors : Lizhen Long, Man Pun
- Link : [http://arxiv.org/abs/2207.11438](http://arxiv.org/abs/2207.11438)
> ABSTRACT  :  Arbitrary style transfer generates an artistic image which combines the structure of a content image and the artistic style of the artwork by using only one trained network. The image representation used in this method contains content structure representation and the style patterns representation, which is usually the features representation of high-level in the pre-trained classification networks. However, the traditional classification networks were designed for classification which usually focus on high-level features and ignore other features. As the result, the stylized images distribute style elements evenly throughout the image and make the overall image structure unrecognizable. To solve this problem, we introduce a novel arbitrary style transfer method with structure **enhancement** by combining the global and local loss. The local structure details are represented by Lapstyle and the global structure is controlled by the image depth. Experimental results demonstrate that our method can generate higher-quality images with impressive visual effects on several common datasets, comparing with other state-of-the-art methods.  
### Improved Regularization of Event-based Learning by Reversing and Drifting. (arXiv:2207.11659v1 [cs.CV])
- Authors : Haibo Shen, Yihao Luo, Xiang Cao, Liangqi Zhang, Juyu Xiao, Tianjiang Wang
- Link : [http://arxiv.org/abs/2207.11659](http://arxiv.org/abs/2207.11659)
> ABSTRACT  :  Event camera has an enormous potential in challenging scenes for its advantages of high temporal resolution, **high dynamic range**, low power consumption, and no motion blur. However, event-based learning is hindered by insufficient generalization ability. In this paper, we first analyze the influence of different brightness variations on event data. Then we propose two novel augmentation methods: EventReverse and EventDrift. By reversing and drifting events to their corresponding positions in the spatiotemporal or polarity domain, the proposed methods generate samples affected by different brightness variations, which improves the robustness of event-based learning and results in a better generalization. Extensive experiments on N-CARS, N-Caltech101 and CIFAR10-DVS datasets demonstrate that our method is general and remarkably effective.  
### FD-MAR: Fourier Dual-domain Network for CT Metal Artifact Reduction. (arXiv:2207.11678v1 [eess.IV])
- Authors : Zilong Li, Qi Gao, Yaping Wu, Chuang Niu, Junping Zhang, Meiyun Wang, Ge Wang, Hongming Shan
- Link : [http://arxiv.org/abs/2207.11678](http://arxiv.org/abs/2207.11678)
> ABSTRACT  :  The presence of high-density objects such as metal implants and dental fillings can introduce severely streak-like artifacts in computed tomography (CT) images, greatly limiting subsequent diagnosis. Although various deep neural networks-based methods have been proposed for metal artifact reduction (MAR), they usually suffer from poor performance due to limited exploitation of global context in the sinogram domain, secondary artifacts introduced in the image domain, and the requirement of precise metal masks. To address these issues, this paper explores fast Fourier convolution for MAR in both sinogram and image domains, and proposes a Fourier dual-domain network for MAR, termed FD-MAR. Specifically, we first propose a Fourier sinogram **restoration** network, which can leverage sinogram-wide receptive context to fill in the metal-corrupted region from uncorrupted region and, hence, is robust to the metal trace. Second, we propose a Fourier refinement network in the image domain, which can refine the reconstructed images in a local-to-global manner by exploring image-wide context information. As a result, the proposed FD-MAR can explore the sinogram- and image-wide receptive fields for MAR. By optimizing FD-MAR with a composite loss function, extensive experimental results demonstrate the superiority of the proposed FD-MAR over the state-of-the-art MAR methods in terms of quantitative metrics and visual comparison. Notably, FD-MAR does not require precise metal masks, which is of great importance in clinical routine.  
### Enhancing Image Rescaling using Dual Latent Variables in Invertible Neural Network. (arXiv:2207.11844v1 [cs.CV])
- Authors : Min Zhang, Zhihong Pan, Xin Zhou, Jay Kuo
- Link : [http://arxiv.org/abs/2207.11844](http://arxiv.org/abs/2207.11844)
> ABSTRACT  :  Normalizing flow models have been used successfully for generative image super-resolution (SR) by approximating complex distribution of natural images to simple tractable distribution in latent space through Invertible Neural Networks (INN). These models can generate multiple realistic SR images from one low-resolution (LR) input using randomly sampled points in the latent space, simulating the ill-posed nature of image upscaling where multiple high-resolution (HR) images correspond to the same LR. Lately, the invertible process in INN has also been used successfully by bidirectional image rescaling models like IRN and HCFlow for joint optimization of downscaling and inverse upscaling, resulting in significant improvements in upscaled image quality. While they are optimized for image downscaling too, the ill-posed nature of image downscaling, where one HR image could be downsized to multiple LR images depending on different interpolation kernels and resampling methods, is not considered. A new downscaling latent variable, in addition to the original one representing uncertainties in image upscaling, is introduced to model variations in the image downscaling process. This dual latent variable **enhancement** is applicable to different image rescaling models and it is shown in extensive experiments that it can improve image upscaling accuracy consistently without sacrificing image quality in downscaled LR images. It is also shown to be effective in enhancing other INN-based models for image **restoration** applications like image hiding.  
### REPNP: Plug-and-Play with Deep Reinforcement Learning Prior for Robust Image **Restoration**. (arXiv:2207.12056v1 [eess.IV])
- Authors : Chong Wang, Rongkai Zhang, Saiprasad Ravishankar, Bihan Wen
- Link : [http://arxiv.org/abs/2207.12056](http://arxiv.org/abs/2207.12056)
> ABSTRACT  :  Image **restoration** schemes based on the pre-trained deep models have received great attention due to their unique flexibility for solving various inverse problems. In particular, the Plug-and-Play (PnP) framework is a popular and powerful tool that can integrate an off-the-shelf deep denoiser for different image **restoration** tasks with known observation models. However, obtaining the observation model that exactly matches the actual one can be challenging in practice. Thus, the PnP schemes with conventional deep denoisers may fail to generate satisfying results in some real-world image **restoration** tasks. We argue that the robustness of the PnP framework is largely limited by using the off-the-shelf deep denoisers that are trained by deterministic optimization. To this end, we propose a novel deep reinforcement learning (DRL) based PnP framework, dubbed RePNP, by leveraging a light-weight DRL-based denoiser for robust image **restoration** tasks. Experimental results demonstrate that the proposed RePNP is robust to the observation model used in the PnP scheme deviating from the actual one. Thus, RePNP can generate more reliable **restoration** results for image deblurring and super resolution tasks. Compared with several state-of-the-art deep image **restoration** baselines, RePNP achieves better results subjective to model deviation with fewer model parameters.  
### Hardware-in-the-loop simulation of a UAV autonomous landing algorithm implemented in SoC FPGA. (arXiv:2207.12198v1 [cs.RO])
- Authors : Hubert Szolc, Tomasz Kryjak
- Link : [http://arxiv.org/abs/2207.12198](http://arxiv.org/abs/2207.12198)
> ABSTRACT  :  This paper presents a system for hardware-in-the-loop (HiL) simulation of unmanned aerial vehicle (UAV) control algorithms implemented on a heterogeneous SoC FPGA computing platforms. The AirSim simulator running on a PC and an Arty Z7 development board with a Zynq SoC chip from AMD Xilinx were used. Communication was carried out via a serial USB link. An application for autonomous landing on a specially marked landing strip was selected as a case study. A landing site detection algorithm was implemented on the Zynq SoC platform. This allowed processing a 1280 x 720 @ 60 fps video stream in **real time**. Performed tests showed that the system works correctly and there are no delays that could negatively affect the stability of the control. The proposed concept is characterised by relative simplicity and low implementation cost. At the same time, it can be applied to test various types of high-level perception and control algorithms for UAV implemented on embedded platforms. We provide the code developed on GitHub, which includes both Python scripts running on the PC and C code running on Arty Z7.  
### PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v3 [eess.IV] UPDATED)
- Authors : Wentao Liu, Tong Tian, Weijin Xu, Huihua Yang, Xipeng Pan, Songlin Yan, Lemeng Wang
- Link : [http://arxiv.org/abs/2203.04568](http://arxiv.org/abs/2203.04568)
> ABSTRACT  :  The success of Transformer in computer vision has attracted increasing attention in the medical imaging community. Especially for medical image segmentation, many excellent hybrid architectures based on convolutional neural networks (CNNs) and Transformer have been presented and achieve impressive performance. However, most of these methods, which embed modular Transformer into CNNs, struggle to reach their full potential. In this paper, we propose a novel hybrid architecture for medical image segmentation called PHTrans, which parallelly hybridizes Transformer and CNN in main building blocks to produce hierarchical representations from global and local features and adaptively aggregate them, aiming to fully exploit their strengths to obtain better segmentation performance. Specifically, PHTrans follows the U-shaped encoder-decoder design and introduces the parallel hybird module in deep stages, where convolution blocks and the modified 3D **Swin** Transformer learn local features and global dependencies separately, then a sequence-to-volume operation unifies the dimensions of the outputs to achieve feature aggregation. Extensive experimental results on both Multi-Atlas Labeling Beyond the Cranial Vault and Automated Cardiac Diagnosis Challeng datasets corroborate its effectiveness, consistently outperforming state-of-the-art methods. The code is available at: https://github.com/lseventeen/PHTrans.  
### Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model. (arXiv:2207.10040v2 [eess.IV] UPDATED)
- Authors : Zhiyuan Mao, Ajay Jaiswal, Zhangyang Wang
- Link : [http://arxiv.org/abs/2207.10040](http://arxiv.org/abs/2207.10040)
> ABSTRACT  :  Image **restoration** algorithms for atmospheric turbulence are known to be much more challenging to design than traditional ones such as blur or noise because the distortion caused by the turbulence is an entanglement of spatially varying blur, geometric distortion, and sensor noise. Existing CNN-based **restoration** methods built upon convolutional kernels with static weights are insufficient to handle the spatially dynamical atmospheric turbulence effect. To address this problem, in this paper, we propose a physics-inspired transformer model for imaging through atmospheric turbulence. The proposed network utilizes the power of transformer blocks to jointly extract a dynamical turbulence distortion map and restore a turbulence-free image. In addition, recognizing the lack of a comprehensive dataset, we collect and present two new real-world turbulence datasets that allow for evaluation with both classical objective metrics (e.g., PSNR and SSIM) and a new task-driven metric using text recognition accuracy. Both real testing sets and all related code will be made publicly available.  
### Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v2 [eess.IV] UPDATED)
- Authors : Emanuele Dalsasso, my Abergel, Florence Tupin
- Link : [http://arxiv.org/abs/2207.11095](http://arxiv.org/abs/2207.11095)
> ABSTRACT  :  Speckle filtering is generally a prerequisite to the analysis of synthetic aperture radar (SAR) images. Tremendous progress has been achieved in the domain of single-image despeckling. Latest techniques rely on deep neural networks to restore the various structures and textures peculiar to SAR images. The availability of time series of SAR images offers the possibility of improving speckle filtering by combining different speckle realizations over the same area. The supervised training of deep neural networks requires ground-truth speckle-free images. Such images can only be obtained indirectly through some form of averaging, by spatial or temporal integration, and are imperfect. Given the potential of very high quality **restoration** reachable by multi-temporal speckle filtering, the limitations of ground-truth images need to be circumvented. We extend a recent self-supervised training strategy for single-look complex SAR images, called MERLIN, to the case of multi-temporal filtering. This requires modeling the sources of statistical dependencies in the spatial and temporal dimensions as well as between the real and imaginary components of the complex amplitudes. Quantitative analysis on datasets with simulated speckle indicates a clear improvement of speckle reduction when additional SAR images are included. Our method is then applied to stacks of TerraSAR-X images and shown to outperform competing multi-temporal speckle filtering approaches. The code of the trained models is made freely available on the Gitlab of the IMAGES team of the LTCI Lab, T\'el\'ecom Paris Institut Polytechnique de Paris (https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/).  
## cs.LG
---
### Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing. (arXiv:2207.11250v1 [cs.CV])
- Authors : Sai Mitheran, Anushri Suresh
- Link : [http://arxiv.org/abs/2207.11250](http://arxiv.org/abs/2207.11250)
> ABSTRACT  :  Single-image haze removal is a long-standing hurdle for computer vision applications. Several works have been focused on transferring advances from image classification, detection, and segmentation to the niche of image dehazing, primarily focusing on contrastive learning and knowledge distillation. However, these approaches prove computationally expensive, raising concern regarding their applicability to on-the-edge use-cases. This work introduces a simple, lightweight, and efficient framework for single-image haze removal, exploiting rich "**dark**-knowledge" information from a lightweight pre-trained super-resolution model via the notion of heterogeneous knowledge distillation. We designed a feature affinity module to maximize the flow of rich feature semantics from the super-resolution teacher to the student dehazing network. In order to evaluate the efficacy of our proposed framework, its performance as a plug-and-play setup to a baseline model is examined. Our experiments are carried out on the RESIDE-Standard dataset to demonstrate the robustness of our framework to the synthetic and real-world domains. The extensive qualitative and quantitative results provided establish the effectiveness of the framework, achieving gains of upto 15\% (PSNR) while reducing the model size by $\sim$20 times.  
### A Taxonomy of Recurrent Learning Rules. (arXiv:2207.11439v1 [cs.LG])
- Authors : Guillermo Mart, Sander Boht, Sebastian Otte
- Link : [http://arxiv.org/abs/2207.11439](http://arxiv.org/abs/2207.11439)
> ABSTRACT  :  Backpropagation through time (BPTT) is the de facto standard for training recurrent neural networks (RNNs), but it is non-causal and non-local. **Real-time** recurrent learning is a causal alternative, but it is highly inefficient. Recently, e-prop was proposed as a causal, local, and efficient practical alternative to these algorithms, providing an approximation of the exact gradient by radically pruning the recurrent dependencies carried over time. Here, we derive RTRL from BPTT using a detailed notation bringing intuition and clarification to how they are connected. Furthermore, we frame e-prop within in the picture, formalising what it approximates. Finally, we derive a family of algorithms of which e-prop is a special case.  
### Anomaly Detection for Fraud in Cryptocurrency Time Series. (arXiv:2207.11466v1 [cs.LG])
- Authors : Eran Kaufman, Andrey Iaremenko
- Link : [http://arxiv.org/abs/2207.11466](http://arxiv.org/abs/2207.11466)
> ABSTRACT  :  Since the inception of Bitcoin in 2009, the market of cryptocurrencies has grown beyond initial expectations as daily trades exceed $10 billion. As industries become automated, the need for an automated fraud detector becomes very apparent. Detecting anomalies in **real time** prevents potential accidents and economic losses. Anomaly detection in multivariate time series data poses a particular challenge because it requires simultaneous consideration of temporal dependencies and relationships between variables. Identifying an anomaly in **real time** is not an easy task specifically because of the exact anomalistic behavior they observe. Some points may present pointwise global or local anomalistic behavior, while others may be anomalistic due to their frequency or seasonal behavior or due to a change in the trend. In this paper we suggested working on **real time** series of trades of Ethereum from specific accounts and surveyed a large variety of different algorithms traditional and new. We categorized them according to the strategy and the anomalistic behavior which they search and showed that when bundling them together to different groups, they can prove to be a good real-time detector with an alarm time of no longer than a few seconds and with very high confidence.  
### Robust Scene Inference under Noise-Blur Dual Corruptions. (arXiv:2207.11643v1 [cs.CV])
- Authors : Bhavya Goyal, ois Lalonde, Yin Li, Mohit Gupta
- Link : [http://arxiv.org/abs/2207.11643](http://arxiv.org/abs/2207.11643)
> ABSTRACT  :  Scene inference under **low-light** is a challenging problem due to severe noise in the captured images. One way to reduce noise is to use longer **exposure** during the capture. However, in the presence of motion (scene or camera motion), longer **exposure**s lead to motion blur, resulting in loss of image information. This creates a trade-off between these two kinds of image degradations: motion blur (due to long **exposure**) vs. noise (due to short **exposure**), also referred as a dual image corruption pair in this paper. With the rise of cameras capable of capturing multiple **exposure**s of the same scene simultaneously, it is possible to overcome this trade-off. Our key observation is that although the amount and nature of degradation varies for these different image captures, the semantic content remains the same across all images. To this end, we propose a method to leverage these multi **exposure** captures for robust inference under **low-light** and motion. Our method builds on a feature consistency loss to encourage similar results from these individual captures, and uses the ensemble of their final predictions for robust visual recognition. We demonstrate the effectiveness of our approach on simulated images as well as real captures with multiple **exposure**s, and across the tasks of object detection and image classification.  
### Improved Regularization of Event-based Learning by Reversing and Drifting. (arXiv:2207.11659v1 [cs.CV])
- Authors : Haibo Shen, Yihao Luo, Xiang Cao, Liangqi Zhang, Juyu Xiao, Tianjiang Wang
- Link : [http://arxiv.org/abs/2207.11659](http://arxiv.org/abs/2207.11659)
> ABSTRACT  :  Event camera has an enormous potential in challenging scenes for its advantages of high temporal resolution, **high dynamic range**, low power consumption, and no motion blur. However, event-based learning is hindered by insufficient generalization ability. In this paper, we first analyze the influence of different brightness variations on event data. Then we propose two novel augmentation methods: EventReverse and EventDrift. By reversing and drifting events to their corresponding positions in the spatiotemporal or polarity domain, the proposed methods generate samples affected by different brightness variations, which improves the robustness of event-based learning and results in a better generalization. Extensive experiments on N-CARS, N-Caltech101 and CIFAR10-DVS datasets demonstrate that our method is general and remarkably effective.  
### Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (arXiv:2207.11694v1 [cs.LG])
- Authors : Quanshi Zhang, Xin Wang, Jie Ren, Xu Cheng, Shuyun Lin, Yisen Wang, Xiangming Zhu
- Link : [http://arxiv.org/abs/2207.11694](http://arxiv.org/abs/2207.11694)
> ABSTRACT  :  Although many methods have been proposed to enhance the transferability of adversarial perturbations, these methods are designed in a heuristic manner, and the essential mechanism for improving adversarial transferability is still unclear. This paper summarizes the common mechanism shared by twelve previous transferability-boosting methods in a unified view, i.e., these methods all reduce game-theoretic interactions between regional adversarial perturbations. To this end, we focus on the attacking utility of all interactions between regional adversarial perturbations, and we first discover and prove the negative correlation between the adversarial transferability and the attacking utility of interactions. Based on this discovery, we theoretically prove and empirically verify that twelve previous transferability-boosting methods all reduce interactions between regional adversarial perturbations. More crucially, we consider the reduction of interactions as the essential reason for the **enhancement** of adversarial transferability. Furthermore, we design the interaction loss to directly penalize interactions between regional adversarial perturbations during attacking. Experimental results show that the interaction loss significantly improves the transferability of adversarial perturbations.  
### Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges. (arXiv:2207.11759v1 [cs.LG])
- Authors : **Lei Zhang**, Guanyu Gao, Huaizheng Zhang
- Link : [http://arxiv.org/abs/2207.11759](http://arxiv.org/abs/2207.11759)
> ABSTRACT  :  Data drift is a thorny challenge when deploying person re-identification (ReID) models into real-world devices, where the data distribution is significantly different from that of the training environment and keeps changing. To tackle this issue, we propose a federated spatial-temporal incremental learning approach, named FedSTIL, which leverages both lifelong learning and federated learning to continuously optimize models deployed on many distributed edge clients. Unlike previous efforts, FedSTIL aims to mine spatial-temporal correlations among the knowledge learnt from different edge clients. Specifically, the edge clients first periodically extract general representations of drifted data to optimize their local models. Then, the learnt knowledge from edge clients will be aggregated by centralized parameter server, where the knowledge will be selectively and attentively distilled from spatial- and temporal-dimension with carefully designed mechanisms. Finally, the distilled informative spatial-temporal knowledge will be sent back to correlated edge clients to further improve the recognition accuracy of each edge client with a lifelong learning method. Extensive experiments on a mixture of five real-world datasets demonstrate that our method outperforms others by nearly 4% in Rank-1 accuracy, while reducing communication cost by 62%. All implementation codes are publicly available on https://github.com/MSNLAB/Federated-Lifelong-Person-ReID  
### R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v2 [cs.CV] UPDATED)
- Authors : Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, Sergey Tulyakov
- Link : [http://arxiv.org/abs/2203.17261](http://arxiv.org/abs/2203.17261)
> ABSTRACT  :  Recent research explosion on Neural Radiance Field (**NeRF**) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of **NeRF** is its prohibitive inference time: Rendering a single pixel requires querying the **NeRF** network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over **NeRF** in novel view synthesis -- the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained **NeRF** model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average PSNR improvement) rendering quality than **NeRF** without any customized parallelism requirement.  
### MaxViT: Multi-Axis Vision Transformer. (arXiv:2204.01697v2 [cs.CV] UPDATED)
- Authors : Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, **Peyman Milanfar**, Alan Bovik, Yinxiao Li
- Link : [http://arxiv.org/abs/2204.01697](http://arxiv.org/abs/2204.01697)
> ABSTRACT  :  Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to ''see'' globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. The source code and trained models will be available at https://github.com/google-research/maxvit.  
## cs.AI
---
### PS-**NeRF**: Neural Inverse Rendering for Multi-view Photometric Stereo. (arXiv:2207.11406v1 [cs.CV])
- Authors : Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang Chen
- Link : [http://arxiv.org/abs/2207.11406](http://arxiv.org/abs/2207.11406)
> ABSTRACT  :  Traditional multi-view photometric stereo (MVPS) methods are often composed of multiple disjoint stages, resulting in noticeable accumulated errors. In this paper, we present a neural inverse rendering method for MVPS based on implicit representation. Given multi-view images of a non-Lambertian object illuminated by multiple unknown directional lights, our method jointly estimates the geometry, materials, and lights. Our method first employs multi-light images to estimate per-view surface normal maps, which are used to regularize the normals derived from the neural radiance field. It then jointly optimizes the surface normals, spatially-varying BRDFs, and lights based on a shadow-aware differentiable rendering layer. After optimization, the reconstructed object can be used for novel-view rendering, relighting, and material editing. Experiments on both synthetic and real datasets demonstrate that our method achieves far more accurate shape reconstruction than existing MVPS and neural rendering methods. Our code and model can be found at https://ywq.github.io/psnerf.  
### High-Resolution **Swin** Transformer for Automatic Medical Image Segmentation. (arXiv:2207.11553v1 [cs.CV])
- Authors : Chen Wei, Shenghan Ren, Kaitai Guo, Haihong Hu, Jimin Liang
- Link : [http://arxiv.org/abs/2207.11553](http://arxiv.org/abs/2207.11553)
> ABSTRACT  :  The Resolution of feature maps is critical for medical image segmentation. Most of the existing Transformer-based networks for medical image segmentation are U-Net-like architecture that contains an encoder that utilizes a sequence of Transformer blocks to convert the input medical image from high-resolution representation into low-resolution feature maps and a decoder that gradually recovers the high-resolution representation from low-resolution feature maps. Unlike previous studies, in this paper, we utilize the network design style from the High-Resolution Network (HRNet), replace the convolutional layers with Transformer blocks, and continuously exchange information from the different resolution feature maps that are generated by Transformer blocks. The newly Transformer-based network presented in this paper is denoted as High-Resolution **Swin** Transformer Network (HRSTNet). Extensive experiments illustrate that HRSTNet can achieve comparable performance with the state-of-the-art Transformer-based U-Net-like architecture on Brain Tumor Segmentation(BraTS) 2021 and the liver dataset from Medical Segmentation Decathlon. The code of HRSTNet will be publicly available at https://github.com/auroua/HRSTNet.  
### Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (arXiv:2207.11694v1 [cs.LG])
- Authors : Quanshi Zhang, Xin Wang, Jie Ren, Xu Cheng, Shuyun Lin, Yisen Wang, Xiangming Zhu
- Link : [http://arxiv.org/abs/2207.11694](http://arxiv.org/abs/2207.11694)
> ABSTRACT  :  Although many methods have been proposed to enhance the transferability of adversarial perturbations, these methods are designed in a heuristic manner, and the essential mechanism for improving adversarial transferability is still unclear. This paper summarizes the common mechanism shared by twelve previous transferability-boosting methods in a unified view, i.e., these methods all reduce game-theoretic interactions between regional adversarial perturbations. To this end, we focus on the attacking utility of all interactions between regional adversarial perturbations, and we first discover and prove the negative correlation between the adversarial transferability and the attacking utility of interactions. Based on this discovery, we theoretically prove and empirically verify that twelve previous transferability-boosting methods all reduce interactions between regional adversarial perturbations. More crucially, we consider the reduction of interactions as the essential reason for the **enhancement** of adversarial transferability. Furthermore, we design the interaction loss to directly penalize interactions between regional adversarial perturbations during attacking. Experimental results show that the interaction loss significantly improves the transferability of adversarial perturbations.  
### Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges. (arXiv:2207.11759v1 [cs.LG])
- Authors : **Lei Zhang**, Guanyu Gao, Huaizheng Zhang
- Link : [http://arxiv.org/abs/2207.11759](http://arxiv.org/abs/2207.11759)
> ABSTRACT  :  Data drift is a thorny challenge when deploying person re-identification (ReID) models into real-world devices, where the data distribution is significantly different from that of the training environment and keeps changing. To tackle this issue, we propose a federated spatial-temporal incremental learning approach, named FedSTIL, which leverages both lifelong learning and federated learning to continuously optimize models deployed on many distributed edge clients. Unlike previous efforts, FedSTIL aims to mine spatial-temporal correlations among the knowledge learnt from different edge clients. Specifically, the edge clients first periodically extract general representations of drifted data to optimize their local models. Then, the learnt knowledge from edge clients will be aggregated by centralized parameter server, where the knowledge will be selectively and attentively distilled from spatial- and temporal-dimension with carefully designed mechanisms. Finally, the distilled informative spatial-temporal knowledge will be sent back to correlated edge clients to further improve the recognition accuracy of each edge client with a lifelong learning method. Extensive experiments on a mixture of five real-world datasets demonstrate that our method outperforms others by nearly 4% in Rank-1 accuracy, while reducing communication cost by 62%. All implementation codes are publicly available on https://github.com/MSNLAB/Federated-Lifelong-Person-ReID  
### Bungee**NeRF**: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. (arXiv:2112.05504v3 [cs.CV] UPDATED)
- Authors : Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, Dahua Lin
- Link : [http://arxiv.org/abs/2112.05504](http://arxiv.org/abs/2112.05504)
> ABSTRACT  :  Neural radiance fields (**NeRF**) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce Bungee**NeRF**, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shallow base block, as training progresses, new blocks are appended to accommodate the emerging details in the increasingly closer views. The strategy progressively activates high-frequency channels in **NeRF**'s positional encoding inputs and successively unfolds more complex details as the training proceeds. We demonstrate the superiority of Bungee**NeRF** in modeling diverse multi-scale scenes with drastically varying views on multiple data sources (city models, synthetic, and drone captured data) and its support for high-quality rendering in different levels of detail.  
### R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v2 [cs.CV] UPDATED)
- Authors : Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, Sergey Tulyakov
- Link : [http://arxiv.org/abs/2203.17261](http://arxiv.org/abs/2203.17261)
> ABSTRACT  :  Recent research explosion on Neural Radiance Field (**NeRF**) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of **NeRF** is its prohibitive inference time: Rendering a single pixel requires querying the **NeRF** network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over **NeRF** in novel view synthesis -- the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained **NeRF** model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average PSNR improvement) rendering quality than **NeRF** without any customized parallelism requirement.  
### MaxViT: Multi-Axis Vision Transformer. (arXiv:2204.01697v2 [cs.CV] UPDATED)
- Authors : Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, **Peyman Milanfar**, Alan Bovik, Yinxiao Li
- Link : [http://arxiv.org/abs/2204.01697](http://arxiv.org/abs/2204.01697)
> ABSTRACT  :  Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to ''see'' globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. The source code and trained models will be available at https://github.com/google-research/maxvit.  
# Paper List
---
## cs.CV
---
**188** new papers in cs.CV:-) 
1. Brain tumor detection using artificial convolutional neural networks. (arXiv:2207.11248v1 [eess.IV])
2. Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing. (arXiv:2207.11250v1 [cs.CV])
3. PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation. (arXiv:2207.11325v1 [cs.CV])
4. Video **Swin** Transformers for Egocentric Video Understanding @ Ego4D Challenges 2022. (arXiv:2207.11329v1 [cs.CV])
5. Dynamic Graph Reasoning for Multi-person 3D Pose Estimation. (arXiv:2207.11341v1 [cs.CV])
6. An Impartial Take to the CNN vs Transformer Robustness Contest. (arXiv:2207.11347v1 [cs.CV])
7. Deep neural network heatmaps capture Alzheimer's disease patterns reported in a large meta-analysis of neuroimaging studies. (arXiv:2207.11352v1 [cs.CV])
8. Egocentric scene context for human-centric environment understanding from video. (arXiv:2207.11365v1 [cs.CV])
9. Neural-Sim: Learning to Generate Training Data with **NeRF**. (arXiv:2207.11368v1 [cs.CV])
10. Evaluation of Different Annotation Strategies for Deployment of Parking Spaces Classification Systems. (arXiv:2207.11372v1 [cs.CV])
11. Do Perceptually Aligned Gradients Imply Adversarial Robustness?. (arXiv:2207.11378v1 [cs.CV])
12. Two-Aspect Information Fusion Model For ABAW4 Multi-task Challenge. (arXiv:2207.11389v1 [cs.CV])
13. Deep Pneumonia: Attention-Based Contrastive Learning for Class-Imbalanced Pneumonia Lesion Recognition in Chest X-rays. (arXiv:2207.11393v1 [cs.CV])
14. Orientation and Context Entangled Network for Retinal Vessel Segmentation. (arXiv:2207.11396v1 [cs.CV])
15. Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations. (arXiv:2207.11401v1 [cs.CL])
16. PS-**NeRF**: Neural Inverse Rendering for Multi-view Photometric Stereo. (arXiv:2207.11406v1 [cs.CV])
17. Halftoning with Multi-Agent Deep Reinforcement Learning. (arXiv:2207.11408v1 [cs.CV])
18. Satellite Detection in Unresolved Space Imagery for Space Domain Awareness Using Neural Networks. (arXiv:2207.11412v1 [cs.CV])
19. Detection and Initial Assessment of Lunar Landing Sites Using Neural Networks. (arXiv:2207.11413v1 [cs.RO])
20. Arbitrary Style Transfer with Structure **Enhancement** by Combining the Global and Local Loss. (arXiv:2207.11438v1 [cs.CV])
21. Meta Spatio-Temporal Debiasing for Video Scene Graph Generation. (arXiv:2207.11441v1 [cs.CV])
22. BuyTheDips: PathLoss for improved topology-preserving deep learning-based image segmentation. (arXiv:2207.11446v1 [cs.CV])
23. UC-OWOD: Unknown-Classified Open World Object Detection. (arXiv:2207.11455v1 [cs.CV])
24. When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2207.11463v1 [cs.CV])
25. Learning Object Placement via Dual-path Graph Completion. (arXiv:2207.11464v1 [cs.CV])
26. CompNVS: Novel View Synthesis with Scene Completion. (arXiv:2207.11467v1 [cs.CV])
27. Progressive Scene Text Erasing with Self-Supervision. (arXiv:2207.11469v1 [cs.CV])
28. 3D Labeling Tool. (arXiv:2207.11479v1 [cs.CV])
29. Multimodal Emotion Recognition with Modality-Pairwise Unsupervised Contrastive Loss. (arXiv:2207.11482v1 [cs.CV])
30. GraphFit: Learning Multi-scale Graph-Convolutional Representation for Point Cloud Normal Estimation. (arXiv:2207.11484v1 [cs.CV])
31. Active Pointly-Supervised Instance Segmentation. (arXiv:2207.11493v1 [cs.CV])
32. Intelligent 3D Network Protocol for Multimedia Data Classification using Deep Learning. (arXiv:2207.11504v1 [cs.CV])
33. SSBNet: Improving Visual Recognition Efficiency by Adaptive Sampling. (arXiv:2207.11511v1 [cs.CV])
34. Combining Hybrid Architecture and Pseudo-label for Semi-supervised Abdominal Organ Segmentation. (arXiv:2207.11512v1 [cs.CV])
35. Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models. (arXiv:2207.11514v1 [cs.CV])
36. Marior: Margin Removal and Iterative Content Rectification for Document Dewarping in the Wild. (arXiv:2207.11515v1 [cs.CV])
37. Contrastive Monotonic Pixel-Level Modulation. (arXiv:2207.11517v1 [cs.CV])
38. Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition. (arXiv:2207.11518v1 [cs.CV])
39. Unstructured Road Segmentation using Hypercolumn based Random Forests of Local experts. (arXiv:2207.11523v1 [cs.CV])
40. Audio-driven Neural Gesture Reenactment with Video Motion Graphs. (arXiv:2207.11524v1 [cs.CV])
41. HPS-Det: Dynamic Sample Assignment with Hyper-Parameter Search for Object Detection. (arXiv:2207.11539v1 [cs.CV])
42. Self-Support Few-Shot Semantic Segmentation. (arXiv:2207.11549v1 [cs.CV])
43. High-Resolution **Swin** Transformer for Automatic Medical Image Segmentation. (arXiv:2207.11553v1 [cs.CV])
44. Towards Open Set 3D Learning: A Benchmark on Object Point Clouds. (arXiv:2207.11554v1 [cs.CV])
45. Robots Enact Malignant Stereotypes. (arXiv:2207.11569v1 [cs.RO])
46. Self-Supervised Learning of Echocardiogram Videos Enables Data-Efficient Clinical Diagnosis. (arXiv:2207.11581v1 [cs.CV])
47. Defining an action of SO(d)-rotations on images generated by projections of d-dimensional objects: Applications to pose inference with Geometric VAEs. (arXiv:2207.11582v1 [cs.CV])
48. Generative Artisan: A Semantic-Aware and Controllable CLIPstyler. (arXiv:2207.11598v1 [cs.CV])
49. Face Deblurring using Dual Camera Fusion on Mobile Phones. (arXiv:2207.11617v1 [cs.CV])
50. A Simplistic and Cost-Effective Design for Real-World Development of an Ambient Assisted Living System for Fall Detection and Indoor Localization: Proof of Concept. (arXiv:2207.11623v1 [cs.HC])
51. Spatial-temporal Analysis for Automated Concrete Workability Estimation. (arXiv:2207.11635v1 [cs.CV])
52. Explored An Effective Methodology for Fine-Grained Snake Recognition. (arXiv:2207.11637v1 [cs.CV])
53. DCT Approximations Based on Chen's Factorization. (arXiv:2207.11638v1 [eess.SP])
54. Robust Scene Inference under Noise-Blur Dual Corruptions. (arXiv:2207.11643v1 [cs.CV])
55. Improved Regularization of Event-based Learning by Reversing and Drifting. (arXiv:2207.11659v1 [cs.CV])
56. MAR: Masked Autoencoders for Efficient Action Recognition. (arXiv:2207.11660v1 [cs.CV])
57. Modeling Associative Plasticity between Synapses to Enhance Learning of Spiking Neural Networks. (arXiv:2207.11670v1 [cs.NE])
58. Learnable Privacy-Preserving Anonymization for Pedestrian Images. (arXiv:2207.11677v1 [cs.CV])
59. FD-MAR: Fourier Dual-domain Network for CT Metal Artifact Reduction. (arXiv:2207.11678v1 [eess.IV])
60. Affective Behaviour Analysis Using Pretrained Model with Facial Priori. (arXiv:2207.11679v1 [cs.CV])
61. Learning Graph Neural Networks for Image Style Transfer. (arXiv:2207.11681v1 [cs.CV])
62. PCA: Semi-supervised Segmentation with Patch Confidence Adversarial Training. (arXiv:2207.11683v1 [eess.IV])
63. Kernel Relative-prototype Spectral Filtering for Few-shot Learning. (arXiv:2207.11685v1 [cs.CV])
64. Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (arXiv:2207.11694v1 [cs.LG])
65. Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v1 [cs.CV])
66. Improving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes. (arXiv:2207.11707v1 [cs.CV])
67. Keypoint-less Camera Calibration for Sports Field Registration in Soccer. (arXiv:2207.11709v1 [cs.CV])
68. TIPS: Text-Induced Pose Synthesis. (arXiv:2207.11718v1 [cs.CV])
69. Combining Internal and External Constraints for Unrolling Shutter in Videos. (arXiv:2207.11725v1 [cs.CV])
70. Can we achieve robustness from data alone?. (arXiv:2207.11727v1 [cs.LG])
71. Improved Super Resolution of MR Images Using CNNs and Vision Transformers. (arXiv:2207.11748v1 [eess.IV])
72. Label-Guided Auxiliary Training Improves 3D Object Detector. (arXiv:2207.11753v1 [cs.CV])
73. Learning Generalizable Light Field Networks from Few Images. (arXiv:2207.11757v1 [cs.CV])
74. Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges. (arXiv:2207.11759v1 [cs.LG])
75. Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis. (arXiv:2207.11770v1 [cs.CV])
76. Image Denoising Using Convolutional Autoencoder. (arXiv:2207.11771v1 [cs.CV])
77. Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection. (arXiv:2207.11789v1 [cs.CV])
78. PatchRD: Detail-Preserving Shape Completion by Learning Patch Retrieval and Deformation. (arXiv:2207.11790v1 [cs.CV])
79. Cross-Modal 3D Shape Generation and Manipulation. (arXiv:2207.11795v1 [cs.CV])
80. Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions. (arXiv:2207.11805v1 [cs.CV])
81. VizWiz-FewShot: Locating Objects in Images Taken by People With Visual Impairments. (arXiv:2207.11810v1 [cs.CV])
82. Object State Change Classification in Egocentric Videos using the Divided Space-Time Attention Mechanism. (arXiv:2207.11814v1 [cs.CV])
83. Inter-model Interpretability: Self-supervised Models as a Case Study. (arXiv:2207.11837v1 [cs.CV])
84. SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions. (arXiv:2207.11838v1 [cs.CV])
85. A Deep Dive into Deep Cluster. (arXiv:2207.11839v1 [cs.CV])
86. Enhancing Image Rescaling using Dual Latent Variables in Invertible Neural Network. (arXiv:2207.11844v1 [cs.CV])
87. Human Gait Database for Normal Walk Collected by Smartphone Accelerometer. (arXiv:1905.03109v3 [eess.SP] UPDATED)
88. Learning Test-time Augmentation for Content-based Image Retrieval. (arXiv:2002.01642v5 [cs.CV] UPDATED)
89. Explaining How Deep Neural Networks Forget by Deep Visualization. (arXiv:2005.01004v3 [cs.LG] UPDATED)
90. SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation. (arXiv:2012.10217v5 [cs.CV] UPDATED)
91. CLASTER: Clustering with Reinforcement Learning for Zero-Shot Action Recognition. (arXiv:2101.07042v3 [cs.CV] UPDATED)
92. Lidar Point Cloud Guided Monocular 3D Object Detection. (arXiv:2104.09035v3 [cs.CV] UPDATED)
93. TextAdaIN: Paying Attention to Shortcut Learning in Text Recognizers. (arXiv:2105.03906v3 [cs.CV] UPDATED)
94. KVT: k-NN Attention for Boosting Vision Transformers. (arXiv:2106.00515v3 [cs.CV] UPDATED)
95. 2D vs. 3D LiDAR-based Person Detection on Mobile Robots. (arXiv:2106.11239v2 [cs.RO] UPDATED)
96. Hierarchical Self-supervised Augmented Knowledge Distillation. (arXiv:2107.13715v2 [cs.CV] UPDATED)
97. VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation. (arXiv:2107.13824v2 [cs.CV] UPDATED)
98. Neural Scene Decoration from a Single Photograph. (arXiv:2108.01806v2 [cs.CV] UPDATED)
99. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors. (arXiv:2109.00182v2 [cs.CV] UPDATED)
100. Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v2 [cs.CV] UPDATED)
101. Clustering performance analysis using a new correlation-based cluster validity index. (arXiv:2109.11172v2 [stat.ML] UPDATED)
102. Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization. (arXiv:2109.14549v2 [cs.RO] UPDATED)
103. Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization. (arXiv:2109.15222v3 [cs.CV] UPDATED)
104. Construction Site Safety Monitoring and Excavator Activity Analysis System. (arXiv:2110.03083v3 [cs.CV] UPDATED)
105. Evaluating generative networks using Gaussian mixtures of image features. (arXiv:2110.05240v2 [cs.CV] UPDATED)
106. Adversarial Attack across Datasets. (arXiv:2110.07718v2 [cs.CV] UPDATED)
107. On Generating Identifiable Virtual Faces. (arXiv:2110.07986v2 [cs.CV] UPDATED)
108. NYU-VPR: Long-Term Visual Place Recognition Benchmark with View Direction and Data Anonymization Influences. (arXiv:2110.09004v2 [cs.CV] UPDATED)
109. IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v4 [cs.CV] UPDATED)
110. Single-Item Fashion Recommender: Towards Cross-Domain Recommendations. (arXiv:2111.00758v2 [cs.IR] UPDATED)
111. Exploring the Semi-supervised Video Object Segmentation Problem from a Cyclic Perspective. (arXiv:2111.01323v2 [cs.CV] UPDATED)
112. Sliced Recursive Transformer. (arXiv:2111.05297v2 [cs.CV] UPDATED)
113. Synthetic Document Generator for Annotation-free Layout Recognition. (arXiv:2111.06016v3 [cs.CV] UPDATED)
114. SWAT: Spatial Structure Within and Among Tokens. (arXiv:2111.13677v2 [cs.CV] UPDATED)
115. OOD-CV: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images. (arXiv:2111.14341v3 [cs.CV] UPDATED)
116. Event Neural Networks. (arXiv:2112.00891v2 [cs.CV] UPDATED)
117. TransFGU: A Top-down Approach to Fine-Grained Unsupervised Semantic Segmentation. (arXiv:2112.01515v2 [cs.CV] UPDATED)
118. AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration. (arXiv:2112.01740v3 [cs.CV] UPDATED)
119. ROCA: Robust CAD Model Retrieval and Alignment from a Single Image. (arXiv:2112.01988v2 [cs.CV] UPDATED)
120. Coupling Vision and Proprioception for Navigation of Legged Robots. (arXiv:2112.02094v2 [cs.RO] UPDATED)
121. BLT: Bidirectional Layout Transformer for Controllable Layout Generation. (arXiv:2112.05112v2 [cs.CV] UPDATED)
122. Bungee**NeRF**: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. (arXiv:2112.05504v3 [cs.CV] UPDATED)
123. COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality. (arXiv:2112.05892v3 [cs.CV] UPDATED)
124. EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices. (arXiv:2112.07642v2 [cs.CV] UPDATED)
125. Multimodal Image Synthesis and Editing: A Survey. (arXiv:2112.13592v2 [cs.CV] UPDATED)
126. Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v3 [cs.CV] UPDATED)
127. Domain Generalization via Frequency-domain-based Feature Disentanglement and Interaction. (arXiv:2201.08029v2 [cs.CV] UPDATED)
128. Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v4 [eess.IV] UPDATED)
129. Interactron: Embodied Adaptive Object Detection. (arXiv:2202.00660v3 [cs.CV] UPDATED)
130. Active Audio-Visual Separation of Dynamic Sound Sources. (arXiv:2202.00850v2 [cs.CV] UPDATED)
131. A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility. (arXiv:2202.02312v2 [cs.CL] UPDATED)
132. The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v2 [cs.CV] UPDATED)
133. Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans on Youtube. (arXiv:2202.10448v2 [cs.RO] UPDATED)
134. On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v3 [cs.LG] UPDATED)
135. PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v3 [eess.IV] UPDATED)
136. PD-Flow: A Point Cloud Denoising Framework with Normalizing Flows. (arXiv:2203.05940v2 [cs.CV] UPDATED)
137. Worst Case Matters for Few-Shot Recognition. (arXiv:2203.06574v2 [cs.CV] UPDATED)
138. SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v3 [cs.CV] UPDATED)
139. A Survey of Historical Document Image Datasets. (arXiv:2203.08504v2 [cs.CV] UPDATED)
140. PressureVision: Estimating Hand Pressure from a Single RGB Image. (arXiv:2203.10385v2 [cs.CV] UPDATED)
141. V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer. (arXiv:2203.10638v2 [cs.CV] UPDATED)
142. HM: Hybrid Masking for Few-Shot Segmentation. (arXiv:2203.12826v2 [cs.CV] UPDATED)
143. A Visual Navigation Perspective for Category-Level Object Pose Estimation. (arXiv:2203.13572v2 [cs.CV] UPDATED)
144. End-to-End Active Speaker Detection. (arXiv:2203.14250v2 [cs.CV] UPDATED)
145. Discovering Human-Object Interaction Concepts via Self-Compositional Learning. (arXiv:2203.14272v2 [cs.CV] UPDATED)
146. Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v2 [cs.CV] UPDATED)
147. Acknowledging the Unknown for Multi-label Learning with Single Positive Labels. (arXiv:2203.16219v2 [cs.CV] UPDATED)
148. SeqTR: A Simple yet Universal Network for Visual Grounding. (arXiv:2203.16265v2 [cs.CV] UPDATED)
149. R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v2 [cs.CV] UPDATED)
150. Digitizing Historical Balance Sheet Data: A Practitioner's Guide. (arXiv:2204.00052v2 [cs.CV] UPDATED)
151. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v3 [cs.CV] UPDATED)
152. Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v2 [cs.CV] UPDATED)
153. MaxViT: Multi-Axis Vision Transformer. (arXiv:2204.01697v2 [cs.CV] UPDATED)
154. Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. (arXiv:2204.03638v2 [cs.CV] UPDATED)
155. Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories. (arXiv:2204.04153v2 [cs.CV] UPDATED)
156. FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring. (arXiv:2204.07820v2 [cs.CV] UPDATED)
157. Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency. (arXiv:2204.10310v3 [cs.CV] UPDATED)
158. Where in the World is this Image? Transformer-based Geo-localization in the Wild. (arXiv:2204.13861v2 [cs.CV] UPDATED)
159. BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking. (arXiv:2205.02301v3 [cs.CV] UPDATED)
160. CLIP-CLOP: CLIP-Guided Collage and Photomontage. (arXiv:2205.03146v3 [cs.CV] UPDATED)
161. Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation. (arXiv:2205.03962v2 [cs.CV] UPDATED)
162. VQFR: Blind Face **Restoration** with Vector-Quantized Dictionary and Parallel Decoder. (arXiv:2205.06803v2 [cs.CV] UPDATED)
163. Unsupervised Segmentation in Real-World Images via Spelke Object Inference. (arXiv:2205.08515v2 [cs.CV] UPDATED)
164. Physically-Based Editing of Indoor Scene Lighting from a Single Image. (arXiv:2205.09343v2 [cs.CV] UPDATED)
165. Human Gender Prediction Based on Deep Transfer Learning from Panoramic Radiograph Images. (arXiv:2205.09850v2 [eess.IV] UPDATED)
166. Salient Skin Lesion Segmentation via Dilated Scale-Wise Feature Fusion Network. (arXiv:2205.10272v2 [cs.CV] UPDATED)
167. Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v5 [cs.CV] UPDATED)
168. SiSPRNet: End-to-End Learning for Single-Shot Phase Retrieval. (arXiv:2205.11434v2 [cs.CV] UPDATED)
169. Robust 3D Object Detection in Cold Weather Conditions. (arXiv:2205.11925v2 [cs.CV] UPDATED)
170. RADNet: Ensemble Model for Robust Glaucoma Classification in Color Fundus Images. (arXiv:2205.12902v2 [eess.IV] UPDATED)
171. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v2 [cs.AI] UPDATED)
172. Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition. (arXiv:2206.04790v2 [cs.CV] UPDATED)
173. A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects. (arXiv:2206.07219v2 [eess.IV] UPDATED)
174. Image Captioning based on Feature Refinement and Reflective Decoding. (arXiv:2206.07986v2 [cs.CV] UPDATED)
175. Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis. (arXiv:2206.13608v2 [cs.CV] UPDATED)
176. Timestamp-Supervised Action Segmentation with Graph Convolutional Networks. (arXiv:2206.15031v2 [cs.CV] UPDATED)
177. Long-term Leap Attention, Short-term Periodic Shift for Video Classification. (arXiv:2207.05526v2 [cs.CV] UPDATED)
178. TransGrasp: Grasp Pose Estimation of a Category of Objects by Transferring Grasps from Only One Labeled Instance. (arXiv:2207.07861v3 [cs.RO] UPDATED)
179. MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask. (arXiv:2207.08046v3 [cs.CV] UPDATED)
180. Latency-Aware Collaborative Perception. (arXiv:2207.08560v4 [cs.CV] UPDATED)
181. Latent Partition Implicit with Surface Codes for 3D Representation. (arXiv:2207.08631v3 [cs.CV] UPDATED)
182. Emotion Recognition based on Multi-Task Learning Framework in the ABAW4 Challenge. (arXiv:2207.09373v2 [cs.CV] UPDATED)
183. Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model. (arXiv:2207.10040v2 [eess.IV] UPDATED)
184. SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v2 [cs.CV] UPDATED)
185. MeshLoc: Mesh-Based Visual Localization. (arXiv:2207.10762v2 [cs.CV] UPDATED)
186. 3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization. (arXiv:2207.10895v2 [cs.CV] UPDATED)
187. Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v2 [eess.IV] UPDATED)
188. STEFANN: Scene Text Editor using Font Adaptive Neural Network. (arXiv:1903.01192v2 [cs.CV] CROSS LISTED)
## eess.IV
---
**40** new papers in eess.IV:-) 
1. Brain tumor detection using artificial convolutional neural networks. (arXiv:2207.11248v1 [eess.IV])
2. Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing. (arXiv:2207.11250v1 [cs.CV])
3. A Supervised Tensor Dimension Reduction-Based Prognostics Model for Applications with Incomplete Imaging Data. (arXiv:2207.11353v1 [cs.LG])
4. Rayleigh Regression Model for Ground Type Detection in SAR Imagery. (arXiv:2207.11397v1 [stat.ME])
5. Wavelength-Resolution SAR Ground Scene Prediction Based on Image Stack. (arXiv:2207.11400v1 [eess.IV])
6. Computer Vision Aided mmWave Beam Alignment in V2X Communications. (arXiv:2207.11409v1 [eess.SP])
7. Arbitrary Style Transfer with Structure **Enhancement** by Combining the Global and Local Loss. (arXiv:2207.11438v1 [cs.CV])
8. DCT Approximations Based on Chen's Factorization. (arXiv:2207.11638v1 [eess.SP])
9. Improved Regularization of Event-based Learning by Reversing and Drifting. (arXiv:2207.11659v1 [cs.CV])
10. FD-MAR: Fourier Dual-domain Network for CT Metal Artifact Reduction. (arXiv:2207.11678v1 [eess.IV])
11. PCA: Semi-supervised Segmentation with Patch Confidence Adversarial Training. (arXiv:2207.11683v1 [eess.IV])
12. Improved Super Resolution of MR Images Using CNNs and Vision Transformers. (arXiv:2207.11748v1 [eess.IV])
13. Image Denoising Using Convolutional Autoencoder. (arXiv:2207.11771v1 [cs.CV])
14. Enhancing Image Rescaling using Dual Latent Variables in Invertible Neural Network. (arXiv:2207.11844v1 [cs.CV])
15. Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation. (arXiv:2207.11860v1 [cs.CV])
16. Sparse-based Domain Adaptation Network for OCTA Image Super-Resolution Reconstruction. (arXiv:2207.11882v1 [eess.IV])
17. Deep learning based non-contact physiological monitoring in Neonatal Intensive Care Unit. (arXiv:2207.11886v1 [eess.IV])
18. Sub-Aperture Feature Adaptation in Single Image Super-resolution Model for Light Field Imaging. (arXiv:2207.11894v1 [cs.CV])
19. Deep dual stream residual network with contextual attention for pansharpening of remote sensing images. (arXiv:2207.12004v1 [cs.CV])
20. REPNP: Plug-and-Play with Deep Reinforcement Learning Prior for Robust Image **Restoration**. (arXiv:2207.12056v1 [eess.IV])
21. Hardware-in-the-loop simulation of a UAV autonomous landing algorithm implemented in SoC FPGA. (arXiv:2207.12198v1 [cs.RO])
22. Cov3d: Detection of the presence and severity of COVID-19 from CT scans using 3D ResNets. (arXiv:2207.12218v1 [eess.IV])
23. OCTAve: 2D en face Optical Coherence Tomography Angiography Vessel Segmentation in Weakly-Supervised Learning with Locality Augmentation. (arXiv:2207.12238v1 [eess.IV])
24. Edge-Aware Autoencoder Design for Real-Time Mixture-of-Experts Image Compression. (arXiv:2207.12348v1 [eess.IV])
25. SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness. (arXiv:2207.12391v1 [cs.CV])
26. Human Gait Database for Normal Walk Collected by Smartphone Accelerometer. (arXiv:1905.03109v3 [eess.SP] UPDATED)
27. Adaptive Compressive Sampling for Mid-infrared Spectroscopic Imaging. (arXiv:2008.00566v3 [eess.IV] UPDATED)
28. Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v4 [eess.IV] UPDATED)
29. Active Audio-Visual Separation of Dynamic Sound Sources. (arXiv:2202.00850v2 [cs.CV] UPDATED)
30. Brain Structural Saliency Over The Ages. (arXiv:2202.11690v3 [q-bio.NC] UPDATED)
31. PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v3 [eess.IV] UPDATED)
32. GATE: Graph CCA for Temporal SElf-supervised Learning for Label-efficient fMRI Analysis. (arXiv:2203.09034v2 [cs.LG] UPDATED)
33. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v3 [cs.CV] UPDATED)
34. Human Gender Prediction Based on Deep Transfer Learning from Panoramic Radiograph Images. (arXiv:2205.09850v2 [eess.IV] UPDATED)
35. SiSPRNet: End-to-End Learning for Single-Shot Phase Retrieval. (arXiv:2205.11434v2 [cs.CV] UPDATED)
36. RADNet: Ensemble Model for Robust Glaucoma Classification in Color Fundus Images. (arXiv:2205.12902v2 [eess.IV] UPDATED)
37. A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects. (arXiv:2206.07219v2 [eess.IV] UPDATED)
38. Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model. (arXiv:2207.10040v2 [eess.IV] UPDATED)
39. Ensemble Learning for Efficient VVC Bitrate Ladder Prediction. (arXiv:2207.10317v2 [eess.IV] UPDATED)
40. Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v2 [eess.IV] UPDATED)
## cs.LG
---
**199** new papers in cs.LG:-) 
1. Brain tumor detection using artificial convolutional neural networks. (arXiv:2207.11248v1 [eess.IV])
2. Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing. (arXiv:2207.11250v1 [cs.CV])
3. Learning Relaxation for Multigrid. (arXiv:2207.11255v1 [cs.LG])
4. PanGu-Coder: Program Synthesis with Function-Level Language Modeling. (arXiv:2207.11280v1 [cs.LG])
5. TRUST-LAPSE: An Explainable & Actionable Mistrust Scoring Framework for Model Monitoring. (arXiv:2207.11290v1 [cs.LG])
6. Accelerated and Quantitative 3D Semisolid MT/CEST Imaging using a Generative Adversarial Network (GAN-CEST). (arXiv:2207.11297v1 [physics.med-ph])
7. Understanding Non-linearity in Graph Neural Networks from the Bayesian-Inference Perspective. (arXiv:2207.11311v1 [cs.LG])
8. A Novel Meta-predictor based Algorithm for Testing VLSI Circuits. (arXiv:2207.11312v1 [cs.LG])
9. A flexible PageRank-based graph embedding framework closely related to spectral eigenvector embeddings. (arXiv:2207.11321v1 [cs.SI])
10. Learning from Multiple Annotator Noisy Labels via Sample-wise Label Fusion. (arXiv:2207.11327v1 [cs.LG])
11. Scalable training of graph convolutional neural networks for fast and accurate predictions of HOMO-LUMO gap in molecules. (arXiv:2207.11333v1 [cs.LG])
12. An Impartial Take to the CNN vs Transformer Robustness Contest. (arXiv:2207.11347v1 [cs.CV])
13. A Supervised Tensor Dimension Reduction-Based Prognostics Model for Applications with Incomplete Imaging Data. (arXiv:2207.11353v1 [cs.LG])
14. Learning Distributed Quantum State Discrimination with Noisy Classical Communications. (arXiv:2207.11354v1 [quant-ph])
15. Machine Learning Modeling to Evaluate the Value of Football Players. (arXiv:2207.11361v1 [cs.LG])
16. Density-Aware Personalized Training for Risk Prediction in Imbalanced Medical Data. (arXiv:2207.11382v1 [cs.LG])
17. Causal Fairness Analysis. (arXiv:2207.11385v1 [cs.AI])
18. Learning to Route in Mobile Wireless Networks. (arXiv:2207.11386v1 [cs.NI])
19. A New Approach to Drifting Games, Based on Asymptotically Optimal Potentials. (arXiv:2207.11405v1 [cs.LG])
20. Satellite Detection in Unresolved Space Imagery for Space Domain Awareness Using Neural Networks. (arXiv:2207.11412v1 [cs.CV])
21. Detection and Initial Assessment of Lunar Landing Sites Using Neural Networks. (arXiv:2207.11413v1 [cs.RO])
22. Multiscale Neural Operator: Learning Fast and Grid-independent PDE Solvers. (arXiv:2207.11417v1 [cs.LG])
23. Driver Dojo: A Benchmark for Generalizable Reinforcement Learning for Autonomous Driving. (arXiv:2207.11432v1 [cs.LG])
24. RIBBON: Cost-Effective and QoS-Aware Deep Learning Model Inference using a Diverse Pool of Cloud Computing Instances. (arXiv:2207.11434v1 [cs.DC])
25. The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks. (arXiv:2207.11437v1 [cs.AR])
26. A Taxonomy of Recurrent Learning Rules. (arXiv:2207.11439v1 [cs.LG])
27. $\mu\text{KG}$: A Library for Multi-source Knowledge Graph Embeddings and Applications. (arXiv:2207.11442v1 [cs.CL])
28. Handling Data Heterogeneity in Federated Learning via Knowledge Fusion. (arXiv:2207.11447v1 [cs.LG])
29. Optimization of the Shape of a Hydrokinetic Turbine's Draft Tube and Hub Assembly Using Design-by-Morphing with Bayesian Optimization. (arXiv:2207.11451v1 [cs.CG])
30. Distributed Nonlinear State Estimation in Electric Power Systems using Graph Neural Networks. (arXiv:2207.11465v1 [cs.LG])
31. Anomaly Detection for Fraud in Cryptocurrency Time Series. (arXiv:2207.11466v1 [cs.LG])
32. Multimodal Emotion Recognition with Modality-Pairwise Unsupervised Contrastive Loss. (arXiv:2207.11482v1 [cs.CV])
33. Time Series Prediction under Distribution Shift using Differentiable Forgetting. (arXiv:2207.11486v1 [cs.LG])
34. SSBNet: Improving Visual Recognition Efficiency by Adaptive Sampling. (arXiv:2207.11511v1 [cs.CV])
35. Supporting peace negotiations in the Yemen war through machine learning. (arXiv:2207.11528v1 [cs.CL])
36. Low-complexity CNNs for Acoustic Scene Classification. (arXiv:2207.11529v1 [eess.AS])
37. FastATDC: Fast Anomalous Trajectory Detection and Classification. (arXiv:2207.11541v1 [cs.LG])
38. Annealed Training for Combinatorial Optimization on Graphs. (arXiv:2207.11542v1 [cs.LG])
39. Learning to Sell a Focal-ancillary Combination. (arXiv:2207.11545v1 [cs.LG])
40. A Ligand-and-structure Dual-driven Deep Learning Method for the Discovery of Highly Potent GnRH1R Antagonist to treat Uterine Diseases. (arXiv:2207.11547v1 [q-bio.BM])
41. Tensor-based Multi-view Spectral Clustering via Shared Latent Space. (arXiv:2207.11559v1 [cs.LG])
42. A general-purpose method for applying Explainable AI for Anomaly Detection. (arXiv:2207.11564v1 [cs.LG])
43. Robots Enact Malignant Stereotypes. (arXiv:2207.11569v1 [cs.RO])
44. Testing the Robustness of Learned Index Structures. (arXiv:2207.11575v1 [cs.DB])
45. Augmented Bilinear Network for Incremental Multi-Stock Time-Series Classification. (arXiv:2207.11577v1 [cs.LG])
46. Boosting the Efficiency of Parametric Detection with Hierarchical Neural Networks. (arXiv:2207.11583v1 [astro-ph.IM])
47. Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning. (arXiv:2207.11584v1 [cs.LG])
48. Thermal half-lives of azobenzene derivatives: virtual screening based on intersystem crossing using a machine learning potential. (arXiv:2207.11592v1 [physics.chem-ph])
49. Exploration in Linear Bandits with Rich Action Sets and its Implications for Inference. (arXiv:2207.11597v1 [cs.LG])
50. Instant Neural Representation for Interactive Volume Rendering. (arXiv:2207.11620v1 [cs.GR])
51. A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors. (arXiv:2207.11621v1 [stat.ML])
52. A Simplistic and Cost-Effective Design for Real-World Development of an Ambient Assisted Living System for Fall Detection and Indoor Localization: Proof of Concept. (arXiv:2207.11623v1 [cs.HC])
53. Prediction Intervals in the Beta Autoregressive Moving Average Model. (arXiv:2207.11628v1 [stat.ME])
54. Reliable amortized variational inference with physics-based latent distribution correction. (arXiv:2207.11640v1 [stat.ML])
55. Robust Scene Inference under Noise-Blur Dual Corruptions. (arXiv:2207.11643v1 [cs.CV])
56. OCTAL: Graph Representation Learning for LTL Model Checking. (arXiv:2207.11649v1 [cs.PL])
57. BPFISH: Blockchain and Privacy-preserving FL Inspired Smart Healthcare. (arXiv:2207.11654v1 [cs.NI])
58. Improved Regularization of Event-based Learning by Reversing and Drifting. (arXiv:2207.11659v1 [cs.CV])
59. Distributed Robust Principal Analysis. (arXiv:2207.11669v1 [cs.DC])
60. Modeling Associative Plasticity between Synapses to Enhance Learning of Spiking Neural Networks. (arXiv:2207.11670v1 [cs.NE])
61. PCA: Semi-supervised Segmentation with Patch Confidence Adversarial Training. (arXiv:2207.11683v1 [eess.IV])
62. HouseX: A Fine-grained House Music Dataset and its Potential in the Music Industry. (arXiv:2207.11690v1 [cs.SD])
63. Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (arXiv:2207.11694v1 [cs.LG])
64. Towards an Improved Understanding of Software Vulnerability Assessment Using Data-Driven Approaches. (arXiv:2207.11708v1 [cs.SE])
65. A Priority Map for Vision-and-Language Navigation with Trajectory Plans and Feature-Location Cues. (arXiv:2207.11717v1 [cs.LG])
66. Gradient-based Bi-level Optimization for Deep Learning: A Survey. (arXiv:2207.11719v1 [cs.LG])
67. Can we achieve robustness from data alone?. (arXiv:2207.11727v1 [cs.LG])
68. AMS-Net: Adaptive Multiscale Sparse Neural Network with Interpretable Basis Expansion for Multiphase Flow Problems. (arXiv:2207.11735v1 [cs.LG])
69. Towards Using Fully Observable Policies for POMDPs. (arXiv:2207.11737v1 [cs.LG])
70. From Multi-label Learning to Cross-Domain Transfer: A Model-Agnostic Approach. (arXiv:2207.11742v1 [cs.LG])
71. Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v1 [cs.SD])
72. Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges. (arXiv:2207.11759v1 [cs.LG])
73. SGAT: Simplicial Graph Attention Network. (arXiv:2207.11761v1 [cs.LG])
74. CODiT: Conformal Out-of-Distribution Detection in Time-Series Data. (arXiv:2207.11769v1 [cs.LG])
75. Image Denoising Using Convolutional Autoencoder. (arXiv:2207.11771v1 [cs.CV])
76. Incorporating Heterogeneous User Behaviors and Social Influences for Predictive Analysis. (arXiv:2207.11776v1 [cs.LG])
77. Physics-Informed Learning of Aerosol Microphysics. (arXiv:2207.11786v1 [cs.LG])
78. Privacy Against Inference Attacks in Vertical Federated Learning. (arXiv:2207.11788v1 [cs.LG])
79. Federated Graph Machine Learning: A Survey of Concepts, Techniques, and Applications. (arXiv:2207.11812v1 [cs.LG])
80. Federated Graph Contrastive Learning. (arXiv:2207.11836v1 [cs.LG])
81. $\textit{FastSVD-ML-ROM}$: A Reduced-Order Modeling Framework based on Machine Learning for Real-Time Applications. (arXiv:2207.11842v1 [cs.LG])
82. Mixture of Input-Output Hidden Markov Models for Heterogeneous Disease Progression Modeling. (arXiv:2207.11846v1 [cs.LG])
83. Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors. (arXiv:1806.05438v4 [stat.ML] UPDATED)
84. Adding Neural Network Controllers to Behavior Trees without Destroying Performance Guarantees. (arXiv:1809.10283v3 [cs.RO] UPDATED)
85. Explaining a prediction in some nonlinear models. (arXiv:1904.09615v4 [cs.LG] UPDATED)
86. Classification of Macromolecule Type Based on Sequences of Amino Acids Using Deep Learning. (arXiv:1907.03532v3 [q-bio.BM] UPDATED)
87. Why Non-myopic Bayesian Optimization is Promising and How Far Should We Look-ahead? A Study via Rollout. (arXiv:1911.01004v3 [cs.LG] UPDATED)
88. Primal and Dual Prediction-Correction Methods for Time-Varying Convex Optimization. (arXiv:2004.11709v3 [math.OC] UPDATED)
89. Knowledge Base Completion: Baseline strikes back (Again). (arXiv:2005.00804v3 [cs.LG] UPDATED)
90. Explaining How Deep Neural Networks Forget by Deep Visualization. (arXiv:2005.01004v3 [cs.LG] UPDATED)
91. How to "Improve" Prediction Using Behavior Modification. (arXiv:2008.12138v4 [cs.CY] UPDATED)
92. Online Stochastic Optimization with Wasserstein Based Non-stationarity. (arXiv:2012.06961v3 [cs.LG] UPDATED)
93. Fast-Convergent Dynamics for Distributed Allocation of Resources Over Switching Sparse Networks with Quantized Communication Links. (arXiv:2012.08181v4 [eess.SY] UPDATED)
94. Universal Approximation Theorems for Differentiable Geometric Deep Learning. (arXiv:2101.05390v4 [cs.LG] UPDATED)
95. Finite Sample Analysis of Minimax Offline Reinforcement Learning: Completeness, Fast Rates and First-Order Efficiency. (arXiv:2102.02981v2 [cs.LG] UPDATED)
96. Inapproximability of a Pair of Forms Defining a Partial Boolean Function. (arXiv:2102.04703v3 [cs.LG] UPDATED)
97. On the Last Iterate Convergence of Momentum Methods. (arXiv:2102.07002v3 [cs.LG] UPDATED)
98. NOMU: Neural Optimization-based Model Uncertainty. (arXiv:2102.13640v4 [cs.LG] UPDATED)
99. Graph Attention Networks for Channel Estimation in RIS-assisted Satellite IoT Communications. (arXiv:2104.00735v4 [cs.NI] UPDATED)
100. Policy Optimization in Dynamic Bayesian Network Hybrid Models of Biomanufacturing Processes. (arXiv:2105.06543v3 [cs.AI] UPDATED)
101. Latent Gaussian Model Boosting. (arXiv:2105.08966v6 [cs.LG] UPDATED)
102. Quantifying and Localizing Usable Information Leakage from Neural Network Gradients. (arXiv:2105.13929v3 [cs.LG] UPDATED)
103. SPANet: Generalized Permutationless Set Assignment for Particle Physics using Symmetry Preserving Attention. (arXiv:2106.03898v4 [hep-ex] UPDATED)
104. MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network. (arXiv:2106.07352v2 [cs.IR] UPDATED)
105. How does Heterophily Impact the Robustness of Graph Neural Networks? Theoretical Connections and Practical Implications. (arXiv:2106.07767v4 [cs.LG] UPDATED)
106. Learning effective stochastic differential equations from microscopic simulations: linking stochastic numerics to deep learning. (arXiv:2106.09004v2 [physics.comp-ph] UPDATED)
107. Prototypical Graph Contrastive Learning. (arXiv:2106.09645v2 [cs.LG] UPDATED)
108. A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions. (arXiv:2108.03357v2 [cs.IR] UPDATED)
109. Optimal Placement of Public Electric Vehicle Charging Stations Using Deep Reinforcement Learning. (arXiv:2108.07772v2 [eess.SY] UPDATED)
110. Task-Oriented Communication for Multi-Device Cooperative Edge Inference. (arXiv:2109.00172v2 [eess.SP] UPDATED)
111. Stochastic Physics-Informed Neural Ordinary Differential Equations. (arXiv:2109.01621v2 [cs.LG] UPDATED)
112. Bayesian Optimization with Clustering and Rollback for CNN Auto Pruning. (arXiv:2109.10591v2 [cs.LG] UPDATED)
113. Clustering performance analysis using a new correlation-based cluster validity index. (arXiv:2109.11172v2 [stat.ML] UPDATED)
114. Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization. (arXiv:2109.14549v2 [cs.RO] UPDATED)
115. Prune Your Model Before Distill It. (arXiv:2109.14960v3 [cs.LG] UPDATED)
116. Evaluating generative networks using Gaussian mixtures of image features. (arXiv:2110.05240v2 [cs.CV] UPDATED)
117. Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models. (arXiv:2110.08536v2 [cs.CL] UPDATED)
118. IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v4 [cs.CV] UPDATED)
119. Single-Item Fashion Recommender: Towards Cross-Domain Recommendations. (arXiv:2111.00758v2 [cs.IR] UPDATED)
120. Constructing Neural Network-Based Models for Simulating Dynamical Systems. (arXiv:2111.01495v2 [cs.LG] UPDATED)
121. Sliced Recursive Transformer. (arXiv:2111.05297v2 [cs.CV] UPDATED)
122. Policy Gradient and Actor-Critic Learning in Continuous Time and Space: Theory and Algorithms. (arXiv:2111.11232v2 [cs.LG] UPDATED)
123. An Exact Algorithm for Semi-supervised Minimum Sum-of-Squares Clustering. (arXiv:2111.15571v2 [math.OC] UPDATED)
124. ROCA: Robust CAD Model Retrieval and Alignment from a Single Image. (arXiv:2112.01988v2 [cs.CV] UPDATED)
125. Coupling Vision and Proprioception for Navigation of Legged Robots. (arXiv:2112.02094v2 [cs.RO] UPDATED)
126. Variational Wasserstein gradient flow. (arXiv:2112.02424v3 [cs.LG] UPDATED)
127. Scaling Structured Inference with Randomization. (arXiv:2112.03638v3 [cs.LG] UPDATED)
128. Test Set Sizing Via Random Matrix Theory. (arXiv:2112.05977v4 [stat.ML] UPDATED)
129. HiClass: a Python library for local hierarchical classification compatible with scikit-learn. (arXiv:2112.06560v6 [cs.LG] UPDATED)
130. Low-resource Learning with Knowledge Graphs: A Comprehensive Survey. (arXiv:2112.10006v5 [cs.LG] UPDATED)
131. Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v3 [cs.CV] UPDATED)
132. Invariant Representation Driven Neural Classifier for Anti-QCD Jet Tagging. (arXiv:2201.07199v3 [hep-ph] UPDATED)
133. Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v5 [cs.CL] UPDATED)
134. Can Wikipedia Help Offline Reinforcement Learning?. (arXiv:2201.12122v3 [cs.LG] UPDATED)
135. Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v4 [eess.IV] UPDATED)
136. Active Audio-Visual Separation of Dynamic Sound Sources. (arXiv:2202.00850v2 [cs.CV] UPDATED)
137. The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance. (arXiv:2202.05791v2 [stat.ML] UPDATED)
138. Rethinking Pareto Frontier for Performance Evaluation of Deep Neural Networks. (arXiv:2202.09275v3 [cs.LG] UPDATED)
139. Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans on Youtube. (arXiv:2202.10448v2 [cs.RO] UPDATED)
140. A Clustering Preserving Transformation for k-Means Algorithm Output. (arXiv:2202.10455v2 [cs.LG] UPDATED)
141. Brain Structural Saliency Over The Ages. (arXiv:2202.11690v3 [q-bio.NC] UPDATED)
142. Private High-Dimensional Hypothesis Testing. (arXiv:2203.01537v2 [cs.DS] UPDATED)
143. On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v3 [cs.LG] UPDATED)
144. SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v3 [cs.CV] UPDATED)
145. GATE: Graph CCA for Temporal SElf-supervised Learning for Label-efficient fMRI Analysis. (arXiv:2203.09034v2 [cs.LG] UPDATED)
146. Deep reinforcement learning guided graph neural networks for brain network analysis. (arXiv:2203.10093v5 [cs.LG] UPDATED)
147. Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents. (arXiv:2203.10592v3 [stat.ML] UPDATED)
148. Discovering Human-Object Interaction Concepts via Self-Compositional Learning. (arXiv:2203.14272v2 [cs.CV] UPDATED)
149. Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v2 [cs.CV] UPDATED)
150. Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation. (arXiv:2203.16104v2 [cs.SD] UPDATED)
151. Hypergraph Convolutional Networks via Equivalency between Hypergraphs and Undirected Graphs. (arXiv:2203.16939v3 [cs.LG] UPDATED)
152. R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v2 [cs.CV] UPDATED)
153. MaxViT: Multi-Axis Vision Transformer. (arXiv:2204.01697v2 [cs.CV] UPDATED)
154. Decentralized Collaborative Learning Framework for Next POI Recommendation. (arXiv:2204.06516v3 [cs.IR] UPDATED)
155. Multi-task Deep Neural Networks for Massive MIMO CSI Feedback. (arXiv:2204.12442v2 [cs.IT] UPDATED)
156. Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and Gain. (arXiv:2204.13883v2 [eess.AS] UPDATED)
157. End-to-End Signal Classification in Signed Cumulative Distribution Transform Space. (arXiv:2205.00348v2 [eess.SP] UPDATED)
158. Autonomy and Intelligence in the Computing Continuum: Challenges, Enablers, and Future Directions for Orchestration. (arXiv:2205.01423v2 [cs.MA] UPDATED)
159. Towards Theoretical Analysis of Transformation Complexity of ReLU DNNs. (arXiv:2205.01940v2 [cs.LG] UPDATED)
160. Perseus: A Simple and Optimal High-Order Method for Variational Inequalities. (arXiv:2205.03202v3 [math.OC] UPDATED)
161. ConfLab: A Rich Multimodal Multisensor Dataset of Free-Standing Social Interactions in the Wild. (arXiv:2205.05177v2 [cs.MM] UPDATED)
162. QHD: A brain-inspired hyperdimensional reinforcement learning algorithm. (arXiv:2205.06978v2 [cs.LG] UPDATED)
163. Generalizing to New Tasks via One-Shot Compositional Subgoals. (arXiv:2205.07716v2 [cs.LG] UPDATED)
164. Utterance Weighted Multi-Dilation Temporal Convolutional Networks for Monaural Speech Dereverberation. (arXiv:2205.08455v3 [cs.SD] UPDATED)
165. Generating Explanations from Deep Reinforcement Learning Using Episodic Memory. (arXiv:2205.08926v2 [cs.AI] UPDATED)
166. Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization. (arXiv:2205.10217v2 [stat.ML] UPDATED)
167. Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v5 [cs.CV] UPDATED)
168. SiSPRNet: End-to-End Learning for Single-Shot Phase Retrieval. (arXiv:2205.11434v2 [cs.CV] UPDATED)
169. RADNet: Ensemble Model for Robust Glaucoma Classification in Color Fundus Images. (arXiv:2205.12902v2 [eess.IV] UPDATED)
170. Friends to Help: Saving Federated Learning from Client Dropout. (arXiv:2205.13222v2 [cs.LG] UPDATED)
171. Experience report of physics-informed neural networks in fluid simulations: pitfalls and frustration. (arXiv:2205.14249v3 [physics.flu-dyn] UPDATED)
172. A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks. (arXiv:2205.15307v2 [cs.LG] UPDATED)
173. Associative Learning Mechanism for Drug-Target Interaction Prediction. (arXiv:2205.15364v3 [q-bio.BM] UPDATED)
174. Predecessor Features. (arXiv:2206.00303v3 [cs.LG] UPDATED)
175. Transfer without Forgetting. (arXiv:2206.00388v2 [cs.LG] UPDATED)
176. Policy Optimization for Markov Games: Unified Framework and Faster Convergence. (arXiv:2206.02640v4 [cs.LG] UPDATED)
177. Physics-Inspired Temporal Learning of Quadrotor Dynamics for Accurate Model Predictive Trajectory Tracking. (arXiv:2206.03305v2 [cs.RO] UPDATED)
178. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v2 [cs.AI] UPDATED)
179. Towards Autonomous Grading In The Real World. (arXiv:2206.06091v2 [cs.RO] UPDATED)
180. A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects. (arXiv:2206.07219v2 [eess.IV] UPDATED)
181. GACT: Activation Compressed Training for Generic Network Architectures. (arXiv:2206.11357v3 [cs.LG] UPDATED)
182. The Real Deal: A Review of Challenges and Opportunities in Moving Reinforcement Learning-Based Traffic Signal Control Systems Towards Reality. (arXiv:2206.11996v2 [cs.AI] UPDATED)
183. BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping. (arXiv:2206.12038v3 [cs.SD] UPDATED)
184. SPI-GAN: Distilling Score-based Generative Models with Straight-Path Interpolations. (arXiv:2206.14464v2 [cs.LG] UPDATED)
185. On the Learning and Learnablity of Quasimetrics. (arXiv:2206.15478v3 [cs.LG] UPDATED)
186. Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis. (arXiv:2207.00813v2 [q-bio.NC] UPDATED)
187. CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. (arXiv:2207.01780v2 [cs.LG] UPDATED)
188. When does SGD favor flat minima? A quantitative characterization via linear stability. (arXiv:2207.02628v2 [stat.ML] UPDATED)
189. An Exploration of How Training Set Composition Bias in Machine Learning Affects Identifying Rare Objects. (arXiv:2207.03207v2 [cs.LG] UPDATED)
190. A Solver + Gradient Descent Training Algorithm for Deep Neural Networks. (arXiv:2207.03264v2 [cs.LG] UPDATED)
191. Whois? Deep Author Name Disambiguation using Bibliographic Data. (arXiv:2207.04772v2 [cs.DL] UPDATED)
192. Equivariant Hypergraph Diffusion Neural Operators. (arXiv:2207.06680v2 [cs.LG] UPDATED)
193. Anomal-E: A Self-Supervised Network Intrusion Detection System based on Graph Neural Networks. (arXiv:2207.06819v4 [cs.LG] UPDATED)
194. Minimum Description Length Control. (arXiv:2207.08258v3 [cs.LG] UPDATED)
195. Upper Limb Movement Recognition utilising EEG and EMG Signals for Rehabilitative Robotics. (arXiv:2207.08650v2 [cs.LG] UPDATED)
196. Controllable Data Generation by Deep Learning: A Review. (arXiv:2207.09542v2 [cs.LG] UPDATED)
197. Journal Impact Factor and Peer Review Thoroughness and Helpfulness: A Supervised Machine Learning Study. (arXiv:2207.09821v2 [cs.DL] UPDATED)
198. SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v2 [cs.CV] UPDATED)
199. Delayed Feedback in Generalised Linear Bandits Revisited. (arXiv:2207.10786v2 [cs.LG] UPDATED)
## cs.AI
---
**116** new papers in cs.AI:-) 
1. Variational Temporal Deconfounder for Individualized Treatment Effect Estimation from Longitudinal Observational Data. (arXiv:2207.11251v1 [cs.AI])
2. PanGu-Coder: Program Synthesis with Function-Level Language Modeling. (arXiv:2207.11280v1 [cs.LG])
3. TRUST-LAPSE: An Explainable & Actionable Mistrust Scoring Framework for Model Monitoring. (arXiv:2207.11290v1 [cs.LG])
4. Receptive Field-based Segmentation for Distributed CNN Inference Acceleration in Collaborative Edge Computing. (arXiv:2207.11293v1 [cs.DC])
5. Distributed Deep Learning Inference Acceleration using Seamless Collaboration in Edge Computing. (arXiv:2207.11294v1 [cs.DC])
6. JAM: The JavaScript Agent Machine for Distributed Computing and Simulation with reactive and mobile Multi-agent Systems -- A Technical Report. (arXiv:2207.11300v1 [cs.AI])
7. Tradeoffs in Preventing Manipulation in Paper Bidding for Reviewer Assignment. (arXiv:2207.11315v1 [cs.AI])
8. Exploring Wasserstein Distance across Concept Embeddings for Ontology Matching. (arXiv:2207.11324v1 [cs.AI])
9. Scalable training of graph convolutional neural networks for fast and accurate predictions of HOMO-LUMO gap in molecules. (arXiv:2207.11333v1 [cs.LG])
10. Learning Distributed Quantum State Discrimination with Noisy Classical Communications. (arXiv:2207.11354v1 [quant-ph])
11. Causal Fairness Analysis. (arXiv:2207.11385v1 [cs.AI])
12. Two-Aspect Information Fusion Model For ABAW4 Multi-task Challenge. (arXiv:2207.11389v1 [cs.CV])
13. PS-**NeRF**: Neural Inverse Rendering for Multi-view Photometric Stereo. (arXiv:2207.11406v1 [cs.CV])
14. Multiscale Neural Operator: Learning Fast and Grid-independent PDE Solvers. (arXiv:2207.11417v1 [cs.LG])
15. Epersist: A Self Balancing Robot Using PID Controller And Deep Reinforcement Learning. (arXiv:2207.11431v1 [cs.RO])
16. $\mu\text{KG}$: A Library for Multi-source Knowledge Graph Embeddings and Applications. (arXiv:2207.11442v1 [cs.CL])
17. When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2207.11463v1 [cs.CV])
18. CompNVS: Novel View Synthesis with Scene Completion. (arXiv:2207.11467v1 [cs.CV])
19. 3D Labeling Tool. (arXiv:2207.11479v1 [cs.CV])
20. Multimodal Emotion Recognition with Modality-Pairwise Unsupervised Contrastive Loss. (arXiv:2207.11482v1 [cs.CV])
21. Towards Smart Fake News Detection Through Explainable AI. (arXiv:2207.11490v1 [cs.AI])
22. Intelligent 3D Network Protocol for Multimedia Data Classification using Deep Learning. (arXiv:2207.11504v1 [cs.CV])
23. SSBNet: Improving Visual Recognition Efficiency by Adaptive Sampling. (arXiv:2207.11511v1 [cs.CV])
24. Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition. (arXiv:2207.11518v1 [cs.CV])
25. Low-complexity CNNs for Acoustic Scene Classification. (arXiv:2207.11529v1 [eess.AS])
26. Comparative Validation of AI and non-AI Methods in MRI Volumetry to Diagnose Parkinsonian Syndromes. (arXiv:2207.11534v1 [cs.AI])
27. A Ligand-and-structure Dual-driven Deep Learning Method for the Discovery of Highly Potent GnRH1R Antagonist to treat Uterine Diseases. (arXiv:2207.11547v1 [q-bio.BM])
28. High-Resolution **Swin** Transformer for Automatic Medical Image Segmentation. (arXiv:2207.11553v1 [cs.CV])
29. A general-purpose method for applying Explainable AI for Anomaly Detection. (arXiv:2207.11564v1 [cs.LG])
30. Context based lemmatizer for Polish language. (arXiv:2207.11565v1 [cs.CL])
31. Robots Enact Malignant Stereotypes. (arXiv:2207.11569v1 [cs.RO])
32. Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning. (arXiv:2207.11584v1 [cs.LG])
33. Exploration in Linear Bandits with Rich Action Sets and its Implications for Inference. (arXiv:2207.11597v1 [cs.LG])
34. Generative Artisan: A Semantic-Aware and Controllable CLIPstyler. (arXiv:2207.11598v1 [cs.CV])
35. A Simplistic and Cost-Effective Design for Real-World Development of an Ambient Assisted Living System for Fall Detection and Indoor Localization: Proof of Concept. (arXiv:2207.11623v1 [cs.HC])
36. Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis. (arXiv:2207.11652v1 [cs.CL])
37. AutoWeird: Weird Translational Scoring Function Identified by Random Search. (arXiv:2207.11673v1 [cs.AI])
38. No More Fine-Tuning? An Experimental Evaluation of Prompt Tuning in Code Intelligence. (arXiv:2207.11680v1 [cs.SE])
39. Kernel Relative-prototype Spectral Filtering for Few-shot Learning. (arXiv:2207.11685v1 [cs.CV])
40. Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (arXiv:2207.11694v1 [cs.LG])
41. Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v1 [cs.AI])
42. Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v1 [cs.CV])
43. A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v1 [cs.CL])
44. Progressive Feature Learning for Realistic Cloth-Changing Gait Recognition. (arXiv:2207.11720v1 [cs.AI])
45. Semantic-guided Multi-Mask Image Harmonization. (arXiv:2207.11722v1 [cs.AI])
46. Adaptive Decision Making at the Intersection for Autonomous Vehicles Based on Skill Discovery. (arXiv:2207.11724v1 [cs.RO])
47. Towards Using Fully Observable Policies for POMDPs. (arXiv:2207.11737v1 [cs.LG])
48. Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v1 [cs.SD])
49. Label-Guided Auxiliary Training Improves 3D Object Detector. (arXiv:2207.11753v1 [cs.CV])
50. Learning Generalizable Light Field Networks from Few Images. (arXiv:2207.11757v1 [cs.CV])
51. Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges. (arXiv:2207.11759v1 [cs.LG])
52. Anti-Overestimation Dialogue Policy Learning for Task-Completion Dialogue System. (arXiv:2207.11762v1 [cs.CL])
53. Neurosymbolic Repair for Low-Code Formula Languages. (arXiv:2207.11765v1 [cs.SE])
54. Incorporating Heterogeneous User Behaviors and Social Influences for Predictive Analysis. (arXiv:2207.11776v1 [cs.LG])
55. Data-driven Models to Anticipate Critical Voltage Events in Power Systems. (arXiv:2207.11803v1 [cs.AI])
56. Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions. (arXiv:2207.11805v1 [cs.CV])
57. ArmanEmo: A Persian Dataset for Text-based Emotion Detection. (arXiv:2207.11808v1 [cs.CL])
58. SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions. (arXiv:2207.11838v1 [cs.CV])
59. Mixture of Input-Output Hidden Markov Models for Heterogeneous Disease Progression Modeling. (arXiv:2207.11846v1 [cs.LG])
60. Adding Neural Network Controllers to Behavior Trees without Destroying Performance Guarantees. (arXiv:1809.10283v3 [cs.RO] UPDATED)
61. Knowledge Base Completion: Baseline strikes back (Again). (arXiv:2005.00804v3 [cs.LG] UPDATED)
62. NOMU: Neural Optimization-based Model Uncertainty. (arXiv:2102.13640v4 [cs.LG] UPDATED)
63. Policy Optimization in Dynamic Bayesian Network Hybrid Models of Biomanufacturing Processes. (arXiv:2105.06543v3 [cs.AI] UPDATED)
64. Latent Gaussian Model Boosting. (arXiv:2105.08966v6 [cs.LG] UPDATED)
65. Prototypical Graph Contrastive Learning. (arXiv:2106.09645v2 [cs.LG] UPDATED)
66. On the Foundations of Grounding in Answer Set Programming. (arXiv:2108.04769v3 [cs.AI] UPDATED)
67. Lyra: A Benchmark for Turducken-Style Code Generation. (arXiv:2108.12144v3 [cs.SE] UPDATED)
68. Stochastic Physics-Informed Neural Ordinary Differential Equations. (arXiv:2109.01621v2 [cs.LG] UPDATED)
69. Bayesian Optimization with Clustering and Rollback for CNN Auto Pruning. (arXiv:2109.10591v2 [cs.LG] UPDATED)
70. IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v4 [cs.CV] UPDATED)
71. Sliced Recursive Transformer. (arXiv:2111.05297v2 [cs.CV] UPDATED)
72. OOD-CV: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images. (arXiv:2111.14341v3 [cs.CV] UPDATED)
73. TransFGU: A Top-down Approach to Fine-Grained Unsupervised Semantic Segmentation. (arXiv:2112.01515v2 [cs.CV] UPDATED)
74. Coupling Vision and Proprioception for Navigation of Legged Robots. (arXiv:2112.02094v2 [cs.RO] UPDATED)
75. Bungee**NeRF**: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. (arXiv:2112.05504v3 [cs.CV] UPDATED)
76. EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices. (arXiv:2112.07642v2 [cs.CV] UPDATED)
77. Low-resource Learning with Knowledge Graphs: A Comprehensive Survey. (arXiv:2112.10006v5 [cs.LG] UPDATED)
78. A Research Agenda for AI Planning in the Field of Flexible Production Systems. (arXiv:2112.15484v5 [cs.AI] UPDATED)
79. Can Wikipedia Help Offline Reinforcement Learning?. (arXiv:2201.12122v3 [cs.LG] UPDATED)
80. Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v3 [cs.AI] UPDATED)
81. Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans on Youtube. (arXiv:2202.10448v2 [cs.RO] UPDATED)
82. Worst Case Matters for Few-Shot Recognition. (arXiv:2203.06574v2 [cs.CV] UPDATED)
83. Deep reinforcement learning guided graph neural networks for brain network analysis. (arXiv:2203.10093v5 [cs.LG] UPDATED)
84. Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v2 [cs.CV] UPDATED)
85. R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v2 [cs.CV] UPDATED)
86. Frequency and Multi-Scale Selective Kernel Attention for Speaker Verification. (arXiv:2204.01005v2 [eess.AS] UPDATED)
87. MaxViT: Multi-Axis Vision Transformer. (arXiv:2204.01697v2 [cs.CV] UPDATED)
88. Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. (arXiv:2204.03638v2 [cs.CV] UPDATED)
89. Multi-task Deep Neural Networks for Massive MIMO CSI Feedback. (arXiv:2204.12442v2 [cs.IT] UPDATED)
90. Autonomy and Intelligence in the Computing Continuum: Challenges, Enablers, and Future Directions for Orchestration. (arXiv:2205.01423v2 [cs.MA] UPDATED)
91. Towards Theoretical Analysis of Transformation Complexity of ReLU DNNs. (arXiv:2205.01940v2 [cs.LG] UPDATED)
92. CLIP-CLOP: CLIP-Guided Collage and Photomontage. (arXiv:2205.03146v3 [cs.CV] UPDATED)
93. QHD: A brain-inspired hyperdimensional reinforcement learning algorithm. (arXiv:2205.06978v2 [cs.LG] UPDATED)
94. Unsupervised Segmentation in Real-World Images via Spelke Object Inference. (arXiv:2205.08515v2 [cs.CV] UPDATED)
95. Generating Explanations from Deep Reinforcement Learning Using Episodic Memory. (arXiv:2205.08926v2 [cs.AI] UPDATED)
96. Robust 3D Object Detection in Cold Weather Conditions. (arXiv:2205.11925v2 [cs.CV] UPDATED)
97. RADNet: Ensemble Model for Robust Glaucoma Classification in Color Fundus Images. (arXiv:2205.12902v2 [eess.IV] UPDATED)
98. Friends to Help: Saving Federated Learning from Client Dropout. (arXiv:2205.13222v2 [cs.LG] UPDATED)
99. Experience report of physics-informed neural networks in fluid simulations: pitfalls and frustration. (arXiv:2205.14249v3 [physics.flu-dyn] UPDATED)
100. A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks. (arXiv:2205.15307v2 [cs.LG] UPDATED)
101. Predecessor Features. (arXiv:2206.00303v3 [cs.LG] UPDATED)
102. Physics-Inspired Temporal Learning of Quadrotor Dynamics for Accurate Model Predictive Trajectory Tracking. (arXiv:2206.03305v2 [cs.RO] UPDATED)
103. EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v2 [cs.AI] UPDATED)
104. Towards Autonomous Grading In The Real World. (arXiv:2206.06091v2 [cs.RO] UPDATED)
105. A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects. (arXiv:2206.07219v2 [eess.IV] UPDATED)
106. The Real Deal: A Review of Challenges and Opportunities in Moving Reinforcement Learning-Based Traffic Signal Control Systems Towards Reality. (arXiv:2206.11996v2 [cs.AI] UPDATED)
107. BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping. (arXiv:2206.12038v3 [cs.SD] UPDATED)
108. Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis. (arXiv:2206.13608v2 [cs.CV] UPDATED)
109. SPI-GAN: Distilling Score-based Generative Models with Straight-Path Interpolations. (arXiv:2206.14464v2 [cs.LG] UPDATED)
110. Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis. (arXiv:2207.00813v2 [q-bio.NC] UPDATED)
111. Dreamento: an open-source dream engineering toolbox for sleep EEG wearables. (arXiv:2207.03977v2 [cs.HC] UPDATED)
112. Anomal-E: A Self-Supervised Network Intrusion Detection System based on Graph Neural Networks. (arXiv:2207.06819v4 [cs.LG] UPDATED)
113. COEM: Cross-Modal Embedding for MetaCell Identification. (arXiv:2207.07734v2 [q-bio.GN] UPDATED)
114. ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale. (arXiv:2207.09078v2 [cs.CL] UPDATED)
115. Controllable Data Generation by Deep Learning: A Review. (arXiv:2207.09542v2 [cs.LG] UPDATED)
116. SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v2 [cs.CV] UPDATED)

