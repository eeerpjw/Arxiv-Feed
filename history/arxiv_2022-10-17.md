# Your interest papers
---
## cs.CV
---
### **Real-time** Action Recognition for Fine-Grained Actions and The Hand Wash Dataset. (arXiv:2210.07400v1 [cs.CV])
- Authors : Akash Nagaraj, Mukund Sood, Chetna Sureka, Gowri Srinivasa
- Link : [http://arxiv.org/abs/2210.07400](http://arxiv.org/abs/2210.07400)
> ABSTRACT  :  In this paper we present a three-stream algorithm for real-time action recognition and a new dataset of handwash videos, with the intent of aligning action recognition with real-world constraints to yield effective conclusions. A three-stream fusion algorithm is proposed, which runs both accurately and efficiently, in real-time even on low-powered systems such as a Raspberry Pi. The cornerstone of the proposed algorithm is the incorporation of both spatial and temporal information, as well as the information of the objects in a video while using an efficient architecture, and Optical Flow computation to achieve commendable results in real-time. The results achieved by this algorithm are benchmarked on the UCF-101 as well as the HMDB-51 datasets, achieving an accuracy of 92.7% and 64.9% respectively. An important point to note is that the algorithm is novel in the aspect that it is also able to learn the intricate differences between extremely similar actions, which would be difficult even for the human eye. Additionally, noticing a dearth in the number of datasets for the recognition of very similar or fine-grained actions, this paper also introduces a new dataset that is made publicly available, the Hand Wash Dataset with the intent of introducing a new benchmark for fine-grained action recognition tasks in the future.  
### Synthetic-to-real Composite Semantic Segmentation in Additive Manufacturing. (arXiv:2210.07466v1 [cs.CV])
- Authors : Aliaksei Petsiuk, Harnoor Singh, Himanshu Dadhwal
- Link : [http://arxiv.org/abs/2210.07466](http://arxiv.org/abs/2210.07466)
> ABSTRACT  :  The application of computer vision and machine learning methods in the field of additive manufacturing (AM) for semantic segmentation of the structural elements of 3-D printed products will improve real-time failure analysis systems and can potentially reduce the number of defects by enabling in situ corrections. This work demonstrates the possibilities of using physics-based rendering for labeled image dataset generation, as well as image-to-image translation capabilities to improve the accuracy of real image segmentation for AM systems. Multi-class semantic segmentation experiments were carried out based on the U-Net model and cycle generative adversarial network. The test results demonstrated the capacity of detecting such structural elements of 3-D printed parts as a top layer, infill, shell, and support. A basis for further segmentation system **enhancement** by utilizing image-to-image style transfer and domain adaptation technologies was also developed. The results indicate that using style transfer as a precursor to domain adaptation can significantly improve real 3-D printing image segmentation in situations where a model trained on synthetic data is the only tool available. The mean intersection over union (mIoU) scores for synthetic test datasets included 94.90% for the entire 3-D printed part, 73.33% for the top layer, 78.93% for the infill, 55.31% for the shell, and 69.45% for supports.  
### Pretrained Transformers Do not Always Improve Robustness. (arXiv:2210.07663v1 [cs.CL])
- Authors : Swaroop Mishra, Bhavdeep Singh, Chitta Baral
- Link : [http://arxiv.org/abs/2210.07663](http://arxiv.org/abs/2210.07663)
> ABSTRACT  :  Pretrained Transformers (PT) have been shown to improve Out of Distribution (OOD) robustness than traditional models such as Bag of Words (BOW), LSTMs, Convolutional Neural Networks (CNN) powered by Word2Vec and Glove embeddings. How does the robustness comparison hold in a real world setting where some part of the dataset can be noisy? Do PT also provide more robust representation than traditional models on **exposure** to noisy data? We perform a comparative study on 10 models and find an empirical evidence that PT provide less robust representation than traditional models on **exposure** to noisy data. We investigate further and augment PT with an adversarial filtering (AF) mechanism that has been shown to improve OOD generalization. However, increase in generalization does not necessarily increase robustness, as we find that noisy data fools the AF method powered by PT.  
### Improved automated lesion segmentation in whole-body FDG/PET-CT via Test-Time Augmentation. (arXiv:2210.07761v1 [eess.IV])
- Authors : Sepideh Amiri, Bulat Ibragimov
- Link : [http://arxiv.org/abs/2210.07761](http://arxiv.org/abs/2210.07761)
> ABSTRACT  :  Numerous oncology indications have extensively quantified metabolically active tumors using positron emission tomography (PET) and computed tomography (CT). F-fluorodeoxyglucose-positron emission tomography (FDG-PET) is frequently utilized in clinical practice and clinical drug research to detect and measure metabolically active malignancies. The assessment of tumor burden using manual or computer-assisted tumor segmentation in FDG-PET images is widespread. Deep learning algorithms have also produced effective solutions in this area. However, there may be a need to improve the performance of a pre-trained deep learning network without the opportunity to modify this network. We investigate the potential benefits of test-time augmentation for segmenting tumors from PET-CT pairings. We applied a new framework of multilevel and multimodal tumor segmentation techniques that can simultaneously consider PET and CT data. In this study, we improve the network using a learnable composition of test time augmentations. We trained U-Net and **Swin** U-Netr on the training database to determine how different test time augmentation improved segmentation performance. We also developed an algorithm that finds an optimal test time augmentation contribution coefficient set. Using the newly trained U-Net and **Swin** U-Netr results, we defined an optimal set of coefficients for test-time augmentation and utilized them in combination with a pre-trained fixed nnU-Net. The ultimate idea is to improve performance at the time of testing when the model is fixed. Averaging the predictions with varying ratios on the augmented data can improve prediction accuracy. Our code will be available at \url{https://github.com/sepidehamiri/pet\_seg\_unet}  
### ISTA-Inspired Network for Image Super-Resolution. (arXiv:2210.07818v1 [eess.IV])
- Authors : Yuqing Liu, Wei Zhang, Weifeng Sun, Zhikai Yu, Jianfeng Wei, Shengquan Li
- Link : [http://arxiv.org/abs/2210.07818](http://arxiv.org/abs/2210.07818)
> ABSTRACT  :  Deep learning for image super-resolution (SR) has been investigated by numerous researchers in recent years. Most of the works concentrate on effective block designs and improve the network representation but lack interpretation. There are also iterative optimization-inspired networks for image SR, which take the solution step as a whole without giving an explicit optimization step. This paper proposes an unfolding iterative shrinkage thresholding algorithm (ISTA) inspired network for interpretable image SR. Specifically, we analyze the problem of image SR and propose a solution based on the ISTA method. Inspired by the mathematical analysis, the ISTA block is developed to conduct the optimization in an end-to-end manner. To make the exploration more effective, a multi-scale exploitation block and multi-scale attention mechanism are devised to build the ISTA block. Experimental results show the proposed ISTA-inspired **restoration** network (ISTAR) achieves competitive or better performances than other optimization-inspired works with fewer parameters and lower computation complexity.  
### Dynamic Token Normalization Improves Vision Transformers. (arXiv:2112.02624v2 [cs.CV] UPDATED)
- Authors : Wenqi Shao, Yixiao Ge, Zhaoyang Zhang, Xuyuan Xu, Xiaogang Wang, Ying Shan, Ping Luo
- Link : [http://arxiv.org/abs/2112.02624](http://arxiv.org/abs/2112.02624)
> ABSTRACT  :  Vision Transformer (ViT) and its variants (e.g., **Swin**, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. {Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, **Swin**, PVT, LeViT, T2T-ViT, BigBird and Reformer. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN by $0.5\%$ - $1.2\%$ top-1 accuracy on ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by $2.3\%$ - $3.9\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\%$ - $0.8\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made public at \url{https://github.com/wqshao126/DTN}  
### Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v2 [cs.CV] UPDATED)
- Authors : Lang Huang, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Toshihiko Yamasaki
- Link : [http://arxiv.org/abs/2205.13515](http://arxiv.org/abs/2205.13515)
> ABSTRACT  :  We present an efficient approach for Masked Image Modeling (MIM) with hierarchical Vision Transformers (ViTs), allowing the hierarchical ViTs to discard masked patches and operate only on the visible ones. Our approach consists of three key designs. First, for window attention, we propose a Group Window Attention scheme following the Divide-and-Conquer strategy. To mitigate the quadratic complexity of the self-attention w.r.t. the number of patches, group attention encourages a uniform partition that visible patches within each local window of arbitrary size can be grouped with equal size, where masked self-attention is then performed within each group. Second, we further improve the grouping strategy via the Dynamic Programming algorithm to minimize the overall computation cost of the attention on the grouped patches. Third, as for the convolution layers, we convert them to the Sparse Convolution that works seamlessly with the sparse data, i.e., the visible patches in MIM. As a result, MIM can now work on most, if not all, hierarchical ViTs in a green and efficient way. For example, we can train the hierarchical ViTs, e.g., **Swin** Transformer and Twins Transformer, about 2.7$\times$ faster and reduce the GPU memory usage by 70%, while still enjoying competitive performance on ImageNet classification and the superiority on downstream COCO object detection benchmarks. Code and pre-trained models have been made publicly available at https://github.com/LayneH/GreenMIM.  
### Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior. (arXiv:2206.03858v3 [cs.CV] UPDATED)
- Authors : Bernhard Egger
- Link : [http://arxiv.org/abs/2206.03858](http://arxiv.org/abs/2206.03858)
> ABSTRACT  :  Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. We propose a conditional neural field representation based on a variational auto-decoder with a SIREN network and, extending Vector Neurons, build equivariance directly into the network. Using this, we develop a rotation-equivariant, **high dynamic range** (**HDR**) neural illumination model that is compact and able to express complex, high-frequency features of natural environment maps. Training our model on a curated dataset of 1.6K **HDR** environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations. A PyTorch implementation, our dataset and trained models can be found at jadgardner.github.io/RENI.  
### E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations. (arXiv:2206.07578v2 [cs.CV] UPDATED)
- Authors : Jongwan Kim, DongJin Lee, Byunggook Na, Seongsik Park, Jeonghee Jo, Sungroh Yoon
- Link : [http://arxiv.org/abs/2206.07578](http://arxiv.org/abs/2206.07578)
> ABSTRACT  :  Event cameras respond to brightness changes in the scene asynchronously and independently for every pixel. Due to the properties, these cameras have distinct features: **high dynamic range** (**HDR**), high temporal resolution, and low power consumption. However, the results of event cameras should be processed into an alternative representation for computer vision tasks. Also, they are usually noisy and cause poor performance in areas with few events. In recent years, numerous researchers have attempted to reconstruct videos from events. However, they do not provide good quality videos due to a lack of temporal information from irregular and discontinuous data. To overcome these difficulties, we introduce an E2V-SDE whose dynamics are governed in a latent space by Stochastic differential equations (SDE). Therefore, E2V-SDE can rapidly reconstruct images at arbitrary time steps and make realistic predictions on unseen data. In addition, we successfully adopted a variety of image composition techniques for improving image clarity and temporal consistency. By conducting extensive experiments on simulated and real-scene datasets, we verify that our model outperforms state-of-the-art approaches under various video reconstruction settings. In terms of image quality, the LPIPS score improves by up to 12% and the reconstruction speed is 87% higher than that of ET-Net.  
### How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v3 [cs.CV] UPDATED)
- Authors : Tobias Fischer, Michael Milford
- Link : [http://arxiv.org/abs/2206.13673](http://arxiv.org/abs/2206.13673)
> ABSTRACT  :  Event cameras continue to attract interest due to desirable characteristics such as **high dynamic range**, low latency, virtually no motion blur, and high energy efficiency. One of the potential applications that would benefit from these characteristics lies in visual place recognition for robot localization, i.e. matching a query observation to the corresponding reference place in the database. In this letter, we explore the distinctiveness of event streams from a small subset of pixels (in the tens or hundreds). We demonstrate that the absolute difference in the number of events at those pixel locations accumulated into event frames can be sufficient for the place recognition task, when pixels that display large variations in the reference set are used. Using such sparse (over image coordinates) but varying (variance over the number of events per pixel location) pixels enables frequent and computationally cheap updates of the location estimates. Furthermore, when event frames contain a constant number of events, our method takes full advantage of the event-driven nature of the sensory stream and displays promising robustness to changes in velocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset in an outdoor driving scenario, as well as the newly contributed indoor QCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a mobile robotic platform. Our results show that our approach achieves competitive performance when compared to several baseline methods on those datasets, and is particularly well suited for compute- and energy-constrained platforms such as interplanetary rovers.  
### Adversarial Pixel **Restoration** as a Pretext Task for Transferable Perturbations. (arXiv:2207.08803v3 [cs.CV] UPDATED)
- Authors : Hashmat Shadab, Muzammal Naseer, Salman Khan, Fahad Shahbaz
- Link : [http://arxiv.org/abs/2207.08803](http://arxiv.org/abs/2207.08803)
> ABSTRACT  :  Transferable adversarial attacks optimize adversaries from a pretrained surrogate model and known label space to fool the unknown black-box models. Therefore, these attacks are restricted by the availability of an effective surrogate model. In this work, we relax this assumption and propose Adversarial Pixel **Restoration** as a self-supervised alternative to train an effective surrogate model from scratch under the condition of no labels and few data samples. Our training approach is based on a min-max scheme which reduces overfitting via an adversarial objective and thus optimizes for a more generalizable surrogate model. Our proposed attack is complimentary to the adversarial pixel **restoration** and is independent of any task specific objective as it can be launched in a self-supervised manner. We successfully demonstrate the adversarial transferability of our approach to Vision Transformers as well as Convolutional Neural Networks for the tasks of classification, object detection, and video segmentation. Our training approach improves the transferability of the baseline unsupervised training method by 16.4% on ImageNet val. set. Our codes &amp; pre-trained surrogate models are available at: https://github.com/HashmatShadab/APR  
### AutoPET Challenge: Combining nn-Unet with **Swin** UNETR Augmented by Maximum Intensity Projection Classifier. (arXiv:2209.01112v2 [eess.IV] UPDATED)
- Authors : Lars Heiliger, Zdravko Marinov, Max Hasin, Jana Fragemann, Kelsey Pomykala, Jacob Murray, David Kersting, Victor Alves, Rainer Stiefelhagen, Jan Egger, Jens Kleesiek
- Link : [http://arxiv.org/abs/2209.01112](http://arxiv.org/abs/2209.01112)
> ABSTRACT  :  Tumor volume and changes in tumor characteristics over time are important biomarkers for cancer therapy. In this context, FDG-PET/CT scans are routinely used for staging and re-staging of cancer, as the radiolabeled fluorodeoxyglucose is taken up in regions of high metabolism. Unfortunately, these regions with high metabolism are not specific to tumors and can also represent physiological uptake by normal functioning organs, inflammation, or infection, making detailed and reliable tumor segmentation in these scans a demanding task. This gap in research is addressed by the AutoPET challenge, which provides a public data set with FDG-PET/CT scans from 900 patients to encourage further improvement in this field. Our contribution to this challenge is an ensemble of two state-of-the-art segmentation models, the nn-Unet and the **Swin** UNETR, augmented by a maximum intensity projection classifier that acts like a gating mechanism. If it predicts the existence of lesions, both segmentations are combined by a late fusion approach. Our solution achieves a Dice score of 72.12\% on patients diagnosed with lung cancer, melanoma, and lymphoma in our cross-validation. Code: https://github.com/heiligerl/autopet_submission  
### From Face to Natural Image: Learning Real Degradation for Blind Image Super-Resolution. (arXiv:2210.00752v2 [cs.CV] UPDATED)
- Authors : Xiaoming Li, Chaofeng Chen, Xianhui Lin, Wangmeng Zuo, **Lei Zhang**
- Link : [http://arxiv.org/abs/2210.00752](http://arxiv.org/abs/2210.00752)
> ABSTRACT  :  How to design proper training pairs is critical for super-resolving real-world low-quality (LQ) images, which suffers from the difficulties in either acquiring paired ground-truth high-quality (HQ) images or synthesizing photo-realistic degraded LQ observations. Recent works mainly focus on modeling the degradation with handcrafted or estimated degradation parameters, which are however incapable to model complicated real-world degradation types, resulting in limited quality improvement. Notably, LQ face images, which may have the same degradation process as natural images, can be robustly restored with photo-realistic textures by exploiting their strong structural priors. This motivates us to use the real-world LQ face images and their restored HQ counterparts to model the complex real-world degradation (namely ReDegNet), and then transfer it to HQ natural images to synthesize their realistic LQ counterparts. By taking these paired HQ-LQ face images as inputs to explicitly predict the degradation-aware and content-independent representations, we could control the degraded image generation, and subsequently transfer these degradation representations from face to natural images to synthesize the degraded LQ natural images. Experiments show that our ReDegNet can well learn the real degradation process from face images. The **restoration** network trained with our synthetic pairs performs favorably against SOTAs. More importantly, our method provides a new way to handle the real-world complex scenarios by learning their degradation representations from the facial portions, which can be used to significantly improve the quality of non-facial areas. The source code is available at https://github.com/csxmli2016/ReDegNet.  
### Automatic **Real-time** Vehicle Classification by Image Colour Component Based Template Matching. (arXiv:2210.06586v2 [cs.CV] UPDATED)
- Authors : Ahmet Orun
- Link : [http://arxiv.org/abs/2210.06586](http://arxiv.org/abs/2210.06586)
> ABSTRACT  :  Selection of appropriate template matching algorithms to run effectively on real-time low-cost systems is always major issue. This is due to unpredictable changes in image scene which often necessitate more sophisticated real-time algorithms to retain image consistency. Inefficiency of low cost auxiliary hardware and time limitations are the major constraints in using these sorts of algorithms. The real-time system introduced here copes with these problems utilising a fast running template matching algorithm, which makes use of best colour band selection. The system uses fast running real-time algorithms to achieve template matching and vehicle classification at about 4 frames /sec. on low-cost hardware. The colour image sequences have been taken by a fixed CCTV camera overlooking a busy multi-lane road  
## eess.IV
---
### Improved automated lesion segmentation in whole-body FDG/PET-CT via Test-Time Augmentation. (arXiv:2210.07761v1 [eess.IV])
- Authors : Sepideh Amiri, Bulat Ibragimov
- Link : [http://arxiv.org/abs/2210.07761](http://arxiv.org/abs/2210.07761)
> ABSTRACT  :  Numerous oncology indications have extensively quantified metabolically active tumors using positron emission tomography (PET) and computed tomography (CT). F-fluorodeoxyglucose-positron emission tomography (FDG-PET) is frequently utilized in clinical practice and clinical drug research to detect and measure metabolically active malignancies. The assessment of tumor burden using manual or computer-assisted tumor segmentation in FDG-PET images is widespread. Deep learning algorithms have also produced effective solutions in this area. However, there may be a need to improve the performance of a pre-trained deep learning network without the opportunity to modify this network. We investigate the potential benefits of test-time augmentation for segmenting tumors from PET-CT pairings. We applied a new framework of multilevel and multimodal tumor segmentation techniques that can simultaneously consider PET and CT data. In this study, we improve the network using a learnable composition of test time augmentations. We trained U-Net and **Swin** U-Netr on the training database to determine how different test time augmentation improved segmentation performance. We also developed an algorithm that finds an optimal test time augmentation contribution coefficient set. Using the newly trained U-Net and **Swin** U-Netr results, we defined an optimal set of coefficients for test-time augmentation and utilized them in combination with a pre-trained fixed nnU-Net. The ultimate idea is to improve performance at the time of testing when the model is fixed. Averaging the predictions with varying ratios on the augmented data can improve prediction accuracy. Our code will be available at \url{https://github.com/sepidehamiri/pet\_seg\_unet}  
### ISTA-Inspired Network for Image Super-Resolution. (arXiv:2210.07818v1 [eess.IV])
- Authors : Yuqing Liu, Wei Zhang, Weifeng Sun, Zhikai Yu, Jianfeng Wei, Shengquan Li
- Link : [http://arxiv.org/abs/2210.07818](http://arxiv.org/abs/2210.07818)
> ABSTRACT  :  Deep learning for image super-resolution (SR) has been investigated by numerous researchers in recent years. Most of the works concentrate on effective block designs and improve the network representation but lack interpretation. There are also iterative optimization-inspired networks for image SR, which take the solution step as a whole without giving an explicit optimization step. This paper proposes an unfolding iterative shrinkage thresholding algorithm (ISTA) inspired network for interpretable image SR. Specifically, we analyze the problem of image SR and propose a solution based on the ISTA method. Inspired by the mathematical analysis, the ISTA block is developed to conduct the optimization in an end-to-end manner. To make the exploration more effective, a multi-scale exploitation block and multi-scale attention mechanism are devised to build the ISTA block. Experimental results show the proposed ISTA-inspired **restoration** network (ISTAR) achieves competitive or better performances than other optimization-inspired works with fewer parameters and lower computation complexity.  
### Data-Limited Tissue Segmentation using Inpainting-Based Self-Supervised Learning. (arXiv:2210.07936v1 [eess.IV])
- Authors : Jeffrey Dominic, Nandita Bhaskhar, Andrew Schmidt, Elka Rubin, Beliz Gunel, Leon Lenchik, Robert Boutin
- Link : [http://arxiv.org/abs/2210.07936](http://arxiv.org/abs/2210.07936)
> ABSTRACT  :  Although supervised learning has enabled high performance for image segmentation, it requires a large amount of labeled training data, which can be difficult to obtain in the medical imaging field. Self-supervised learning (SSL) methods involving pretext tasks have shown promise in overcoming this requirement by first pretraining models using unlabeled data. In this work, we evaluate the efficacy of two SSL methods (inpainting-based pretext tasks of context prediction and context **restoration**) for CT and MRI image segmentation in label-limited scenarios, and investigate the effect of implementation design choices for SSL on downstream segmentation performance. We demonstrate that optimally trained and easy-to-implement inpainting-based SSL segmentation models can outperform classically supervised methods for MRI and CT tissue segmentation in label-limited scenarios, for both clinically-relevant metrics and the traditional Dice score.  
### Wide Range MRI Artifact Removal with Transformers. (arXiv:2210.07976v1 [eess.IV])
- Authors : Lennart Alexander, Van der, Kevin Smith
- Link : [http://arxiv.org/abs/2210.07976](http://arxiv.org/abs/2210.07976)
> ABSTRACT  :  Artifacts on magnetic resonance scans are a serious challenge for both radiologists and computer-aided diagnosis systems. Most commonly, artifacts are caused by motion of the patients, but can also arise from device-specific abnormalities such as noise patterns. Irrespective of the source, artifacts can not only render a scan useless, but can potentially induce misdiagnoses if left unnoticed. For instance, an artifact may masquerade as a tumor or other abnormality. Retrospective artifact correction (RAC) is concerned with removing artifacts after the scan has already been taken. In this work, we propose a method capable of retrospectively removing eight common artifacts found in native-resolution MR imagery. Knowledge of the presence or location of a specific artifact is not assumed and the system is, by design, capable of undoing interactions of multiple artifacts. Our method is realized through the design of a novel volumetric transformer-based neural network that generalizes a \emph{window-centered} approach popularized by the **Swin** transformer. Unlike **Swin**, our method is (i) natively volumetric, (ii) geared towards dense prediction tasks instead of classification, and (iii), uses a novel and more global mechanism to enable information exchange between windows. Our experiments show that our reconstructions are considerably better than those attained by ResNet, V-Net, MobileNet-v2, DenseNet, CycleGAN and BicycleGAN. Moreover, we show that the reconstructed images from our model improves the accuracy of FSL BET, a standard skull-stripping method typically applied in diagnostic workflows.  
### E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations. (arXiv:2206.07578v2 [cs.CV] UPDATED)
- Authors : Jongwan Kim, DongJin Lee, Byunggook Na, Seongsik Park, Jeonghee Jo, Sungroh Yoon
- Link : [http://arxiv.org/abs/2206.07578](http://arxiv.org/abs/2206.07578)
> ABSTRACT  :  Event cameras respond to brightness changes in the scene asynchronously and independently for every pixel. Due to the properties, these cameras have distinct features: **high dynamic range** (**HDR**), high temporal resolution, and low power consumption. However, the results of event cameras should be processed into an alternative representation for computer vision tasks. Also, they are usually noisy and cause poor performance in areas with few events. In recent years, numerous researchers have attempted to reconstruct videos from events. However, they do not provide good quality videos due to a lack of temporal information from irregular and discontinuous data. To overcome these difficulties, we introduce an E2V-SDE whose dynamics are governed in a latent space by Stochastic differential equations (SDE). Therefore, E2V-SDE can rapidly reconstruct images at arbitrary time steps and make realistic predictions on unseen data. In addition, we successfully adopted a variety of image composition techniques for improving image clarity and temporal consistency. By conducting extensive experiments on simulated and real-scene datasets, we verify that our model outperforms state-of-the-art approaches under various video reconstruction settings. In terms of image quality, the LPIPS score improves by up to 12% and the reconstruction speed is 87% higher than that of ET-Net.  
### AutoPET Challenge: Combining nn-Unet with **Swin** UNETR Augmented by Maximum Intensity Projection Classifier. (arXiv:2209.01112v2 [eess.IV] UPDATED)
- Authors : Lars Heiliger, Zdravko Marinov, Max Hasin, Jana Fragemann, Kelsey Pomykala, Jacob Murray, David Kersting, Victor Alves, Rainer Stiefelhagen, Jan Egger, Jens Kleesiek
- Link : [http://arxiv.org/abs/2209.01112](http://arxiv.org/abs/2209.01112)
> ABSTRACT  :  Tumor volume and changes in tumor characteristics over time are important biomarkers for cancer therapy. In this context, FDG-PET/CT scans are routinely used for staging and re-staging of cancer, as the radiolabeled fluorodeoxyglucose is taken up in regions of high metabolism. Unfortunately, these regions with high metabolism are not specific to tumors and can also represent physiological uptake by normal functioning organs, inflammation, or infection, making detailed and reliable tumor segmentation in these scans a demanding task. This gap in research is addressed by the AutoPET challenge, which provides a public data set with FDG-PET/CT scans from 900 patients to encourage further improvement in this field. Our contribution to this challenge is an ensemble of two state-of-the-art segmentation models, the nn-Unet and the **Swin** UNETR, augmented by a maximum intensity projection classifier that acts like a gating mechanism. If it predicts the existence of lesions, both segmentations are combined by a late fusion approach. Our solution achieves a Dice score of 72.12\% on patients diagnosed with lung cancer, melanoma, and lymphoma in our cross-validation. Code: https://github.com/heiligerl/autopet_submission  
## cs.LG
---
### Accelerating RNN-based Speech **Enhancement** on a Multi-Core MCU with Mixed FP16-INT8 Post-Training Quantization. (arXiv:2210.07692v1 [cs.SD])
- Authors : Manuele Rusci, Marco Fariselli, Martin Croome, Francesco Paci, Eric Flamand
- Link : [http://arxiv.org/abs/2210.07692](http://arxiv.org/abs/2210.07692)
> ABSTRACT  :  This paper presents an optimized methodology to design and deploy Speech **Enhancement** (SE) algorithms based on Recurrent Neural Networks (RNNs) on a state-of-the-art MicroController Unit (MCU), with 1+8 general-purpose RISC-V cores. To achieve low-latency execution, we propose an optimized software pipeline interleaving parallel computation of LSTM or GRU recurrent blocks, featuring vectorized 8-bit integer (INT8) and 16-bit floating-point (FP16) compute units, with manually-managed memory transfers of model parameters. To ensure minimal accuracy degradation with respect to the full-precision models, we propose a novel FP16-INT8 Mixed-Precision Post-Training Quantization (PTQ) scheme that compresses the recurrent layers to 8-bit while the bit precision of remaining layers is kept to FP16. Experiments are conducted on multiple LSTM and GRU based SE models trained on the Valentini dataset, featuring up to 1.24M parameters. Thanks to the proposed approaches, we speed-up the computation by up to 4x with respect to the lossless FP16 baselines. Differently from a uniform 8-bit quantization that degrades the PESQ score by 0.3 on average, the Mixed-Precision PTQ scheme leads to a low-degradation of only 0.06, while achieving a 1.4-1.7x memory saving. Thanks to this compression, we cut the power cost of the external memory by fitting the large models on the limited on-chip non-volatile memory and we gain a MCU power saving of up to 2.5x by reducing the supply voltage from 0.8V to 0.65V while still matching the real-time constraints. Our design results 10x more energy efficient than state-of-the-art SE solutions deployed on single-core MCUs that make use of smaller models and quantization-aware training.  
### Simpson's Paradox in Recommender Fairness: Reconciling differences between per-user and aggregated evaluations. (arXiv:2210.07755v1 [cs.IR])
- Authors : Flavien Prost, Ben Packer, Jilin Chen, Li Wei, Pierre Kremp, Nicholas Blumm, Susan Wang, Tulsee Doshi, Tonia Osadebe, Lukasz Heldt, Alex Beutel
- Link : [http://arxiv.org/abs/2210.07755](http://arxiv.org/abs/2210.07755)
> ABSTRACT  :  There has been a flurry of research in recent years on notions of fairness in ranking and recommender systems, particularly on how to evaluate if a recommender allocates **exposure** equally across groups of relevant items (also known as provider fairness). While this research has laid an important foundation, it gave rise to different approaches depending on whether relevant items are compared per-user/per-query or aggregated across users. Despite both being established and intuitive, we discover that these two notions can lead to opposite conclusions, a form of Simpson's Paradox. We reconcile these notions and show that the tension is due to differences in distributions of users where items are relevant, and break down the important factors of the user's recommendations. Based on this new understanding, practitioners might be interested in either notions, but might face challenges with the per-user metric due to partial observability of the relevance and user satisfaction, typical in real-world recommenders. We describe a technique based on distribution matching to estimate it in such a scenario. We demonstrate on simulated and real-world recommender data the effectiveness and usefulness of such an approach.  
### Improved automated lesion segmentation in whole-body FDG/PET-CT via Test-Time Augmentation. (arXiv:2210.07761v1 [eess.IV])
- Authors : Sepideh Amiri, Bulat Ibragimov
- Link : [http://arxiv.org/abs/2210.07761](http://arxiv.org/abs/2210.07761)
> ABSTRACT  :  Numerous oncology indications have extensively quantified metabolically active tumors using positron emission tomography (PET) and computed tomography (CT). F-fluorodeoxyglucose-positron emission tomography (FDG-PET) is frequently utilized in clinical practice and clinical drug research to detect and measure metabolically active malignancies. The assessment of tumor burden using manual or computer-assisted tumor segmentation in FDG-PET images is widespread. Deep learning algorithms have also produced effective solutions in this area. However, there may be a need to improve the performance of a pre-trained deep learning network without the opportunity to modify this network. We investigate the potential benefits of test-time augmentation for segmenting tumors from PET-CT pairings. We applied a new framework of multilevel and multimodal tumor segmentation techniques that can simultaneously consider PET and CT data. In this study, we improve the network using a learnable composition of test time augmentations. We trained U-Net and **Swin** U-Netr on the training database to determine how different test time augmentation improved segmentation performance. We also developed an algorithm that finds an optimal test time augmentation contribution coefficient set. Using the newly trained U-Net and **Swin** U-Netr results, we defined an optimal set of coefficients for test-time augmentation and utilized them in combination with a pre-trained fixed nnU-Net. The ultimate idea is to improve performance at the time of testing when the model is fixed. Averaging the predictions with varying ratios on the augmented data can improve prediction accuracy. Our code will be available at \url{https://github.com/sepidehamiri/pet\_seg\_unet}  
### Intra-session Context-aware Feed Recommendation in Live Systems. (arXiv:2210.07815v1 [cs.IR])
- Authors : Luo Ji, Gao Liu, Mingyang Yin, Hongxia Yang
- Link : [http://arxiv.org/abs/2210.07815](http://arxiv.org/abs/2210.07815)
> ABSTRACT  :  Feed recommendation allows users to constantly browse items until feel uninterested and leave the session, which differs from traditional recommendation scenarios. Within a session, user's decision to continue browsing or not substantially affects occurrences of later clicks. However, such type of **exposure** bias is generally ignored or not explicitly modeled in most feed recommendation studies. In this paper, we model this effect as part of intra-session context, and propose a novel intra-session Context-aware Feed Recommendation (INSCAFER) framework to maximize the total views and total clicks simultaneously. User click and browsing decisions are jointly learned by a multi-task setting, and the intra-session context is encoded by the session-wise exposed item sequence. We deploy our model on Alipay with all key business benchmarks improved. Our method sheds some lights on feed recommendation studies which aim to optimize session-level click and view metrics.  
### Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v2 [cs.CV] UPDATED)
- Authors : Lang Huang, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Toshihiko Yamasaki
- Link : [http://arxiv.org/abs/2205.13515](http://arxiv.org/abs/2205.13515)
> ABSTRACT  :  We present an efficient approach for Masked Image Modeling (MIM) with hierarchical Vision Transformers (ViTs), allowing the hierarchical ViTs to discard masked patches and operate only on the visible ones. Our approach consists of three key designs. First, for window attention, we propose a Group Window Attention scheme following the Divide-and-Conquer strategy. To mitigate the quadratic complexity of the self-attention w.r.t. the number of patches, group attention encourages a uniform partition that visible patches within each local window of arbitrary size can be grouped with equal size, where masked self-attention is then performed within each group. Second, we further improve the grouping strategy via the Dynamic Programming algorithm to minimize the overall computation cost of the attention on the grouped patches. Third, as for the convolution layers, we convert them to the Sparse Convolution that works seamlessly with the sparse data, i.e., the visible patches in MIM. As a result, MIM can now work on most, if not all, hierarchical ViTs in a green and efficient way. For example, we can train the hierarchical ViTs, e.g., **Swin** Transformer and Twins Transformer, about 2.7$\times$ faster and reduce the GPU memory usage by 70%, while still enjoying competitive performance on ImageNet classification and the superiority on downstream COCO object detection benchmarks. Code and pre-trained models have been made publicly available at https://github.com/LayneH/GreenMIM.  
### Gradient Obfuscation Gives a False Sense of Security in Federated Learning. (arXiv:2206.04055v2 [cs.CR] UPDATED)
- Authors : Kai Yue, Richeng Jin, Wai Wong, Dror Baron, Huaiyu Dai
- Link : [http://arxiv.org/abs/2206.04055](http://arxiv.org/abs/2206.04055)
> ABSTRACT  :  Federated learning has been proposed as a privacy-preserving machine learning framework that enables multiple clients to collaborate without sharing raw data. However, client privacy protection is not guaranteed by design in this framework. Prior work has shown that the gradient sharing strategies in federated learning can be vulnerable to data reconstruction attacks. In practice, though, clients may not transmit raw gradients considering the high communication cost or due to privacy **enhancement** requirements. Empirical studies have demonstrated that gradient obfuscation, including intentional obfuscation via gradient noise injection and unintentional obfuscation via gradient compression, can provide more privacy protection against reconstruction attacks. In this work, we present a new data reconstruction attack framework targeting the image classification task in federated learning. We show that commonly adopted gradient postprocessing procedures, such as gradient quantization, gradient sparsification, and gradient perturbation, may give a false sense of security in federated learning. Contrary to prior studies, we argue that privacy **enhancement** should not be treated as a byproduct of gradient compression. Additionally, we design a new method under the proposed framework to reconstruct the image at the semantic level. We quantify the semantic privacy leakage and compare with conventional based on image similarity scores. Our comparisons challenge the image data leakage evaluation schemes in the literature. The results emphasize the importance of revisiting and redesigning the privacy protection mechanisms for client data in existing federated learning algorithms.  
### E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations. (arXiv:2206.07578v2 [cs.CV] UPDATED)
- Authors : Jongwan Kim, DongJin Lee, Byunggook Na, Seongsik Park, Jeonghee Jo, Sungroh Yoon
- Link : [http://arxiv.org/abs/2206.07578](http://arxiv.org/abs/2206.07578)
> ABSTRACT  :  Event cameras respond to brightness changes in the scene asynchronously and independently for every pixel. Due to the properties, these cameras have distinct features: **high dynamic range** (**HDR**), high temporal resolution, and low power consumption. However, the results of event cameras should be processed into an alternative representation for computer vision tasks. Also, they are usually noisy and cause poor performance in areas with few events. In recent years, numerous researchers have attempted to reconstruct videos from events. However, they do not provide good quality videos due to a lack of temporal information from irregular and discontinuous data. To overcome these difficulties, we introduce an E2V-SDE whose dynamics are governed in a latent space by Stochastic differential equations (SDE). Therefore, E2V-SDE can rapidly reconstruct images at arbitrary time steps and make realistic predictions on unseen data. In addition, we successfully adopted a variety of image composition techniques for improving image clarity and temporal consistency. By conducting extensive experiments on simulated and real-scene datasets, we verify that our model outperforms state-of-the-art approaches under various video reconstruction settings. In terms of image quality, the LPIPS score improves by up to 12% and the reconstruction speed is 87% higher than that of ET-Net.  
## cs.AI
---
### **Real-time** Action Recognition for Fine-Grained Actions and The Hand Wash Dataset. (arXiv:2210.07400v1 [cs.CV])
- Authors : Akash Nagaraj, Mukund Sood, Chetna Sureka, Gowri Srinivasa
- Link : [http://arxiv.org/abs/2210.07400](http://arxiv.org/abs/2210.07400)
> ABSTRACT  :  In this paper we present a three-stream algorithm for real-time action recognition and a new dataset of handwash videos, with the intent of aligning action recognition with real-world constraints to yield effective conclusions. A three-stream fusion algorithm is proposed, which runs both accurately and efficiently, in real-time even on low-powered systems such as a Raspberry Pi. The cornerstone of the proposed algorithm is the incorporation of both spatial and temporal information, as well as the information of the objects in a video while using an efficient architecture, and Optical Flow computation to achieve commendable results in real-time. The results achieved by this algorithm are benchmarked on the UCF-101 as well as the HMDB-51 datasets, achieving an accuracy of 92.7% and 64.9% respectively. An important point to note is that the algorithm is novel in the aspect that it is also able to learn the intricate differences between extremely similar actions, which would be difficult even for the human eye. Additionally, noticing a dearth in the number of datasets for the recognition of very similar or fine-grained actions, this paper also introduces a new dataset that is made publicly available, the Hand Wash Dataset with the intent of introducing a new benchmark for fine-grained action recognition tasks in the future.  
### Simpson's Paradox in Recommender Fairness: Reconciling differences between per-user and aggregated evaluations. (arXiv:2210.07755v1 [cs.IR])
- Authors : Flavien Prost, Ben Packer, Jilin Chen, Li Wei, Pierre Kremp, Nicholas Blumm, Susan Wang, Tulsee Doshi, Tonia Osadebe, Lukasz Heldt, Alex Beutel
- Link : [http://arxiv.org/abs/2210.07755](http://arxiv.org/abs/2210.07755)
> ABSTRACT  :  There has been a flurry of research in recent years on notions of fairness in ranking and recommender systems, particularly on how to evaluate if a recommender allocates **exposure** equally across groups of relevant items (also known as provider fairness). While this research has laid an important foundation, it gave rise to different approaches depending on whether relevant items are compared per-user/per-query or aggregated across users. Despite both being established and intuitive, we discover that these two notions can lead to opposite conclusions, a form of Simpson's Paradox. We reconcile these notions and show that the tension is due to differences in distributions of users where items are relevant, and break down the important factors of the user's recommendations. Based on this new understanding, practitioners might be interested in either notions, but might face challenges with the per-user metric due to partial observability of the relevance and user satisfaction, typical in real-world recommenders. We describe a technique based on distribution matching to estimate it in such a scenario. We demonstrate on simulated and real-world recommender data the effectiveness and usefulness of such an approach.  
### Dynamic Token Normalization Improves Vision Transformers. (arXiv:2112.02624v2 [cs.CV] UPDATED)
- Authors : Wenqi Shao, Yixiao Ge, Zhaoyang Zhang, Xuyuan Xu, Xiaogang Wang, Ying Shan, Ping Luo
- Link : [http://arxiv.org/abs/2112.02624](http://arxiv.org/abs/2112.02624)
> ABSTRACT  :  Vision Transformer (ViT) and its variants (e.g., **Swin**, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. {Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, **Swin**, PVT, LeViT, T2T-ViT, BigBird and Reformer. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN by $0.5\%$ - $1.2\%$ top-1 accuracy on ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by $2.3\%$ - $3.9\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\%$ - $0.8\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made public at \url{https://github.com/wqshao126/DTN}  
### Gradient Obfuscation Gives a False Sense of Security in Federated Learning. (arXiv:2206.04055v2 [cs.CR] UPDATED)
- Authors : Kai Yue, Richeng Jin, Wai Wong, Dror Baron, Huaiyu Dai
- Link : [http://arxiv.org/abs/2206.04055](http://arxiv.org/abs/2206.04055)
> ABSTRACT  :  Federated learning has been proposed as a privacy-preserving machine learning framework that enables multiple clients to collaborate without sharing raw data. However, client privacy protection is not guaranteed by design in this framework. Prior work has shown that the gradient sharing strategies in federated learning can be vulnerable to data reconstruction attacks. In practice, though, clients may not transmit raw gradients considering the high communication cost or due to privacy **enhancement** requirements. Empirical studies have demonstrated that gradient obfuscation, including intentional obfuscation via gradient noise injection and unintentional obfuscation via gradient compression, can provide more privacy protection against reconstruction attacks. In this work, we present a new data reconstruction attack framework targeting the image classification task in federated learning. We show that commonly adopted gradient postprocessing procedures, such as gradient quantization, gradient sparsification, and gradient perturbation, may give a false sense of security in federated learning. Contrary to prior studies, we argue that privacy **enhancement** should not be treated as a byproduct of gradient compression. Additionally, we design a new method under the proposed framework to reconstruct the image at the semantic level. We quantify the semantic privacy leakage and compare with conventional based on image similarity scores. Our comparisons challenge the image data leakage evaluation schemes in the literature. The results emphasize the importance of revisiting and redesigning the privacy protection mechanisms for client data in existing federated learning algorithms.  
### How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v3 [cs.CV] UPDATED)
- Authors : Tobias Fischer, Michael Milford
- Link : [http://arxiv.org/abs/2206.13673](http://arxiv.org/abs/2206.13673)
> ABSTRACT  :  Event cameras continue to attract interest due to desirable characteristics such as **high dynamic range**, low latency, virtually no motion blur, and high energy efficiency. One of the potential applications that would benefit from these characteristics lies in visual place recognition for robot localization, i.e. matching a query observation to the corresponding reference place in the database. In this letter, we explore the distinctiveness of event streams from a small subset of pixels (in the tens or hundreds). We demonstrate that the absolute difference in the number of events at those pixel locations accumulated into event frames can be sufficient for the place recognition task, when pixels that display large variations in the reference set are used. Using such sparse (over image coordinates) but varying (variance over the number of events per pixel location) pixels enables frequent and computationally cheap updates of the location estimates. Furthermore, when event frames contain a constant number of events, our method takes full advantage of the event-driven nature of the sensory stream and displays promising robustness to changes in velocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset in an outdoor driving scenario, as well as the newly contributed indoor QCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a mobile robotic platform. Our results show that our approach achieves competitive performance when compared to several baseline methods on those datasets, and is particularly well suited for compute- and energy-constrained platforms such as interplanetary rovers.  
### ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding. (arXiv:2210.06155v2 [cs.CL] UPDATED)
- Authors : Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo, Zhenyu Zhang, Zhengjie Huang, Teng Hu, Weichong Yin, Yongfeng Chen, Yin Zhang, Shikun Feng, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang
- Link : [http://arxiv.org/abs/2210.06155](http://arxiv.org/abs/2210.06155)
> ABSTRACT  :  Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel document pre-training solution with layout knowledge **enhancement** in the whole workflow, to learn better representations that combine the features from text, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at <a href="http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout.">this http URL</a>  
### Automatic **Real-time** Vehicle Classification by Image Colour Component Based Template Matching. (arXiv:2210.06586v2 [cs.CV] UPDATED)
- Authors : Ahmet Orun
- Link : [http://arxiv.org/abs/2210.06586](http://arxiv.org/abs/2210.06586)
> ABSTRACT  :  Selection of appropriate template matching algorithms to run effectively on real-time low-cost systems is always major issue. This is due to unpredictable changes in image scene which often necessitate more sophisticated real-time algorithms to retain image consistency. Inefficiency of low cost auxiliary hardware and time limitations are the major constraints in using these sorts of algorithms. The real-time system introduced here copes with these problems utilising a fast running template matching algorithm, which makes use of best colour band selection. The system uses fast running real-time algorithms to achieve template matching and vehicle classification at about 4 frames /sec. on low-cost hardware. The colour image sequences have been taken by a fixed CCTV camera overlooking a busy multi-lane road  
# Paper List
---
## cs.CV
---
**140** new papers in cs.CV:-) 
1. The Hidden Uniform Cluster Prior in Self-Supervised Learning. (arXiv:2210.07277v1 [cs.LG])
2. Tumor-location-guided CNNs for Pediatric Low-grade Glioma Molecular Biomarker Classification Using MRI. (arXiv:2210.07287v1 [cs.CV])
3. 3D GAN Inversion with Pose Optimization. (arXiv:2210.07301v1 [cs.CV])
4. Demystifying Self-supervised Trojan Attacks. (arXiv:2210.07346v1 [cs.CR])
5. Finding Islands of Predictability in Action Forecasting. (arXiv:2210.07354v1 [cs.CV])
6. Consistency and Accuracy of CelebA Attribute Values. (arXiv:2210.07356v1 [cs.CV])
7. SWFormer: Sparse Window Transformer for 3D Object Detection in Point Clouds. (arXiv:2210.07372v1 [cs.CV])
8. Amortized Inference for Heterogeneous Reconstruction in Cryo-EM. (arXiv:2210.07387v1 [cs.CV])
9. Caption supervision enables robust learners. (arXiv:2210.07396v1 [cs.CV])
10. **Real-time** Action Recognition for Fine-Grained Actions and The Hand Wash Dataset. (arXiv:2210.07400v1 [cs.CV])
11. A Novel Supervised Contrastive Regression Framework for Prediction of Neurocognitive Measures Using Multi-Site Harmonized Diffusion MRI Tractography. (arXiv:2210.07411v1 [cs.CV])
12. Task Grouping for Multilingual Text Recognition. (arXiv:2210.07423v1 [cs.CV])
13. Autoregressive Uncertainty Modeling for 3D Bounding Box Prediction. (arXiv:2210.07424v1 [cs.CV])
14. NOCaL: Calibration-Free Semi-Supervised Learning of Odometry and Camera Intrinsics. (arXiv:2210.07435v1 [cs.RO])
15. Smart Headset, Computer Vision and Machine Learning for Efficient Prawn Farm Management. (arXiv:2210.07436v1 [cs.CV])
16. Frame Mining: a Free Lunch for Learning Robotic Manipulation from 3D Point Clouds. (arXiv:2210.07442v1 [cs.RO])
17. Evaluating Out-of-Distribution Performance on Document Image Classifiers. (arXiv:2210.07448v1 [cs.CV])
18. ExAug: Robot-Conditioned Navigation Policies via Geometric Experience Augmentation. (arXiv:2210.07450v1 [cs.RO])
19. Neural Network Compression by Joint Sparsity Promotion and Redundancy Reduction. (arXiv:2210.07451v1 [cs.CV])
20. Polycentric Clustering and Structural Regularization for Source-free Unsupervised Domain Adaptation. (arXiv:2210.07463v1 [cs.CV])
21. Synthetic-to-real Composite Semantic Segmentation in Additive Manufacturing. (arXiv:2210.07466v1 [cs.CV])
22. SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v1 [cs.CV])
23. InFIP: An Explainable DNN Intellectual Property Protection Method based on Intrinsic Features. (arXiv:2210.07481v1 [cs.CV])
24. The Surprisingly Straightforward Scene Text Removal Method With Gated Attention and Region of Interest Generation: A Comprehensive Prominent Model Analysis. (arXiv:2210.07489v1 [cs.CV])
25. Exploring Vanilla U-Net for Lesion Segmentation from Whole-body FDG-PET/CT Scans. (arXiv:2210.07490v1 [eess.IV])
26. STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition. (arXiv:2210.07503v1 [cs.CV])
27. Learning Active Camera for Multi-Object Navigation. (arXiv:2210.07505v1 [cs.CV])
28. Weakly-Supervised Multi-Granularity Map Learning for Vision-and-Language Navigation. (arXiv:2210.07506v1 [cs.CV])
29. Boosting Performance of a Baseline Visual Place Recognition Technique by Predicting the Maximally Complementary Technique. (arXiv:2210.07509v1 [cs.CV])
30. Superpixel Perception Graph Neural Network for Intelligent Defect Detection. (arXiv:2210.07539v1 [cs.CV])
31. When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture. (arXiv:2210.07540v1 [cs.CV])
32. Transformer-Based Speech Synthesizer Attribution in an Open Set Scenario. (arXiv:2210.07546v1 [cs.SD])
33. Reconstructed Student-Teacher and Discriminative Networks for Anomaly Detection. (arXiv:2210.07548v1 [cs.CV])
34. TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers. (arXiv:2210.07562v1 [cs.CV])
35. A Survey of Parameters Associated with the Quality of Benchmarks in NLP. (arXiv:2210.07566v1 [cs.CL])
36. Mix and Reason: Reasoning over Semantic Topology with Data Mixing for Domain Generalization. (arXiv:2210.07571v1 [cs.CV])
37. Cross-Scale Context Extracted Hashing for Fine-Grained Image Binary Encoding. (arXiv:2210.07572v1 [cs.CV])
38. Is synthetic data from generative models ready for image recognition?. (arXiv:2210.07574v1 [cs.CV])
39. MonoDVPS: A Self-Supervised Monocular Depth Estimation Approach to Depth-aware Video Panoptic Segmentation. (arXiv:2210.07577v1 [cs.CV])
40. Deep PatchMatch MVS with Learned Patch Coplanarity, Geometric Consistency and Adaptive Pixel Sampling. (arXiv:2210.07582v1 [cs.CV])
41. See Blue Sky: Deep Image Dehaze Using Paired and Unpaired Training Images. (arXiv:2210.07594v1 [cs.CV])
42. Lightweight Stepless Super-Resolution of Remote Sensing Images via Saliency-Aware Dynamic Routing Strategy. (arXiv:2210.07598v1 [cs.CV])
43. MCTNet: A Multi-Scale CNN-Transformer Network for Change Detection in Optical Remote Sensing Images. (arXiv:2210.07601v1 [cs.CV])
44. Self-Supervised 2D/3D Registration for X-Ray to CT Image Fusion. (arXiv:2210.07611v1 [eess.IV])
45. Hardness of Samples Need to be Quantified for a Reliable Evaluation System: Exploring Potential Opportunities with a New Task. (arXiv:2210.07631v1 [cs.CL])
46. Pareto-aware Neural Architecture Generation for Diverse Computational Budgets. (arXiv:2210.07634v1 [cs.LG])
47. Vision Transformer Visualization: What Neurons Tell and How Neurons Behave?. (arXiv:2210.07646v1 [cs.CV])
48. DART: Articulated Hand Model with Diverse Accessories and Rich Textures. (arXiv:2210.07650v1 [cs.CV])
49. Towards Transformer-based Homogenization of Satellite Imagery for Landsat-8 and Sentinel-2. (arXiv:2210.07654v1 [cs.CV])
50. Pretrained Transformers Do not Always Improve Robustness. (arXiv:2210.07663v1 [cs.CL])
51. Multi-View Photometric Stereo Revisited. (arXiv:2210.07670v1 [cs.CV])
52. Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v1 [cs.CV])
53. Quo Vadis: Is Trajectory Forecasting the Key Towards Long-Term Multi-Object Tracking?. (arXiv:2210.07681v1 [cs.CV])
54. Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training. (arXiv:2210.07688v1 [cs.CL])
55. Multi-Task Learning based Video Anomaly Detection with Attention. (arXiv:2210.07697v1 [cs.CV])
56. Motion-related Artefact Classification Using Patch-based Ensemble and Transfer Learning in Cardiac MRI. (arXiv:2210.07717v1 [eess.IV])
57. Model-Based Imitation Learning for Urban Driving. (arXiv:2210.07729v1 [cs.CV])
58. Blind Super-Resolution for Remote Sensing Images via Conditional Stochastic Normalizing Flows. (arXiv:2210.07751v1 [eess.IV])
59. Lightweight Alpha Matting Network Using Distillation-Based Channel Pruning. (arXiv:2210.07760v1 [cs.CV])
60. Improved automated lesion segmentation in whole-body FDG/PET-CT via Test-Time Augmentation. (arXiv:2210.07761v1 [eess.IV])
61. Controllable Style Transfer via Test-time Training of Implicit Neural Representation. (arXiv:2210.07762v1 [cs.CV])
62. Intel Labs at Ego4D Challenge 2022: A Better Baseline for Audio-Visual Diarization. (arXiv:2210.07764v1 [cs.CV])
63. EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning. (arXiv:2210.07795v1 [cs.CL])
64. Comparison of different automatic solutions for resection cavity segmentation in postoperative MRI volumes including longitudinal acquisitions. (arXiv:2210.07806v1 [cs.CV])
65. A Consistent and Differentiable Lp Canonical Calibration Error Estimator. (arXiv:2210.07810v1 [stat.ML])
66. SAILOR: Scaling Anchors via Insights into Latent Object. (arXiv:2210.07811v1 [cs.CV])
67. Surface abnormality detection in medical and inspection systems using energy variations in co-occurrence matrixes. (arXiv:2210.07812v1 [cs.CV])
68. ISTA-Inspired Network for Image Super-Resolution. (arXiv:2210.07818v1 [eess.IV])
69. Parameter-Free Average Attention Improves Convolutional Neural Network Performance (Almost) Free of Charge. (arXiv:2210.07828v1 [cs.CV])
70. Asymmetric Student-Teacher Networks for Industrial Anomaly Detection. (arXiv:2210.07829v1 [cs.LG])
71. Contrastive Audio-Visual Masked Autoencoder. (arXiv:2210.07839v1 [cs.MM])
72. Realizing Flame State Monitoring with Very Few Visual or Infrared Images via Few-Shot Learning. (arXiv:2210.07845v1 [cs.CV])
73. Convolutional Neural Networks: Basic Concepts and Applications in Manufacturing. (arXiv:2210.07848v1 [cs.CV])
74. Unsupervised Dense Nuclei Detection and Segmentation with Prior Self-activation Map For Histology Images. (arXiv:2210.07862v1 [cs.CV])
75. Hierarchical Approach for Joint Semantic, Plant Instance, and Leaf Instance Segmentation in the Agricultural Domain. (arXiv:2210.07879v1 [cs.CV])
76. One Model to Edit Them All: Free-Form Text-Driven Image Manipulation with Semantic Modulations. (arXiv:2210.07883v1 [cs.CV])
77. PedFormer: Pedestrian Behavior Prediction via Cross-Modal Attention Modulation and Gated Multitask Learning. (arXiv:2210.07886v1 [cs.CV])
78. Text Detection Forgot About Document OCR. (arXiv:2210.07903v1 [cs.CV])
79. Post-Training Quantization for Energy Efficient Realization of Deep Neural Networks. (arXiv:2210.07906v1 [cs.LG])
80. Cumulo: A Dataset for Learning Cloud Classes. (arXiv:1911.04227v3 [physics.ao-ph] UPDATED)
81. Look Twice: A Generalist Computational Model Predicts Return Fixations across Tasks and Species. (arXiv:2101.01611v2 [cs.CV] UPDATED)
82. Self-Adaptive Training: Bridging Supervised and Self-Supervised Learning. (arXiv:2101.08732v3 [cs.LG] UPDATED)
83. Deep Hierarchical Super Resolution for Scientific Data. (arXiv:2107.00462v3 [eess.IV] UPDATED)
84. Dynamic Convolution for 3D Point Cloud Instance Segmentation. (arXiv:2107.08392v3 [cs.CV] UPDATED)
85. Dynamic Token Normalization Improves Vision Transformers. (arXiv:2112.02624v2 [cs.CV] UPDATED)
86. An Interpretive Constrained Linear Model for ResNet and MgNet. (arXiv:2112.07441v2 [cs.CV] UPDATED)
87. Self-Supervised Robustifying Guidance for Monocular 3D Face Reconstruction. (arXiv:2112.14382v2 [cs.CV] UPDATED)
88. The cluster structure function. (arXiv:2201.01222v3 [cs.LG] UPDATED)
89. Similarity and Generalization: From Noise to Corruption. (arXiv:2201.12803v2 [cs.LG] UPDATED)
90. An Efficient Smoothing and Thresholding Image Segmentation Framework with Weighted Anisotropic-Isotropic Total Variation. (arXiv:2202.10115v3 [cs.CV] UPDATED)
91. STPLS3D: A Large-Scale Synthetic and Real Aerial Photogrammetry 3D Point Cloud Dataset. (arXiv:2203.09065v3 [cs.CV] UPDATED)
92. Multi-Domain Multi-Definition Landmark Localization for Small Datasets. (arXiv:2203.10358v3 [cs.CV] UPDATED)
93. Towards Device Efficient Conditional Image Generation. (arXiv:2203.10363v2 [cs.CV] UPDATED)
94. Physics-Driven Deep Learning for Computational Magnetic Resonance Imaging. (arXiv:2203.12215v3 [eess.IV] UPDATED)
95. A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques. (arXiv:2203.16915v3 [cs.CV] UPDATED)
96. Unconditional Image-Text Pair Generation with Multimodal Cross Quantizer. (arXiv:2204.07537v2 [cs.CV] UPDATED)
97. Neural Implicit Representations for Physical Parameter Inference from a Single Video. (arXiv:2204.14030v2 [cs.CV] UPDATED)
98. Review on Panoramic Imaging and Its Applications in Scene Understanding. (arXiv:2205.05570v2 [cs.CV] UPDATED)
99. Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion. (arXiv:2205.07844v2 [cs.CV] UPDATED)
100. UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes. (arXiv:2205.10337v3 [cs.CV] UPDATED)
101. Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v2 [cs.CV] UPDATED)
102. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training. (arXiv:2205.14401v2 [cs.CV] UPDATED)
103. Towards Efficient 3D Object Detection with Knowledge Distillation. (arXiv:2205.15156v3 [cs.CV] UPDATED)
104. Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering. (arXiv:2206.02721v2 [cs.CV] UPDATED)
105. Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior. (arXiv:2206.03858v3 [cs.CV] UPDATED)
106. E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations. (arXiv:2206.07578v2 [cs.CV] UPDATED)
107. Embodied Scene-aware Human Pose Estimation. (arXiv:2206.09106v3 [cs.CV] UPDATED)
108. Counting Varying Density Crowds Through Density Guided Adaptive Selection CNN and Transformer Estimation. (arXiv:2206.10075v2 [cs.CV] UPDATED)
109. How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v3 [cs.CV] UPDATED)
110. Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera. (arXiv:2206.15258v2 [cs.CV] UPDATED)
111. Drone Detection and Tracking in Real-Time by Fusion of Different Sensing Modalities. (arXiv:2207.01927v2 [cs.CV] UPDATED)
112. Exploring the sequence length bottleneck in the Transformer for Image Captioning. (arXiv:2207.03327v4 [cs.CV] UPDATED)
113. 2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds. (arXiv:2207.04397v3 [cs.CV] UPDATED)
114. Vision Transformer for NeRF-Based View Synthesis from a Single Input Image. (arXiv:2207.05736v2 [cs.CV] UPDATED)
115. Trans4Map: Revisiting Holistic Bird's-Eye-View Mapping from Egocentric Images to Allocentric Semantics with Vision Transformers. (arXiv:2207.06205v2 [cs.CV] UPDATED)
116. EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations. (arXiv:2207.06635v3 [cs.CV] UPDATED)
117. Adversarial Pixel **Restoration** as a Pretext Task for Transferable Perturbations. (arXiv:2207.08803v3 [cs.CV] UPDATED)
118. Probable Domain Generalization via Quantile Risk Minimization. (arXiv:2207.09944v2 [stat.ML] UPDATED)
119. Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP. (arXiv:2208.05516v2 [cs.LG] UPDATED)
120. USB: A Unified Semi-supervised Learning Benchmark for Classification. (arXiv:2208.07204v2 [cs.LG] UPDATED)
121. Blind Users Accessing Their Training Images in Teachable Object Recognizers. (arXiv:2208.07968v2 [cs.HC] UPDATED)
122. Towards Open-vocabulary Scene Graph Generation with Prompt-based Finetuning. (arXiv:2208.08165v3 [cs.CV] UPDATED)
123. AutoPET Challenge: Combining nn-Unet with **Swin** UNETR Augmented by Maximum Intensity Projection Classifier. (arXiv:2209.01112v2 [eess.IV] UPDATED)
124. Generalized One-shot Domain Adaptation of Generative Adversarial Networks. (arXiv:2209.03665v2 [cs.CV] UPDATED)
125. EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v2 [cs.CV] UPDATED)
126. SOCRATES: A Stereo Camera Trap for Monitoring of Biodiversity. (arXiv:2209.09070v2 [cs.CV] UPDATED)
127. Poisson Flow Generative Models. (arXiv:2209.11178v3 [cs.LG] UPDATED)
128. From Face to Natural Image: Learning Real Degradation for Blind Image Super-Resolution. (arXiv:2210.00752v2 [cs.CV] UPDATED)
129. Dual-Domain Cross-Iteration Squeeze-Excitation Network for Sparse Reconstruction of Brain MRI. (arXiv:2210.02523v2 [cs.CV] UPDATED)
130. Pre-trained Adversarial Perturbations. (arXiv:2210.03372v2 [cs.CV] UPDATED)
131. Robust Graph Structure Learning over Images via Multiple Statistical Tests. (arXiv:2210.03956v2 [cs.CV] UPDATED)
132. Deep object detection for waterbird monitoring using aerial imagery. (arXiv:2210.04868v2 [cs.CV] UPDATED)
133. Loop Unrolled Shallow Equilibrium Regularizer (LUSER) -- A Memory-Efficient Inverse Problem Solver. (arXiv:2210.04987v2 [eess.IV] UPDATED)
134. Frequency-Aware Self-Supervised Monocular Depth Estimation. (arXiv:2210.05479v2 [cs.CV] UPDATED)
135. Oflib: Facilitating Operations with and on Optical Flow Fields in Python. (arXiv:2210.05635v2 [cs.CV] UPDATED)
136. Uplift and Upsample: Efficient 3D Human Pose Estimation with Uplifting Transformers. (arXiv:2210.06110v2 [cs.CV] UPDATED)
137. AISFormer: Amodal Instance Segmentation with Transformer. (arXiv:2210.06323v2 [cs.CV] UPDATED)
138. S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces. (arXiv:2210.06583v2 [cs.CV] UPDATED)
139. Automatic **Real-time** Vehicle Classification by Image Colour Component Based Template Matching. (arXiv:2210.06586v2 [cs.CV] UPDATED)
140. QMRNet: Quality Metric Regression for EO Image Quality Assessment and Super-Resolution. (arXiv:2210.06618v2 [cs.CV] UPDATED)
## eess.IV
---
**24** new papers in eess.IV:-) 
1. Spline Sketches: An Efficient Approach for Photon Counting Lidar. (arXiv:2210.07314v1 [eess.IV])
2. Exploring Vanilla U-Net for Lesion Segmentation from Whole-body FDG-PET/CT Scans. (arXiv:2210.07490v1 [eess.IV])
3. Superpixel Perception Graph Neural Network for Intelligent Defect Detection. (arXiv:2210.07539v1 [cs.CV])
4. See Blue Sky: Deep Image Dehaze Using Paired and Unpaired Training Images. (arXiv:2210.07594v1 [cs.CV])
5. Self-Supervised 2D/3D Registration for X-Ray to CT Image Fusion. (arXiv:2210.07611v1 [eess.IV])
6. Towards Transformer-based Homogenization of Satellite Imagery for Landsat-8 and Sentinel-2. (arXiv:2210.07654v1 [cs.CV])
7. End-to-end joint optimization of metasurface and image processing for compact snapshot hyperspectral imaging. (arXiv:2210.07684v1 [physics.optics])
8. Motion-related Artefact Classification Using Patch-based Ensemble and Transfer Learning in Cardiac MRI. (arXiv:2210.07717v1 [eess.IV])
9. On Benefits and Challenges of Conditional Interframe Video Coding in Light of Information Theory. (arXiv:2210.07737v1 [cs.IT])
10. Blind Super-Resolution for Remote Sensing Images via Conditional Stochastic Normalizing Flows. (arXiv:2210.07751v1 [eess.IV])
11. Improved automated lesion segmentation in whole-body FDG/PET-CT via Test-Time Augmentation. (arXiv:2210.07761v1 [eess.IV])
12. An Efficient FPGA Accelerator for Point Cloud. (arXiv:2210.07803v1 [eess.SP])
13. ISTA-Inspired Network for Image Super-Resolution. (arXiv:2210.07818v1 [eess.IV])
14. Data-Limited Tissue Segmentation using Inpainting-Based Self-Supervised Learning. (arXiv:2210.07936v1 [eess.IV])
15. Wide Range MRI Artifact Removal with Transformers. (arXiv:2210.07976v1 [eess.IV])
16. Deep Hierarchical Super Resolution for Scientific Data. (arXiv:2107.00462v3 [eess.IV] UPDATED)
17. Towards Device Efficient Conditional Image Generation. (arXiv:2203.10363v2 [cs.CV] UPDATED)
18. Physics-Driven Deep Learning for Computational Magnetic Resonance Imaging. (arXiv:2203.12215v3 [eess.IV] UPDATED)
19. Review on Panoramic Imaging and Its Applications in Scene Understanding. (arXiv:2205.05570v2 [cs.CV] UPDATED)
20. E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations. (arXiv:2206.07578v2 [cs.CV] UPDATED)
21. AutoPET Challenge: Combining nn-Unet with **Swin** UNETR Augmented by Maximum Intensity Projection Classifier. (arXiv:2209.01112v2 [eess.IV] UPDATED)
22. Loop Unrolled Shallow Equilibrium Regularizer (LUSER) -- A Memory-Efficient Inverse Problem Solver. (arXiv:2210.04987v2 [eess.IV] UPDATED)
23. S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces. (arXiv:2210.06583v2 [cs.CV] UPDATED)
24. QMRNet: Quality Metric Regression for EO Image Quality Assessment and Super-Resolution. (arXiv:2210.06618v2 [cs.CV] UPDATED)
## cs.LG
---
**223** new papers in cs.LG:-) 
1. Topics in Deep Learning and Optimization Algorithms for IoT Applications in Smart Transportation. (arXiv:2210.07246v1 [cs.LG])
2. BLOX: Macro Neural Architecture Search Benchmark and Algorithms. (arXiv:2210.07271v1 [cs.LG])
3. The Hidden Uniform Cluster Prior in Self-Supervised Learning. (arXiv:2210.07277v1 [cs.LG])
4. Meta-Uncertainty in Bayesian Model Comparison. (arXiv:2210.07278v1 [stat.ML])
5. Tumor-location-guided CNNs for Pediatric Low-grade Glioma Molecular Biomarker Classification Using MRI. (arXiv:2210.07287v1 [cs.CV])
6. A Dual Control Variate for doubly stochastic optimization and black-box variational inference. (arXiv:2210.07290v1 [cs.LG])
7. Joint Reasoning on Hybrid-knowledge sources for Task-Oriented Dialog. (arXiv:2210.07295v1 [cs.CL])
8. AMP: Automatically Finding Model Parallel Strategies with Heterogeneity Awareness. (arXiv:2210.07297v1 [cs.LG])
9. Deep Reinforcement Learning-based Rebalancing Policies for Profit Maximization of Relay Nodes in Payment Channel Networks. (arXiv:2210.07302v1 [cs.DC])
10. SHINE: SubHypergraph Inductive Neural nEtwork. (arXiv:2210.07309v1 [cs.LG])
11. Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning. (arXiv:2210.07312v1 [cs.LG])
12. Bootstrapping Multilingual Semantic Parsers using Large Language Models. (arXiv:2210.07313v1 [cs.CL])
13. MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v1 [cs.CL])
14. A Large-Scale Annotated Multivariate Time Series Aviation Maintenance Dataset from the NGAFID. (arXiv:2210.07317v1 [cs.LG])
15. Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. (arXiv:2210.07321v1 [cs.CL])
16. HuBERT-TR: Reviving Turkish Automatic Speech Recognition with Self-supervised Speech Representation Learning. (arXiv:2210.07323v1 [cs.CL])
17. Secure Multiparty Computation for Synthetic Data Generation from Distributed Data. (arXiv:2210.07332v1 [cs.CR])
18. Reinforcement Learning with Unbiased Policy Evaluation and Linear Function Approximation. (arXiv:2210.07338v1 [cs.LG])
19. LEAVES: Learning Views for Time-Series Data in Contrastive Learning. (arXiv:2210.07340v1 [cs.LG])
20. Demystifying Self-supervised Trojan Attacks. (arXiv:2210.07346v1 [cs.CR])
21. Disentanglement of Correlated Factors via Hausdorff Factorized Support. (arXiv:2210.07347v1 [cs.LG])
22. Predicting Fine-Tuning Performance with Probing. (arXiv:2210.07352v1 [cs.CL])
23. Finding Islands of Predictability in Action Forecasting. (arXiv:2210.07354v1 [cs.CV])
24. Reducing Action Space: Reference-Model-Assisted Deep Reinforcement Learning for Inverter-based Volt-Var Control. (arXiv:2210.07360v1 [eess.SY])
25. ScionFL: Secure Quantized Aggregation for Federated Learning. (arXiv:2210.07376v1 [cs.CR])
26. Amortized Inference for Heterogeneous Reconstruction in Cryo-EM. (arXiv:2210.07387v1 [cs.CV])
27. Efficiently Computing Local Lipschitz Constants of Neural Networks via Bound Propagation. (arXiv:2210.07394v1 [cs.LG])
28. Estimation of the Sample Frechet Mean: A Convolutional Neural Network Approach. (arXiv:2210.07401v1 [cs.LG])
29. Invariance-adapted decomposition and Lasso-type contrastive learning. (arXiv:2210.07413v1 [stat.ML])
30. GLACIAL: Granger and Learning-based Causality Analysis for Longitudinal Studies. (arXiv:2210.07416v1 [cs.LG])
31. Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v1 [cs.RO])
32. Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v1 [cs.LG])
33. CaloDVAE : Discrete Variational Autoencoders for Fast Calorimeter Shower Simulation. (arXiv:2210.07430v1 [physics.ins-det])
34. PCFG-based Natural Language Interface Improves Generalization for Controlled Text Generation. (arXiv:2210.07431v1 [cs.CL])
35. Monte Carlo Augmented Actor-Critic for Sparse Reward Deep Reinforcement Learning from Suboptimal Demonstrations. (arXiv:2210.07432v1 [cs.LG])
36. Smart Headset, Computer Vision and Machine Learning for Efficient Prawn Farm Management. (arXiv:2210.07436v1 [cs.CV])
37. Characterizing the Influence of Graph Elements. (arXiv:2210.07441v1 [cs.LG])
38. G2A2: An Automated Graph Generator with Attributes and Anomalies. (arXiv:2210.07449v1 [cs.LG])
39. ExAug: Robot-Conditioned Navigation Policies via Geometric Experience Augmentation. (arXiv:2210.07450v1 [cs.RO])
40. Using Graph Algorithms to Pretrain Graph Completion Transformers. (arXiv:2210.07453v1 [cs.LG])
41. Communication-Efficient Adam-Type Algorithms for Distributed Data Mining. (arXiv:2210.07454v1 [cs.LG])
42. Robust Candidate Generation for Entity Linking on Short Social Media Texts. (arXiv:2210.07472v1 [cs.CL])
43. SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v1 [cs.CV])
44. Latent Temporal Flows for Multivariate Analysis of Wearables Data. (arXiv:2210.07475v1 [cs.LG])
45. Mutual Information Regularized Offline Reinforcement Learning. (arXiv:2210.07484v1 [cs.LG])
46. A Scalable Finite Difference Method for Deep Reinforcement Learning. (arXiv:2210.07487v1 [cs.LG])
47. A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking. (arXiv:2210.07494v1 [cs.LG])
48. Hierarchical Diffusion Models for Singing Voice Neural Vocoder. (arXiv:2210.07508v1 [cs.SD])
49. Continuous-in-time Limit for Bayesian Bandits. (arXiv:2210.07513v1 [math.OC])
50. Counterfactual Neural Temporal Point Process for Estimating Causal Influence of Misinformation on Social Media. (arXiv:2210.07518v1 [cs.LG])
51. Spatiotemporal Classification with limited labels using Constrained Clustering for large datasets. (arXiv:2210.07522v1 [cs.LG])
52. Provable Subspace Identification Under Post-Nonlinear Mixtures. (arXiv:2210.07532v1 [cs.LG])
53. AutoMoE: Neural Architecture Search for Efficient Sparsely Activated Transformers. (arXiv:2210.07535v1 [cs.CL])
54. A Reinforcement Learning Approach to Estimating Long-term Treatment Effects. (arXiv:2210.07536v1 [cs.LG])
55. When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture. (arXiv:2210.07540v1 [cs.CV])
56. Watermarking Pre-trained Language Models with Backdooring. (arXiv:2210.07543v1 [cs.CL])
57. Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding. (arXiv:2210.07547v1 [cs.CL])
58. Safe Model-Based Reinforcement Learning with an Uncertainty-Aware Reachability Certificate. (arXiv:2210.07553v1 [cs.RO])
59. DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation. (arXiv:2210.07558v1 [cs.CL])
60. Mix and Reason: Reasoning over Semantic Topology with Data Mixing for Domain Generalization. (arXiv:2210.07571v1 [cs.CV])
61. Model-based Safe Deep Reinforcement Learning via a Constrained Proximal Policy Optimization Algorithm. (arXiv:2210.07573v1 [cs.LG])
62. Distributed Distributionally Robust Optimization with Non-Convex Objectives. (arXiv:2210.07588v1 [cs.LG])
63. See Blue Sky: Deep Image Dehaze Using Paired and Unpaired Training Images. (arXiv:2210.07594v1 [cs.CV])
64. Revisiting Heterophily For Graph Neural Networks. (arXiv:2210.07606v1 [cs.LG])
65. Self-Supervised 2D/3D Registration for X-Ray to CT Image Fusion. (arXiv:2210.07611v1 [eess.IV])
66. Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes. (arXiv:2210.07612v1 [stat.ML])
67. FedFM: Anchor-based Feature Matching for Data Heterogeneity in Federated Learning. (arXiv:2210.07615v1 [cs.LG])
68. Quantifying Quality of Class-Conditional Generative Models in Time-Series Domain. (arXiv:2210.07617v1 [cs.LG])
69. The Invariant Ground Truth of Affect. (arXiv:2210.07630v1 [cs.AI])
70. Pareto-aware Neural Architecture Generation for Diverse Computational Budgets. (arXiv:2210.07634v1 [cs.LG])
71. Distributional Reward Estimation for Effective Multi-Agent Deep Reinforcement Learning. (arXiv:2210.07636v1 [cs.LG])
72. Training speech emotion classifier without categorical annotations. (arXiv:2210.07642v1 [cs.SD])
73. Vision Transformer Visualization: What Neurons Tell and How Neurons Behave?. (arXiv:2210.07646v1 [cs.CV])
74. Machine Learning in Transaction Monitoring: The Prospect of xAI. (arXiv:2210.07648v1 [cs.HC])
75. Towards Transformer-based Homogenization of Satellite Imagery for Landsat-8 and Sentinel-2. (arXiv:2210.07654v1 [cs.CV])
76. Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization. (arXiv:2210.07658v1 [cs.LG])
77. Automated dysgraphia detection by deep learning with SensoGrip. (arXiv:2210.07659v1 [cs.LG])
78. CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling. (arXiv:2210.07661v1 [cs.LG])
79. Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v1 [cs.CV])
80. Learning Generalizable Models for Vehicle Routing Problems via Knowledge Distillation. (arXiv:2210.07686v1 [cs.LG])
81. Accelerating RNN-based Speech **Enhancement** on a Multi-Core MCU with Mixed FP16-INT8 Post-Training Quantization. (arXiv:2210.07692v1 [cs.SD])
82. Theory and Approximate Solvers for Branched Optimal Transport with Multiple Sources. (arXiv:2210.07702v1 [cs.LG])
83. Hybrid Decentralized Optimization: First- and Zeroth-Order Optimizers Can Be Jointly Leveraged For Faster Convergence. (arXiv:2210.07703v1 [cs.LG])
84. An Empirical Evaluation of Multivariate Time Series Classification with Input Transformation across Different Dimensions. (arXiv:2210.07713v1 [cs.LG])
85. Close the Gate: Detecting Backdoored Models in Federated Learning based on Client-Side Deep Layer Output Analysis. (arXiv:2210.07714v1 [cs.CR])
86. Not All Neighbors Are Worth Attending to: Graph Selective Attention Networks for Semi-supervised Learning. (arXiv:2210.07715v1 [cs.LG])
87. A Lightweight Moving Target Defense Framework for Multi-purpose Malware Affecting IoT Devices. (arXiv:2210.07719v1 [cs.CR])
88. (1,1)-Cluster Editing is Polynomial-time Solvable. (arXiv:2210.07722v1 [cs.DS])
89. Privacy-Preserving and Lossless Distributed Estimation of High-Dimensional Generalized Additive Mixed Models. (arXiv:2210.07723v1 [stat.ML])
90. Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning. (arXiv:2210.07733v1 [cs.CL])
91. Bandwidth-efficient distributed neural network architectures with application to body sensor networks. (arXiv:2210.07750v1 [cs.LG])
92. Simpson's Paradox in Recommender Fairness: Reconciling differences between per-user and aggregated evaluations. (arXiv:2210.07755v1 [cs.IR])
93. Improved automated lesion segmentation in whole-body FDG/PET-CT via Test-Time Augmentation. (arXiv:2210.07761v1 [eess.IV])
94. HGARN: Hierarchical Graph Attention Recurrent Network for Human Mobility Prediction. (arXiv:2210.07765v1 [cs.LG])
95. FeatureBox: Feature Engineering on GPUs for Massive-Scale Ads Systems. (arXiv:2210.07768v1 [cs.IR])
96. Flattened Graph Convolutional Networks For Recommendation. (arXiv:2210.07769v1 [cs.IR])
97. Diversified Recommendations for Agents with Adaptive Preferences. (arXiv:2210.07773v1 [cs.IR])
98. Learning To Rank Diversely. (arXiv:2210.07774v1 [cs.IR])
99. PrivMVMF: Privacy-Preserving Multi-View Matrix Factorization for Recommender Systems. (arXiv:2210.07775v1 [cs.IR])
100. LEATHER: A Framework for Learning to Generate Human-like Text in Dialogue. (arXiv:2210.07777v1 [cs.CL])
101. Federated Best Arm Identification with Heterogeneous Clients. (arXiv:2210.07780v1 [cs.LG])
102. Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue. (arXiv:2210.07783v1 [cs.CL])
103. Object-Category Aware Reinforcement Learning. (arXiv:2210.07802v1 [cs.LG])
104. Meta-Query-Net: Resolving Purity-Informativeness Dilemma in Open-set Active Learning. (arXiv:2210.07805v1 [cs.LG])
105. Optimal AdaBoost Converges. (arXiv:2210.07808v1 [stat.ML])
106. A Sequence-Aware Recommendation Method Based on Complex Networks. (arXiv:2210.07814v1 [cs.IR])
107. Intra-session Context-aware Feed Recommendation in Live Systems. (arXiv:2210.07815v1 [cs.IR])
108. A Recommendation Approach based on Similarity-Popularity Models of Complex Networks. (arXiv:2210.07816v1 [cs.IR])
109. Asymmetric Student-Teacher Networks for Industrial Anomaly Detection. (arXiv:2210.07829v1 [cs.LG])
110. Commutativity and Disentanglement from the Manifold Perspective. (arXiv:2210.07857v1 [stat.ML])
111. Revisiting Optimal Convergence Rate for Smooth and Non-convex Stochastic Decentralized Optimization. (arXiv:2210.07863v1 [cs.LG])
112. Tunable Complexity Benchmarks for Evaluating Physics-Informed Neural Networks on Coupled Ordinary Differential Equations. (arXiv:2210.07880v1 [stat.ML])
113. Communication-Efficient Topologies for Decentralized Learning with $O(1)$ Consensus Rate. (arXiv:2210.07881v1 [math.OC])
114. E2R: a Hierarchical-Learning inspired Novelty-Search method to generate diverse repertoires of grasping trajectories. (arXiv:2210.07887v1 [cs.RO])
115. Hierarchical Policy Blending as Inference for Reactive Robot Control. (arXiv:2210.07890v1 [cs.RO])
116. Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v1 [stat.ML])
117. Post-Training Quantization for Energy Efficient Realization of Deep Neural Networks. (arXiv:2210.07906v1 [cs.LG])
118. Efficiently Controlling Multiple Risks with Pareto Testing. (arXiv:2210.07913v1 [cs.LG])
119. Optimal Auctions through Deep Learning: Advances in Differentiable Economics. (arXiv:1706.03459v6 [cs.GT] UPDATED)
120. Sarcasm Detection using Hybrid Neural Network. (arXiv:1908.07414v2 [cs.LG] UPDATED)
121. Nonasymptotic estimates for Stochastic Gradient Langevin Dynamics under local conditions in nonconvex optimization. (arXiv:1910.02008v5 [math.ST] UPDATED)
122. Cumulo: A Dataset for Learning Cloud Classes. (arXiv:1911.04227v3 [physics.ao-ph] UPDATED)
123. Semiparametric Inference For Causal Effects In Graphical Models With Hidden Variables. (arXiv:2003.12659v3 [stat.ML] UPDATED)
124. Self-Adaptive Training: Bridging Supervised and Self-Supervised Learning. (arXiv:2101.08732v3 [cs.LG] UPDATED)
125. Deep Hierarchical Super Resolution for Scientific Data. (arXiv:2107.00462v3 [eess.IV] UPDATED)
126. NTS-NOTEARS: Learning Nonparametric DBNs With Prior Knowledge. (arXiv:2109.04286v2 [cs.LG] UPDATED)
127. Generalized Anomaly Detection. (arXiv:2110.15108v2 [cs.LG] UPDATED)
128. A Concentration Bound for LSPE($\lambda$). (arXiv:2111.02644v4 [cs.LG] UPDATED)
129. Consistent Sufficient Explanations and Minimal Local Rules for explaining regression and classification models. (arXiv:2111.04658v2 [stat.ML] UPDATED)
130. Reliability Assessment and Safety Arguments for Machine Learning Components in System Assurance. (arXiv:2112.00646v2 [cs.SE] UPDATED)
131. An Interpretive Constrained Linear Model for ResNet and MgNet. (arXiv:2112.07441v2 [cs.CV] UPDATED)
132. Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations. (arXiv:2112.08125v2 [math.NA] UPDATED)
133. The cluster structure function. (arXiv:2201.01222v3 [cs.LG] UPDATED)
134. FedDTG:Federated Data-Free Knowledge Distillation via Three-Player Generative Adversarial Networks. (arXiv:2201.03169v3 [cs.LG] UPDATED)
135. When is Offline Two-Player Zero-Sum Markov Game Solvable?. (arXiv:2201.03522v2 [cs.LG] UPDATED)
136. Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering. (arXiv:2201.05077v4 [cs.SE] UPDATED)
137. Interpretable and Effective Reinforcement Learning for Attacking against Graph-based Rumor Detection. (arXiv:2201.05819v2 [cs.LG] UPDATED)
138. GRPE: Relative Positional Encoding for Graph Transformer. (arXiv:2201.12787v3 [cs.LG] UPDATED)
139. Similarity and Generalization: From Noise to Corruption. (arXiv:2201.12803v2 [cs.LG] UPDATED)
140. BEER: Fast $O(1/T)$ Rate for Decentralized Nonconvex Optimization with Communication Compression. (arXiv:2201.13320v3 [cs.LG] UPDATED)
141. Learning entanglement breakdown as a phase transition by confusion. (arXiv:2202.00348v3 [quant-ph] UPDATED)
142. Covariate-informed Representation Learning to Prevent Posterior Collapse of iVAE. (arXiv:2202.04206v3 [stat.ML] UPDATED)
143. Finding Optimal Arms in Non-stochastic Combinatorial Bandits with Semi-bandit Feedback and Finite Budget. (arXiv:2202.04487v2 [cs.LG] UPDATED)
144. Augmenting Neural Networks with Priors on Function Values. (arXiv:2202.04798v4 [cs.LG] UPDATED)
145. Mixture-of-Experts with Expert Choice Routing. (arXiv:2202.09368v2 [cs.LG] UPDATED)
146. COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v3 [cs.CL] UPDATED)
147. Spatial-Temporal Attention Fusion Network for short-term passenger flow prediction on holidays in urban rail transit systems. (arXiv:2203.00007v3 [cs.LG] UPDATED)
148. Discrete Optimal Transport with Independent Marginals is #P-Hard. (arXiv:2203.01161v2 [math.OC] UPDATED)
149. Quantity over Quality: Training an AV Motion Planner with Large Scale Commodity Vision Data. (arXiv:2203.01681v2 [cs.RO] UPDATED)
150. projUNN: efficient method for training deep networks with unitary matrices. (arXiv:2203.05483v3 [cs.LG] UPDATED)
151. Physics-Driven Deep Learning for Computational Magnetic Resonance Imaging. (arXiv:2203.12215v3 [eess.IV] UPDATED)
152. Unconditional Image-Text Pair Generation with Multimodal Cross Quantizer. (arXiv:2204.07537v2 [cs.CV] UPDATED)
153. Posterior Collapse of a Linear Latent Variable Model. (arXiv:2205.04009v2 [cs.LG] UPDATED)
154. Scalable Stochastic Parametric Verification with Stochastic Variational Smoothed Model Checking. (arXiv:2205.05398v2 [cs.LG] UPDATED)
155. Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel. (arXiv:2205.07384v4 [cs.LG] UPDATED)
156. Ergodic variational flows. (arXiv:2205.07475v2 [stat.ML] UPDATED)
157. DouFu: A Double Fusion Joint Learning Method For Driving Trajectory Representation. (arXiv:2205.08356v2 [cs.LG] UPDATED)
158. ODBO: Bayesian Optimization with Search Space Prescreening for Directed Protein Evolution. (arXiv:2205.09548v3 [q-bio.BM] UPDATED)
159. Deep Learning Methods for Proximal Inference via Maximum Moment Restriction. (arXiv:2205.09824v3 [stat.ML] UPDATED)
160. Towards Understanding Grokking: An Effective Theory of Representation Learning. (arXiv:2205.10343v2 [cs.LG] UPDATED)
161. CELEST: Federated Learning for Globally Coordinated Threat Detection. (arXiv:2205.11459v2 [cs.CR] UPDATED)
162. Recognition Models to Learn Dynamics from Partial Observations with Neural ODEs. (arXiv:2205.12550v2 [eess.SY] UPDATED)
163. Improving Subgraph Representation Learning via Multi-View Augmentation. (arXiv:2205.13038v3 [cs.LG] UPDATED)
164. Towards Learning Universal Hyperparameter Optimizers with Transformers. (arXiv:2205.13320v2 [cs.LG] UPDATED)
165. Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v2 [cs.CV] UPDATED)
166. Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power. (arXiv:2205.13863v3 [cs.LG] UPDATED)
167. A Continuous Time Framework for Discrete Denoising Models. (arXiv:2205.14987v2 [stat.ML] UPDATED)
168. Towards Efficient 3D Object Detection with Knowledge Distillation. (arXiv:2205.15156v3 [cs.CV] UPDATED)
169. Comparing interpretation methods in mental state decoding analyses with deep learning models. (arXiv:2205.15581v2 [q-bio.NC] UPDATED)
170. Feature Learning in $L_{2}$-regularized DNNs: Attraction/Repulsion and Sparsity. (arXiv:2205.15809v2 [stat.ML] UPDATED)
171. Provably Efficient Offline Multi-agent Reinforcement Learning via Strategy-wise Bonus. (arXiv:2206.00159v2 [cs.LG] UPDATED)
172. Graph Machine Learning for Design of High-Octane Fuels. (arXiv:2206.00619v2 [cs.LG] UPDATED)
173. Nest Your Adaptive Algorithm for Parameter-Agnostic Nonconvex Minimax Optimization. (arXiv:2206.00743v2 [math.OC] UPDATED)
174. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. (arXiv:2206.00927v3 [cs.LG] UPDATED)
175. Indirect Active Learning. (arXiv:2206.01454v2 [math.ST] UPDATED)
176. Neural Differential Equations for Learning to Program Neural Nets Through Continuous Learning Rules. (arXiv:2206.01649v2 [cs.LG] UPDATED)
177. Learning in Congestion Games with Bandit Feedback. (arXiv:2206.01880v2 [cs.GT] UPDATED)
178. Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering. (arXiv:2206.02721v2 [cs.CV] UPDATED)
179. Autoregressive Perturbations for Data Poisoning. (arXiv:2206.03693v3 [cs.LG] UPDATED)
180. Gradient Obfuscation Gives a False Sense of Security in Federated Learning. (arXiv:2206.04055v2 [cs.CR] UPDATED)
181. Markov Chain Score Ascent: A Unifying Framework of Variational Inference with Markovian Gradients. (arXiv:2206.06295v4 [cs.LG] UPDATED)
182. E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations. (arXiv:2206.07578v2 [cs.CV] UPDATED)
183. Double Sampling Randomized Smoothing. (arXiv:2206.07912v3 [cs.LG] UPDATED)
184. MAGIC: Microlensing Analysis Guided by Intelligent Computation. (arXiv:2206.08199v2 [astro-ph.IM] UPDATED)
185. Beyond Real-world Benchmark Datasets: An Empirical Study of Node Classification with GNNs. (arXiv:2206.09144v5 [cs.LG] UPDATED)
186. Beyond IID: data-driven decision-making in heterogeneous environments. (arXiv:2206.09642v2 [cs.LG] UPDATED)
187. Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries. (arXiv:2206.11140v3 [cs.LG] UPDATED)
188. Spherical Channels for Modeling Atomic Interactions. (arXiv:2206.14331v2 [physics.chem-ph] UPDATED)
189. PAC Prediction Sets for Meta-Learning. (arXiv:2207.02440v2 [cs.LG] UPDATED)
190. Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v2 [stat.ML] UPDATED)
191. Probable Domain Generalization via Quantile Risk Minimization. (arXiv:2207.09944v2 [stat.ML] UPDATED)
192. PAN: Pulse Ansatz on NISQ Machines. (arXiv:2208.01215v2 [quant-ph] UPDATED)
193. On the non-universality of deep learning: quantifying the cost of symmetry. (arXiv:2208.03113v2 [cs.LG] UPDATED)
194. Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP. (arXiv:2208.05516v2 [cs.LG] UPDATED)
195. USB: A Unified Semi-supervised Learning Benchmark for Classification. (arXiv:2208.07204v2 [cs.LG] UPDATED)
196. Geometric Scattering on Measure Spaces. (arXiv:2208.08561v2 [stat.ML] UPDATED)
197. Interpretable (not just posthoc-explainable) medical claims modeling for discharge placement to prevent avoidable all-cause readmissions or death. (arXiv:2208.12814v2 [cs.CY] UPDATED)
198. B\'ezier Gaussian Processes for Tall and Wide Data. (arXiv:2209.00343v2 [stat.ML] UPDATED)
199. MaxWeight With Discounted UCB: A Provably Stable Scheduling Policy for Nonstationary Multi-Server Systems With Unknown Statistics. (arXiv:2209.01126v2 [cs.LG] UPDATED)
200. Generalization Properties of NAS under Activation and Skip Connection Search. (arXiv:2209.07238v2 [cs.LG] UPDATED)
201. Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization). (arXiv:2209.07263v2 [cs.LG] UPDATED)
202. Optimal Scaling for Locally Balanced Proposals in Discrete Spaces. (arXiv:2209.08183v2 [cs.LG] UPDATED)
203. Walk-and-Relate: A Random-Walk-based Algorithm for Representation Learning on Sparse Knowledge Graphs. (arXiv:2209.08769v2 [cs.LG] UPDATED)
204. EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v2 [cs.CV] UPDATED)
205. SOCRATES: A Stereo Camera Trap for Monitoring of Biodiversity. (arXiv:2209.09070v2 [cs.CV] UPDATED)
206. Poisson Flow Generative Models. (arXiv:2209.11178v3 [cs.LG] UPDATED)
207. Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models. (arXiv:2209.13325v2 [cs.LG] UPDATED)
208. Momentum Gradient Descent Federated Learning with Local Differential Privacy. (arXiv:2209.14086v2 [cs.LG] UPDATED)
209. Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v3 [stat.ML] UPDATED)
210. AI-Assisted Discovery of Quantitative and Formal Models in Social Science. (arXiv:2210.00563v2 [cs.SC] UPDATED)
211. Unsupervised Model Selection for Time-series Anomaly Detection. (arXiv:2210.01078v2 [cs.LG] UPDATED)
212. Conformalized Fairness via Quantile Regression. (arXiv:2210.02015v2 [stat.ML] UPDATED)
213. Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis. (arXiv:2210.04714v2 [cs.CL] UPDATED)
214. Block Format Error Bounds and Optimal Block Size Selection. (arXiv:2210.05470v2 [cs.LG] UPDATED)
215. Frequency-Aware Self-Supervised Monocular Depth Estimation. (arXiv:2210.05479v2 [cs.CV] UPDATED)
216. Schedule-Robust Online Continual Learning. (arXiv:2210.05561v2 [cs.LG] UPDATED)
217. An Experimental Study on Private Aggregation of Teacher Ensemble Learning for End-to-End Speech Recognition. (arXiv:2210.05614v2 [cs.SD] UPDATED)
218. Finding and Listing Front-door Adjustment Sets. (arXiv:2210.05816v2 [stat.ME] UPDATED)
219. S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces. (arXiv:2210.06583v2 [cs.CV] UPDATED)
220. BLADERUNNER: Rapid Countermeasure for Synthetic (AI-Generated) StyleGAN Faces. (arXiv:2210.06587v2 [cs.CR] UPDATED)
221. Efficient circuit implementation for coined quantum walks on binary trees and application to reinforcement learning. (arXiv:2210.06784v2 [cs.ET] UPDATED)
222. CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing. (arXiv:2210.07074v2 [cs.CL] UPDATED)
223. Global Explainability of GNNs via Logic Combination of Learned Concepts. (arXiv:2210.07147v2 [cs.LG] UPDATED)
## cs.AI
---
**114** new papers in cs.AI:-) 
1. The Hidden Uniform Cluster Prior in Self-Supervised Learning. (arXiv:2210.07277v1 [cs.LG])
2. Harfang3D Dog-Fight Sandbox: A Reinforcement Learning Research Platform for the Customized Control Tasks of Fighter Aircrafts. (arXiv:2210.07282v1 [cs.RO])
3. SHINE: SubHypergraph Inductive Neural nEtwork. (arXiv:2210.07309v1 [cs.LG])
4. Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning. (arXiv:2210.07312v1 [cs.LG])
5. Machine Learning vs. Deep Learning in 5G Networks -- A Comparison of Scientific Impact. (arXiv:2210.07327v1 [cs.DL])
6. Sample Efficient Dynamics Learning for Symmetrical Legged Robots:Leveraging Physics Invariance and Geometric Symmetries. (arXiv:2210.07329v1 [cs.RO])
7. FOON Creation and Traversal for Recipe Generation. (arXiv:2210.07335v1 [cs.RO])
8. LEAVES: Learning Views for Time-Series Data in Contrastive Learning. (arXiv:2210.07340v1 [cs.LG])
9. Scientific Impact of Graph-Based Approaches in Deep Learning Studies -- A Bibliometric Comparison. (arXiv:2210.07343v1 [cs.DL])
10. Predicting Fine-Tuning Performance with Probing. (arXiv:2210.07352v1 [cs.CL])
11. Reducing Action Space: Reference-Model-Assisted Deep Reinforcement Learning for Inverter-based Volt-Var Control. (arXiv:2210.07360v1 [eess.SY])
12. A Relational Macrostate Theory Guides Artificial Intelligence to Learn Macro and Design Micro. (arXiv:2210.07374v1 [cs.AI])
13. Behavior Cloned Transformers are Neurosymbolic Reasoners. (arXiv:2210.07382v1 [cs.CL])
14. A Concise Introduction to Reinforcement Learning in Robotics. (arXiv:2210.07397v1 [cs.RO])
15. **Real-time** Action Recognition for Fine-Grained Actions and The Hand Wash Dataset. (arXiv:2210.07400v1 [cs.CV])
16. Quantification of entanglement with Siamese convolutional neural networks. (arXiv:2210.07410v1 [quant-ph])
17. Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v1 [cs.RO])
18. Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v1 [cs.LG])
19. Monte Carlo Augmented Actor-Critic for Sparse Reward Deep Reinforcement Learning from Suboptimal Demonstrations. (arXiv:2210.07432v1 [cs.LG])
20. Risk-Awareness in Learning Neural Controllers for Temporal Logic Objectives. (arXiv:2210.07439v1 [eess.SY])
21. ExAug: Robot-Conditioned Navigation Policies via Geometric Experience Augmentation. (arXiv:2210.07450v1 [cs.RO])
22. Robust Candidate Generation for Entity Linking on Short Social Media Texts. (arXiv:2210.07472v1 [cs.CL])
23. SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v1 [cs.CV])
24. Mutual Information Regularized Offline Reinforcement Learning. (arXiv:2210.07484v1 [cs.LG])
25. Holistic Sentence Embeddings for Better Out-of-Distribution Detection. (arXiv:2210.07485v1 [cs.CL])
26. A Scalable Finite Difference Method for Deep Reinforcement Learning. (arXiv:2210.07487v1 [cs.LG])
27. Psychology-guided Controllable Story Generation. (arXiv:2210.07493v1 [cs.CL])
28. A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking. (arXiv:2210.07494v1 [cs.LG])
29. STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition. (arXiv:2210.07503v1 [cs.CV])
30. Counterfactual Neural Temporal Point Process for Estimating Causal Influence of Misinformation on Social Media. (arXiv:2210.07518v1 [cs.LG])
31. Can Language Representation Models Think in Bets?. (arXiv:2210.07519v1 [cs.CL])
32. Distributed Distributionally Robust Optimization with Non-Convex Objectives. (arXiv:2210.07588v1 [cs.LG])
33. BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation. (arXiv:2210.07626v1 [cs.CL])
34. The Invariant Ground Truth of Affect. (arXiv:2210.07630v1 [cs.AI])
35. DART: Articulated Hand Model with Diverse Accessories and Rich Textures. (arXiv:2210.07650v1 [cs.CV])
36. Decentralized Policy Gradient for Nash Equilibria Learning of General-sum Stochastic Games. (arXiv:2210.07651v1 [eess.SY])
37. Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values. (arXiv:2210.07652v1 [cs.CL])
38. Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v1 [cs.CV])
39. TransFusion: Transcribing Speech with Multinomial Diffusion. (arXiv:2210.07677v1 [eess.AS])
40. Learning Generalizable Models for Vehicle Routing Problems via Knowledge Distillation. (arXiv:2210.07686v1 [cs.LG])
41. Multi-Task Learning based Video Anomaly Detection with Attention. (arXiv:2210.07697v1 [cs.CV])
42. Generative Adversarial Learning for Trusted and Secure Clustering in Industrial Wireless Sensor Networks. (arXiv:2210.07707v1 [cs.NI])
43. Model-Based Imitation Learning for Urban Driving. (arXiv:2210.07729v1 [cs.CV])
44. Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning. (arXiv:2210.07733v1 [cs.CL])
45. Simpson's Paradox in Recommender Fairness: Reconciling differences between per-user and aggregated evaluations. (arXiv:2210.07755v1 [cs.IR])
46. Extracting Cultural Commonsense Knowledge at Scale. (arXiv:2210.07763v1 [cs.CL])
47. Towards Trustworthy AI-Empowered Real-Time Bidding for Online Advertisement Auctioning. (arXiv:2210.07770v1 [cs.IR])
48. Learning To Rank Diversely. (arXiv:2210.07774v1 [cs.IR])
49. PrivMVMF: Privacy-Preserving Multi-View Matrix Factorization for Recommender Systems. (arXiv:2210.07775v1 [cs.IR])
50. Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue. (arXiv:2210.07783v1 [cs.CL])
51. Object-Category Aware Reinforcement Learning. (arXiv:2210.07802v1 [cs.LG])
52. Comparison of different automatic solutions for resection cavity segmentation in postoperative MRI volumes including longitudinal acquisitions. (arXiv:2210.07806v1 [cs.CV])
53. Discussion about Attacks and Defenses for Fair and Robust Recommendation System Design. (arXiv:2210.07817v1 [cs.IR])
54. Shadfa 0.1: The Iranian Movie Knowledge Graph and Graph-Embedding-Based Recommender System. (arXiv:2210.07822v1 [cs.IR])
55. Asymmetric Student-Teacher Networks for Industrial Anomaly Detection. (arXiv:2210.07829v1 [cs.LG])
56. Learning to Autonomously Reach Objects with NICO and Grow-When-Required Networks. (arXiv:2210.07851v1 [cs.RO])
57. Unsupervised Dense Nuclei Detection and Segmentation with Prior Self-activation Map For Histology Images. (arXiv:2210.07862v1 [cs.CV])
58. One Graph to Rule them All: Using NLP and Graph Neural Networks to analyse Tolkien's Legendarium. (arXiv:2210.07871v1 [cs.CL])
59. Post-Training Quantization for Energy Efficient Realization of Deep Neural Networks. (arXiv:2210.07906v1 [cs.LG])
60. Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks. (arXiv:2210.07907v1 [cs.CL])
61. Efficiently Controlling Multiple Risks with Pareto Testing. (arXiv:2210.07913v1 [cs.LG])
62. Optimal Auctions through Deep Learning: Advances in Differentiable Economics. (arXiv:1706.03459v6 [cs.GT] UPDATED)
63. NTS-NOTEARS: Learning Nonparametric DBNs With Prior Knowledge. (arXiv:2109.04286v2 [cs.LG] UPDATED)
64. Generalized Anomaly Detection. (arXiv:2110.15108v2 [cs.LG] UPDATED)
65. Template Filling for Controllable Commonsense Reasoning. (arXiv:2111.00539v3 [cs.CL] UPDATED)
66. Reliability Assessment and Safety Arguments for Machine Learning Components in System Assurance. (arXiv:2112.00646v2 [cs.SE] UPDATED)
67. Dynamic Token Normalization Improves Vision Transformers. (arXiv:2112.02624v2 [cs.CV] UPDATED)
68. An Interpretive Constrained Linear Model for ResNet and MgNet. (arXiv:2112.07441v2 [cs.CV] UPDATED)
69. Self-Supervised Robustifying Guidance for Monocular 3D Face Reconstruction. (arXiv:2112.14382v2 [cs.CV] UPDATED)
70. When is Offline Two-Player Zero-Sum Markov Game Solvable?. (arXiv:2201.03522v2 [cs.LG] UPDATED)
71. GRPE: Relative Positional Encoding for Graph Transformer. (arXiv:2201.12787v3 [cs.LG] UPDATED)
72. Similarity and Generalization: From Noise to Corruption. (arXiv:2201.12803v2 [cs.LG] UPDATED)
73. Covariate-informed Representation Learning to Prevent Posterior Collapse of iVAE. (arXiv:2202.04206v3 [stat.ML] UPDATED)
74. Mixture-of-Experts with Expert Choice Routing. (arXiv:2202.09368v2 [cs.LG] UPDATED)
75. COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v3 [cs.CL] UPDATED)
76. projUNN: efficient method for training deep networks with unitary matrices. (arXiv:2203.05483v3 [cs.LG] UPDATED)
77. Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v3 [cs.CL] UPDATED)
78. Fictitious Play with Maximin Initialization. (arXiv:2203.10774v4 [cs.GT] UPDATED)
79. Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel. (arXiv:2205.07384v4 [cs.LG] UPDATED)
80. DouFu: A Double Fusion Joint Learning Method For Driving Trajectory Representation. (arXiv:2205.08356v2 [cs.LG] UPDATED)
81. Towards Understanding Grokking: An Effective Theory of Representation Learning. (arXiv:2205.10343v2 [cs.LG] UPDATED)
82. BBTv2: Towards a Gradient-Free Future with Large Language Models. (arXiv:2205.11200v2 [cs.CL] UPDATED)
83. Unsupervised Tokenization Learning. (arXiv:2205.11443v3 [cs.CL] UPDATED)
84. Improving Subgraph Representation Learning via Multi-View Augmentation. (arXiv:2205.13038v3 [cs.LG] UPDATED)
85. Towards Learning Universal Hyperparameter Optimizers with Transformers. (arXiv:2205.13320v2 [cs.LG] UPDATED)
86. Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power. (arXiv:2205.13863v3 [cs.LG] UPDATED)
87. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training. (arXiv:2205.14401v2 [cs.CV] UPDATED)
88. Towards Efficient 3D Object Detection with Knowledge Distillation. (arXiv:2205.15156v3 [cs.CV] UPDATED)
89. Feature Learning in $L_{2}$-regularized DNNs: Attraction/Repulsion and Sparsity. (arXiv:2205.15809v2 [stat.ML] UPDATED)
90. Provably Efficient Offline Multi-agent Reinforcement Learning via Strategy-wise Bonus. (arXiv:2206.00159v2 [cs.LG] UPDATED)
91. Learning Distributed and Fair Policies for Network Load Balancing as Markov Potential Game. (arXiv:2206.01451v3 [cs.AI] UPDATED)
92. Gradient Obfuscation Gives a False Sense of Security in Federated Learning. (arXiv:2206.04055v2 [cs.CR] UPDATED)
93. Markov Chain Score Ascent: A Unifying Framework of Variational Inference with Markovian Gradients. (arXiv:2206.06295v4 [cs.LG] UPDATED)
94. Equivariant Descriptor Fields: SE(3)-Equivariant Energy-Based Models for End-to-End Visual Robotic Manipulation Learning. (arXiv:2206.08321v2 [cs.RO] UPDATED)
95. How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v3 [cs.CV] UPDATED)
96. Probable Domain Generalization via Quantile Risk Minimization. (arXiv:2207.09944v2 [stat.ML] UPDATED)
97. USB: A Unified Semi-supervised Learning Benchmark for Classification. (arXiv:2208.07204v2 [cs.LG] UPDATED)
98. Interpretable (not just posthoc-explainable) medical claims modeling for discharge placement to prevent avoidable all-cause readmissions or death. (arXiv:2208.12814v2 [cs.CY] UPDATED)
99. Generalized One-shot Domain Adaptation of Generative Adversarial Networks. (arXiv:2209.03665v2 [cs.CV] UPDATED)
100. Generalization Properties of NAS under Activation and Skip Connection Search. (arXiv:2209.07238v2 [cs.LG] UPDATED)
101. Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization). (arXiv:2209.07263v2 [cs.LG] UPDATED)
102. Walk-and-Relate: A Random-Walk-based Algorithm for Representation Learning on Sparse Knowledge Graphs. (arXiv:2209.08769v2 [cs.LG] UPDATED)
103. Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v3 [stat.ML] UPDATED)
104. Physical computation and compositionality. (arXiv:2210.00392v2 [quant-ph] UPDATED)
105. COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models. (arXiv:2210.01963v3 [cs.CL] UPDATED)
106. Dual-Domain Cross-Iteration Squeeze-Excitation Network for Sparse Reconstruction of Brain MRI. (arXiv:2210.02523v2 [cs.CV] UPDATED)
107. Pre-trained Adversarial Perturbations. (arXiv:2210.03372v2 [cs.CV] UPDATED)
108. Frequency-Aware Self-Supervised Monocular Depth Estimation. (arXiv:2210.05479v2 [cs.CV] UPDATED)
109. Finding and Listing Front-door Adjustment Sets. (arXiv:2210.05816v2 [stat.ME] UPDATED)
110. ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding. (arXiv:2210.06155v2 [cs.CL] UPDATED)
111. Automatic **Real-time** Vehicle Classification by Image Colour Component Based Template Matching. (arXiv:2210.06586v2 [cs.CV] UPDATED)
112. QMRNet: Quality Metric Regression for EO Image Quality Assessment and Super-Resolution. (arXiv:2210.06618v2 [cs.CV] UPDATED)
113. CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing. (arXiv:2210.07074v2 [cs.CL] UPDATED)
114. Global Explainability of GNNs via Logic Combination of Learned Concepts. (arXiv:2210.07147v2 [cs.LG] UPDATED)

