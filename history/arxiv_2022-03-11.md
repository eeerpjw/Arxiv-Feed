# Your interest papers
---
## cs.CV
---
### Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v1 [eess.IV])
- Authors : Dezhao Wang, **Wenhan Yang**, Yueyu Hu, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2203.04963](http://arxiv.org/abs/2203.04963)
> ABSTRACT  :  Learned image compression has achieved great success due to its excellent modeling capacity, but seldom further considers the Rate-Distortion Optimization (RDO) of each input image. To explore this potential in the learned codec, we make the first attempt to build a neural data-dependent transform and introduce a continuous online mode decision mechanism to jointly optimize the coding efficiency for each individual image. Specifically, apart from the image content stream, we employ an additional model stream to generate the transform parameters at the decoder side. The presence of a model stream enables our model to learn more abstract neural-syntax, which helps cluster the latent representations of images more compactly. Beyond the transform stage, we also adopt neural-syntax based post-processing for the scenarios that require higher quality reconstructions regardless of extra decoding overhead. Moreover, the involvement of the model stream further makes it possible to optimize both the representation and the decoder in an online way, i.e. RDO at the testing time. It is equivalent to a continuous online mode decision, like coding modes in the traditional codecs, to improve the coding efficiency based on the individual input image. The experimental results show the effectiveness of the proposed neural-syntax design and the continuous online mode decision mechanism, demonstrating the superiority of our method in coding efficiency compared to the latest conventional standard Versatile Video Coding (VVC) and other state-of-the-art learning-based methods.  
### Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v1 [cs.CV])
- Authors : Xiuwei Xu, Yifan Wang, Yu Zheng, Yongming Rao, Jiwen Lu, Jie Zhou
- Link : [http://arxiv.org/abs/2203.05238](http://arxiv.org/abs/2203.05238)
> ABSTRACT  :  In this paper, we propose a weakly-supervised approach for 3D object detection, which makes it possible to train strong 3D detector with position-level annotations (i.e. annotations of object centers). In order to remedy the information loss from box annotations to centers, our method, namely Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak labels into fully-annotated virtual scenes as stronger supervision, and in turn utilizes the perfect virtual labels to complement and refine the real labels. Specifically, we first assemble 3D shapes into physically reasonable virtual scenes according to the coarse scene layout extracted from position-level annotations. Then we go back to reality by applying a virtual-to-real domain adaptation method, which refine the weak labels and additionally supervise the training of detector with the virtual scenes. Furthermore, we propose a more challenging benckmark for indoor 3D object detection with more diversity in object sizes to better show the potential of BR. With less than 5% of the labeling labor, we achieve comparable detection performance with some popular fully-supervised approaches on the widely used ScanNet dataset. Code is available at: https://github.com/xuxw98/BackToReality  
### **Real-time** Scene Text Detection Based on Global Level and Word Level Features. (arXiv:2203.05251v1 [cs.CV])
- Authors : Fuqiang Zhao, Jionghua Yu, Enjun Xing, Wenming Song, Xue Xu
- Link : [http://arxiv.org/abs/2203.05251](http://arxiv.org/abs/2203.05251)
> ABSTRACT  :  It is an extremely challenging task to detect arbitrary shape text in natural scenes on high accuracy and efficiency. In this paper, we propose a scene text detection framework, namely GWNet, which mainly includes two modules: Global module and RCNN module. Specifically, Global module improves the adaptive performance of the DB (Differentiable Binarization) module by adding k submodule and shift submodule. Two submodules enhance the adaptability of amplifying factor k, accelerate the convergence of models and help to produce more accurate detection results. RCNN module fuses global-level and word-level features. The word-level label is generated by obtaining the minimum axis-aligned rectangle boxes of the shrunk polygon. In the inference period, GWNet only uses global-level features to output simple polygon detections. Experiments on four benchmark datasets, including the MSRA-TD500, Total-Text, ICDAR2015 and CTW-1500, demonstrate that our GWNet outperforms the state-of-the-art detectors. Specifically, with a backbone of ResNet-50, we achieve an F-measure of 88.6% on MSRA- TD500, 87.9% on Total-Text, 89.2% on ICDAR2015 and 87.5% on CTW-1500.  
### Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework. (arXiv:2202.06767v2 [cs.CV] UPDATED)
- Authors : Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, Hang Xu
- Link : [http://arxiv.org/abs/2202.06767](http://arxiv.org/abs/2202.06767)
> ABSTRACT  :  Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, containing 100 million Chinese image-text pairs from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/**Swin**T) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a deep benchmarking of different downstream tasks are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, our model achieves an average accuracy of 73.03%. For the image-text retrieval task,our model achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than the result of WenLan 2.0. More information can refer to https://wukong-dataset.github.io/wukong-dataset/.  
### Text-DIAE: Degradation Invariant Autoencoders for Text Recognition and Document **Enhancement**. (arXiv:2203.04814v2 [cs.CV] UPDATED)
- Authors : Mohamed Ali, Sanket Biswas, Andres Mafla, Ali Furkan, Alicia Forn, Yousri Kessentini, Josep Llad, Lluis Gomez, Dimosthenis Karatzas
- Link : [http://arxiv.org/abs/2203.04814](http://arxiv.org/abs/2203.04814)
> ABSTRACT  :  In this work, we propose Text-Degradation Invariant Auto Encoder (Text-DIAE) aimed to solve two tasks, text recognition (handwritten or scene-text) and document image **enhancement**. We define three pretext tasks as learning objectives to be optimized during pre-training without the usage of labelled data. Each of the pre-text objectives is specifically tailored for the final downstream tasks. We conduct several ablation experiments that show the importance of each degradation for a specific domain. Exhaustive experimentation shows that our method does not have limitations of previous state-of-the-art based on contrastive losses while at the same time requiring essentially fewer data samples to converge. Finally, we demonstrate that our method surpasses the state-of-the-art significantly in existing supervised and self-supervised settings in handwritten and scene text recognition and document image **enhancement**. Our code and trained models will be made publicly available at~\url{ <a href="http://Upon_Acceptance">this http URL</a>}.  
## eess.IV
---
### Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v1 [eess.IV])
- Authors : Dezhao Wang, **Wenhan Yang**, Yueyu Hu, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2203.04963](http://arxiv.org/abs/2203.04963)
> ABSTRACT  :  Learned image compression has achieved great success due to its excellent modeling capacity, but seldom further considers the Rate-Distortion Optimization (RDO) of each input image. To explore this potential in the learned codec, we make the first attempt to build a neural data-dependent transform and introduce a continuous online mode decision mechanism to jointly optimize the coding efficiency for each individual image. Specifically, apart from the image content stream, we employ an additional model stream to generate the transform parameters at the decoder side. The presence of a model stream enables our model to learn more abstract neural-syntax, which helps cluster the latent representations of images more compactly. Beyond the transform stage, we also adopt neural-syntax based post-processing for the scenarios that require higher quality reconstructions regardless of extra decoding overhead. Moreover, the involvement of the model stream further makes it possible to optimize both the representation and the decoder in an online way, i.e. RDO at the testing time. It is equivalent to a continuous online mode decision, like coding modes in the traditional codecs, to improve the coding efficiency based on the individual input image. The experimental results show the effectiveness of the proposed neural-syntax design and the continuous online mode decision mechanism, demonstrating the superiority of our method in coding efficiency compared to the latest conventional standard Versatile Video Coding (VVC) and other state-of-the-art learning-based methods.  
## cs.LG
---
### Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition. (arXiv:2203.05008v1 [cs.CL])
- Authors : Ronny Huang, Cal Peyser, Ruoming Pang, Trevor Strohman, Shankar Kumar
- Link : [http://arxiv.org/abs/2203.05008](http://arxiv.org/abs/2203.05008)
> ABSTRACT  :  Language model fusion helps smart assistants recognize words which are rare in acoustic data but abundant in text-only corpora (typed search logs). However, such corpora have properties that hinder downstream performance, including being (1) too large, (2) beset with domain-mismatched content, and (3) heavy-headed rather than heavy-tailed (excessively many duplicate search queries such as "weather"). We show that three simple strategies for selecting language modeling data can dramatically improve rare-word recognition without harming overall performance. First, to address the heavy-headedness, we downsample the data according to a soft log function, which tunably reduces high frequency (head) sentences. Second, to encourage rare-word **exposure**, we explicitly filter for words rare in the acoustic data. Finally, we tackle domain-mismatch via perplexity-based contrastive selection, filtering for examples matched to the target domain. We down-select a large corpus of web search queries by a factor of 53x and achieve better LM perplexities than without down-selection. When shallow-fused with a state-of-the-art, production speech engine, our LM achieves WER reductions of up to 24% relative on rare-word sentences (without changing overall WER) compared to a baseline LM trained on the raw corpus. These gains are further validated through favorable side-by-side evaluations on live voice search traffic.  
### Power-of-Two Quantization for Low Bitwidth and Hardware Compliant Neural Networks. (arXiv:2203.05025v1 [cs.LG])
- Authors : Dominika Przewlocka, Syed Shakib, Ekin Sumbul, Yuecheng Li, Barbara De
- Link : [http://arxiv.org/abs/2203.05025](http://arxiv.org/abs/2203.05025)
> ABSTRACT  :  Deploying Deep Neural Networks in low-power embedded devices for **real time**-constrained applications requires optimization of memory and computational complexity of the networks, usually by quantizing the weights. Most of the existing works employ linear quantization which causes considerable degradation in accuracy for weight bit widths lower than 8. Since the distribution of weights is usually non-uniform (with most weights concentrated around zero), other methods, such as logarithmic quantization, are more suitable as they are able to preserve the shape of the weight distribution more precise. Moreover, using base-2 logarithmic representation allows optimizing the multiplication by replacing it with bit shifting. In this paper, we explore non-linear quantization techniques for exploiting lower bit precision and identify favorable hardware implementation options. We developed the Quantization Aware Training (QAT) algorithm that allowed training of low bit width Power-of-Two (PoT) networks and achieved accuracies on par with state-of-the-art floating point models for different tasks. We explored PoT weight encoding techniques and investigated hardware designs of MAC units for three different quantization schemes - uniform, PoT and Additive-PoT (APoT) - to show the increased efficiency when using the proposed approach. Eventually, the experiments showed that for low bit width precision, non-uniform quantization performs better than uniform, and at the same time, PoT quantization vastly reduces the computational complexity of the neural network.  
### Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v1 [cs.CV])
- Authors : Xiuwei Xu, Yifan Wang, Yu Zheng, Yongming Rao, Jiwen Lu, Jie Zhou
- Link : [http://arxiv.org/abs/2203.05238](http://arxiv.org/abs/2203.05238)
> ABSTRACT  :  In this paper, we propose a weakly-supervised approach for 3D object detection, which makes it possible to train strong 3D detector with position-level annotations (i.e. annotations of object centers). In order to remedy the information loss from box annotations to centers, our method, namely Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak labels into fully-annotated virtual scenes as stronger supervision, and in turn utilizes the perfect virtual labels to complement and refine the real labels. Specifically, we first assemble 3D shapes into physically reasonable virtual scenes according to the coarse scene layout extracted from position-level annotations. Then we go back to reality by applying a virtual-to-real domain adaptation method, which refine the weak labels and additionally supervise the training of detector with the virtual scenes. Furthermore, we propose a more challenging benckmark for indoor 3D object detection with more diversity in object sizes to better show the potential of BR. With less than 5% of the labeling labor, we achieve comparable detection performance with some popular fully-supervised approaches on the widely used ScanNet dataset. Code is available at: https://github.com/xuxw98/BackToReality  
### Exploiting the Potential of Datasets: A Data-Centric Approach for Model Robustness. (arXiv:2203.05323v1 [cs.LG])
- Authors : Yiqi Zhong, Lei Wu, Xianming Liu, Junjun Jiang
- Link : [http://arxiv.org/abs/2203.05323](http://arxiv.org/abs/2203.05323)
> ABSTRACT  :  Robustness of deep neural networks (DNNs) to malicious perturbations is a hot topic in trustworthy AI. Existing techniques obtain robust models given fixed datasets, either by modifying model structures, or by optimizing the process of inference or training. While significant improvements have been made, the possibility of constructing a high-quality dataset for model robustness remain unexplored. Follow the campaign of data-centric AI launched by Andrew Ng, we propose a novel algorithm for dataset **enhancement** that works well for many existing DNN models to improve robustness. Transferable adversarial examples and 14 kinds of common corruptions are included in our optimized dataset. In the data-centric robust learning competition hosted by Alibaba Group and Tsinghua University, our algorithm came third out of more than 3000 competitors in the first stage while we ranked fourth in the second stage. Our code is available at \url{https://github.com/hncszyq/tianchi_challenge}.  
### SoftSNN: Low-Cost Fault Tolerance for Spiking Neural Network Accelerators under Soft Errors. (arXiv:2203.05523v1 [cs.AR])
- Authors : Rachmad Vidya, Wicaksana Putra, Muhammad Abdullah, Muhammad Shafique
- Link : [http://arxiv.org/abs/2203.05523](http://arxiv.org/abs/2203.05523)
> ABSTRACT  :  Specialized hardware accelerators have been designed and employed to maximize the performance efficiency of Spiking Neural Networks (SNNs). However, such accelerators are vulnerable to transient faults (i.e., soft errors), which occur due to high-energy particle strikes, and manifest as bit flips at the hardware layer. These errors can change the weight values and neuron operations in the compute engine of SNN accelerators, thereby leading to incorrect outputs and accuracy degradation. However, the impact of soft errors in the compute engine and the respective mitigation techniques have not been thoroughly studied yet for SNNs. A potential solution is employing redundant executions (re-execution) for ensuring correct outputs, but it leads to huge latency and energy overheads. Toward this, we propose SoftSNN, a novel methodology to mitigate soft errors in the weight registers (synapses) and neurons of SNN accelerators without re-execution, thereby maintaining the accuracy with low latency and energy overheads. Our SoftSNN methodology employs the following key steps: (1) analyzing the SNN characteristics under soft errors to identify faulty weights and neuron operations, which are required for recognizing faulty SNN behavior; (2) a Bound-and-Protect technique that leverages this analysis to improve the SNN fault tolerance by bounding the weight values and protecting the neurons from faulty operations; and (3) devising lightweight hardware **enhancement**s for the neural hardware accelerator to efficiently support the proposed technique. The experimental results show that, for a 900-neuron network with even a high fault rate, our SoftSNN maintains the accuracy degradation below 3%, while reducing latency and energy by up to 3x and 2.3x respectively, as compared to the re-execution technique.  
### Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle Choices: New Insights into Chronic Disease Prevention Literacy. (arXiv:2104.14281v6 [cs.CY] UPDATED)
- Authors : Yongzhen Wang, Xiaozhong Liu, Jun Lin, Yingnan Ju, Changlong Sun, Luo Si
- Link : [http://arxiv.org/abs/2104.14281](http://arxiv.org/abs/2104.14281)
> ABSTRACT  :  Objective: Ubiquitous internet access is reshaping the way we live, but it is accompanied by unprecedented challenges in preventing chronic diseases that are usually planted by long **exposure** to unhealthy lifestyles. This paper proposes leveraging online shopping behaviors as a proxy for personal lifestyle choices to improve chronic disease prevention literacy, targeted for times when e-commerce user experience has been assimilated into most people's everyday lives.    Methods: Longitudinal query logs and purchase records from 15 million online shoppers were accessed, constructing a broad spectrum of lifestyle features covering various product categories and buyer personas. Using the lifestyle-related information preceding online shoppers' first purchases of specific prescription drugs, we could determine associations between their past lifestyle choices and whether they suffered from a particular chronic disease.    Results: Novel lifestyle risk factors were discovered in two exemplars--depression and type 2 diabetes, most of which showed reasonable consistency with existing healthcare knowledge. Further, such empirical findings could be adopted to locate online shoppers at higher risk of these chronic diseases with decent accuracy [i.e., (area under the receiver operating characteristic curve) AUC=0.68 for depression and AUC=0.70 for type 2 diabetes], closely matching the performance of screening surveys benchmarked against medical diagnosis.    Conclusions: Mining online shopping behaviors can point medical experts to a series of lifestyle issues associated with chronic diseases that are less explored to date. Hopefully, unobtrusive chronic disease surveillance via e-commerce sites can grant consenting individuals a privilege to be connected more readily with the medical profession and sophistication.  
### Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework. (arXiv:2202.06767v2 [cs.CV] UPDATED)
- Authors : Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, Hang Xu
- Link : [http://arxiv.org/abs/2202.06767](http://arxiv.org/abs/2202.06767)
> ABSTRACT  :  Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, containing 100 million Chinese image-text pairs from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/**Swin**T) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a deep benchmarking of different downstream tasks are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, our model achieves an average accuracy of 73.03%. For the image-text retrieval task,our model achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than the result of WenLan 2.0. More information can refer to https://wukong-dataset.github.io/wukong-dataset/.  
### Targeted Data Poisoning Attack on News Recommendation System by Content Perturbation. (arXiv:2203.03560v2 [cs.CR] UPDATED)
- Authors : Xudong Zhang, Zan Wang, Jingke Zhao, Lanjun Wang
- Link : [http://arxiv.org/abs/2203.03560](http://arxiv.org/abs/2203.03560)
> ABSTRACT  :  News Recommendation System(NRS) has become a fundamental technology to many online news services. Meanwhile, several studies show that recommendation systems(RS) are vulnerable to data poisoning attacks, and the attackers have the ability to mislead the system to perform as their desires. A widely studied attack approach, injecting fake users, can be applied on the NRS when the NRS is treated the same as the other systems whose items are fixed. However, in the NRS, as each item (i.e. news) is more informative, we propose a novel approach to poison the NRS, which is to perturb contents of some browsed news that results in the manipulation of the rank of the target news. Intuitively, an attack is useless if it is highly likely to be caught, i.e., exposed. To address this, we introduce a notion of the **exposure** risk and propose a novel problem of attacking a history news dataset by means of perturbations where the goal is to maximize the manipulation of the target news rank while keeping the risk of **exposure** under a given budget. We design a reinforcement learning framework, called TDP-CP, which contains a two-stage hierarchical model to reduce the searching space. Meanwhile, influence estimation is also applied to save the time on retraining the NRS for rewards. We test the performance of TDP-CP under three NRSs and on different target news. Our experiments show that TDP-CP can increase the rank of the target news successfully with a limited **exposure** budget.  
## cs.AI
---
### Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v1 [cs.CV])
- Authors : Xiuwei Xu, Yifan Wang, Yu Zheng, Yongming Rao, Jiwen Lu, Jie Zhou
- Link : [http://arxiv.org/abs/2203.05238](http://arxiv.org/abs/2203.05238)
> ABSTRACT  :  In this paper, we propose a weakly-supervised approach for 3D object detection, which makes it possible to train strong 3D detector with position-level annotations (i.e. annotations of object centers). In order to remedy the information loss from box annotations to centers, our method, namely Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak labels into fully-annotated virtual scenes as stronger supervision, and in turn utilizes the perfect virtual labels to complement and refine the real labels. Specifically, we first assemble 3D shapes into physically reasonable virtual scenes according to the coarse scene layout extracted from position-level annotations. Then we go back to reality by applying a virtual-to-real domain adaptation method, which refine the weak labels and additionally supervise the training of detector with the virtual scenes. Furthermore, we propose a more challenging benckmark for indoor 3D object detection with more diversity in object sizes to better show the potential of BR. With less than 5% of the labeling labor, we achieve comparable detection performance with some popular fully-supervised approaches on the widely used ScanNet dataset. Code is available at: https://github.com/xuxw98/BackToReality  
### Targeted Data Poisoning Attack on News Recommendation System by Content Perturbation. (arXiv:2203.03560v2 [cs.CR] UPDATED)
- Authors : Xudong Zhang, Zan Wang, Jingke Zhao, Lanjun Wang
- Link : [http://arxiv.org/abs/2203.03560](http://arxiv.org/abs/2203.03560)
> ABSTRACT  :  News Recommendation System(NRS) has become a fundamental technology to many online news services. Meanwhile, several studies show that recommendation systems(RS) are vulnerable to data poisoning attacks, and the attackers have the ability to mislead the system to perform as their desires. A widely studied attack approach, injecting fake users, can be applied on the NRS when the NRS is treated the same as the other systems whose items are fixed. However, in the NRS, as each item (i.e. news) is more informative, we propose a novel approach to poison the NRS, which is to perturb contents of some browsed news that results in the manipulation of the rank of the target news. Intuitively, an attack is useless if it is highly likely to be caught, i.e., exposed. To address this, we introduce a notion of the **exposure** risk and propose a novel problem of attacking a history news dataset by means of perturbations where the goal is to maximize the manipulation of the target news rank while keeping the risk of **exposure** under a given budget. We design a reinforcement learning framework, called TDP-CP, which contains a two-stage hierarchical model to reduce the searching space. Meanwhile, influence estimation is also applied to save the time on retraining the NRS for rewards. We test the performance of TDP-CP under three NRSs and on different target news. Our experiments show that TDP-CP can increase the rank of the target news successfully with a limited **exposure** budget.  
# Paper List
---
## cs.CV
---
**115** new papers in cs.CV:-) 
1. Fluid registration between lung CT and stationary chest tomosynthesis images. (arXiv:2203.04958v1 [eess.IV])
2. ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities. (arXiv:2203.04959v1 [eess.IV])
3. Memory-augmented Deep Unfolding Network for Guided Image Super-resolution. (arXiv:2203.04960v1 [eess.IV])
4. Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification. (arXiv:2203.04961v1 [eess.IV])
5. Learning the Degradation Distribution for Blind Image Super-Resolution. (arXiv:2203.04962v1 [eess.IV])
6. Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v1 [eess.IV])
7. Metastatic Cancer Outcome Prediction with Injective Multiple Instance Pooling. (arXiv:2203.04964v1 [eess.IV])
8. UNeXt: MLP-based Rapid Medical Image Segmentation Network. (arXiv:2203.04967v1 [eess.IV])
9. Resource-Efficient Invariant Networks: Exponential Gains by Unrolled Optimization. (arXiv:2203.05006v1 [cs.CV])
10. Dynamic Instance Domain Adaptation. (arXiv:2203.05028v1 [cs.CV])
11. Adaptive Trajectory Prediction via Transferable GNN. (arXiv:2203.05046v1 [cs.CV])
12. Evaluating Proposed Fairness Models for Face Recognition Algorithms. (arXiv:2203.05051v1 [cs.CV])
13. Optical Flow Training under Limited Label Budget via Active Learning. (arXiv:2203.05053v1 [cs.CV])
14. SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving. (arXiv:2203.05056v1 [cs.CV])
15. The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v1 [cs.LG])
16. NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks. (arXiv:2203.05081v1 [cs.CV])
17. A Tree-Structured Multi-Task Model Recommender. (arXiv:2203.05092v1 [cs.LG])
18. Improving Neural ODEs via Knowledge Distillation. (arXiv:2203.05103v1 [cs.CV])
19. OpenTAL: Towards Open Set Temporal Action Localization. (arXiv:2203.05114v1 [cs.CV])
20. Semi-supervision semantic segmentation with uncertainty-guided self cross supervision. (arXiv:2203.05118v1 [cs.CV])
21. MetAug: Contrastive Learning via Meta Feature Augmentation. (arXiv:2203.05119v1 [cs.CV])
22. DEER: Detection-agnostic End-to-End Recognizer for Scene Text Spotting. (arXiv:2203.05122v1 [cs.CV])
23. Manifold Modeling in Quotient Space: Learning An Invariant Mapping with Decodability of Image Patches. (arXiv:2203.05134v1 [cs.CV])
24. Cross-modal Map Learning for Vision and Language Navigation. (arXiv:2203.05137v1 [cs.CV])
25. Intention-aware Feature Propagation Network for Interactive Segmentation. (arXiv:2203.05145v1 [cs.CV])
26. Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v1 [cs.CV])
27. Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v1 [cs.CV])
28. Zero-Shot Action Recognition with Transformer-based Video Semantic Embedding. (arXiv:2203.05156v1 [cs.CV])
29. MVP: Multimodality-guided Visual Pre-training. (arXiv:2203.05175v1 [cs.CV])
30. An Audio-Visual Attention Based Multimodal Network for Fake Talking Face Videos Detection. (arXiv:2203.05178v1 [cs.CV])
31. Towards Open-Set Text Recognition via Label-to-Prototype Learning. (arXiv:2203.05179v1 [cs.CV])
32. Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability. (arXiv:2203.05180v1 [cs.CV])
33. Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding. (arXiv:2203.05186v1 [cs.CV])
34. Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection. (arXiv:2203.05187v1 [cs.RO])
35. NeRFocus: Neural Radiance Field for 3D Synthetic Defocus. (arXiv:2203.05189v1 [cs.CV])
36. Adaptive Background Matting Using Background Matching. (arXiv:2203.05193v1 [cs.CV])
37. A Screen-Shooting Resilient Document Image Watermarking Scheme using Deep Neural Network. (arXiv:2203.05198v1 [cs.CV])
38. Hyperspectral Imaging for cherry tomato. (arXiv:2203.05199v1 [cs.CV])
39. Online Deep Metric Learning via Mutual Distillation. (arXiv:2203.05201v1 [cs.CV])
40. MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v1 [cs.CV])
41. Crowd Source Scene Change Detection and Local Map Update. (arXiv:2203.05205v1 [cs.CV])
42. ReF -- Rotation Equivariant Features for Local Feature Matching. (arXiv:2203.05206v1 [cs.CV])
43. Transferring Dual Stochastic Graph Convolutional Network for Facial Micro-expression Recognition. (arXiv:2203.05208v1 [cs.CV])
44. Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation. (arXiv:2203.05212v1 [cs.CV])
45. Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v1 [cs.CV])
46. A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach. (arXiv:2203.05243v1 [cs.CV])
47. **Real-time** Scene Text Detection Based on Global Level and Word Level Features. (arXiv:2203.05251v1 [cs.CV])
48. Contrastive Boundary Learning for Point Cloud Segmentation. (arXiv:2203.05272v1 [cs.CV])
49. Domain Generalisation for Object Detection. (arXiv:2203.05294v1 [cs.CV])
50. BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v1 [cs.CV])
51. GrainSpace: A Large-scale Dataset for Fine-grained and Domain-adaptive Recognition of Cereal Grains. (arXiv:2203.05306v1 [cs.CV])
52. StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v1 [cs.CV])
53. Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking. (arXiv:2203.05328v1 [cs.CV])
54. SelfTune: Metrically Scaled Monocular Depth Estimation through Self-Supervised Learning. (arXiv:2203.05332v1 [cs.CV])
55. Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects. (arXiv:2203.05334v1 [cs.CV])
56. Non-generative Generalized Zero-shot Learning via Task-correlated Disentanglement and Controllable Samples Synthesis. (arXiv:2203.05335v1 [cs.CV])
57. TrueType Transformer: Character and Font Style Recognition in Outline Format. (arXiv:2203.05338v1 [cs.CV])
58. Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v1 [cs.CV])
59. EyeLoveGAN: Exploiting domain-shifts to boost network learning with cycleGANs. (arXiv:2203.05344v1 [cs.CV])
60. Knowledge-enriched Attention Network with Group-wise Semantic for Visual Storytelling. (arXiv:2203.05346v1 [cs.CV])
61. Two-stream Hierarchical Similarity Reasoning for Image-text Matching. (arXiv:2203.05349v1 [cs.MM])
62. Temporal Context for Robust Maritime Obstacle Detection. (arXiv:2203.05352v1 [cs.CV])
63. Spatial Commonsense Graph for Object Localisation in Partial Scenes. (arXiv:2203.05380v1 [cs.CV])
64. Annotation Efficient Person Re-Identification with Diverse Cluster-Based Pair Selection. (arXiv:2203.05395v1 [cs.CV])
65. Representation Compensation Networks for Continual Semantic Segmentation. (arXiv:2203.05402v1 [cs.CV])
66. LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval. (arXiv:2203.05465v1 [cs.CV])
67. Prediction-Guided Distillation for Dense Object Detection. (arXiv:2203.05469v1 [cs.CV])
68. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v1 [cs.LG])
69. Towards Less Constrained Macro-Neural Architecture Search. (arXiv:2203.05508v1 [cs.CV])
70. AGCN: Augmented Graph Convolutional Network for Lifelong Multi-label Image Recognition. (arXiv:2203.05534v1 [cs.CV])
71. An Empirical Investigation of 3D Anomaly Detection and Segmentation. (arXiv:2203.05550v1 [cs.CV])
72. Transfer of Representations to Video Label Propagation: Implementation Factors Matter. (arXiv:2203.05553v1 [cs.CV])
73. Conditional Prompt Learning for Vision-Language Models. (arXiv:2203.05557v1 [cs.CV])
74. Learning from Noisy Labels with Deep Neural Networks: A Survey. (arXiv:2007.08199v7 [cs.LG] UPDATED)
75. Conceptual Compression via Deep Structure and Texture Synthesis. (arXiv:2011.04976v2 [cs.CV] UPDATED)
76. Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery. (arXiv:2011.13913v3 [cs.CV] UPDATED)
77. HANA: A HAndwritten NAme Database for Offline Handwritten Text Recognition. (arXiv:2101.10862v2 [cs.CV] UPDATED)
78. Toward Compact Deep Neural Networks via Energy-Aware Pruning. (arXiv:2103.10858v2 [cs.CV] UPDATED)
79. AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v2 [eess.IV] UPDATED)
80. Patchwork: Concentric Zone-based Region-wise Ground Segmentation with Ground Likelihood Estimation Using a 3D LiDAR Sensor. (arXiv:2108.05560v2 [cs.RO] UPDATED)
81. A review and experimental evaluation of deep learning methods for MRI reconstruction. (arXiv:2109.08618v3 [eess.IV] UPDATED)
82. Learning Contrastive Representation for Semantic Correspondence. (arXiv:2109.10967v2 [cs.CV] UPDATED)
83. Cartoon Explanations of Image Classifiers. (arXiv:2110.03485v4 [cs.AI] UPDATED)
84. EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Vitals Measurement. (arXiv:2110.04447v2 [cs.CV] UPDATED)
85. Revisit Dictionary Learning for Video Compressive Sensing under the Plug-and-Play Framework. (arXiv:2110.04966v2 [cs.CV] UPDATED)
86. MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v2 [cs.CV] UPDATED)
87. MAC-ReconNet: A Multiple Acquisition Context based Convolutional Neural Network for MR Image Reconstruction using Dynamic Weight Prediction. (arXiv:2111.05055v2 [eess.IV] UPDATED)
88. SequentialPointNet: A strong frame-level parallel point cloud sequence network for 3D action recognition. (arXiv:2111.08492v2 [cs.CV] UPDATED)
89. TraSw: Tracklet-Switch Adversarial Attacks against Multi-Object Tracking. (arXiv:2111.08954v2 [cs.CV] UPDATED)
90. IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization. (arXiv:2111.09136v5 [cs.CV] UPDATED)
91. Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v2 [eess.IV] UPDATED)
92. Hierarchical Modular Network for Video Captioning. (arXiv:2111.12476v3 [cs.CV] UPDATED)
93. Neural Collaborative Graph Machines for Table Structure Recognition. (arXiv:2111.13359v2 [cs.CV] UPDATED)
94. Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v2 [cs.LG] UPDATED)
95. Diffusion Autoencoders: Toward a Meaningful and Decodable Representation. (arXiv:2111.15640v3 [cs.CV] UPDATED)
96. Curvature-guided dynamic scale networks for Multi-view Stereo. (arXiv:2112.05999v3 [cs.CV] UPDATED)
97. Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking. (arXiv:2112.14016v2 [cs.CV] UPDATED)
98. Dense Depth Estimation from Multiple 360-degree Images Using Virtual Depth. (arXiv:2112.14931v2 [cs.CV] UPDATED)
99. Efficient Non-Local Contrastive Attention for Image Super-Resolution. (arXiv:2201.03794v2 [cs.CV] UPDATED)
100. Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution. (arXiv:2201.04358v2 [cs.CV] UPDATED)
101. Toward Data-Driven STAP Radar. (arXiv:2201.10712v2 [cs.CV] UPDATED)
102. Access Control of Object Detection Models Using Encrypted Feature Maps. (arXiv:2202.00265v2 [cs.CV] UPDATED)
103. Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework. (arXiv:2202.06767v2 [cs.CV] UPDATED)
104. When, Why, and Which Pretrained GANs Are Useful?. (arXiv:2202.08937v2 [cs.LG] UPDATED)
105. Outlier-based Autism Detection using Longitudinal Structural MRI. (arXiv:2202.09988v2 [eess.IV] UPDATED)
106. Pattern Based Multivariable Regression using Deep Learning (PBMR-DP). (arXiv:2202.13541v3 [cs.CV] UPDATED)
107. Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey. (arXiv:2203.00190v3 [cs.CV] UPDATED)
108. Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work. (arXiv:2203.01536v2 [cs.CV] UPDATED)
109. Interactive Image Synthesis with Panoptic Layout Generation. (arXiv:2203.02104v2 [cs.CV] UPDATED)
110. HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v2 [cs.CV] UPDATED)
111. Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v3 [cs.CV] UPDATED)
112. SingleSketch2Mesh : Generating 3D Mesh model from Sketch. (arXiv:2203.03157v2 [cs.CV] UPDATED)
113. Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks. (arXiv:2203.03844v2 [eess.IV] UPDATED)
114. Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition. (arXiv:2203.04623v2 [cs.CV] UPDATED)
115. Text-DIAE: Degradation Invariant Autoencoders for Text Recognition and Document **Enhancement**. (arXiv:2203.04814v2 [cs.CV] UPDATED)
## eess.IV
---
**22** new papers in eess.IV:-) 
1. Fluid registration between lung CT and stationary chest tomosynthesis images. (arXiv:2203.04958v1 [eess.IV])
2. ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities. (arXiv:2203.04959v1 [eess.IV])
3. Memory-augmented Deep Unfolding Network for Guided Image Super-resolution. (arXiv:2203.04960v1 [eess.IV])
4. Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification. (arXiv:2203.04961v1 [eess.IV])
5. Learning the Degradation Distribution for Blind Image Super-Resolution. (arXiv:2203.04962v1 [eess.IV])
6. Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v1 [eess.IV])
7. Metastatic Cancer Outcome Prediction with Injective Multiple Instance Pooling. (arXiv:2203.04964v1 [eess.IV])
8. UNeXt: MLP-based Rapid Medical Image Segmentation Network. (arXiv:2203.04967v1 [eess.IV])
9. Action-Constrained Reinforcement Learning for Frame-Level Bit Allocation in HEVC/H.265 through Frank-Wolfe Policy Optimization. (arXiv:2203.05127v1 [eess.IV])
10. Transferring Dual Stochastic Graph Convolutional Network for Facial Micro-expression Recognition. (arXiv:2203.05208v1 [cs.CV])
11. Conceptual Compression via Deep Structure and Texture Synthesis. (arXiv:2011.04976v2 [cs.CV] UPDATED)
12. AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v2 [eess.IV] UPDATED)
13. A review and experimental evaluation of deep learning methods for MRI reconstruction. (arXiv:2109.08618v3 [eess.IV] UPDATED)
14. Revisit Dictionary Learning for Video Compressive Sensing under the Plug-and-Play Framework. (arXiv:2110.04966v2 [cs.CV] UPDATED)
15. MAC-ReconNet: A Multiple Acquisition Context based Convolutional Neural Network for MR Image Reconstruction using Dynamic Weight Prediction. (arXiv:2111.05055v2 [eess.IV] UPDATED)
16. Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v2 [eess.IV] UPDATED)
17. Dense Depth Estimation from Multiple 360-degree Images Using Virtual Depth. (arXiv:2112.14931v2 [cs.CV] UPDATED)
18. Dynamic optical contrast imaging for real-time delineation of tumor resection margins using head and neck cancer as a model. (arXiv:2202.07108v3 [eess.IV] UPDATED)
19. Outlier-based Autism Detection using Longitudinal Structural MRI. (arXiv:2202.09988v2 [eess.IV] UPDATED)
20. Pattern Based Multivariable Regression using Deep Learning (PBMR-DP). (arXiv:2202.13541v3 [cs.CV] UPDATED)
21. HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v2 [cs.CV] UPDATED)
22. Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks. (arXiv:2203.03844v2 [eess.IV] UPDATED)
## cs.LG
---
**115** new papers in cs.LG:-) 
1. Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification. (arXiv:2203.04961v1 [eess.IV])
2. Resource-Efficient Invariant Networks: Exponential Gains by Unrolled Optimization. (arXiv:2203.05006v1 [cs.CV])
3. Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition. (arXiv:2203.05008v1 [cs.CL])
4. Learning to control from expert demonstrations. (arXiv:2203.05012v1 [eess.SY])
5. Power-of-Two Quantization for Low Bitwidth and Hardware Compliant Neural Networks. (arXiv:2203.05025v1 [cs.LG])
6. Transfer Learning as an Essential Tool for Digital Twins in Renewable Energy Systems. (arXiv:2203.05026v1 [cs.LG])
7. Evaluating Proposed Fairness Models for Face Recognition Algorithms. (arXiv:2203.05051v1 [cs.CV])
8. Universal Regression with Adversarial Responses. (arXiv:2203.05067v1 [cs.LG])
9. On the influence of over-parameterization in manifold based surrogates and deep neural operators. (arXiv:2203.05071v1 [cs.LG])
10. The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v1 [cs.LG])
11. Connecting sufficient conditions for domain adaptation: source-guided uncertainty, relaxed divergences and discrepancy localization. (arXiv:2203.05076v1 [cs.LG])
12. SAGE: Generating Symbolic Goals for Myopic Models in Deep Reinforcement Learning. (arXiv:2203.05079v1 [cs.LG])
13. Detecting and Diagnosing Terrestrial Gravitational-Wave Mimics Through Feature Learning. (arXiv:2203.05086v1 [astro-ph.IM])
14. A Tree-Structured Multi-Task Model Recommender. (arXiv:2203.05092v1 [cs.LG])
15. Model-Architecture Co-Design for High Performance Temporal GNN Inference on FPGA. (arXiv:2203.05095v1 [cs.AR])
16. Improving Neural ODEs via Knowledge Distillation. (arXiv:2203.05103v1 [cs.CV])
17. Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models. (arXiv:2203.05104v1 [cs.LG])
18. Librarian-in-the-Loop: A Natural Language Processing Paradigm for Detecting Informal Mentions of Research Data in Academic Literature. (arXiv:2203.05112v1 [cs.DL])
19. Internet-augmented language models through few-shot prompting for open-domain question answering. (arXiv:2203.05115v1 [cs.CL])
20. Optimal Methods for Risk Averse Distributed Optimization. (arXiv:2203.05117v1 [math.OC])
21. Collusion Detection in Team-Based Multiplayer Games. (arXiv:2203.05121v1 [cs.LG])
22. Multi-Task Adversarial Learning for Treatment Effect Estimation in Basket Trials. (arXiv:2203.05123v1 [cs.LG])
23. PACTran: PAC-Bayesian Metrics for Estimating the Transferability of Pretrained Models to Classification Tasks. (arXiv:2203.05126v1 [cs.LG])
24. Action-Constrained Reinforcement Learning for Frame-Level Bit Allocation in HEVC/H.265 through Frank-Wolfe Policy Optimization. (arXiv:2203.05127v1 [eess.IV])
25. Manifold Modeling in Quotient Space: Learning An Invariant Mapping with Decodability of Image Patches. (arXiv:2203.05134v1 [cs.CV])
26. IAE-Net: Integral Autoencoders for Discretization-Invariant Learning. (arXiv:2203.05142v1 [cs.LG])
27. TiSAT: Time Series Anomaly Transformer. (arXiv:2203.05167v1 [cs.LG])
28. TextConvoNet:A Convolutional Neural Network based Architecture for Text Classification. (arXiv:2203.05173v1 [cs.CL])
29. Assessing Phenotype Definitions for Algorithmic Fairness. (arXiv:2203.05174v1 [q-bio.OT])
30. A Review of Open Source Software Tools for Time Series Analysis. (arXiv:2203.05195v1 [cs.LG])
31. Clustering Label Inference Attack against Practical Split Learning. (arXiv:2203.05222v1 [cs.LG])
32. Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v1 [cs.CV])
33. Conditional Synthetic Data Generation for Personal Thermal Comfort Models. (arXiv:2203.05242v1 [cs.LG])
34. API: Boosting Multi-Agent Reinforcement Learning via Agent-Permutation-Invariant Networks. (arXiv:2203.05285v1 [cs.LG])
35. BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v1 [cs.CV])
36. Exploiting the Potential of Datasets: A Data-Centric Approach for Model Robustness. (arXiv:2203.05323v1 [cs.LG])
37. AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities. (arXiv:2203.05325v1 [cs.CL])
38. Parsimonious Random Projection Neural Networks for the Numerical Solution of Initial-Value Problems of ODEs and index-1 DAEs. (arXiv:2203.05337v1 [math.NA])
39. Differentially Private Learning Needs Hidden State (Or Much Faster Convergence). (arXiv:2203.05363v1 [stat.ML])
40. A Contribution-based Device Selection Scheme in Federated Learning. (arXiv:2203.05369v1 [cs.LG])
41. Forecasting the abnormal events at well drilling with machine learning. (arXiv:2203.05378v1 [cs.LG])
42. Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Regression. (arXiv:2203.05400v1 [math.ST])
43. Robustness Analysis of Classification Using Recurrent Neural Networks with Perturbed Sequential Input. (arXiv:2203.05403v1 [cs.LG])
44. Blind Extraction of Equitable Partitions from Graph Signals. (arXiv:2203.05407v1 [math.OC])
45. Deep Regression Ensembles. (arXiv:2203.05417v1 [stat.ML])
46. Semantic Norm Recognition and its application to Portuguese Law. (arXiv:2203.05425v1 [cs.CL])
47. Near-optimal Deep Reinforcement Learning Policies from Data for Zone Temperature Control. (arXiv:2203.05434v1 [cs.LG])
48. Bias-variance decomposition of overparameterized regression with random linear features. (arXiv:2203.05443v1 [stat.ML])
49. LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval. (arXiv:2203.05465v1 [cs.CV])
50. CoCo-FL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization. (arXiv:2203.05468v1 [cs.LG])
51. Prediction-Guided Distillation for Dense Object Detection. (arXiv:2203.05469v1 [cs.CV])
52. Fully Adaptive Composition in Differential Privacy. (arXiv:2203.05481v1 [cs.LG])
53. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v1 [cs.LG])
54. projUNN: efficient method for training deep networks with unitary matrices. (arXiv:2203.05483v1 [cs.LG])
55. Geometric and Topological Inference for Deep Representations of Complex Networks. (arXiv:2203.05488v1 [cs.LG])
56. An Empirical Study of Low Precision Quantization for TinyML. (arXiv:2203.05492v1 [cs.LG])
57. Towards Less Constrained Macro-Neural Architecture Search. (arXiv:2203.05508v1 [cs.CV])
58. SoftSNN: Low-Cost Fault Tolerance for Spiking Neural Network Accelerators under Soft Errors. (arXiv:2203.05523v1 [cs.AR])
59. Context is Everything: Implicit Identification for Dynamics Adaptation. (arXiv:2203.05549v1 [cs.RO])
60. On Embeddings for Numerical Features in Tabular Deep Learning. (arXiv:2203.05556v1 [cs.LG])
61. Conditional Prompt Learning for Vision-Language Models. (arXiv:2203.05557v1 [cs.CV])
62. Supervised ML Solution for Band Assignment in Dual-Band Systems with Omnidirectional and Directional Antennas. (arXiv:1902.10890v2 [cs.LG] UPDATED)
63. A General Pairwise Comparison Model for Extremely Sparse Networks. (arXiv:2002.08853v3 [stat.ML] UPDATED)
64. A Locally Adaptive Interpretable Regression. (arXiv:2005.03350v3 [stat.ML] UPDATED)
65. Learning from Noisy Labels with Deep Neural Networks: A Survey. (arXiv:2007.08199v7 [cs.LG] UPDATED)
66. On the finite representation of group equivariant operators via permutant measures. (arXiv:2008.06340v2 [math.GR] UPDATED)
67. The Nonconvex Geometry of Linear Inverse Problems. (arXiv:2101.02776v2 [math.OC] UPDATED)
68. Toward Compact Deep Neural Networks via Energy-Aware Pruning. (arXiv:2103.10858v2 [cs.CV] UPDATED)
69. Computationally Efficient Learning of Statistical Manifolds. (arXiv:2103.11773v2 [cs.LG] UPDATED)
70. deepregression: a Flexible Neural Network Framework for Semi-Structured Deep Distributional Regression. (arXiv:2104.02705v3 [stat.ML] UPDATED)
71. Deep Learning for Bayesian Optimization of Scientific Problems with High-Dimensional Structure. (arXiv:2104.11667v2 [cs.LG] UPDATED)
72. Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle Choices: New Insights into Chronic Disease Prevention Literacy. (arXiv:2104.14281v6 [cs.CY] UPDATED)
73. SLGPT: Using Transfer Learning to Directly Generate Simulink Model Files and Find Bugs in the Simulink Toolchain. (arXiv:2105.07465v3 [cs.SE] UPDATED)
74. JUMBO: Scalable Multi-task Bayesian Optimization using Offline Data. (arXiv:2106.00942v2 [cs.LG] UPDATED)
75. Reliable Adversarial Distillation with Unreliable Teachers. (arXiv:2106.04928v3 [cs.LG] UPDATED)
76. A Deep Variational Approach to Clustering Survival Data. (arXiv:2106.05763v3 [cs.LG] UPDATED)
77. Sparse Multi-Reference Alignment : Phase Retrieval, Uniform Uncertainty Principles and the Beltway Problem. (arXiv:2106.12996v3 [math.ST] UPDATED)
78. Hierarchical Few-Shot Imitation with Skill Transition Models. (arXiv:2107.08981v2 [cs.LG] UPDATED)
79. Filament Plots for Data Visualization. (arXiv:2107.10869v3 [cs.HC] UPDATED)
80. AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v2 [eess.IV] UPDATED)
81. Lower Bounds on the Total Variation Distance Between Mixtures of Two Gaussians. (arXiv:2109.01064v2 [math.PR] UPDATED)
82. A review and experimental evaluation of deep learning methods for MRI reconstruction. (arXiv:2109.08618v3 [eess.IV] UPDATED)
83. Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming. (arXiv:2109.11410v2 [cs.LG] UPDATED)
84. Distributed Deep Reinforcement Learning for Adaptive Medium Access and Modulation in Shared Spectrum. (arXiv:2109.11723v2 [eess.SP] UPDATED)
85. Topologically-Informed Atlas Learning. (arXiv:2110.00429v2 [cs.RO] UPDATED)
86. TensorPlan and the Few Actions Lower Bound for Planning in MDPs under Linear Realizability of Optimal Value Functions. (arXiv:2110.02195v2 [cs.LG] UPDATED)
87. MarS-FL: Enabling Competitors to Collaborate in Federated Learning. (arXiv:2110.13464v2 [cs.LG] UPDATED)
88. Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v2 [eess.IV] UPDATED)
89. Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v2 [cs.LG] UPDATED)
90. Path Integral Sampler: a stochastic control approach for sampling. (arXiv:2111.15141v2 [cs.LG] UPDATED)
91. Diffusion Autoencoders: Toward a Meaningful and Decodable Representation. (arXiv:2111.15640v3 [cs.CV] UPDATED)
92. Anti-Money Laundering Alert Optimization Using Machine Learning with Graphs. (arXiv:2112.07508v2 [cs.LG] UPDATED)
93. Skip Vectors for RDF Data: Extraction Based on the Complexity of Feature Patterns. (arXiv:2201.01996v3 [cs.LG] UPDATED)
94. The Dataset Nutrition Label (2nd Gen): Leveraging Context to Mitigate Harms in Artificial Intelligence. (arXiv:2201.03954v2 [cs.LG] UPDATED)
95. Differentially Describing Groups of Graphs. (arXiv:2201.04064v2 [cs.SI] UPDATED)
96. Access Control of Object Detection Models Using Encrypted Feature Maps. (arXiv:2202.00265v2 [cs.CV] UPDATED)
97. To Impute or not to Impute? Missing Data in Treatment Effect Estimation. (arXiv:2202.02096v2 [stat.ML] UPDATED)
98. Compute Trends Across Three Eras of Machine Learning. (arXiv:2202.05924v2 [cs.LG] UPDATED)
99. Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework. (arXiv:2202.06767v2 [cs.CV] UPDATED)
100. Learning Transferrable Representations of Career Trajectories for Economic Prediction. (arXiv:2202.08370v2 [cs.LG] UPDATED)
101. When, Why, and Which Pretrained GANs Are Useful?. (arXiv:2202.08937v2 [cs.LG] UPDATED)
102. Bayes-Optimal Classifiers under Group Fairness. (arXiv:2202.09724v2 [stat.ML] UPDATED)
103. Outlier-based Autism Detection using Longitudinal Structural MRI. (arXiv:2202.09988v2 [eess.IV] UPDATED)
104. CD-ROM: Complementary Deep-Reduced Order Model. (arXiv:2202.10746v2 [physics.flu-dyn] UPDATED)
105. Pattern Based Multivariable Regression using Deep Learning (PBMR-DP). (arXiv:2202.13541v3 [cs.CV] UPDATED)
106. Distributional Reinforcement Learning for Scheduling of Chemical Production Processes. (arXiv:2203.00636v2 [eess.SY] UPDATED)
107. Engineering the Neural Automatic Passenger Counter. (arXiv:2203.01156v2 [cs.LG] UPDATED)
108. Analysis of closed-loop inertial gradient dynamics. (arXiv:2203.02140v2 [math.OC] UPDATED)
109. Exploring Scalable, Distributed Real-Time Anomaly Detection for Bridge Health Monitoring. (arXiv:2203.02380v2 [cs.NI] UPDATED)
110. Kernel Packet: An Exact and Scalable Algorithm for Gaussian Process Regression with Mat\'ern Correlations. (arXiv:2203.03116v2 [stat.ML] UPDATED)
111. Detecting data-driven robust statistical arbitrage strategies with deep neural networks. (arXiv:2203.03179v2 [q-fin.CP] UPDATED)
112. Targeted Data Poisoning Attack on News Recommendation System by Content Perturbation. (arXiv:2203.03560v2 [cs.CR] UPDATED)
113. Estimating the average causal effect of intervention in continuous variables using machine learning. (arXiv:2203.03916v3 [stat.ML] UPDATED)
114. Data-driven detector signal characterization with constrained bottleneck autoencoders. (arXiv:2203.04604v2 [physics.ins-det] UPDATED)
115. Investigation of Factorized Optical Flows as Mid-Level Representations. (arXiv:2203.04927v2 [cs.LG] UPDATED)
## cs.AI
---
**62** new papers in cs.AI:-) 
1. Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification. (arXiv:2203.04961v1 [eess.IV])
2. On Linking Level Segments. (arXiv:2203.05057v1 [cs.AI])
3. HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing. (arXiv:2203.05061v1 [cs.CL])
4. The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v1 [cs.LG])
5. SAGE: Generating Symbolic Goals for Myopic Models in Deep Reinforcement Learning. (arXiv:2203.05079v1 [cs.LG])
6. Givens Coordinate Descent Methods for Rotation Matrix Learning in Trainable Embedding Indexes. (arXiv:2203.05082v1 [cs.IR])
7. Compilable Neural Code Generation with Compiler Feedback. (arXiv:2203.05132v1 [cs.CL])
8. Zero-Shot Action Recognition with Transformer-based Video Semantic Embedding. (arXiv:2203.05156v1 [cs.CV])
9. TextConvoNet:A Convolutional Neural Network based Architecture for Text Classification. (arXiv:2203.05173v1 [cs.CL])
10. Assessing Phenotype Definitions for Algorithmic Fairness. (arXiv:2203.05174v1 [q-bio.OT])
11. Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding. (arXiv:2203.05186v1 [cs.CV])
12. A Systematic Literature Review on Blockchain Enabled Federated Learning Framework for Internet of Vehicles. (arXiv:2203.05192v1 [cs.CR])
13. Learning Torque Control for Quadrupedal Locomotion. (arXiv:2203.05194v1 [cs.RO])
14. A Review of Open Source Software Tools for Time Series Analysis. (arXiv:2203.05195v1 [cs.LG])
15. Hyperspectral Imaging for cherry tomato. (arXiv:2203.05199v1 [cs.CV])
16. MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v1 [cs.CV])
17. Clustering Label Inference Attack against Practical Split Learning. (arXiv:2203.05222v1 [cs.LG])
18. Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label **Enhancement**. (arXiv:2203.05238v1 [cs.CV])
19. API: Boosting Multi-Agent Reinforcement Learning via Agent-Permutation-Invariant Networks. (arXiv:2203.05285v1 [cs.LG])
20. SoK: On the Semantic AI Security in Autonomous Driving. (arXiv:2203.05314v1 [cs.CR])
21. AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities. (arXiv:2203.05325v1 [cs.CL])
22. EyeLoveGAN: Exploiting domain-shifts to boost network learning with cycleGANs. (arXiv:2203.05344v1 [cs.CV])
23. Attacks as Defenses: Designing Robust Audio CAPTCHAs Using Attacks on Automatic Speech Recognition Systems. (arXiv:2203.05408v1 [cs.CR])
24. OneRel:Joint Entity and Relation Extraction with One Module in One Step. (arXiv:2203.05412v1 [cs.CL])
25. Near-optimal Deep Reinforcement Learning Policies from Data for Zone Temperature Control. (arXiv:2203.05434v1 [cs.LG])
26. IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages. (arXiv:2203.05437v1 [cs.CL])
27. Artificial Intelligence in Vehicular Wireless Networks: A Case Study Using ns-3. (arXiv:2203.05449v1 [cs.NI])
28. LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval. (arXiv:2203.05465v1 [cs.CV])
29. projUNN: efficient method for training deep networks with unitary matrices. (arXiv:2203.05483v1 [cs.LG])
30. Towards Less Constrained Macro-Neural Architecture Search. (arXiv:2203.05508v1 [cs.CV])
31. Data-driven Abstractions with Probabilistic Guarantees for Linear PETC Systems. (arXiv:2203.05522v1 [eess.SY])
32. AGCN: Augmented Graph Convolutional Network for Lifelong Multi-label Image Recognition. (arXiv:2203.05534v1 [cs.CV])
33. Context is Everything: Implicit Identification for Dynamics Adaptation. (arXiv:2203.05549v1 [cs.RO])
34. Conditional Prompt Learning for Vision-Language Models. (arXiv:2203.05557v1 [cs.CV])
35. A Locally Adaptive Interpretable Regression. (arXiv:2005.03350v3 [stat.ML] UPDATED)
36. From Extreme Multi-label to Multi-class: A Hierarchical Approach for Automated ICD-10 Coding Using Phrase-level Attention. (arXiv:2102.09136v2 [cs.CL] UPDATED)
37. JUMBO: Scalable Multi-task Bayesian Optimization using Offline Data. (arXiv:2106.00942v2 [cs.LG] UPDATED)
38. Hierarchical Few-Shot Imitation with Skill Transition Models. (arXiv:2107.08981v2 [cs.LG] UPDATED)
39. Topologically-Informed Atlas Learning. (arXiv:2110.00429v2 [cs.RO] UPDATED)
40. TensorPlan and the Few Actions Lower Bound for Planning in MDPs under Linear Realizability of Optimal Value Functions. (arXiv:2110.02195v2 [cs.LG] UPDATED)
41. Cartoon Explanations of Image Classifiers. (arXiv:2110.03485v4 [cs.AI] UPDATED)
42. EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Vitals Measurement. (arXiv:2110.04447v2 [cs.CV] UPDATED)
43. Program Transfer for Answering Complex Questions over Knowledge Bases. (arXiv:2110.05743v3 [cs.AI] UPDATED)
44. The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail. (arXiv:2110.08300v3 [cs.CL] UPDATED)
45. Planning for Risk-Aversion and Expected Value in MDPs. (arXiv:2110.12746v2 [cs.AI] UPDATED)
46. Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v2 [cs.LG] UPDATED)
47. An unsupervised extractive summarization method based on multi-round computation. (arXiv:2112.03203v4 [cs.CL] UPDATED)
48. Curvature-guided dynamic scale networks for Multi-view Stereo. (arXiv:2112.05999v3 [cs.CV] UPDATED)
49. Dense Depth Estimation from Multiple 360-degree Images Using Virtual Depth. (arXiv:2112.14931v2 [cs.CV] UPDATED)
50. Efficient Non-Local Contrastive Attention for Image Super-Resolution. (arXiv:2201.03794v2 [cs.CV] UPDATED)
51. The Dataset Nutrition Label (2nd Gen): Leveraging Context to Mitigate Harms in Artificial Intelligence. (arXiv:2201.03954v2 [cs.LG] UPDATED)
52. Compute Trends Across Three Eras of Machine Learning. (arXiv:2202.05924v2 [cs.LG] UPDATED)
53. Trustworthy Autonomous Systems (TAS): Engaging TAS experts in curriculum design. (arXiv:2202.07447v3 [cs.CY] UPDATED)
54. Pattern Based Multivariable Regression using Deep Learning (PBMR-DP). (arXiv:2202.13541v3 [cs.CV] UPDATED)
55. Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey. (arXiv:2203.00190v3 [cs.CV] UPDATED)
56. Engineering the Neural Automatic Passenger Counter. (arXiv:2203.01156v2 [cs.LG] UPDATED)
57. A Fully Memristive Spiking Neural Network with Unsupervised Learning. (arXiv:2203.01416v2 [cs.NE] UPDATED)
58. SPICEprop: Backpropagating Errors Through Memristive Spiking Neural Networks. (arXiv:2203.01426v3 [cs.NE] UPDATED)
59. Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work. (arXiv:2203.01536v2 [cs.CV] UPDATED)
60. Piloting Diversity and Inclusion Workshops in Artificial Intelligence and Robotics for Children. (arXiv:2203.03204v2 [cs.RO] UPDATED)
61. Targeted Data Poisoning Attack on News Recommendation System by Content Perturbation. (arXiv:2203.03560v2 [cs.CR] UPDATED)
62. Investigation of Factorized Optical Flows as Mid-Level Representations. (arXiv:2203.04927v2 [cs.LG] UPDATED)

