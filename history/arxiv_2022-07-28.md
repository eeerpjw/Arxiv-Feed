# Your interest papers
---
## cs.CV
---
### Mid-level Representation **Enhancement** and Graph Embedded Uncertainty Suppressing for Facial Expression Recognition. (arXiv:2207.13235v1 [cs.CV])
- Authors : Jie Lei, Zhao Liu, Zeyu Zou, Tong Li, Xu Juan, Shuaiwei Wang, Guoyu Yang, Zunlei Feng
- Link : [http://arxiv.org/abs/2207.13235](http://arxiv.org/abs/2207.13235)
> ABSTRACT  :  Facial expression is an essential factor in conveying human emotional states and intentions. Although remarkable advancement has been made in facial expression recognition (FER) task, challenges due to large variations of expression patterns and unavoidable data uncertainties still remain. In this paper, we propose mid-level representation **enhancement** (MRE) and graph embedded uncertainty suppressing (GUS) addressing these issues. On one hand, MRE is introduced to avoid expression representation learning being dominated by a limited number of highly discriminative patterns. On the other hand, GUS is introduced to suppress the feature ambiguity in the representation space. The proposed method not only has stronger generalization capability to handle different variations of expression patterns but also more robustness to capture expression representations. Experimental evaluation on Aff-Wild2 have verified the effectiveness of the proposed method.  
### Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition. (arXiv:2207.13259v1 [cs.CV])
- Authors : Wangmeng Xiang, Chao Li, Biao Wang, Xihan Wei, Sheng Hua, **Lei Zhang**
- Link : [http://arxiv.org/abs/2207.13259](http://arxiv.org/abs/2207.13259)
> ABSTRACT  :  Transformer-based methods have recently achieved great advancement on 2D image-based vision tasks. For 3D video-based tasks such as action recognition, however, directly applying spatiotemporal transformers on video data will bring heavy computation and memory burdens due to the largely increased number of patches and the quadratic complexity of self-attention computation. How to efficiently and effectively model the 3D self-attention of video data has been a great challenge for transformers. In this paper, we propose a Temporal Patch Shift (TPS) method for efficient 3D self-attention modeling in transformers for video-based action recognition. TPS shifts part of patches with a specific mosaic pattern in the temporal dimension, thus converting a vanilla spatial self-attention operation to a spatiotemporal one with little additional cost. As a result, we can compute 3D self-attention using nearly the same computation and memory cost as 2D self-attention. TPS is a plug-and-play module and can be inserted into existing 2D transformer models to enhance spatiotemporal feature learning. The proposed method achieves competitive performance with state-of-the-arts on Something-something V1 &amp; V2, Diving-48, and Kinetics400 while being much more efficient on computation and memory cost. The source code of TPS can be found at https://github.com/MartinXM/TPS.  
### GPS-GLASS: Learning **Night**time Semantic Segmentation Using Daytime Video and GPS data. (arXiv:2207.13297v1 [cs.CV])
- Authors : Hongjae Lee, Changwoo Han, Won Jung
- Link : [http://arxiv.org/abs/2207.13297](http://arxiv.org/abs/2207.13297)
> ABSTRACT  :  Semantic segmentation for autonomous driving should be robust against various in-the-wild environments. **Night**time semantic segmentation is especially challenging due to a lack of annotated **night**time images and a large domain gap from daytime images with sufficient annotation. In this paper, we propose a novel GPS-based training framework for **night**time semantic segmentation. Given GPS-aligned pairs of daytime and **night**time images, we perform cross-domain correspondence matching to obtain pixel-level pseudo supervision. Moreover, we conduct flow estimation between daytime video frames and apply GPS-based scaling to acquire another pixel-level pseudo supervision. Using these pseudo supervisions with a confidence map, we train a **night**time semantic segmentation network without any annotation from **night**time images. Experimental results demonstrate the effectiveness of the proposed method on several **night**time semantic segmentation datasets. Our source code is available at https://github.com/jimmy9704/GPS-GLASS.  
### Is Attention All **NeRF** Needs?. (arXiv:2207.13298v1 [cs.CV])
- Authors : Mukund Varma, Peihao Wang, Xuxi Chen, Tianlong Chen, Subhashini Venugopalan, Zhangyang Wang
- Link : [http://arxiv.org/abs/2207.13298](http://arxiv.org/abs/2207.13298)
> ABSTRACT  :  We present Generalizable **NeRF** Transformer (GNT), a pure, unified transformer-based architecture that efficiently reconstructs Neural Radiance Fields (**NeRF**s) on the fly from source views. Unlike prior works on **NeRF** that optimize a per-scene implicit representation by inverting a handcrafted rendering equation, GNT achieves generalizable neural scene representation and rendering, by encapsulating two transformer-based stages. The first stage of GNT, called view transformer, leverages multi-view geometry as an inductive bias for attention-based scene representation, and predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views. The second stage of GNT, named ray transformer, renders novel views by ray marching and directly decodes the sequence of sampled point features using the attention mechanism. Our experiments demonstrate that when optimized on a single scene, GNT can successfully reconstruct **NeRF** without explicit rendering formula, and even improve the PSNR by ~1.3dB on complex scenes due to the learnable ray renderer. When trained across various scenes, GNT consistently achieves the state-of-the-art performance when transferring to forward-facing LLFF dataset (LPIPS ~20%, SSIM ~25%$) and synthetic blender dataset (LPIPS ~20%, SSIM ~4%). In addition, we show that depth and occlusion can be inferred from the learned attention maps, which implies that the pure attention mechanism is capable of learning a physically-grounded rendering process. All these results bring us one step closer to the tantalizing hope of utilizing transformers as the "universal modeling tool" even for graphics. Please refer to our project page for video results: https://vita-group.github.io/GNT/.  
### Traffic Sign Detection With Event Cameras and DCNN. (arXiv:2207.13345v1 [cs.CV])
- Authors : Piotr Wzorek, Tomasz Kryjak
- Link : [http://arxiv.org/abs/2207.13345](http://arxiv.org/abs/2207.13345)
> ABSTRACT  :  In recent years, event cameras (DVS - Dynamic Vision Sensors) have been used in vision systems as an alternative or supplement to traditional cameras. They are characterised by **high dynamic range**, high temporal resolution, low latency, and reliable performance in limited lighting conditions -- parameters that are particularly important in the context of advanced driver assistance systems (ADAS) and self-driving cars. In this work, we test whether these rather novel sensors can be applied to the popular task of traffic sign detection. To this end, we analyse different representations of the event data: event frame, event frequency, and the exponentially decaying time surface, and apply video frame reconstruction using a deep neural network called FireNet. We use the deep convolutional neural network YOLOv4 as a detector. For particular representations, we obtain a detection accuracy in the range of 86.9-88.9% mAP@0.5. The use of a fusion of the considered representations allows us to obtain a detector with higher accuracy of 89.9% mAP@0.5. In comparison, the detector for the frames reconstructed with FireNet is characterised by an accuracy of 72.67% mAP@0.5. The results obtained illustrate the potential of event cameras in automotive applications, either as standalone sensors or in close cooperation with typical frame-based cameras.  
### Efficient Video Deblurring Guided by Motion Magnitude. (arXiv:2207.13374v1 [cs.CV])
- Authors : Yusheng Wang, Yunfan Lu, Ye Gao, Lin Wang, Zhihang Zhong, Yinqiang Zheng, Atsushi Yamashita
- Link : [http://arxiv.org/abs/2207.13374](http://arxiv.org/abs/2207.13374)
> ABSTRACT  :  Video deblurring is a highly under-constrained problem due to the spatially and temporally varying blur. An intuitive approach for video deblurring includes two steps: a) detecting the blurry region in the current frame; b) utilizing the information from clear regions in adjacent frames for current frame deblurring. To realize this process, our idea is to detect the pixel-wise blur level of each frame and combine it with video deblurring. To this end, we propose a novel framework that utilizes the motion magnitude prior (MMP) as guidance for efficient deep video deblurring. Specifically, as the pixel movement along its trajectory during the **exposure** time is positively correlated to the level of motion blur, we first use the average magnitude of optical flow from the high-frequency sharp frames to generate the synthetic blurry frames and their corresponding pixel-wise motion magnitude maps. We then build a dataset including the blurry frame and MMP pairs. The MMP is then learned by a compact CNN by regression. The MMP consists of both spatial and temporal blur level information, which can be further integrated into an efficient recurrent neural network (RNN) for video deblurring. We conduct intensive experiments to validate the effectiveness of the proposed methods on the public datasets.  
### A Proper Orthogonal Decomposition approach for parameters reduction of Single Shot Detector networks. (arXiv:2207.13551v1 [cs.CV])
- Authors : Laura Meneghetti, Nicola Demo, Gianluigi Rozza
- Link : [http://arxiv.org/abs/2207.13551](http://arxiv.org/abs/2207.13551)
> ABSTRACT  :  As a major breakthrough in artificial intelligence and deep learning, Convolutional Neural Networks have achieved an impressive success in solving many problems in several fields including computer vision and image processing. **Real-time** performance, robustness of algorithms and fast training processes remain open problems in these contexts. In addition object recognition and detection are challenging tasks for resource-constrained embedded systems, commonly used in the industrial sector. To overcome these issues, we propose a dimensionality reduction framework based on Proper Orthogonal Decomposition, a classical model order reduction technique, in order to gain a reduction in the number of hyperparameters of the net. We have applied such framework to SSD300 architecture using PASCAL VOC dataset, demonstrating a reduction of the network dimension and a remarkable speedup in the fine-tuning of the network in a transfer learning context.  
### Meta-Interpolation: Time-Arbitrary Frame Interpolation via Dual Meta-Learning. (arXiv:2207.13670v1 [cs.CV])
- Authors : Shixing Yu, Yiyang Ma, **Wenhan Yang**, Wei Xiang, **Jiaying Liu**
- Link : [http://arxiv.org/abs/2207.13670](http://arxiv.org/abs/2207.13670)
> ABSTRACT  :  Existing video frame interpolation methods can only interpolate the frame at a given intermediate time-step, e.g. 1/2. In this paper, we aim to explore a more generalized kind of video frame interpolation, that at an arbitrary time-step. To this end, we consider processing different time-steps with adaptively generated convolutional kernels in a unified way with the help of meta-learning. Specifically, we develop a dual meta-learned frame interpolation framework to synthesize intermediate frames with the guidance of context information and optical flow as well as taking the time-step as side information. First, a content-aware meta-learned flow refinement module is built to improve the accuracy of the optical flow estimation based on the down-sampled version of the input frames. Second, with the refined optical flow and the time-step as the input, a motion-aware meta-learned frame interpolation module generates the convolutional kernels for every pixel used in the convolution operations on the feature map of the coarse warped version of the input frames to generate the predicted frame. Extensive qualitative and quantitative evaluations, as well as ablation studies, demonstrate that, via introducing meta-learning in our framework in such a well-designed way, our method not only achieves superior performance to state-of-the-art frame interpolation approaches but also owns an extended capacity to support the interpolation at an arbitrary time-step.  
### Stacked BNAS: Rethinking Broad Convolutional Neural Network for Neural Architecture Search. (arXiv:2111.07722v3 [cs.CV] UPDATED)
- Authors : Zixiang Ding, Yaran Chen, Nannan Li, Dongbin Zhao, Philip Chen
- Link : [http://arxiv.org/abs/2111.07722](http://arxiv.org/abs/2111.07722)
> ABSTRACT  :  Different from other deep scalable architecture-based NAS approaches, Broad Neural Architecture Search (BNAS) proposes a broad scalable architecture which consists of convolution and **enhancement** blocks, dubbed Broad Convolutional Neural Network (BCNN), as the search space for amazing efficiency improvement. BCNN reuses the topologies of cells in the convolution block so that BNAS can employ few cells for efficient search. Moreover, multi-scale feature fusion and knowledge embedding are proposed to improve the performance of BCNN with shallow topology. However, BNAS suffers some drawbacks: 1) insufficient representation diversity for feature fusion and **enhancement** and 2) time consumption of knowledge embedding design by human experts. This paper proposes Stacked BNAS, whose search space is a developed broad scalable architecture named Stacked BCNN, with better performance than BNAS. On the one hand, Stacked BCNN treats mini BCNN as a basic block to preserve comprehensive representation and deliver powerful feature extraction ability. For multi-scale feature **enhancement**, each mini BCNN feeds the outputs of deep and broad cells to the **enhancement** cell. For multi-scale feature fusion, each mini BCNN feeds the outputs of deep, broad and **enhancement** cells to the output node. On the other hand, Knowledge Embedding Search (KES) is proposed to learn appropriate knowledge embeddings in a differentiable way. Moreover, the basic unit of KES is an over-parameterized knowledge embedding module that consists of all possible candidate knowledge embeddings. Experimental results show that 1) Stacked BNAS obtains better performance than BNAS-v2 on both CIFAR-10 and ImageNet, 2) the proposed KES algorithm contributes to reducing the parameters of the learned architecture with satisfactory performance, and 3) Stacked BNAS delivers a state-of-the-art efficiency of 0.02 GPU days.  
### Multimodal Image Synthesis and Editing: A Survey. (arXiv:2112.13592v3 [cs.CV] UPDATED)
- Authors : Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian Theobalt, Eric Xing
- Link : [http://arxiv.org/abs/2112.13592](http://arxiv.org/abs/2112.13592)
> ABSTRACT  :  As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modelling the interaction among multimodal information, multimodal image synthesis and editing has become a hot research topic in recent years. Instead of providing explicit guidance for network training, multimodal guidance offers intuitive and flexible means for image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of features with inherent modality gaps, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis and editing and formulate taxonomies according to data modality and model architectures. We start with an introduction to different types of guidance modalities in image synthesis and editing. We then describe multimodal image synthesis and editing approaches extensively with detailed frameworks including Generative Adversarial Networks (GANs), Auto-regressive models, Diffusion models, Neural Radiance Fields (**NeRF**) and other methods. This is followed by a comprehensive description of benchmark datasets and corresponding evaluation metrics as widely adopted in multimodal image synthesis and editing, as well as detailed comparisons of various synthesis methods with analysis of respective advantages and limitations. Finally, we provide insights about the current research challenges and possible directions for future research. We hope this survey could lay a sound and valuable foundation for future development of multimodal image synthesis and editing. A project associated with this survey is available at https://github.com/fnzhan/MISE.  
### Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v3 [cs.CV] UPDATED)
- Authors : Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, Marc Toussaint
- Link : [http://arxiv.org/abs/2202.11855](http://arxiv.org/abs/2202.11855)
> ABSTRACT  :  We present a method to learn compositional multi-object dynamics models from image observations based on implicit object encoders, Neural Radiance Fields (**NeRF**s), and graph neural networks. **NeRF**s have become a popular choice for representing scenes due to their strong 3D prior. However, most **NeRF** approaches are trained on a single scene, representing the whole scene with a global model, making generalization to novel scenes, containing different numbers of objects, challenging. Instead, we present a compositional, object-centric auto-encoder framework that maps multiple views of the scene to a set of latent vectors representing each object separately. The latent vectors parameterize individual **NeRF**s from which the scene can be reconstructed. Based on those latent vectors, we train a graph neural network dynamics model in the latent space to achieve compositionality for dynamics prediction. A key feature of our approach is that the latent vectors are forced to encode 3D information through the **NeRF** decoder, which enables us to incorporate structural priors in learning the dynamics models, making long-term predictions more stable compared to several baselines. Simulated and real world experiments show that our method can model and learn the dynamics of compositional scenes including rigid and deformable objects. Video: https://dannydriess.github.io/compnerfdyn/  
### MPS-**NeRF**: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v2 [cs.CV] UPDATED)
- Authors : Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng, Zicheng Liu, Xin Tong
- Link : [http://arxiv.org/abs/2203.16875](http://arxiv.org/abs/2203.16875)
> ABSTRACT  :  There has been rapid progress recently on 3D human rendering, including novel view synthesis and pose animation, based on the advances of neural radiance fields (**NeRF**). However, most existing methods focus on person-specific training and their training typically requires multi-view videos. This paper deals with a new challenging task -- rendering novel views and novel poses for a person unseen in training, using only multiview images as input. For this task, we propose a simple yet effective method to train a generalizable **NeRF** with multiview images as conditional input. The key ingredient is a dedicated representation combining a canonical **NeRF** and a volume deformation scheme. Using a canonical space enables our method to learn shared properties of human and easily generalize to different people. Volume deformation is used to connect the canonical space with input and target images and query image features for radiance and density prediction. We leverage the parametric 3D human model fitted on the input images to derive the deformation, which works quite well in practice when combined with our canonical **NeRF**. The experiments on both real and synthetic data with the novel view synthesis and pose animation tasks collectively demonstrate the efficacy of our method.  
### Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v3 [cs.CV] UPDATED)
- Authors : Jiawang Bai, Li Yuan, Tao Xia, Shuicheng Yan, Zhifeng Li, Wei Liu
- Link : [http://arxiv.org/abs/2204.00993](http://arxiv.org/abs/2204.00993)
> ABSTRACT  :  The transformer models have shown promising effectiveness in dealing with various vision tasks. However, compared with training Convolutional Neural Network (CNN) models, training Vision Transformer (ViT) models is more difficult and relies on the large-scale training set. To explain this observation we make a hypothesis that \textit{ViT models are less effective in capturing the high-frequency components of images than CNN models}, and verify it by a frequency analysis. Inspired by this finding, we first investigate the effects of existing techniques for improving ViT models from a new frequency perspective, and find that the success of some techniques (e.g., RandAugment) can be attributed to the better usage of the high-frequency components. Then, to compensate for this insufficient ability of ViT models, we propose HAT, which directly augments high-frequency components of images via adversarial training. We show that HAT can consistently boost the performance of various ViT models (e.g., +1.2% for ViT-B, +0.5% for **Swin**-B), and especially enhance the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the superiority can also be maintained on out-of-distribution data and transferred to downstream tasks. The code is available at: https://github.com/jiawangbai/HAT.  
### Simple Baselines for Image **Restoration**. (arXiv:2204.04676v3 [cs.CV] UPDATED)
- Authors : Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, Jian Sun
- Link : [http://arxiv.org/abs/2204.04676](http://arxiv.org/abs/2204.04676)
> ABSTRACT  :  Although there have been significant advances in the field of image **restoration** recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that the nonlinear activation functions, e.g. Sigmoid, ReLU, GELU, Softmax, etc. are not necessary: they could be replaced by multiplication or removed. Thus, we derive a Nonlinear Activation Free Network, namely NAFNet, from the baseline. SOTA results are achieved on various challenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring), exceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs; 40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28 dB with less than half of its computational costs. The code and the pre-trained models are released at https://github.com/megvii-research/NAFNet.  
### MM-RealSR: Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v2 [cs.CV] UPDATED)
- Authors : Chong Mou, Yanze Wu, Xintao Wang, Chao Dong, Jian Zhang, Ying Shan
- Link : [http://arxiv.org/abs/2205.05065](http://arxiv.org/abs/2205.05065)
> ABSTRACT  :  Interactive image **restoration** aims to restore images by adjusting several controlling coefficients, which determine the **restoration** strength. Existing methods are restricted in learning the controllable functions under the supervision of known degradation types and levels. They usually suffer from a severe performance drop when the real degradation is different from their assumptions. Such a limitation is due to the complexity of real-world degradations, which can not provide explicit supervision to the interactive modulation during training. However, how to realize the interactive modulation in real-world super-resolution has not yet been studied. In this work, we present a Metric Learning based Interactive Modulation for Real-World Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised degradation estimation strategy to estimate the degradation level in real-world scenarios. Instead of using known degradation levels as explicit supervision to the interactive mechanism, we propose a metric learning strategy to map the unquantifiable degradation levels in real-world scenarios to a metric space, which is trained in an unsupervised manner. Moreover, we introduce an anchor point strategy in the metric learning process to normalize the distribution of metric space. Extensive experiments demonstrate that the proposed MM-RealSR achieves excellent modulation and **restoration** performance in real-world super-resolution. Codes are available at https://github.com/TencentARC/MM-RealSR.  
### VQFR: Blind Face **Restoration** with Vector-Quantized Dictionary and Parallel Decoder. (arXiv:2205.06803v3 [cs.CV] UPDATED)
- Authors : Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, Ming Cheng
- Link : [http://arxiv.org/abs/2205.06803](http://arxiv.org/abs/2205.06803)
> ABSTRACT  :  Although generative facial prior and geometric prior have recently demonstrated high-quality results for blind face **restoration**, producing fine-grained facial details faithful to inputs remains a challenging problem. Motivated by the classical dictionary-based methods and the recent vector quantization (VQ) technique, we propose a VQ-based face **restoration** method - VQFR. VQFR takes advantage of high-quality low-level feature banks extracted from high-quality faces and can thus help recover realistic facial details. However, the simple application of the VQ codebook cannot achieve good results with faithful details and identity preservation. Therefore, we further introduce two special network designs. 1). We first investigate the compression patch size in the VQ codebook and find that the VQ codebook designed with a proper compression patch size is crucial to balance the quality and fidelity. 2). To further fuse low-level features from inputs while not "contaminating" the realistic details generated from the VQ codebook, we proposed a parallel decoder consisting of a texture decoder and a main decoder. Those two decoders then interact with a texture warping module with deformable convolution. Equipped with the VQ codebook as a facial detail dictionary and the parallel decoder design, the proposed VQFR can largely enhance the restored quality of facial details while keeping the fidelity to previous methods.  
## eess.IV
---
### Fast optical refocusing through multimode fiber bend using Cake-Cutting Hadamard encoding algorithm to improve robustness. (arXiv:2207.13334v1 [physics.optics])
- Authors : Chuncheng Zhang, Zheyi Yao, Zhengyue Qin, Guohua Gu, Qian Chen, Zhihua Xie, Guodong Liu, Xiubao Sui
- Link : [http://arxiv.org/abs/2207.13334](http://arxiv.org/abs/2207.13334)
> ABSTRACT  :  Multimode fibres offer the advantages of high resolution and miniaturization over single mode fibers in the field of optical imaging. However, multimode fibre's imaging is susceptible to perturbations of MMF that can lead to secondary spatial distortions in the transmitted image. Perturbations include random disturbances in the fiber as well as environmental noise. Here, we exploit the fast focusing capability of the Cake-Cutting Hadamard coding algorithm to counteract the effects of perturbations and improve the system's robustness. Simulation shows that it can approach the theoretical **enhancement** at 2000 measurements. Experimental results show that the algorithm can help the system to refocus in a short time when MMFs are perturbed. This research will further contribute to using multimode fibres in medicine, communication, and detection.  
### Traffic Sign Detection With Event Cameras and DCNN. (arXiv:2207.13345v1 [cs.CV])
- Authors : Piotr Wzorek, Tomasz Kryjak
- Link : [http://arxiv.org/abs/2207.13345](http://arxiv.org/abs/2207.13345)
> ABSTRACT  :  In recent years, event cameras (DVS - Dynamic Vision Sensors) have been used in vision systems as an alternative or supplement to traditional cameras. They are characterised by **high dynamic range**, high temporal resolution, low latency, and reliable performance in limited lighting conditions -- parameters that are particularly important in the context of advanced driver assistance systems (ADAS) and self-driving cars. In this work, we test whether these rather novel sensors can be applied to the popular task of traffic sign detection. To this end, we analyse different representations of the event data: event frame, event frequency, and the exponentially decaying time surface, and apply video frame reconstruction using a deep neural network called FireNet. We use the deep convolutional neural network YOLOv4 as a detector. For particular representations, we obtain a detection accuracy in the range of 86.9-88.9% mAP@0.5. The use of a fusion of the considered representations allows us to obtain a detector with higher accuracy of 89.9% mAP@0.5. In comparison, the detector for the frames reconstructed with FireNet is characterised by an accuracy of 72.67% mAP@0.5. The results obtained illustrate the potential of event cameras in automotive applications, either as standalone sensors or in close cooperation with typical frame-based cameras.  
### MM-RealSR: Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v2 [cs.CV] UPDATED)
- Authors : Chong Mou, Yanze Wu, Xintao Wang, Chao Dong, Jian Zhang, Ying Shan
- Link : [http://arxiv.org/abs/2205.05065](http://arxiv.org/abs/2205.05065)
> ABSTRACT  :  Interactive image **restoration** aims to restore images by adjusting several controlling coefficients, which determine the **restoration** strength. Existing methods are restricted in learning the controllable functions under the supervision of known degradation types and levels. They usually suffer from a severe performance drop when the real degradation is different from their assumptions. Such a limitation is due to the complexity of real-world degradations, which can not provide explicit supervision to the interactive modulation during training. However, how to realize the interactive modulation in real-world super-resolution has not yet been studied. In this work, we present a Metric Learning based Interactive Modulation for Real-World Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised degradation estimation strategy to estimate the degradation level in real-world scenarios. Instead of using known degradation levels as explicit supervision to the interactive mechanism, we propose a metric learning strategy to map the unquantifiable degradation levels in real-world scenarios to a metric space, which is trained in an unsupervised manner. Moreover, we introduce an anchor point strategy in the metric learning process to normalize the distribution of metric space. Extensive experiments demonstrate that the proposed MM-RealSR achieves excellent modulation and **restoration** performance in real-world super-resolution. Codes are available at https://github.com/TencentARC/MM-RealSR.  
## cs.LG
---
### Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition. (arXiv:2207.13259v1 [cs.CV])
- Authors : Wangmeng Xiang, Chao Li, Biao Wang, Xihan Wei, Sheng Hua, **Lei Zhang**
- Link : [http://arxiv.org/abs/2207.13259](http://arxiv.org/abs/2207.13259)
> ABSTRACT  :  Transformer-based methods have recently achieved great advancement on 2D image-based vision tasks. For 3D video-based tasks such as action recognition, however, directly applying spatiotemporal transformers on video data will bring heavy computation and memory burdens due to the largely increased number of patches and the quadratic complexity of self-attention computation. How to efficiently and effectively model the 3D self-attention of video data has been a great challenge for transformers. In this paper, we propose a Temporal Patch Shift (TPS) method for efficient 3D self-attention modeling in transformers for video-based action recognition. TPS shifts part of patches with a specific mosaic pattern in the temporal dimension, thus converting a vanilla spatial self-attention operation to a spatiotemporal one with little additional cost. As a result, we can compute 3D self-attention using nearly the same computation and memory cost as 2D self-attention. TPS is a plug-and-play module and can be inserted into existing 2D transformer models to enhance spatiotemporal feature learning. The proposed method achieves competitive performance with state-of-the-arts on Something-something V1 &amp; V2, Diving-48, and Kinetics400 while being much more efficient on computation and memory cost. The source code of TPS can be found at https://github.com/MartinXM/TPS.  
### A Proper Orthogonal Decomposition approach for parameters reduction of Single Shot Detector networks. (arXiv:2207.13551v1 [cs.CV])
- Authors : Laura Meneghetti, Nicola Demo, Gianluigi Rozza
- Link : [http://arxiv.org/abs/2207.13551](http://arxiv.org/abs/2207.13551)
> ABSTRACT  :  As a major breakthrough in artificial intelligence and deep learning, Convolutional Neural Networks have achieved an impressive success in solving many problems in several fields including computer vision and image processing. **Real-time** performance, robustness of algorithms and fast training processes remain open problems in these contexts. In addition object recognition and detection are challenging tasks for resource-constrained embedded systems, commonly used in the industrial sector. To overcome these issues, we propose a dimensionality reduction framework based on Proper Orthogonal Decomposition, a classical model order reduction technique, in order to gain a reduction in the number of hyperparameters of the net. We have applied such framework to SSD300 architecture using PASCAL VOC dataset, demonstrating a reduction of the network dimension and a remarkable speedup in the fine-tuning of the network in a transfer learning context.  
### Efficient Personalized Speech **Enhancement** through Self-Supervised Learning. (arXiv:2104.02017v2 [eess.AS] UPDATED)
- Authors : Aswin Sivaraman, Minje Kim
- Link : [http://arxiv.org/abs/2104.02017](http://arxiv.org/abs/2104.02017)
> ABSTRACT  :  This work presents self-supervised learning methods for developing monaural speaker-specific (i.e., personalized) speech **enhancement** models. While generalist models must broadly address many speakers, specialist models can adapt their **enhancement** function towards a particular speaker's voice, expecting to solve a narrower problem. Hence, specialists are capable of achieving more optimal performance in addition to reducing computational complexity. However, naive personalization methods can require clean speech from the target user, which is inconvenient to acquire, e.g., due to subpar recording conditions. To this end, we pose personalization as either a zero-shot task, in which no additional clean speech of the target speaker is used for training, or a few-shot learning task, in which the goal is to minimize the duration of the clean speech used for transfer learning. With this paper, we propose self-supervised learning methods as a solution to both zero- and few-shot personalization tasks. The proposed methods are designed to learn the personalized speech features from unlabeled data (i.e., in-the-wild noisy recordings from the target user) without knowing the corresponding clean sources. Our experiments investigate three different self-supervised learning mechanisms. The results show that self-supervised models achieve zero-shot and few-shot personalization using fewer model parameters and less clean data from the target user, achieving the data efficiency and model compression goals.  
### Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v3 [cs.CV] UPDATED)
- Authors : Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, Marc Toussaint
- Link : [http://arxiv.org/abs/2202.11855](http://arxiv.org/abs/2202.11855)
> ABSTRACT  :  We present a method to learn compositional multi-object dynamics models from image observations based on implicit object encoders, Neural Radiance Fields (**NeRF**s), and graph neural networks. **NeRF**s have become a popular choice for representing scenes due to their strong 3D prior. However, most **NeRF** approaches are trained on a single scene, representing the whole scene with a global model, making generalization to novel scenes, containing different numbers of objects, challenging. Instead, we present a compositional, object-centric auto-encoder framework that maps multiple views of the scene to a set of latent vectors representing each object separately. The latent vectors parameterize individual **NeRF**s from which the scene can be reconstructed. Based on those latent vectors, we train a graph neural network dynamics model in the latent space to achieve compositionality for dynamics prediction. A key feature of our approach is that the latent vectors are forced to encode 3D information through the **NeRF** decoder, which enables us to incorporate structural priors in learning the dynamics models, making long-term predictions more stable compared to several baselines. Simulated and real world experiments show that our method can model and learn the dynamics of compositional scenes including rigid and deformable objects. Video: https://dannydriess.github.io/compnerfdyn/  
## cs.AI
---
### Mid-level Representation **Enhancement** and Graph Embedded Uncertainty Suppressing for Facial Expression Recognition. (arXiv:2207.13235v1 [cs.CV])
- Authors : Jie Lei, Zhao Liu, Zeyu Zou, Tong Li, Xu Juan, Shuaiwei Wang, Guoyu Yang, Zunlei Feng
- Link : [http://arxiv.org/abs/2207.13235](http://arxiv.org/abs/2207.13235)
> ABSTRACT  :  Facial expression is an essential factor in conveying human emotional states and intentions. Although remarkable advancement has been made in facial expression recognition (FER) task, challenges due to large variations of expression patterns and unavoidable data uncertainties still remain. In this paper, we propose mid-level representation **enhancement** (MRE) and graph embedded uncertainty suppressing (GUS) addressing these issues. On one hand, MRE is introduced to avoid expression representation learning being dominated by a limited number of highly discriminative patterns. On the other hand, GUS is introduced to suppress the feature ambiguity in the representation space. The proposed method not only has stronger generalization capability to handle different variations of expression patterns but also more robustness to capture expression representations. Experimental evaluation on Aff-Wild2 have verified the effectiveness of the proposed method.  
### Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition. (arXiv:2207.13259v1 [cs.CV])
- Authors : Wangmeng Xiang, Chao Li, Biao Wang, Xihan Wei, Sheng Hua, **Lei Zhang**
- Link : [http://arxiv.org/abs/2207.13259](http://arxiv.org/abs/2207.13259)
> ABSTRACT  :  Transformer-based methods have recently achieved great advancement on 2D image-based vision tasks. For 3D video-based tasks such as action recognition, however, directly applying spatiotemporal transformers on video data will bring heavy computation and memory burdens due to the largely increased number of patches and the quadratic complexity of self-attention computation. How to efficiently and effectively model the 3D self-attention of video data has been a great challenge for transformers. In this paper, we propose a Temporal Patch Shift (TPS) method for efficient 3D self-attention modeling in transformers for video-based action recognition. TPS shifts part of patches with a specific mosaic pattern in the temporal dimension, thus converting a vanilla spatial self-attention operation to a spatiotemporal one with little additional cost. As a result, we can compute 3D self-attention using nearly the same computation and memory cost as 2D self-attention. TPS is a plug-and-play module and can be inserted into existing 2D transformer models to enhance spatiotemporal feature learning. The proposed method achieves competitive performance with state-of-the-arts on Something-something V1 &amp; V2, Diving-48, and Kinetics400 while being much more efficient on computation and memory cost. The source code of TPS can be found at https://github.com/MartinXM/TPS.  
### MPS-**NeRF**: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v2 [cs.CV] UPDATED)
- Authors : Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng, Zicheng Liu, Xin Tong
- Link : [http://arxiv.org/abs/2203.16875](http://arxiv.org/abs/2203.16875)
> ABSTRACT  :  There has been rapid progress recently on 3D human rendering, including novel view synthesis and pose animation, based on the advances of neural radiance fields (**NeRF**). However, most existing methods focus on person-specific training and their training typically requires multi-view videos. This paper deals with a new challenging task -- rendering novel views and novel poses for a person unseen in training, using only multiview images as input. For this task, we propose a simple yet effective method to train a generalizable **NeRF** with multiview images as conditional input. The key ingredient is a dedicated representation combining a canonical **NeRF** and a volume deformation scheme. Using a canonical space enables our method to learn shared properties of human and easily generalize to different people. Volume deformation is used to connect the canonical space with input and target images and query image features for radiance and density prediction. We leverage the parametric 3D human model fitted on the input images to derive the deformation, which works quite well in practice when combined with our canonical **NeRF**. The experiments on both real and synthetic data with the novel view synthesis and pose animation tasks collectively demonstrate the efficacy of our method.  
# Paper List
---
## cs.CV
---
**122** new papers in cs.CV:-) 
1. LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity. (arXiv:2207.13129v1 [cs.LG])
2. Bayesian Evidential Learning for Few-Shot Classification. (arXiv:2207.13137v1 [cs.CV])
3. Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining. (arXiv:2207.13148v1 [eess.IV])
4. TINYCD: A (Not So) Deep Learning Model For Change Detection. (arXiv:2207.13159v1 [cs.CV])
5. Retrieval-Augmented Transformer for Image Captioning. (arXiv:2207.13162v1 [cs.CV])
6. YOLO and Mask R-CNN for Vehicle Number Plate Identification. (arXiv:2207.13165v1 [cs.CV])
7. SAR-to-EO Image Translation with Multi-Conditional Adversarial Networks. (arXiv:2207.13184v1 [cs.CV])
8. Learning-Based Keypoint Registration for Fetoscopic Mosaicking. (arXiv:2207.13185v1 [eess.IV])
9. Deep Model-Based Architectures for Inverse Problems under Mismatched Priors. (arXiv:2207.13200v1 [eess.IV])
10. Point-McBert: A Multi-choice Self-supervised Framework for Point Cloud Pre-training. (arXiv:2207.13226v1 [cs.CV])
11. Mid-level Representation **Enhancement** and Graph Embedded Uncertainty Suppressing for Facial Expression Recognition. (arXiv:2207.13235v1 [cs.CV])
12. Contrastive Image Synthesis and Self-supervised Feature Adaptation for Cross-Modality Biomedical Image Segmentation. (arXiv:2207.13240v1 [cs.CV])
13. Uncertainty-based Visual Question Answering: Estimating Semantic Inconsistency between Image and Knowledge Base. (arXiv:2207.13242v1 [cs.CV])
14. Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v1 [cs.LG])
15. Concurrent Subsidiary Supervision for Unsupervised Source-Free Domain Adaptation. (arXiv:2207.13247v1 [cs.CV])
16. AADG: Automatic Augmentation for Domain Generalization on Retinal Image Segmentation. (arXiv:2207.13249v1 [eess.IV])
17. Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition. (arXiv:2207.13259v1 [cs.CV])
18. Instance-specific 6-DoF Object Pose Estimation from Minimal Annotations. (arXiv:2207.13264v1 [cs.CV])
19. Fault Detection and Classification of Aerospace Sensors using a VGG16-based Deep Neural Network. (arXiv:2207.13267v1 [cs.CV])
20. End-to-end Graph-constrained Vectorized Floorplan Generation with Panoptic Refinement. (arXiv:2207.13268v1 [cs.CV])
21. Vector Quantized Image-to-Image Translation. (arXiv:2207.13286v1 [cs.CV])
22. Applied Computer Vision on 2-Dimensional Lung X-Ray Images for Assisted Medical Diagnosis of Pneumonia. (arXiv:2207.13295v1 [eess.IV])
23. GPS-GLASS: Learning **Night**time Semantic Segmentation Using Daytime Video and GPS data. (arXiv:2207.13297v1 [cs.CV])
24. Is Attention All **NeRF** Needs?. (arXiv:2207.13298v1 [cs.CV])
25. Object-ABN: Learning to Generate Sharp Attention Maps for Action Recognition. (arXiv:2207.13306v1 [cs.CV])
26. Federated Selective Aggregation for Knowledge Amalgamation. (arXiv:2207.13309v1 [cs.CV])
27. Portrait Interpretation and a Benchmark. (arXiv:2207.13315v1 [cs.CV])
28. NICEST: Noisy Label Correction and Training for Robust Scene Graph Generation. (arXiv:2207.13316v1 [cs.CV])
29. Convolutional Embedding Makes Hierarchical Vision Transformer Stronger. (arXiv:2207.13317v1 [cs.CV])
30. Generator Knows What Discriminator Should Learn in Unconditional GANs. (arXiv:2207.13320v1 [cs.CV])
31. DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking. (arXiv:2207.13321v1 [cs.CR])
32. SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding. (arXiv:2207.13325v1 [cs.CV])
33. Point Cloud Attacks in Graph Spectral Domain: When 3D Geometry Meets Graph Signal Processing. (arXiv:2207.13326v1 [cs.CV])
34. Two-Stream UNET Networks for Semantic Segmentation in Medical Images. (arXiv:2207.13337v1 [cs.CV])
35. ALBench: A Framework for Evaluating Active Learning in Object Detection. (arXiv:2207.13339v1 [cs.CV])
36. PointFix: Learning to Fix Domain Bias for Robust Online Stereo Adaptation. (arXiv:2207.13340v1 [cs.CV])
37. Inverse Airborne Optical Sectioning. (arXiv:2207.13344v1 [cs.CV])
38. Traffic Sign Detection With Event Cameras and DCNN. (arXiv:2207.13345v1 [cs.CV])
39. One-Trimap Video Matting. (arXiv:2207.13353v1 [cs.CV])
40. Learning Appearance-motion Normality for Video Anomaly Detection. (arXiv:2207.13361v1 [cs.CV])
41. Camouflaged Object Detection via Context-aware Cross-level Fusion. (arXiv:2207.13362v1 [cs.CV])
42. Deep Clustering with Features from Self-Supervised Pretraining. (arXiv:2207.13364v1 [cs.CV])
43. Optimizing transformations for contrastive learning in a differentiable framework. (arXiv:2207.13367v1 [cs.LG])
44. Efficient Video Deblurring Guided by Motion Magnitude. (arXiv:2207.13374v1 [cs.CV])
45. Identifying Hard Noise in Long-Tailed Sample Distribution. (arXiv:2207.13378v1 [cs.CV])
46. Look Closer to Your Enemy: Learning to Attack via Teacher-student Mimicking. (arXiv:2207.13381v1 [cs.CV])
47. Statistical Keystroke Synthesis for Improved Bot Detection. (arXiv:2207.13394v1 [cs.LG])
48. Post-Train Adaptive MobileNet for Fast Anti-Spoofing. (arXiv:2207.13410v1 [cs.CV])
49. TransNorm: Transformer Provides a Strong Spatial Normalization Mechanism for a Deep Segmentation Model. (arXiv:2207.13415v1 [cs.CV])
50. Hardly Perceptible Trojan Attack against Neural Networks with Bit Flips. (arXiv:2207.13417v1 [cs.CV])
51. Rethinking Efficacy of Softmax for Lightweight Non-Local Neural Networks. (arXiv:2207.13423v1 [cs.CV])
52. Efficient Pix2Vox++ for 3D Cardiac Reconstruction from 2D echo views. (arXiv:2207.13424v1 [eess.IV])
53. Leveraging GAN Priors for Few-Shot Part Segmentation. (arXiv:2207.13428v1 [cs.CV])
54. Concept Drift Challenge in Multimedia Anomaly Detection: A Case Study with Facial Datasets. (arXiv:2207.13430v1 [cs.CV])
55. End-To-End Audiovisual Feature Fusion for Active Speaker Detection. (arXiv:2207.13434v1 [cs.SD])
56. Iterative Scene Graph Generation. (arXiv:2207.13440v1 [cs.CV])
57. Skimming, Locating, then Perusing: A Human-Like Framework for Natural Language Video Localization. (arXiv:2207.13450v1 [cs.CV])
58. Reducing the Vision and Language Bias for Temporal Sentence Grounding. (arXiv:2207.13457v1 [cs.CV])
59. VICTOR: Visual Incompatibility Detection with Transformers and Fashion-specific contrastive pre-training. (arXiv:2207.13458v1 [cs.CV])
60. Adaptive sampling for scanning pixel cameras. (arXiv:2207.13460v1 [cs.CV])
61. Towards the Probabilistic Fusion of Learned Priors into Standard Pipelines for 3D Reconstruction. (arXiv:2207.13464v1 [cs.CV])
62. PASTA-GAN++: A Versatile Framework for High-Resolution Unpaired Virtual Try-on. (arXiv:2207.13475v1 [cs.CV])
63. AutoTransition: Learning to Recommend Video Transition Effects. (arXiv:2207.13479v1 [cs.CV])
64. Time to augment contrastive learning. (arXiv:2207.13492v1 [cs.LG])
65. Generalizable multi-task, multi-domain deep segmentation of sparse pediatric imaging datasets via multi-scale contrastive regularization and multi-joint anatomical priors. (arXiv:2207.13502v1 [eess.IV])
66. Multi-Forgery Detection Challenge 2022: Push the Frontier of Unconstrained and Diverse Forgery Detection. (arXiv:2207.13505v1 [cs.CV])
67. Satellite Image Based Cross-view Localization for Autonomous Vehicle. (arXiv:2207.13506v1 [cs.CV])
68. Online Continual Learning with Contrastive Vision Transformer. (arXiv:2207.13516v1 [cs.LG])
69. Future Unruptured Intracranial Aneurysm Growth Prediction using Mesh Convolutional Neural Networks. (arXiv:2207.13518v1 [eess.IV])
70. Contrastive Masked Autoencoders are Stronger Vision Learners. (arXiv:2207.13532v1 [cs.CV])
71. Abstracting Sketches through Simple Primitives. (arXiv:2207.13543v1 [cs.CV])
72. A Proper Orthogonal Decomposition approach for parameters reduction of Single Shot Detector networks. (arXiv:2207.13551v1 [cs.CV])
73. D3C2-Net: Dual-Domain Deep Convolutional Coding Network for Compressive Sensing. (arXiv:2207.13560v1 [cs.CV])
74. Lightweight and Progressively-Scalable Networks for Semantic Segmentation. (arXiv:2207.13600v1 [cs.CV])
75. Neural Radiance Transfer Fields for Relightable Novel-view Synthesis with Global Illumination. (arXiv:2207.13607v1 [cs.CV])
76. A Semi-automatic Cell Tracking Process Towards Completing the 4D Atlas of C. elegans Development. (arXiv:2207.13611v1 [cs.CV])
77. Using Deep Learning to Detecting Deepfakes. (arXiv:2207.13644v1 [cs.CV])
78. Meta-Interpolation: Time-Arbitrary Frame Interpolation via Dual Meta-Learning. (arXiv:2207.13670v1 [cs.CV])
79. Multi-layer Representation Learning for Robust OOD Image Classification. (arXiv:2207.13678v1 [cs.CV])
80. Shift-tolerant Perceptual Similarity Metric. (arXiv:2207.13686v1 [cs.CV])
81. ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization. (arXiv:2207.13691v1 [cs.CV])
82. Human Gait Database for Normal Walk Collected by Smartphone Accelerometer. (arXiv:1905.03109v4 [eess.SP] UPDATED)
83. Spectral Analysis for Semantic Segmentation with Applications on Feature Truncation and Weak Annotation. (arXiv:2012.14123v4 [cs.CV] UPDATED)
84. Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS Imagery. (arXiv:2103.12434v3 [cs.CV] UPDATED)
85. Kernel Adversarial Learning for Real-world Image Super-resolution. (arXiv:2104.09008v2 [cs.CV] UPDATED)
86. A new perspective on the approximation capability of GNNs. (arXiv:2106.08992v4 [cs.LG] UPDATED)
87. Scalable Certified Segmentation via Randomized Smoothing. (arXiv:2107.00228v2 [cs.LG] UPDATED)
88. Stacked BNAS: Rethinking Broad Convolutional Neural Network for Neural Architecture Search. (arXiv:2111.07722v3 [cs.CV] UPDATED)
89. UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling. (arXiv:2111.12085v2 [cs.CV] UPDATED)
90. PTQ4ViT: Post-Training Quantization Framework for Vision Transformers with Twin Uniform Quantization. (arXiv:2111.12293v2 [cs.CV] UPDATED)
91. Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v3 [cs.LG] UPDATED)
92. Pose Representations for Deep Skeletal Animation. (arXiv:2111.13907v2 [cs.CV] UPDATED)
93. Extract Free Dense Labels from CLIP. (arXiv:2112.01071v2 [cs.CV] UPDATED)
94. How to Synthesize a Large-Scale and Trainable Micro-Expression Dataset?. (arXiv:2112.01730v5 [cs.CV] UPDATED)
95. Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval. (arXiv:2112.01832v3 [cs.MM] UPDATED)
96. Neural Style Transfer and Unpaired Image-to-Image Translation to deal with the Domain Shift Problem on Spheroid Segmentation. (arXiv:2112.09043v2 [cs.CV] UPDATED)
97. Multimodal Image Synthesis and Editing: A Survey. (arXiv:2112.13592v3 [cs.CV] UPDATED)
98. Adversarial Imitation Learning from Video using a State Observer. (arXiv:2202.00243v2 [cs.RO] UPDATED)
99. Mining the manifolds of deep generative models for multiple data-consistent solutions of ill-posed tomographic imaging problems. (arXiv:2202.05311v2 [eess.IV] UPDATED)
100. Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v3 [cs.CV] UPDATED)
101. Object discovery and representation networks. (arXiv:2203.08777v3 [cs.CV] UPDATED)
102. High-fidelity GAN Inversion with Padding Space. (arXiv:2203.11105v2 [cs.CV] UPDATED)
103. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v3 [cs.LG] UPDATED)
104. FedVLN: Privacy-preserving Federated Vision-and-Language Navigation. (arXiv:2203.14936v2 [cs.AI] UPDATED)
105. MPS-**NeRF**: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v2 [cs.CV] UPDATED)
106. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v4 [cs.CV] UPDATED)
107. Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v3 [cs.CV] UPDATED)
108. Co-Teaching for Unsupervised Domain Adaptation and Expansion. (arXiv:2204.01210v2 [cs.CV] UPDATED)
109. TALLFormer: Temporal Action Localization with a Long-memory Transformer. (arXiv:2204.01680v2 [cs.CV] UPDATED)
110. Simple Baselines for Image **Restoration**. (arXiv:2204.04676v3 [cs.CV] UPDATED)
111. MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v3 [cs.CV] UPDATED)
112. TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v3 [cs.CV] UPDATED)
113. MM-RealSR: Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v2 [cs.CV] UPDATED)
114. VQFR: Blind Face **Restoration** with Vector-Quantized Dictionary and Parallel Decoder. (arXiv:2205.06803v3 [cs.CV] UPDATED)
115. Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v4 [cs.CV] UPDATED)
116. SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning. (arXiv:2207.03677v2 [cs.CV] UPDATED)
117. PseudoClick: Interactive Image Segmentation with Click Imitation. (arXiv:2207.05282v2 [cs.CV] UPDATED)
118. DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2207.10271v2 [cs.CV] UPDATED)
119. Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation. (arXiv:2207.11860v2 [cs.CV] UPDATED)
120. PTGCF: Printing Texture Guided Color Fusion for Impressionism Oil Painting Style Rendering. (arXiv:2207.12585v2 [cs.CV] UPDATED)
121. Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis. (arXiv:2207.13064v2 [cs.CV] UPDATED)
122. Improving Generalization of Batch Whitening by Convolutional Unit Optimization. (arXiv:2108.10629v2 [cs.CV] CROSS LISTED)
## eess.IV
---
**27** new papers in eess.IV:-) 
1. Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining. (arXiv:2207.13148v1 [eess.IV])
2. TINYCD: A (Not So) Deep Learning Model For Change Detection. (arXiv:2207.13159v1 [cs.CV])
3. SAR-to-EO Image Translation with Multi-Conditional Adversarial Networks. (arXiv:2207.13184v1 [cs.CV])
4. Learning-Based Keypoint Registration for Fetoscopic Mosaicking. (arXiv:2207.13185v1 [eess.IV])
5. Inverse-designed Metastructures Together with Reconfigurable Couplers to Compute Forward Scattering. (arXiv:2207.13189v1 [physics.optics])
6. Deep Model-Based Architectures for Inverse Problems under Mismatched Priors. (arXiv:2207.13200v1 [eess.IV])
7. XADLiME: eXplainable Alzheimer's Disease Likelihood Map Estimation via Clinically-guided Prototype Learning. (arXiv:2207.13223v1 [cs.LG])
8. AADG: Automatic Augmentation for Domain Generalization on Retinal Image Segmentation. (arXiv:2207.13249v1 [eess.IV])
9. Applied Computer Vision on 2-Dimensional Lung X-Ray Images for Assisted Medical Diagnosis of Pneumonia. (arXiv:2207.13295v1 [eess.IV])
10. Regularity-constrained Fast Sine Transforms. (arXiv:2207.13301v1 [eess.SP])
11. Arbitrary unitary rotation of three-dimensional pixellated images. (arXiv:2207.13308v1 [physics.comp-ph])
12. Point Cloud Attacks in Graph Spectral Domain: When 3D Geometry Meets Graph Signal Processing. (arXiv:2207.13326v1 [cs.CV])
13. Fast optical refocusing through multimode fiber bend using Cake-Cutting Hadamard encoding algorithm to improve robustness. (arXiv:2207.13334v1 [physics.optics])
14. Inverse Airborne Optical Sectioning. (arXiv:2207.13344v1 [cs.CV])
15. Traffic Sign Detection With Event Cameras and DCNN. (arXiv:2207.13345v1 [cs.CV])
16. TransNorm: Transformer Provides a Strong Spatial Normalization Mechanism for a Deep Segmentation Model. (arXiv:2207.13415v1 [cs.CV])
17. Efficient Pix2Vox++ for 3D Cardiac Reconstruction from 2D echo views. (arXiv:2207.13424v1 [eess.IV])
18. Generalizable multi-task, multi-domain deep segmentation of sparse pediatric imaging datasets via multi-scale contrastive regularization and multi-joint anatomical priors. (arXiv:2207.13502v1 [eess.IV])
19. Future Unruptured Intracranial Aneurysm Growth Prediction using Mesh Convolutional Neural Networks. (arXiv:2207.13518v1 [eess.IV])
20. A Hybrid Deep Animation Codec for Low-bitrate Video Conferencing. (arXiv:2207.13530v1 [cs.MM])
21. D3C2-Net: Dual-Domain Deep Convolutional Coding Network for Compressive Sensing. (arXiv:2207.13560v1 [cs.CV])
22. Human Gait Database for Normal Walk Collected by Smartphone Accelerometer. (arXiv:1905.03109v4 [eess.SP] UPDATED)
23. Neural Style Transfer and Unpaired Image-to-Image Translation to deal with the Domain Shift Problem on Spheroid Segmentation. (arXiv:2112.09043v2 [cs.CV] UPDATED)
24. Mining the manifolds of deep generative models for multiple data-consistent solutions of ill-posed tomographic imaging problems. (arXiv:2202.05311v2 [eess.IV] UPDATED)
25. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v4 [cs.CV] UPDATED)
26. MM-RealSR: Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v2 [cs.CV] UPDATED)
27. Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation. (arXiv:2207.11860v2 [cs.CV] UPDATED)
## cs.LG
---
**116** new papers in cs.LG:-) 
1. VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations. (arXiv:2207.13091v1 [cs.GR])
2. Analysis and Design of Quadratic Neural Networks for Regression, Classification, and Lyapunov Control of Dynamical Systems. (arXiv:2207.13120v1 [cs.LG])
3. The Sample Complexity of Forecast Aggregation. (arXiv:2207.13126v1 [cs.LG])
4. LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity. (arXiv:2207.13129v1 [cs.LG])
5. Semi-analytical Industrial Cooling System Model for Reinforcement Learning. (arXiv:2207.13131v1 [cs.AI])
6. TINYCD: A (Not So) Deep Learning Model For Change Detection. (arXiv:2207.13159v1 [cs.CV])
7. One Simple Trick to Fix Your Bayesian Neural Network. (arXiv:2207.13167v1 [stat.ML])
8. Initial Orbit Determination for the CR3BP using Particle Swarm Optimization. (arXiv:2207.13175v1 [physics.comp-ph])
9. Sliced Wasserstein Variational Inference. (arXiv:2207.13177v1 [stat.ML])
10. Unsupervised Learning under Latent Label Shift. (arXiv:2207.13179v1 [cs.LG])
11. On Missing Labels, Long-tails and Propensities in Extreme Multi-label Classification. (arXiv:2207.13186v1 [cs.LG])
12. GCN-WP -- Semi-Supervised Graph Convolutional Networks for Win Prediction in Esports. (arXiv:2207.13191v1 [cs.LG])
13. Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception. (arXiv:2207.13192v1 [cs.SD])
14. Deep Model-Based Architectures for Inverse Problems under Mismatched Priors. (arXiv:2207.13200v1 [eess.IV])
15. Information We Can Extract About a User From 'One Minute Mobile Application Usage'. (arXiv:2207.13222v1 [cs.LG])
16. XADLiME: eXplainable Alzheimer's Disease Likelihood Map Estimation via Clinically-guided Prototype Learning. (arXiv:2207.13223v1 [cs.LG])
17. PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations. (arXiv:2207.13224v1 [cs.RO])
18. Atomic structure generation from reconstructing structural fingerprints. (arXiv:2207.13227v1 [cond-mat.mtrl-sci])
19. Uncertainty-based Visual Question Answering: Estimating Semantic Inconsistency between Image and Knowledge Base. (arXiv:2207.13242v1 [cs.CV])
20. Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v1 [cs.LG])
21. Concurrent Subsidiary Supervision for Unsupervised Source-Free Domain Adaptation. (arXiv:2207.13247v1 [cs.CV])
22. Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition. (arXiv:2207.13259v1 [cs.CV])
23. Fault Detection and Classification of Aerospace Sensors using a VGG16-based Deep Neural Network. (arXiv:2207.13267v1 [cs.CV])
24. Learning from Positive and Unlabeled Data with Augmented Classes. (arXiv:2207.13274v1 [cs.LG])
25. INTERACT: Achieving Low Sample and Communication Complexities in Decentralized Bilevel Learning over Networks. (arXiv:2207.13283v1 [cs.LG])
26. Detecting Concept Drift in the Presence of Sparsity -- A Case Study of Automated Change Risk Assessment System. (arXiv:2207.13287v1 [cs.LG])
27. Learning the Evolution of Correlated Stochastic Power System Dynamics. (arXiv:2207.13310v1 [cs.LG])
28. JDRec: Practical Actor-Critic Framework for Online Combinatorial Recommender System. (arXiv:2207.13311v1 [cs.IR])
29. Should Bank Stress Tests Be Fair?. (arXiv:2207.13319v1 [stat.ML])
30. DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking. (arXiv:2207.13321v1 [cs.CR])
31. Gaia: Graph Neural Network with Temporal Shift aware Attention for Gross Merchandise Value Forecast in E-commerce. (arXiv:2207.13329v1 [cs.LG])
32. PointFix: Learning to Fix Domain Bias for Robust Online Stereo Adaptation. (arXiv:2207.13340v1 [cs.CV])
33. Towards Clear Expectations for Uncertainty Estimation. (arXiv:2207.13341v1 [cs.LG])
34. Towards Soft Fairness in Restless Multi-Armed Bandits. (arXiv:2207.13343v1 [cs.LG])
35. Deep Clustering with Features from Self-Supervised Pretraining. (arXiv:2207.13364v1 [cs.CV])
36. Optimizing transformations for contrastive learning in a differentiable framework. (arXiv:2207.13367v1 [cs.LG])
37. Statistical Keystroke Synthesis for Improved Bot Detection. (arXiv:2207.13394v1 [cs.LG])
38. Post-Train Adaptive MobileNet for Fast Anti-Spoofing. (arXiv:2207.13410v1 [cs.CV])
39. Time Series Forecasting Models Copy the Past: How to Mitigate. (arXiv:2207.13441v1 [cs.LG])
40. Dynamic Shielding for Reinforcement Learning in Black-Box Environments. (arXiv:2207.13446v1 [cs.LG])
41. Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms. (arXiv:2207.13453v1 [cs.LG])
42. Time to augment contrastive learning. (arXiv:2207.13492v1 [cs.LG])
43. Learning with Combinatorial Optimization Layers: a Probabilistic Approach. (arXiv:2207.13513v1 [stat.ML])
44. Online Continual Learning with Contrastive Vision Transformer. (arXiv:2207.13516v1 [cs.LG])
45. A Variational AutoEncoder for Transformers with Nonparametric Variational Information Bottleneck. (arXiv:2207.13529v1 [cs.LG])
46. Learned Label Aggregation for Weak Supervision. (arXiv:2207.13545v1 [cs.LG])
47. A Proper Orthogonal Decomposition approach for parameters reduction of Single Shot Detector networks. (arXiv:2207.13551v1 [cs.CV])
48. D3C2-Net: Dual-Domain Deep Convolutional Coding Network for Compressive Sensing. (arXiv:2207.13560v1 [cs.CV])
49. Correlations Between COVID-19 and Dengue. (arXiv:2207.13561v1 [q-bio.PE])
50. Membership Inference Attacks via Adversarial Examples. (arXiv:2207.13572v1 [cs.LG])
51. Encoding Concepts in Graph Neural Networks. (arXiv:2207.13586v1 [cs.LG])
52. Fairness and Randomness in Machine Learning: Statistical Independence and Relativization. (arXiv:2207.13596v1 [cs.LG])
53. Using Deep Learning to Detecting Deepfakes. (arXiv:2207.13644v1 [cs.CV])
54. Do Quantum Circuit Born Machines Generalize?. (arXiv:2207.13645v1 [quant-ph])
55. Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes. (arXiv:2207.13649v1 [cs.LG])
56. Visualizing Confidence Intervals for Critical Point Probabilities in 2D Scalar Field Ensembles. (arXiv:2207.13661v1 [cs.HC])
57. Unsupervised Training for Neural TSP Solver. (arXiv:2207.13667v1 [cs.LG])
58. Fast expansion into harmonics on the disk: a steerable basis with fast radial convolutions. (arXiv:2207.13674v1 [math.NA])
59. Open Source Vizier: Distributed Infrastructure and API for Reliable and Flexible Blackbox Optimization. (arXiv:2207.13676v1 [cs.LG])
60. Multi-layer Representation Learning for Robust OOD Image Classification. (arXiv:2207.13678v1 [cs.CV])
61. ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization. (arXiv:2207.13691v1 [cs.CV])
62. Deep Partial Updating: Towards Communication Efficient Updating for On-device Inference. (arXiv:2007.03071v3 [cs.LG] UPDATED)
63. The Computational Limits of Deep Learning. (arXiv:2007.05558v2 [cs.LG] UPDATED)
64. Buffer Pool Aware Query Scheduling via Deep Reinforcement Learning. (arXiv:2007.10568v3 [cs.DB] UPDATED)
65. Handling Hard Affine SDP Shape Constraints in RKHSs. (arXiv:2101.01519v2 [stat.ML] UPDATED)
66. TINKER: A framework for Open source Cyberthreat Intelligence. (arXiv:2102.05571v5 [cs.CR] UPDATED)
67. The Implications of the No-Free-Lunch Theorems for Meta-induction. (arXiv:2103.11956v3 [cs.LG] UPDATED)
68. Efficient Personalized Speech **Enhancement** through Self-Supervised Learning. (arXiv:2104.02017v2 [eess.AS] UPDATED)
69. A hybrid ensemble method with negative correlation learning for regression. (arXiv:2104.02317v3 [cs.LG] UPDATED)
70. Intelligent Zero Trust Architecture for 5G/6G Networks: Principles, Challenges, and the Role of Machine Learning in the context of O-RAN. (arXiv:2105.01478v3 [cs.NI] UPDATED)
71. The Randomness of Input Data Spaces is an A Priori Predictor for Generalization. (arXiv:2106.04181v2 [cs.LG] UPDATED)
72. A new perspective on the approximation capability of GNNs. (arXiv:2106.08992v4 [cs.LG] UPDATED)
73. Cascade Decoders-Based Autoencoders for Image Reconstruction. (arXiv:2107.00002v2 [cs.LG] UPDATED)
74. Scalable Certified Segmentation via Randomized Smoothing. (arXiv:2107.00228v2 [cs.LG] UPDATED)
75. Accelerating the Learning of TAMER with Counterfactual Explanations. (arXiv:2108.01358v2 [cs.AI] UPDATED)
76. Fast TreeSHAP: Accelerating SHAP Value Computation for Trees. (arXiv:2109.09847v3 [cs.LG] UPDATED)
77. Understanding Convolutional Neural Networks from Volterra Convolution Perspective. (arXiv:2110.09902v2 [cs.LG] UPDATED)
78. Towards noise robust trigger-word detection with contrastive learning pre-task for fast on-boarding of new trigger-words. (arXiv:2111.03971v3 [cs.SD] UPDATED)
79. Reasonable Effectiveness of Random Weighting: A Litmus Test for Multi-Task Learning. (arXiv:2111.10603v2 [cs.LG] UPDATED)
80. Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v3 [cs.LG] UPDATED)
81. Neural Style Transfer and Unpaired Image-to-Image Translation to deal with the Domain Shift Problem on Spheroid Segmentation. (arXiv:2112.09043v2 [cs.CV] UPDATED)
82. Representation Learning for Dynamic Hyperedges. (arXiv:2112.10154v2 [cs.LG] UPDATED)
83. Graph Neural Networks for Communication Networks: Context, Use Cases and Opportunities. (arXiv:2112.14792v2 [cs.NI] UPDATED)
84. On generalization bounds for deep networks based on loss surface implicit regularization. (arXiv:2201.04545v2 [stat.ML] UPDATED)
85. Adversarial Imitation Learning from Video using a State Observer. (arXiv:2202.00243v2 [cs.RO] UPDATED)
86. Transporters with Visual Foresight for Solving Unseen Rearrangement Tasks. (arXiv:2202.10765v3 [cs.RO] UPDATED)
87. Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v3 [cs.CV] UPDATED)
88. Statistically Efficient Advantage Learning for Offline Reinforcement Learning in Infinite Horizons. (arXiv:2202.13163v2 [stat.ML] UPDATED)
89. Object discovery and representation networks. (arXiv:2203.08777v3 [cs.CV] UPDATED)
90. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v3 [cs.LG] UPDATED)
91. FedVLN: Privacy-preserving Federated Vision-and-Language Navigation. (arXiv:2203.14936v2 [cs.AI] UPDATED)
92. Accurate detection of sepsis at ED triage using machine learning with clinical natural language processing. (arXiv:2204.07657v3 [cs.LG] UPDATED)
93. Dynamical simulation via quantum machine learning with provable generalization. (arXiv:2204.10269v2 [quant-ph] UPDATED)
94. Faster online calibration without randomization: interval forecasts and the power of two choices. (arXiv:2204.13087v2 [cs.LG] UPDATED)
95. TracInAD: Measuring Influence for Anomaly Detection. (arXiv:2205.01362v3 [cs.LG] UPDATED)
96. ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks. (arXiv:2205.08119v2 [cs.LG] UPDATED)
97. Time Series Anomaly Detection via Reinforcement Learning-Based Model Selection. (arXiv:2205.09884v4 [cs.LG] UPDATED)
98. Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v4 [cs.CV] UPDATED)
99. Evaluation of creating scoring opportunities for teammates in soccer via trajectory prediction. (arXiv:2206.01899v3 [cs.AI] UPDATED)
100. Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation. (arXiv:2206.02368v2 [cs.CL] UPDATED)
101. Efficient Resource Allocation with Fairness Constraints in Restless Multi-Armed Bandits. (arXiv:2206.03883v2 [cs.LG] UPDATED)
102. Exploring Representation of Horn Clauses using GNNs (Extended Technical Report). (arXiv:2206.06986v4 [cs.AI] UPDATED)
103. Multi-Objective Hyperparameter Optimization -- An Overview. (arXiv:2206.07438v2 [cs.LG] UPDATED)
104. Bioinspired random projections for robust, sparse classification. (arXiv:2206.09222v2 [stat.ML] UPDATED)
105. A generalized regionalization framework for geographical modelling and its application in spatial regression. (arXiv:2206.09429v2 [stat.ME] UPDATED)
106. CEN : Cooperatively Evolving Networks. (arXiv:2207.02192v4 [cs.LG] UPDATED)
107. SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning. (arXiv:2207.03677v2 [cs.CV] UPDATED)
108. Robust Newsvendor Problem in Global Market: Stable Operation Strategy for a Two-Market Stochastic System. (arXiv:2207.03801v2 [cs.LG] UPDATED)
109. GT4SD: Generative Toolkit for Scientific Discovery. (arXiv:2207.03928v2 [cs.LG] UPDATED)
110. Emergence of Novelty in Evolutionary Algorithms. (arXiv:2207.04857v2 [cs.NE] UPDATED)
111. A Meta-learning Formulation of the Autoencoder Problem for Non-linear Dimensionality Reduction. (arXiv:2207.06676v2 [cs.LG] UPDATED)
112. Understanding Non-linearity in Graph Neural Networks from the Bayesian-Inference Perspective. (arXiv:2207.11311v2 [cs.LG] UPDATED)
113. Thermal half-lives of azobenzene derivatives: virtual screening based on intersystem crossing using a machine learning potential. (arXiv:2207.11592v2 [physics.chem-ph] UPDATED)
114. BPFISH: Blockchain and Privacy-preserving FL Inspired Smart Healthcare. (arXiv:2207.11654v2 [cs.NI] UPDATED)
115. Improved and Interpretable Defense to Transferred Adversarial Examples by Jacobian Norm with Selective Input Gradient Regularization. (arXiv:2207.13036v2 [cs.LG] UPDATED)
116. Fixed-Time Convergence for a Class of Nonconvex-Nonconcave Min-Max Problems. (arXiv:2207.12845v1 [math.OC] CROSS LISTED)
## cs.AI
---
**58** new papers in cs.AI:-) 
1. VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations. (arXiv:2207.13091v1 [cs.GR])
2. Semi-analytical Industrial Cooling System Model for Reinforcement Learning. (arXiv:2207.13131v1 [cs.AI])
3. Retrieval-Augmented Transformer for Image Captioning. (arXiv:2207.13162v1 [cs.CV])
4. Evaluation of key impression of resilient supply chain based on artificial intelligence of things (AIoT). (arXiv:2207.13174v1 [eess.SY])
5. Planning and Learning: A Review of Methods involving Path-Planning for Autonomous Vehicles. (arXiv:2207.13181v1 [cs.AI])
6. Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception. (arXiv:2207.13192v1 [cs.SD])
7. PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations. (arXiv:2207.13224v1 [cs.RO])
8. Mid-level Representation **Enhancement** and Graph Embedded Uncertainty Suppressing for Facial Expression Recognition. (arXiv:2207.13235v1 [cs.CV])
9. Uncertainty-based Visual Question Answering: Estimating Semantic Inconsistency between Image and Knowledge Base. (arXiv:2207.13242v1 [cs.CV])
10. Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v1 [cs.LG])
11. Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition. (arXiv:2207.13259v1 [cs.CV])
12. Sparse Deep Neural Network for Nonlinear Partial Differential Equations. (arXiv:2207.13266v1 [math.NA])
13. Marker and source-marker reprogramming of Most Permissive Boolean networks and ensembles with BoNesis. (arXiv:2207.13307v1 [eess.SY])
14. JDRec: Practical Actor-Critic Framework for Online Combinatorial Recommender System. (arXiv:2207.13311v1 [cs.IR])
15. Convolutional Embedding Makes Hierarchical Vision Transformer Stronger. (arXiv:2207.13317v1 [cs.CV])
16. Towards Soft Fairness in Restless Multi-Armed Bandits. (arXiv:2207.13343v1 [cs.LG])
17. Evolutionary Multiparty Distance Minimization. (arXiv:2207.13390v1 [cs.NE])
18. Emergent social NPC interactions in the Social NPCs Skyrim mod and beyond. (arXiv:2207.13398v1 [cs.AI])
19. Rethinking Efficacy of Softmax for Lightweight Non-Local Neural Networks. (arXiv:2207.13423v1 [cs.CV])
20. Efficient Pix2Vox++ for 3D Cardiac Reconstruction from 2D echo views. (arXiv:2207.13424v1 [eess.IV])
21. A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation. (arXiv:2207.13438v1 [cs.RO])
22. Time Series Forecasting Models Copy the Past: How to Mitigate. (arXiv:2207.13441v1 [cs.LG])
23. Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms. (arXiv:2207.13453v1 [cs.LG])
24. Online Continual Learning with Contrastive Vision Transformer. (arXiv:2207.13516v1 [cs.LG])
25. Future Unruptured Intracranial Aneurysm Growth Prediction using Mesh Convolutional Neural Networks. (arXiv:2207.13518v1 [eess.IV])
26. Abstracting Sketches through Simple Primitives. (arXiv:2207.13543v1 [cs.CV])
27. Membership Inference Attacks via Adversarial Examples. (arXiv:2207.13572v1 [cs.LG])
28. Towards the Neuroevolution of Low-level Artificial General Intelligence. (arXiv:2207.13583v1 [cs.AI])
29. Encoding Concepts in Graph Neural Networks. (arXiv:2207.13586v1 [cs.LG])
30. Using Deep Learning to Detecting Deepfakes. (arXiv:2207.13644v1 [cs.CV])
31. Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes. (arXiv:2207.13649v1 [cs.LG])
32. Causal foundations of bias, disparity and fairness. (arXiv:2207.13665v1 [cs.DL])
33. TINKER: A framework for Open source Cyberthreat Intelligence. (arXiv:2102.05571v5 [cs.CR] UPDATED)
34. Active Inference Tree Search in Large POMDPs. (arXiv:2103.13860v2 [cs.AI] UPDATED)
35. Accelerating the Learning of TAMER with Counterfactual Explanations. (arXiv:2108.01358v2 [cs.AI] UPDATED)
36. Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v3 [cs.LG] UPDATED)
37. Adversarial Imitation Learning from Video using a State Observer. (arXiv:2202.00243v2 [cs.RO] UPDATED)
38. Malleable Agents for Re-Configurable Robotic Manipulators. (arXiv:2202.02395v2 [cs.RO] UPDATED)
39. Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v6 [cs.CL] UPDATED)
40. Transporters with Visual Foresight for Solving Unseen Rearrangement Tasks. (arXiv:2202.10765v3 [cs.RO] UPDATED)
41. Object discovery and representation networks. (arXiv:2203.08777v3 [cs.CV] UPDATED)
42. Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v3 [cs.LG] UPDATED)
43. FedVLN: Privacy-preserving Federated Vision-and-Language Navigation. (arXiv:2203.14936v2 [cs.AI] UPDATED)
44. MPS-**NeRF**: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v2 [cs.CV] UPDATED)
45. MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v3 [cs.CV] UPDATED)
46. TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v3 [cs.CV] UPDATED)
47. ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks. (arXiv:2205.08119v2 [cs.LG] UPDATED)
48. Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v4 [cs.CV] UPDATED)
49. Evaluation of creating scoring opportunities for teammates in soccer via trajectory prediction. (arXiv:2206.01899v3 [cs.AI] UPDATED)
50. Exploring Representation of Horn Clauses using GNNs (Extended Technical Report). (arXiv:2206.06986v4 [cs.AI] UPDATED)
51. GT4SD: Generative Toolkit for Scientific Discovery. (arXiv:2207.03928v2 [cs.LG] UPDATED)
52. Emergence of Novelty in Evolutionary Algorithms. (arXiv:2207.04857v2 [cs.NE] UPDATED)
53. FLDetector: Defending Federated Learning Against Model Poisoning Attacks via Detecting Malicious Clients. (arXiv:2207.09209v3 [cs.CR] UPDATED)
54. XInsight: eXplainable Data Analysis Through The Lens of Causality. (arXiv:2207.12718v2 [cs.DB] UPDATED)
55. Improved and Interpretable Defense to Transferred Adversarial Examples by Jacobian Norm with Selective Input Gradient Regularization. (arXiv:2207.13036v2 [cs.LG] UPDATED)
56. Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis. (arXiv:2207.13064v2 [cs.CV] UPDATED)
57. Improving Generalization of Batch Whitening by Convolutional Unit Optimization. (arXiv:2108.10629v2 [cs.CV] CROSS LISTED)
58. Fixed-Time Convergence for a Class of Nonconvex-Nonconcave Min-Max Problems. (arXiv:2207.12845v1 [math.OC] CROSS LISTED)

