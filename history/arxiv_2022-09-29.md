# Your interest papers
---
## cs.CV
---
### Streaming Video Temporal Action Segmentation In Real Time. (arXiv:2209.13808v1 [cs.CV])
- Authors : Wujun Wen, Yunheng Li, Zhuben Dong, Lin Feng, Wanxiao Yang, Shenlan Liu
- Link : [http://arxiv.org/abs/2209.13808](http://arxiv.org/abs/2209.13808)
> ABSTRACT  :  Temporal action segmentation (TAS) is a critical step toward long-term video understanding. Recent studies follow a pattern that builds models based on features instead of raw video picture information. However, we claim those models are trained complicatedly and limit application scenarios. It is hard for them to segment human actions of video in **real time** because they must work after the full video features are extracted. As the real-time action segmentation task is different from TAS task, we define it as streaming video real-time temporal action segmentation (SVTAS) task. In this paper, we propose a real-time end-to-end multi-modality model for SVTAS task. More specifically, under the circumstances that we cannot get any future information, we segment the current human action of streaming video chunk in **real time**. Furthermore, the model we propose combines the last steaming video chunk feature extracted by language model with the current image feature extracted by image model to improve the quantity of real-time temporal action segmentation. To the best of our knowledge, it is the first multi-modality real-time temporal action segmentation model. Under the same evaluation criteria as full video temporal action segmentation, our model segments human action in **real time** with less than 40% of state-of-the-art model computation and achieves 90% of the accuracy of the full video state-of-the-art model.  
### Learning Deep Representations via Contrastive Learning for Instance Retrieval. (arXiv:2209.13832v1 [cs.CV])
- Authors : Tao Wu, Tie Luo, Donald Wunsch
- Link : [http://arxiv.org/abs/2209.13832](http://arxiv.org/abs/2209.13832)
> ABSTRACT  :  Instance-level Image Retrieval (IIR), or simply Instance Retrieval, deals with the problem of finding all the images within an dataset that contain a query instance (e.g. an object). This paper makes the first attempt that tackles this problem using instance-discrimination based contrastive learning (CL). While CL has shown impressive performance for many computer vision tasks, the similar success has never been found in the field of IIR. In this work, we approach this problem by exploring the capability of deriving discriminative representations from pre-trained and fine-tuned CL models. To begin with, we investigate the efficacy of transfer learning in IIR, by comparing off-the-shelf features learned by a pre-trained deep neural network (DNN) classifier with features learned by a CL model. The findings inspired us to propose a new training strategy that optimizes CL towards learning IIR-oriented features, by using an Average Precision (AP) loss together with a fine-tuning method to learn contrastive feature representations that are tailored to IIR. Our empirical evaluation demonstrates significant performance **enhancement** over the off-the-shelf features learned from a pre-trained DNN classifier on the challenging Oxford and Paris datasets.  
### Thinking Hallucination for Video Captioning. (arXiv:2209.13853v1 [cs.CV])
- Authors : Nasib Ullah, Partha Pratim
- Link : [http://arxiv.org/abs/2209.13853](http://arxiv.org/abs/2209.13853)
> ABSTRACT  :  With the advent of rich visual representations and pre-trained language models, video captioning has seen continuous improvement over time. Despite the performance improvement, video captioning models are prone to hallucination. Hallucination refers to the generation of highly pathological descriptions that are detached from the source material. In video captioning, there are two kinds of hallucination: object and action hallucination. Instead of endeavoring to learn better representations of a video, in this work, we investigate the fundamental sources of the hallucination problem. We identify three main factors: (i) inadequate visual features extracted from pre-trained models, (ii) improper influences of source and target contexts during multi-modal fusion, and (iii) **exposure** bias in the training strategy. To alleviate these problems, we propose two robust solutions: (a) the introduction of auxiliary heads trained in multi-label settings on top of the extracted visual features and (b) the addition of context gates, which dynamically select the features during fusion. The standard evaluation metrics for video captioning measures similarity with ground truth captions and do not adequately capture object and action relevance. To this end, we propose a new metric, COAHA (caption object and action hallucination assessment), which assesses the degree of hallucination. Our method achieves state-of-the-art performance on the MSR-Video to Text (MSR-VTT) and the Microsoft Research Video Description Corpus (MSVD) datasets, especially by a massive margin in CIDEr score.  
### Strong Instance Segmentation Pipeline for MMSports Challenge. (arXiv:2209.13899v1 [cs.CV])
- Authors : Bo Yan, Fengliang Qi, Zhuang Li, Yadong Li, Hongbin Wang
- Link : [http://arxiv.org/abs/2209.13899](http://arxiv.org/abs/2209.13899)
> ABSTRACT  :  The goal of ACM MMSports2022 DeepSportRadar Instance Segmentation Challenge is to tackle the segmentation of individual humans including players, coaches and referees on a basketball court. And the main characteristics of this challenge are there is a high level of occlusions between players and the amount of data is quite limited. In order to address these problems, we designed a strong instance segmentation pipeline. Firstly, we employed a proper data augmentation strategy for this task mainly including photometric distortion transform and copy-paste strategy, which can generate more image instances with a wider distribution. Secondly, we employed a strong segmentation model, Hybrid Task Cascade based detector on the **Swin**-Base based CBNetV2 backbone, and we add MaskIoU head to HTCMaskHead that can simply and effectively improve the performance of instance segmentation. Finally, the SWA training strategy was applied to improve the performance further. Experimental results demonstrate the proposed pipeline can achieve a competitive result on the DeepSportRadar challenge, with 0.768AP@0.50:0.95 on the challenge set. Source code is available at https://github.com/YJingyu/Instanc_Segmentation_Pro.  
### DPNet: Dual-Path Network for **Real-time** Object Detection with Lightweight Attention. (arXiv:2209.13933v1 [cs.CV])
- Authors : Quan Zhou, Huimin Shi, Weikang Xiang, Bin Kang, Xiaofu Wu, Longin Jan
- Link : [http://arxiv.org/abs/2209.13933](http://arxiv.org/abs/2209.13933)
> ABSTRACT  :  The recent advances of compressing high-accuracy convolution neural networks (CNNs) have witnessed remarkable progress for real-time object detection. To accelerate detection speed, lightweight detectors always have few convolution layers using single-path backbone. Single-path architecture, however, involves continuous pooling and downsampling operations, always resulting in coarse and inaccurate feature maps that are disadvantageous to locate objects. On the other hand, due to limited network capacity, recent lightweight networks are often weak in representing large scale visual data. To address these problems, this paper presents a dual-path network, named DPNet, with a lightweight attention scheme for real-time object detection. The dual-path architecture enables us to parallelly extract high-level semantic features and low-level object details. Although DPNet has nearly duplicated shape with respect to single-path detectors, the computational costs and model size are not significantly increased. To enhance representation capability, a lightweight self-correlation module (LSCM) is designed to capture global interactions, with only few computational overheads and network parameters. In neck, LSCM is extended into a lightweight crosscorrelation module (LCCM), capturing mutual dependencies among neighboring scale features. We have conducted exhaustive experiments on MS COCO and Pascal VOC 2007 datasets. The experimental results demonstrate that DPNet achieves state-of the-art trade-off between detection accuracy and implementation efficiency. Specifically, DPNet achieves 30.5% AP on MS COCO test-dev and 81.5% mAP on Pascal VOC 2007 test set, together mwith nearly 2.5M model size, 1.04 GFLOPs, and 164 FPS and 196 FPS for 320 x 320 input images of two datasets.  
### PTSD in the Wild: A Video Database for Studying Post-Traumatic Stress Disorder Recognition in Unconstrained Environments. (arXiv:2209.14085v1 [cs.HC])
- Authors : Moctar Abdoul, Latif Sawadogo, Furkan Pala, Gurkirat Singh, Imen Selmi, Pauline Puteaux, Alice Othmani
- Link : [http://arxiv.org/abs/2209.14085](http://arxiv.org/abs/2209.14085)
> ABSTRACT  :  POST-traumatic stress disorder (PTSD) is a chronic and debilitating mental condition that is developed in response to catastrophic life events, such as military combat, sexual assault, and natural disasters. PTSD is characterized by flashbacks of past traumatic events, intrusive thoughts, **night**mares, hypervigilance, and sleep disturbance, all of which affect a person's life and lead to considerable social, occupational, and interpersonal dysfunction. The diagnosis of PTSD is done by medical professionals using self-assessment questionnaire of PTSD symptoms as defined in the Diagnostic and Statistical Manual of Mental Disorders (DSM). In this paper, and for the first time, we collected, annotated, and prepared for public distribution a new video database for automatic PTSD diagnosis, called PTSD in the wild dataset. The database exhibits "natural" and big variability in acquisition conditions with different pose, facial expression, lighting, focus, resolution, age, gender, race, occlusions and background. In addition to describing the details of the dataset collection, we provide a benchmark for evaluating computer vision and machine learning based approaches on PTSD in the wild dataset. In addition, we propose and we evaluate a deep learning based approach for PTSD detection in respect to the given benchmark. The proposed approach shows very promising results. Interested researcher can download a copy of PTSD-in-the wild dataset from: <a href="http://www.lissi.fr/PTSD-Dataset/">this http URL</a>  
### CALIP: Zero-Shot **Enhancement** of CLIP with Parameter-free Attention. (arXiv:2209.14169v1 [cs.CV])
- Authors : Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, Bin Cui
- Link : [http://arxiv.org/abs/2209.14169](http://arxiv.org/abs/2209.14169)
> ABSTRACT  :  Contrastive Language-Image Pre-training (CLIP) has been shown to learn visual representations with great transferability, which achieves promising accuracy for zero-shot classification. To further improve its downstream performance, existing works propose additional learnable modules upon CLIP and fine-tune them by few-shot training sets. However, the resulting extra training cost and data requirement severely hinder the efficiency for model deployment and knowledge transfer. In this paper, we introduce a free-lunch **enhancement** method, CALIP, to boost CLIP's zero-shot performance via a parameter-free Attention module. Specifically, we guide visual and textual representations to interact with each other and explore cross-modal informative features via attention. As the pre-training has largely reduced the embedding distances between two modalities, we discard all learnable parameters in the attention and bidirectionally update the multi-modal features, enabling the whole process to be parameter-free and training-free. In this way, the images are blended with textual-aware signals and the text representations become visual-guided for better adaptive zero-shot alignment. We evaluate CALIP on various benchmarks of 14 datasets for both 2D image and 3D point cloud few-shot classification, showing consistent zero-shot performance improvement over CLIP. Based on that, we further insert a small number of linear layers in CALIP's attention module and verify our robustness under the few-shot settings, which also achieves leading performance compared to existing methods. Those extensive experiments demonstrate the superiority of our approach for efficient **enhancement** of CLIP.  
## eess.IV
---
## cs.LG
---
### SGTM 2.0: Autonomously Untangling Long Cables using Interactive Perception. (arXiv:2209.13706v1 [cs.RO])
- Authors : Kaushik Shivakumar, Vainavi Viswanath, Anrui Gu, Yahav Avigal, Justin Kerr, Jeffrey Ichnowski, Richard Cheng, Thomas Kollar, Ken Goldberg
- Link : [http://arxiv.org/abs/2209.13706](http://arxiv.org/abs/2209.13706)
> ABSTRACT  :  Cables are commonplace in homes, hospitals, and industrial warehouses and are prone to tangling. This paper extends prior work on autonomously untangling long cables by introducing novel uncertainty quantification metrics and actions that interact with the cable to reduce perception uncertainty. We present Sliding and Grasping for Tangle Manipulation 2.0 (SGTM 2.0), a system that autonomously untangles cables approximately 3 meters in length with a **bilateral** robot using estimates of uncertainty at each step to inform actions. By interactively reducing uncertainty, Sliding and Grasping for Tangle Manipulation 2.0 (SGTM 2.0) reduces the number of state-resetting moves it must take, significantly speeding up run-time. Experiments suggest that SGTM 2.0 can achieve 83% untangling success on cables with 1 or 2 overhand and figure-8 knots, and 70% termination detection success across these configurations, outperforming SGTM 1.0 by 43% in untangling accuracy and 200% in full rollout speed. Supplementary material, visualizations, and videos can be found at sites.google.com/view/sgtm2.  
### Label Distribution Learning via Implicit Distribution Representation. (arXiv:2209.13824v1 [cs.LG])
- Authors : Zhuoran Zheng, Xiuyi Jia
- Link : [http://arxiv.org/abs/2209.13824](http://arxiv.org/abs/2209.13824)
> ABSTRACT  :  In contrast to multi-label learning, label distribution learning characterizes the polysemy of examples by a label distribution to represent richer semantics. In the learning process of label distribution, the training data is collected mainly by manual annotation or label **enhancement** algorithms to generate label distribution. Unfortunately, the complexity of the manual annotation task or the inaccuracy of the label **enhancement** algorithm leads to noise and uncertainty in the label distribution training set. To alleviate this problem, we introduce the implicit distribution in the label distribution learning framework to characterize the uncertainty of each label value. Specifically, we use deep implicit representation learning to construct a label distribution matrix with Gaussian prior constraints, where each row component corresponds to the distribution estimate of each label value, and this row component is constrained by a prior Gaussian distribution to moderate the noise and uncertainty interference of the label distribution dataset. Finally, each row component of the label distribution matrix is transformed into a standard label distribution form by using the self-attention algorithm. In addition, some approaches with regularization characteristics are conducted in the training phase to improve the performance of the model.  
### Learning Deep Representations via Contrastive Learning for Instance Retrieval. (arXiv:2209.13832v1 [cs.CV])
- Authors : Tao Wu, Tie Luo, Donald Wunsch
- Link : [http://arxiv.org/abs/2209.13832](http://arxiv.org/abs/2209.13832)
> ABSTRACT  :  Instance-level Image Retrieval (IIR), or simply Instance Retrieval, deals with the problem of finding all the images within an dataset that contain a query instance (e.g. an object). This paper makes the first attempt that tackles this problem using instance-discrimination based contrastive learning (CL). While CL has shown impressive performance for many computer vision tasks, the similar success has never been found in the field of IIR. In this work, we approach this problem by exploring the capability of deriving discriminative representations from pre-trained and fine-tuned CL models. To begin with, we investigate the efficacy of transfer learning in IIR, by comparing off-the-shelf features learned by a pre-trained deep neural network (DNN) classifier with features learned by a CL model. The findings inspired us to propose a new training strategy that optimizes CL towards learning IIR-oriented features, by using an Average Precision (AP) loss together with a fine-tuning method to learn contrastive feature representations that are tailored to IIR. Our empirical evaluation demonstrates significant performance **enhancement** over the off-the-shelf features learned from a pre-trained DNN classifier on the challenging Oxford and Paris datasets.  
### PTSD in the Wild: A Video Database for Studying Post-Traumatic Stress Disorder Recognition in Unconstrained Environments. (arXiv:2209.14085v1 [cs.HC])
- Authors : Moctar Abdoul, Latif Sawadogo, Furkan Pala, Gurkirat Singh, Imen Selmi, Pauline Puteaux, Alice Othmani
- Link : [http://arxiv.org/abs/2209.14085](http://arxiv.org/abs/2209.14085)
> ABSTRACT  :  POST-traumatic stress disorder (PTSD) is a chronic and debilitating mental condition that is developed in response to catastrophic life events, such as military combat, sexual assault, and natural disasters. PTSD is characterized by flashbacks of past traumatic events, intrusive thoughts, **night**mares, hypervigilance, and sleep disturbance, all of which affect a person's life and lead to considerable social, occupational, and interpersonal dysfunction. The diagnosis of PTSD is done by medical professionals using self-assessment questionnaire of PTSD symptoms as defined in the Diagnostic and Statistical Manual of Mental Disorders (DSM). In this paper, and for the first time, we collected, annotated, and prepared for public distribution a new video database for automatic PTSD diagnosis, called PTSD in the wild dataset. The database exhibits "natural" and big variability in acquisition conditions with different pose, facial expression, lighting, focus, resolution, age, gender, race, occlusions and background. In addition to describing the details of the dataset collection, we provide a benchmark for evaluating computer vision and machine learning based approaches on PTSD in the wild dataset. In addition, we propose and we evaluate a deep learning based approach for PTSD detection in respect to the given benchmark. The proposed approach shows very promising results. Interested researcher can download a copy of PTSD-in-the wild dataset from: <a href="http://www.lissi.fr/PTSD-Dataset/">this http URL</a>  
### Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v7 [cs.CL] UPDATED)
- Authors : Terrence Sejnowski
- Link : [http://arxiv.org/abs/2207.14382](http://arxiv.org/abs/2207.14382)
> ABSTRACT  :  Large Language Models (LLMs) have been transformative. They are pre-trained foundational models that are self-supervised and can be adapted with fine tuning to a wide ranger of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and more recently LaMDA can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a Reverse Turing Test. If so, then by studying interviews we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable they may transform the way we interact with machines and how they interact with each other, but several **enhancement**s are needed for autonomy. LLMs can talk the talk, but can they walk the walk?  
## cs.AI
---
### Intercepting A Flying Target While Avoiding Moving Obstacles: A Unified Control Framework With Deep Manifold Learning. (arXiv:2209.13628v1 [cs.RO])
- Authors : Apan Dastider, Mingjie Lin
- Link : [http://arxiv.org/abs/2209.13628](http://arxiv.org/abs/2209.13628)
> ABSTRACT  :  **Real-time** interception of a fast-moving object by a robotic arm in cluttered environments filled with static or dynamic obstacles permits only tens of milliseconds for reaction times, hence quite challenging and arduous for state-of-the-art robotic planning algorithms to perform multiple robotic skills, for instance, catching the dynamic object and avoiding obstacles, in parallel. This paper proposes an unified framework of robotic path planning through embedding the high-dimensional temporal information contained in the event stream to distinguish between safe and colliding trajectories into a low-dimension space manifested with a pre-constructed 2D densely connected graph. We then leverage a fast graph-traversing strategy to generate the motor commands necessary to effectively avoid the approaching obstacles while simultaneously intercepting a fast-moving objects. The most distinctive feature of our methodology is to conduct both object interception and obstacle avoidance within the same algorithm framework based on deep manifold learning. By leveraging a highly efficient diffusion-map based variational autoencoding and Extended Kalman Filter(EKF), we demonstrate the effectiveness of our approach on an autonomous 7-DoF robotic arm using only onboard sensing and computation. Our robotic manipulator was capable of avoiding multiple obstacles of different sizes and shapes while successfully capturing a fast-moving soft ball thrown by hand at normal speed in different angles. Complete video demonstrations of our experiments can be found in https://sites.google.com/view/multirobotskill/home.  
### SGTM 2.0: Autonomously Untangling Long Cables using Interactive Perception. (arXiv:2209.13706v1 [cs.RO])
- Authors : Kaushik Shivakumar, Vainavi Viswanath, Anrui Gu, Yahav Avigal, Justin Kerr, Jeffrey Ichnowski, Richard Cheng, Thomas Kollar, Ken Goldberg
- Link : [http://arxiv.org/abs/2209.13706](http://arxiv.org/abs/2209.13706)
> ABSTRACT  :  Cables are commonplace in homes, hospitals, and industrial warehouses and are prone to tangling. This paper extends prior work on autonomously untangling long cables by introducing novel uncertainty quantification metrics and actions that interact with the cable to reduce perception uncertainty. We present Sliding and Grasping for Tangle Manipulation 2.0 (SGTM 2.0), a system that autonomously untangles cables approximately 3 meters in length with a **bilateral** robot using estimates of uncertainty at each step to inform actions. By interactively reducing uncertainty, Sliding and Grasping for Tangle Manipulation 2.0 (SGTM 2.0) reduces the number of state-resetting moves it must take, significantly speeding up run-time. Experiments suggest that SGTM 2.0 can achieve 83% untangling success on cables with 1 or 2 overhand and figure-8 knots, and 70% termination detection success across these configurations, outperforming SGTM 1.0 by 43% in untangling accuracy and 200% in full rollout speed. Supplementary material, visualizations, and videos can be found at sites.google.com/view/sgtm2.  
### CALIP: Zero-Shot **Enhancement** of CLIP with Parameter-free Attention. (arXiv:2209.14169v1 [cs.CV])
- Authors : Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, Bin Cui
- Link : [http://arxiv.org/abs/2209.14169](http://arxiv.org/abs/2209.14169)
> ABSTRACT  :  Contrastive Language-Image Pre-training (CLIP) has been shown to learn visual representations with great transferability, which achieves promising accuracy for zero-shot classification. To further improve its downstream performance, existing works propose additional learnable modules upon CLIP and fine-tune them by few-shot training sets. However, the resulting extra training cost and data requirement severely hinder the efficiency for model deployment and knowledge transfer. In this paper, we introduce a free-lunch **enhancement** method, CALIP, to boost CLIP's zero-shot performance via a parameter-free Attention module. Specifically, we guide visual and textual representations to interact with each other and explore cross-modal informative features via attention. As the pre-training has largely reduced the embedding distances between two modalities, we discard all learnable parameters in the attention and bidirectionally update the multi-modal features, enabling the whole process to be parameter-free and training-free. In this way, the images are blended with textual-aware signals and the text representations become visual-guided for better adaptive zero-shot alignment. We evaluate CALIP on various benchmarks of 14 datasets for both 2D image and 3D point cloud few-shot classification, showing consistent zero-shot performance improvement over CLIP. Based on that, we further insert a small number of linear layers in CALIP's attention module and verify our robustness under the few-shot settings, which also achieves leading performance compared to existing methods. Those extensive experiments demonstrate the superiority of our approach for efficient **enhancement** of CLIP.  
### Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v7 [cs.CL] UPDATED)
- Authors : Terrence Sejnowski
- Link : [http://arxiv.org/abs/2207.14382](http://arxiv.org/abs/2207.14382)
> ABSTRACT  :  Large Language Models (LLMs) have been transformative. They are pre-trained foundational models that are self-supervised and can be adapted with fine tuning to a wide ranger of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and more recently LaMDA can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a Reverse Turing Test. If so, then by studying interviews we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable they may transform the way we interact with machines and how they interact with each other, but several **enhancement**s are needed for autonomy. LLMs can talk the talk, but can they walk the walk?  
# Paper List
---
## cs.CV
---
**97** new papers in cs.CV:-) 
1. Scalable and Equivariant Spherical CNNs by Discrete-Continuous (DISCO) Convolutions. (arXiv:2209.13603v1 [cs.CV])
2. LapGM: A Multisequence MR Bias Correction and Normalization Model. (arXiv:2209.13619v1 [physics.med-ph])
3. Reconstruction-guided attention improves the robustness and shape processing of neural networks. (arXiv:2209.13620v1 [cs.CV])
4. Rethinking Clustering-Based Pseudo-Labeling for Unsupervised Meta-Learning. (arXiv:2209.13635v1 [cs.LG])
5. CEC-CNN: A Consecutive Expansion-Contraction Convolutional Network for Very Small Resolution Medical Image Classification. (arXiv:2209.13661v1 [cs.CV])
6. Mixed-domain Training Improves Multi-Mission Terrain Segmentation. (arXiv:2209.13674v1 [cs.CV])
7. V2XP-ASG: Generating Adversarial Scenes for Vehicle-to-Everything Perception. (arXiv:2209.13679v1 [cs.CV])
8. What Does DALL-E 2 Know About Radiology?. (arXiv:2209.13696v1 [cs.CV])
9. An Overview of the Data-Loader Landscape: Comparative Performance Analysis. (arXiv:2209.13705v1 [cs.DC])
10. Deep Learning Based Detection of Enlarged Perivascular Spaces on Brain MRI. (arXiv:2209.13727v1 [eess.IV])
11. Towards Regression-Free Neural Networks for Diverse Compute Platforms. (arXiv:2209.13740v1 [cs.CV])
12. MTU-Net: Multi-level TransUNet for Space-based Infrared Tiny Ship Detection. (arXiv:2209.13756v1 [cs.CV])
13. Image Compressed Sensing with Multi-scale Dilated Convolutional Neural Network. (arXiv:2209.13761v1 [eess.IV])
14. Target Features Affect Visual Search, A Study of Eye Fixations. (arXiv:2209.13771v1 [cs.CV])
15. An Embarrassingly Simple Approach to Semi-Supervised Few-Shot Learning. (arXiv:2209.13777v1 [cs.CV])
16. CourtNet for Infrared Small-Target Detection. (arXiv:2209.13780v1 [cs.CV])
17. Attacking Compressed Vision Transformers. (arXiv:2209.13785v1 [cs.LG])
18. A Machine Learning Approach for DeepFake Detection. (arXiv:2209.13792v1 [cs.CV])
19. PCB-RandNet: Rethinking Random Sampling for LIDAR Semantic Segmentation in Autonomous Driving Scene. (arXiv:2209.13797v1 [cs.CV])
20. Analysis and prediction of heart stroke from ejection fraction and serum creatinine using LSTM deep learning approach. (arXiv:2209.13799v1 [cs.CV])
21. Translation, Scale and Rotation: Cross-Modal Alignment Meets RGB-Infrared Vehicle Detection. (arXiv:2209.13801v1 [cs.CV])
22. Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention. (arXiv:2209.13802v1 [cs.CV])
23. Streaming Video Temporal Action Segmentation In Real Time. (arXiv:2209.13808v1 [cs.CV])
24. Denoising of 3D MR images using a voxel-wise hybrid residual MLP-CNN model to improve small lesion diagnostic confidence. (arXiv:2209.13818v1 [eess.IV])
25. TokenFlow: Rethinking Fine-grained Cross-modal Alignment in Vision-Language Retrieval. (arXiv:2209.13822v1 [cs.CV])
26. Learning Deep Representations via Contrastive Learning for Instance Retrieval. (arXiv:2209.13832v1 [cs.CV])
27. SEMICON: A Learning-to-hash Solution for Large-scale Fine-grained Image Retrieval. (arXiv:2209.13833v1 [cs.CV])
28. Multi-Sample Training for Neural Image Compression. (arXiv:2209.13834v1 [cs.CV])
29. Deeply Supervised Layer Selective Attention Network: Towards Label-Efficient Learning for Medical Image Classification. (arXiv:2209.13844v1 [cs.CV])
30. Deep Learning based Automatic Quantification of Urethral Plate Quality using the Plate Objective Scoring Tool (POST). (arXiv:2209.13848v1 [cs.CV])
31. Thinking Hallucination for Video Captioning. (arXiv:2209.13853v1 [cs.CV])
32. USEEK: Unsupervised SE(3)-Equivariant 3D Keypoints for Generalizable Manipulation. (arXiv:2209.13864v1 [cs.RO])
33. Rethinking Blur Synthesis for Deep Real-World Image Deblurring. (arXiv:2209.13866v1 [cs.CV])
34. Unified Loss of Pair Similarity Optimization for Vision-Language Retrieval. (arXiv:2209.13869v1 [cs.CV])
35. A General Scattering Phase Function for Inverse Rendering. (arXiv:2209.13875v1 [cs.CV])
36. Strong Instance Segmentation Pipeline for MMSports Challenge. (arXiv:2209.13899v1 [cs.CV])
37. SmartMocap: Joint Estimation of Human and Camera Motion using Uncalibrated RGB Cameras. (arXiv:2209.13906v1 [cs.CV])
38. DeViT: Deformed Vision Transformers in Video Inpainting. (arXiv:2209.13925v1 [cs.CV])
39. Attention Spiking Neural Networks. (arXiv:2209.13929v1 [cs.CV])
40. DPNet: Dual-Path Network for **Real-time** Object Detection with Lightweight Attention. (arXiv:2209.13933v1 [cs.CV])
41. Racial Bias in the Beautyverse. (arXiv:2209.13939v1 [cs.AI])
42. Obj2Seq: Formatting Objects as Sequences with Class Prompt for Visual Tasks. (arXiv:2209.13948v1 [cs.CV])
43. Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding. (arXiv:2209.13959v1 [cs.CV])
44. 3D Neural Sculpting (3DNS): Editing Neural Signed Distance Functions. (arXiv:2209.13971v1 [cs.GR])
45. Medical Image Captioning via Generative Pretrained Transformers. (arXiv:2209.13983v1 [cs.CV])
46. A Review of Modern Approaches for Coronary Angiography Imaging Analysis. (arXiv:2209.13997v1 [eess.IV])
47. Vision based Crop Row Navigation under Varying Field Conditions in Arable Fields. (arXiv:2209.14003v1 [cs.CV])
48. Leveraging machine learning for less developed languages: Progress on Urdu text detection. (arXiv:2209.14022v1 [cs.CV])
49. Motion Transformer for Unsupervised Image Animation. (arXiv:2209.14024v1 [cs.CV])
50. Adma-GAN: Attribute-Driven Memory Augmented GANs for Text-to-Image Generation. (arXiv:2209.14046v1 [cs.CV])
51. Inducing Data Amplification Using Auxiliary Datasets in Adversarial Training. (arXiv:2209.14053v1 [cs.CV])
52. City-scale Incremental Neural Mapping with Three-layer Sampling and Panoptic Representation. (arXiv:2209.14072v1 [cs.CV])
53. Recipro-CAM: Gradient-free reciprocal class activation map. (arXiv:2209.14074v1 [cs.CV])
54. PTSD in the Wild: A Video Database for Studying Post-Traumatic Stress Disorder Recognition in Unconstrained Environments. (arXiv:2209.14085v1 [cs.HC])
55. Data Augmentation using Feature Generation for Volumetric Medical Images. (arXiv:2209.14097v1 [eess.IV])
56. Deepfake audio detection by speaker verification. (arXiv:2209.14098v1 [cs.SD])
57. Weighted Contrastive Hashing. (arXiv:2209.14099v1 [cs.CV])
58. CSSAM: U-net Network for Application and Segmentation of Welding Engineering Drawings. (arXiv:2209.14102v1 [cs.CV])
59. Cyclegan Network for Sheet Metal Welding Drawing Translation. (arXiv:2209.14106v1 [cs.CV])
60. Multi-scale Attention Network for Image Super-Resolution. (arXiv:2209.14145v1 [eess.IV])
61. TVLT: Textless Vision-Language Transformer. (arXiv:2209.14156v1 [cs.CV])
62. CALIP: Zero-Shot **Enhancement** of CLIP with Parameter-free Attention. (arXiv:2209.14169v1 [cs.CV])
63. Spatial Pruned Sparse Convolution for Efficient 3D Object Detection. (arXiv:2209.14201v1 [cs.CV])
64. Prompt-driven efficient Open-set Semi-supervised Learning. (arXiv:2209.14205v1 [cs.CV])
65. Automated Quality Controlled Analysis of 2D Phase Contrast Cardiovascular Magnetic Resonance Imaging. (arXiv:2209.14212v1 [eess.IV])
66. Longitudinal Variability Analysis on Low-dose Abdominal CT with Deep Learning-based Segmentation. (arXiv:2209.14217v1 [cs.CV])
67. Road Rutting Detection using Deep Learning on Images. (arXiv:2209.14225v1 [cs.CV])
68. A Survey on Physical Adversarial Attack in Computer Vision. (arXiv:2209.14262v1 [cs.CV])
69. 360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance. (arXiv:2209.14265v1 [cs.CV])
70. Less is More: Rethinking Few-Shot Learning and Recurrent Neural Nets. (arXiv:2209.14267v1 [cs.LG])
71. Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v1 [cs.LG])
72. DexTransfer: Real World Multi-fingered Dexterous Grasping with Minimal Human Demonstrations. (arXiv:2209.14284v1 [cs.CV])
73. Zeus: Efficiently Localizing Actions in Videos using Reinforcement Learning. (arXiv:2104.06142v3 [cs.CV] UPDATED)
74. Chromatic and spatial analysis of one-pixel attacks against an image classifier. (arXiv:2105.13771v4 [cs.CV] UPDATED)
75. DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing. (arXiv:2109.10737v3 [cs.CV] UPDATED)
76. TreeGCN-ED: Encoding Point Cloud using a Tree-Structured Graph Network. (arXiv:2110.03170v3 [cs.CV] UPDATED)
77. Adaptive Image Transformations for Transfer-based Adversarial Attack. (arXiv:2111.13844v4 [cs.CV] UPDATED)
78. Make an Omelette with Breaking Eggs: Zero-Shot Learning for Novel Attribute Synthesis. (arXiv:2111.14182v5 [cs.CV] UPDATED)
79. Periodic Residual Learning for Crowd Flow Forecasting. (arXiv:2112.06132v2 [cs.LG] UPDATED)
80. Learning Semantic Ambiguities for Zero-Shot Learning. (arXiv:2201.01823v3 [cs.CV] UPDATED)
81. Towards Predicting Fine Finger Motions from Ultrasound Images via Kinematic Representation. (arXiv:2202.05204v2 [cs.RO] UPDATED)
82. COLA: COarse LAbel pre-training for 3D semantic segmentation of sparse LiDAR datasets. (arXiv:2202.06884v2 [cs.CV] UPDATED)
83. Optical Flow Training under Limited Label Budget via Active Learning. (arXiv:2203.05053v2 [cs.CV] UPDATED)
84. Selective Cross-Task Distillation. (arXiv:2204.11526v3 [cs.LG] UPDATED)
85. Towards view-invariant vehicle speed detection from driving simulator images. (arXiv:2206.00343v2 [cs.CV] UPDATED)
86. AutoMerge: A Framework for Map Assembling and Smoothing in City-scale Environments. (arXiv:2207.06965v3 [cs.RO] UPDATED)
87. RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation. (arXiv:2208.00237v2 [cs.CV] UPDATED)
88. Character decomposition to resolve class imbalance problem in Hangul OCR. (arXiv:2208.06079v2 [cs.CV] UPDATED)
89. Training Strategies for Improved Lip-reading. (arXiv:2209.01383v2 [cs.CV] UPDATED)
90. ADTR: Anomaly Detection Transformer with Feature Reconstruction. (arXiv:2209.01816v2 [cs.CV] UPDATED)
91. Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe. (arXiv:2209.05324v2 [cs.CV] UPDATED)
92. NAAP-440 Dataset and Baseline for Neural Architecture Accuracy Prediction. (arXiv:2209.06626v3 [cs.CV] UPDATED)
93. ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection. (arXiv:2209.09178v2 [cs.CV] UPDATED)
94. Learning from Mixed Datasets: A Monotonic Image Quality Assessment Model. (arXiv:2209.10451v2 [cs.CV] UPDATED)
95. A Tightly Coupled LiDAR-IMU Odometry through Iterated Point-Level Undistortion. (arXiv:2209.12249v2 [cs.RO] UPDATED)
96. Accurate and Efficient Stereo Matching via Attention Concatenation Volume. (arXiv:2209.12699v2 [cs.CV] UPDATED)
97. Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion. (arXiv:2209.13360v2 [cs.CV] UPDATED)
## eess.IV
---
**13** new papers in eess.IV:-) 
1. Quality Assurance of Weld Seams Using Laser Triangulation Imaging and Deep Neural Networks. (arXiv:2209.13648v1 [eess.IV])
2. What Does DALL-E 2 Know About Radiology?. (arXiv:2209.13696v1 [cs.CV])
3. Deep Learning Based Detection of Enlarged Perivascular Spaces on Brain MRI. (arXiv:2209.13727v1 [eess.IV])
4. Image Compressed Sensing with Multi-scale Dilated Convolutional Neural Network. (arXiv:2209.13761v1 [eess.IV])
5. Denoising of 3D MR images using a voxel-wise hybrid residual MLP-CNN model to improve small lesion diagnostic confidence. (arXiv:2209.13818v1 [eess.IV])
6. A Review of Modern Approaches for Coronary Angiography Imaging Analysis. (arXiv:2209.13997v1 [eess.IV])
7. Leveraging machine learning for less developed languages: Progress on Urdu text detection. (arXiv:2209.14022v1 [cs.CV])
8. Data Augmentation using Feature Generation for Volumetric Medical Images. (arXiv:2209.14097v1 [eess.IV])
9. CSSAM: U-net Network for Application and Segmentation of Welding Engineering Drawings. (arXiv:2209.14102v1 [cs.CV])
10. Multi-scale Attention Network for Image Super-Resolution. (arXiv:2209.14145v1 [eess.IV])
11. Automated Quality Controlled Analysis of 2D Phase Contrast Cardiovascular Magnetic Resonance Imaging. (arXiv:2209.14212v1 [eess.IV])
12. Learning from Mixed Datasets: A Monotonic Image Quality Assessment Model. (arXiv:2209.10451v2 [cs.CV] UPDATED)
13. Detection and hypothesis testing of features in extremely noisy image series using topological data analysis, with applications to nanoparticle videos. (arXiv:2209.13584v2 [stat.AP] UPDATED)
## cs.LG
---
**152** new papers in cs.LG:-) 
1. Deep learning forward and reverse primer design to detect SARS-CoV-2 emerging variants. (arXiv:2209.13591v1 [q-bio.GN])
2. DVGAN: Stabilize Wasserstein GAN training for time-domain Gravitational Wave physics. (arXiv:2209.13592v1 [astro-ph.IM])
3. Scalable and Equivariant Spherical CNNs by Discrete-Continuous (DISCO) Convolutions. (arXiv:2209.13603v1 [cs.CV])
4. Efficiently Learning Recoveries from Failures Under Partial Observability. (arXiv:2209.13605v1 [cs.RO])
5. Reconstruction-guided attention improves the robustness and shape processing of neural networks. (arXiv:2209.13620v1 [cs.CV])
6. Rethinking Clustering-Based Pseudo-Labeling for Unsupervised Meta-Learning. (arXiv:2209.13635v1 [cs.LG])
7. MPC-Pipe: an Efficient Pipeline Scheme for Secure Multi-party Machine Learning Inference. (arXiv:2209.13643v1 [cs.CR])
8. Modeling Polyp Activity of Paragorgia arborea Using Supervised Learning. (arXiv:2209.13644v1 [q-bio.PE])
9. PearNet: A Pearson Correlation-based Graph Attention Network for Sleep Stage Recognition. (arXiv:2209.13645v1 [eess.SP])
10. Deep learning based sferics recognition for AMT data processing in the dead band. (arXiv:2209.13647v1 [eess.SP])
11. CEC-CNN: A Consecutive Expansion-Contraction Convolutional Network for Very Small Resolution Medical Image Classification. (arXiv:2209.13661v1 [cs.CV])
12. FAIR-FATE: Fair Federated Learning with Momentum. (arXiv:2209.13678v1 [cs.LG])
13. A Doubly Optimistic Strategy for Safe Linear Bandits. (arXiv:2209.13694v1 [cs.LG])
14. Reasoning over Multi-view Knowledge Graphs. (arXiv:2209.13702v1 [cs.AI])
15. An Overview of the Data-Loader Landscape: Comparative Performance Analysis. (arXiv:2209.13705v1 [cs.DC])
16. SGTM 2.0: Autonomously Untangling Long Cables using Interactive Perception. (arXiv:2209.13706v1 [cs.RO])
17. Falsification before Extrapolation in Causal Effect Estimation. (arXiv:2209.13708v1 [cs.LG])
18. Hamiltonian Adaptive Importance Sampling. (arXiv:2209.13716v1 [cs.LG])
19. Deep Learning Based Detection of Enlarged Perivascular Spaces on Brain MRI. (arXiv:2209.13727v1 [eess.IV])
20. Towards Regression-Free Neural Networks for Diverse Compute Platforms. (arXiv:2209.13740v1 [cs.CV])
21. Consensus Knowledge Graph Learning via Multi-view Sparse Low Rank Block Model. (arXiv:2209.13762v1 [stat.ML])
22. ButterflyFlow: Building Invertible Layers with Butterfly Matrices. (arXiv:2209.13774v1 [cs.LG])
23. An Embarrassingly Simple Approach to Semi-Supervised Few-Shot Learning. (arXiv:2209.13777v1 [cs.CV])
24. Attacking Compressed Vision Transformers. (arXiv:2209.13785v1 [cs.LG])
25. A Parameter-free Nonconvex Low-rank Tensor Completion Model for Spatiotemporal Traffic Data Recovery. (arXiv:2209.13786v1 [cs.LG])
26. TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method. (arXiv:2209.13791v1 [cs.LG])
27. Analysis and prediction of heart stroke from ejection fraction and serum creatinine using LSTM deep learning approach. (arXiv:2209.13799v1 [cs.CV])
28. FedVeca: Federated Vectorized Averaging on Non-IID Data with Adaptive Bi-directional Global Objective. (arXiv:2209.13803v1 [cs.LG])
29. Revisiting Few-Shot Learning from a Causal Perspective. (arXiv:2209.13816v1 [cs.LG])
30. Label Distribution Learning via Implicit Distribution Representation. (arXiv:2209.13824v1 [cs.LG])
31. Supervised Class-pairwise NMF for Data Representation and Classification. (arXiv:2209.13831v1 [cs.LG])
32. Learning Deep Representations via Contrastive Learning for Instance Retrieval. (arXiv:2209.13832v1 [cs.CV])
33. Mutual Information and Ensemble Based Feature Recommender for Renal Cancer Stage Classification. (arXiv:2209.13836v1 [cs.LG])
34. Online Policy Optimization for Robust MDP. (arXiv:2209.13841v1 [cs.LG])
35. VREN: Volleyball Rally Dataset with Expression Notation Language. (arXiv:2209.13846v1 [cs.LG])
36. Shape-constrained Symbolic Regression with NSGA-III. (arXiv:2209.13851v1 [cs.LG])
37. Identifying Differential Equations to predict Blood Glucose using Sparse Identification of Nonlinear Systems. (arXiv:2209.13852v1 [cs.LG])
38. Variance Tolerance Factors For Interpreting Neural Networks. (arXiv:2209.13858v1 [cs.LG])
39. Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes. (arXiv:2209.13860v1 [cs.CL])
40. Disentangling Transfer in Continual Reinforcement Learning. (arXiv:2209.13900v1 [cs.LG])
41. An Efficient Multitask Learning Architecture for Affective Vocal Burst Analysis. (arXiv:2209.13914v1 [cs.SD])
42. A simple but strong baseline for online continual learning: Repeated Augmented Rehearsal. (arXiv:2209.13917v1 [cs.LG])
43. Experimental study of time series forecasting methods for groundwater level prediction. (arXiv:2209.13927v1 [cs.LG])
44. ArNLI: Arabic Natural Language Inference for Entailment and Contradiction Detection. (arXiv:2209.13953v1 [cs.CL])
45. Forecasting Sensor Values in Waste-To-Fuel Plants: a Case Study. (arXiv:2209.13957v1 [cs.AI])
46. Big data analysis and distributed deep learning for next-generation intrusion detection system optimization. (arXiv:2209.13961v1 [cs.CR])
47. Machine Beats Machine: Machine Learning Models to Defend Against Adversarial Attacks. (arXiv:2209.13963v1 [cs.LG])
48. Graph Soft-Contrastive Learning via Neighborhood Ranking. (arXiv:2209.13964v1 [cs.LG])
49. Anomaly detection optimization using big data and deep learning to reduce false-positive. (arXiv:2209.13965v1 [cs.AI])
50. SoftTreeMax: Policy Gradient with Tree Search. (arXiv:2209.13966v1 [cs.LG])
51. Toward Certification of Machine-Learning Systems for Low Criticality Airborne Applications. (arXiv:2209.13975v1 [cs.LG])
52. Argumentative Reward Learning: Reasoning About Human Preferences. (arXiv:2209.14010v1 [cs.AI])
53. On the Robustness of Ensemble-Based Machine Learning Against Data Poisoning. (arXiv:2209.14013v1 [cs.LG])
54. Leveraging machine learning for less developed languages: Progress on Urdu text detection. (arXiv:2209.14022v1 [cs.CV])
55. LL-GNN: Low Latency Graph Neural Networks on FPGAs for Particle Detectors. (arXiv:2209.14065v1 [cs.AR])
56. Efficient block contrastive learning via parameter-free meta-node approximation. (arXiv:2209.14067v1 [cs.LG])
57. Recipro-CAM: Gradient-free reciprocal class activation map. (arXiv:2209.14074v1 [cs.CV])
58. Backward Reachability Analysis of Neural Feedback Loops: Techniques for Linear and Nonlinear Systems. (arXiv:2209.14076v1 [eess.SY])
59. Global Weighted Tensor Nuclear Norm for Tensor Robust Principal Component Analysis. (arXiv:2209.14084v1 [cs.LG])
60. PTSD in the Wild: A Video Database for Studying Post-Traumatic Stress Disorder Recognition in Unconstrained Environments. (arXiv:2209.14085v1 [cs.HC])
61. Momentum Gradient Descent Federated Learning with Local Differential Privacy. (arXiv:2209.14086v1 [cs.LG])
62. Reinforcement Learning with Tensor Networks: Application to Dynamical Large Deviations. (arXiv:2209.14089v1 [cond-mat.stat-mech])
63. Offensive Language Detection on Twitter. (arXiv:2209.14091v1 [cs.CL])
64. Securing Federated Learning against Overwhelming Collusive Attackers. (arXiv:2209.14093v1 [cs.LG])
65. Data Augmentation using Feature Generation for Volumetric Medical Images. (arXiv:2209.14097v1 [eess.IV])
66. CSSAM: U-net Network for Application and Segmentation of Welding Engineering Drawings. (arXiv:2209.14102v1 [cs.CV])
67. Exploring the Relationship between Architecture and Adversarially Robust Generalization. (arXiv:2209.14105v1 [cs.LG])
68. Cyclegan Network for Sheet Metal Welding Drawing Translation. (arXiv:2209.14106v1 [cs.CV])
69. Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure. (arXiv:2209.14107v1 [cs.LG])
70. Deep learning for gradient flows using the Brezis-Ekeland principle. (arXiv:2209.14115v1 [math.NA])
71. Spectral Diffusion Processes. (arXiv:2209.14125v1 [stat.ML])
72. Evaluation of Time-Series Forecasting Models for Chickenpox Cases Estimation in Hungary. (arXiv:2209.14129v1 [cs.AI])
73. Mobile Edge Computing, Metaverse, 6G Wireless Communications, Artificial Intelligence, and Blockchain: Survey and Their Convergence. (arXiv:2209.14147v1 [cs.DC])
74. Guiding Safe Exploration with Weakest Preconditions. (arXiv:2209.14148v1 [cs.LG])
75. Automatic Analysis of Available Source Code of Top Artificial Intelligence Conference Papers. (arXiv:2209.14155v1 [cs.SE])
76. Learning Filter-Based Compressed Blind-Deconvolution. (arXiv:2209.14165v1 [eess.SP])
77. Class-Imbalanced Complementary-Label Learning via Weighted Loss. (arXiv:2209.14189v1 [cs.LG])
78. Active Transfer Prototypical Network: An Efficient Labeling Algorithm for Time-Series Data. (arXiv:2209.14199v1 [cs.LG])
79. Online Subset Selection using $\alpha$-Core with no Augmented Regret. (arXiv:2209.14222v1 [cs.LG])
80. Knowledge-Aware Bayesian Deep Topic Model. (arXiv:2209.14228v1 [cs.CL])
81. Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments. (arXiv:2209.14233v1 [cs.RO])
82. Accuracy, Fairness, and Interpretability of Machine Learning Criminal Recidivism Models. (arXiv:2209.14237v1 [cs.CY])
83. How to solve a classification problem using a cooperative tiling Multi-Agent System?. (arXiv:2209.14239v1 [cs.MA])
84. A Closer Look at Evaluating the Bit-Flip Attack Against Deep Neural Networks. (arXiv:2209.14243v1 [cs.CR])
85. Score Modeling for Simulation-based Inference. (arXiv:2209.14249v1 [cs.LG])
86. B2B Advertising: Joint Dynamic Scoring of Account and Users. (arXiv:2209.14250v1 [cs.LG])
87. A Multi-scale Graph Signature for Persistence Diagrams based on Return Probabilities of Random Walks. (arXiv:2209.14264v1 [cs.LG])
88. Less is More: Rethinking Few-Shot Learning and Recurrent Neural Nets. (arXiv:2209.14267v1 [cs.LG])
89. On the Generalization of Deep Reinforcement Learning Methods in the Problem of Local Navigation. (arXiv:2209.14271v1 [cs.RO])
90. Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v1 [cs.LG])
91. Multilingual Search with Subword TF-IDF. (arXiv:2209.14281v1 [cs.CL])
92. Data-driven soliton mappings for integrable fractional nonlinear wave equations via deep learning with Fourier neural operator. (arXiv:2209.14291v1 [nlin.SI])
93. Conformal Prediction is Robust to Label Noise. (arXiv:2209.14295v1 [cs.LG])
94. A deep learning approach for the computation of curvature in the level-set method. (arXiv:2002.02804v4 [math.NA] UPDATED)
95. Distance-based Positive and Unlabeled Learning for Ranking. (arXiv:2005.10700v3 [cs.LG] UPDATED)
96. Meta Clustering for Collaborative Learning. (arXiv:2006.00082v3 [cs.LG] UPDATED)
97. A General Framework for Analyzing Stochastic Dynamics in Learning Algorithms. (arXiv:2006.06171v3 [math.OC] UPDATED)
98. Double Double Descent: On Generalization Errors in Transfer Learning between Linear Regression Tasks. (arXiv:2006.07002v8 [cs.LG] UPDATED)
99. A Survey on Ensemble Learning under the Era of Deep Learning. (arXiv:2101.08387v6 [cs.LG] UPDATED)
100. Causal Inference Under Unmeasured Confounding With Negative Controls: A Minimax Learning Approach. (arXiv:2103.14029v3 [stat.ML] UPDATED)
101. A hybrid inference system for improved curvature estimation in the level-set method using machine learning. (arXiv:2104.02951v5 [cs.LG] UPDATED)
102. Learning Dissipative Dynamics in Chaotic Systems. (arXiv:2106.06898v2 [cs.LG] UPDATED)
103. Sample-Efficient Safety Assurances using Conformal Prediction. (arXiv:2109.14082v3 [cs.RO] UPDATED)
104. Graph Condensation for Graph Neural Networks. (arXiv:2110.07580v4 [cs.LG] UPDATED)
105. One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v7 [cs.LG] UPDATED)
106. Error-correcting neural networks for semi-Lagrangian advection in the level-set method. (arXiv:2110.11611v4 [cs.LG] UPDATED)
107. Learn one size to infer all: Exploiting translational symmetries in delay-dynamical and spatio-temporal systems using scalable neural networks. (arXiv:2111.03706v2 [cs.LG] UPDATED)
108. Importance of Kernel Bandwidth in Quantum Machine Learning. (arXiv:2111.05451v4 [quant-ph] UPDATED)
109. Sharing to learn and learning to share -- Fitting together Meta-Learning, Multi-Task Learning, and Transfer Learning: A meta review. (arXiv:2111.12146v4 [cs.LG] UPDATED)
110. Make an Omelette with Breaking Eggs: Zero-Shot Learning for Novel Attribute Synthesis. (arXiv:2111.14182v5 [cs.CV] UPDATED)
111. Imbalanced Graph Classification via Graph-of-Graph Neural Networks. (arXiv:2112.00238v2 [cs.LG] UPDATED)
112. Periodic Residual Learning for Crowd Flow Forecasting. (arXiv:2112.06132v2 [cs.LG] UPDATED)
113. Joint Learning of Linear Time-Invariant Dynamical Systems. (arXiv:2112.10955v4 [stat.ML] UPDATED)
114. CausalSim: A Causal Inference Framework for Unbiased Trace-Driven Simulation. (arXiv:2201.01811v3 [cs.LG] UPDATED)
115. Multiblock ADMM for nonsmooth nonconvex optimization with nonlinear coupling constraints. (arXiv:2201.07657v2 [math.OC] UPDATED)
116. Error-Correcting Neural Networks for Two-Dimensional Curvature Computation in the Level-Set Method. (arXiv:2201.12342v3 [math.NA] UPDATED)
117. On the Implicit Bias Towards Minimal Depth of Deep Neural Networks. (arXiv:2202.09028v9 [cs.LG] UPDATED)
118. Estimators of Entropy and Information via Inference in Probabilistic Models. (arXiv:2202.12363v3 [stat.ML] UPDATED)
119. SHiFT: An Efficient, Flexible Search Engine for Transfer Learning. (arXiv:2204.01457v2 [cs.LG] UPDATED)
120. Selective Cross-Task Distillation. (arXiv:2204.11526v3 [cs.LG] UPDATED)
121. Topological Data Analysis in Time Series: Temporal Filtration and Application to Single-Cell Genomics. (arXiv:2204.14048v2 [cs.LG] UPDATED)
122. TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning. (arXiv:2205.00293v2 [cs.LG] UPDATED)
123. ASTROMER: A transformer-based embedding for the representation of light curves. (arXiv:2205.01677v2 [astro-ph.IM] UPDATED)
124. Analyzing Lottery Ticket Hypothesis from PAC-Bayesian Theory Perspective. (arXiv:2205.07320v3 [cs.LG] UPDATED)
125. Constraint-Based Causal Structure Learning from Undersampled Graphs. (arXiv:2205.09235v3 [stat.ML] UPDATED)
126. Differentially Private Covariance Revisited. (arXiv:2205.14324v3 [cs.CR] UPDATED)
127. First-Order Algorithms for Min-Max Optimization in Geodesic Metric Spaces. (arXiv:2206.02041v2 [math.OC] UPDATED)
128. SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks. (arXiv:2206.05794v2 [cs.LG] UPDATED)
129. Contrastive Learning for Unsupervised Domain Adaptation of Time Series. (arXiv:2206.06243v2 [cs.LG] UPDATED)
130. Beyond Real-world Benchmark Datasets: An Empirical Study of Node Classification with GNNs. (arXiv:2206.09144v3 [cs.LG] UPDATED)
131. On the Limitations of Stochastic Pre-processing Defenses. (arXiv:2206.09491v2 [cs.LG] UPDATED)
132. Randomized K-FACs: Speeding up K-FAC with Randomized Numerical Linear Algebra. (arXiv:2206.15397v2 [cs.LG] UPDATED)
133. Cooperate or Compete: A New Perspective on Training of Generative Networks. (arXiv:2207.02192v6 [cs.LG] UPDATED)
134. Collaboration-Aware Graph Convolutional Network for Recommender Systems. (arXiv:2207.06221v2 [cs.IR] UPDATED)
135. Automatic Sleep Scoring from Large-scale Multi-channel Pediatric EEG. (arXiv:2207.06921v3 [eess.SP] UPDATED)
136. FLOWGEN: Fast and slow graph generation. (arXiv:2207.07656v4 [cs.LG] UPDATED)
137. p-Adic Statistical Field Theory and Deep Belief Networks. (arXiv:2207.13877v2 [math-ph] UPDATED)
138. Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v7 [cs.CL] UPDATED)
139. Wave simulation in non-smooth media by PINN with quadratic neural network and PML condition. (arXiv:2208.08276v2 [physics.geo-ph] UPDATED)
140. Training Strategies for Improved Lip-reading. (arXiv:2209.01383v2 [cs.CV] UPDATED)
141. A Novel Nearest Neighbors Algorithm Based on Power Muirhead Mean. (arXiv:2209.01514v2 [cs.LG] UPDATED)
142. Applying Machine Learning to Life Insurance: some knowledge sharing to master it. (arXiv:2209.02057v3 [stat.ML] UPDATED)
143. Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe. (arXiv:2209.05324v2 [cs.CV] UPDATED)
144. NAAP-440 Dataset and Baseline for Neural Architecture Accuracy Prediction. (arXiv:2209.06626v3 [cs.CV] UPDATED)
145. Semi-Counterfactual Risk Minimization Via Neural Networks. (arXiv:2209.07148v2 [cs.LG] UPDATED)
146. STPOTR: Simultaneous Human Trajectory and Pose Prediction Using a Non-Autoregressive Transformer for Robot Following Ahead. (arXiv:2209.07600v3 [cs.RO] UPDATED)
147. Importance Tempering: Group Robustness for Overparameterized Models. (arXiv:2209.08745v2 [cs.LG] UPDATED)
148. Learning from Mixed Datasets: A Monotonic Image Quality Assessment Model. (arXiv:2209.10451v2 [cs.CV] UPDATED)
149. Quantification before Selection: Active Dynamics Preference for Robust Reinforcement Learning. (arXiv:2209.11596v2 [cs.LG] UPDATED)
150. Factual and Informative Review Generation for Explainable Recommendation. (arXiv:2209.12613v2 [cs.CL] UPDATED)
151. Hierarchical Sliced Wasserstein Distance. (arXiv:2209.13570v2 [stat.ML] UPDATED)
152. End-to-End Intelligent Framework for Rockfall Detection. (arXiv:2102.06491v1 [cs.LG] CROSS LISTED)
## cs.AI
---
**76** new papers in cs.AI:-) 
1. Efficiently Learning Recoveries from Failures Under Partial Observability. (arXiv:2209.13605v1 [cs.RO])
2. Reconstruction-guided attention improves the robustness and shape processing of neural networks. (arXiv:2209.13620v1 [cs.CV])
3. A critical appraisal of equity in conversational AI: Evidence from auditing GPT-3's dialogues with different publics on climate change and Black Lives Matter. (arXiv:2209.13627v1 [cs.AI])
4. Intercepting A Flying Target While Avoiding Moving Obstacles: A Unified Control Framework With Deep Manifold Learning. (arXiv:2209.13628v1 [cs.RO])
5. FAIR-FATE: Fair Federated Learning with Momentum. (arXiv:2209.13678v1 [cs.LG])
6. What Does DALL-E 2 Know About Radiology?. (arXiv:2209.13696v1 [cs.CV])
7. Reasoning over Multi-view Knowledge Graphs. (arXiv:2209.13702v1 [cs.AI])
8. SGTM 2.0: Autonomously Untangling Long Cables using Interactive Perception. (arXiv:2209.13706v1 [cs.RO])
9. Towards Human-Compatible XAI: Explaining Data Differentials with Concept Induction over Background Knowledge. (arXiv:2209.13710v1 [cs.AI])
10. mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark. (arXiv:2209.13738v1 [cs.CL])
11. Clustering-Induced Generative Incomplete Image-Text Clustering (CIGIT-C). (arXiv:2209.13763v1 [cs.AI])
12. ButterflyFlow: Building Invertible Layers with Butterfly Matrices. (arXiv:2209.13774v1 [cs.LG])
13. Revisiting Few-Shot Learning from a Causal Perspective. (arXiv:2209.13816v1 [cs.LG])
14. Deep Learning based Automatic Quantification of Urethral Plate Quality using the Plate Objective Scoring Tool (POST). (arXiv:2209.13848v1 [cs.CV])
15. InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference. (arXiv:2209.13873v1 [cs.AI])
16. MLink: Linking Black-Box Models from Multiple Domains for Collaborative Inference. (arXiv:2209.13883v1 [cs.AI])
17. UCEpic: Unifying Aspect Planning and Lexical Constraints for Explainable Recommendation. (arXiv:2209.13885v1 [cs.AI])
18. Hierarchical MixUp Multi-label Classification with Imbalanced Interdisciplinary Research Proposals. (arXiv:2209.13912v1 [cs.CL])
19. A simple but strong baseline for online continual learning: Repeated Augmented Rehearsal. (arXiv:2209.13917v1 [cs.LG])
20. Racial Bias in the Beautyverse. (arXiv:2209.13939v1 [cs.AI])
21. ArNLI: Arabic Natural Language Inference for Entailment and Contradiction Detection. (arXiv:2209.13953v1 [cs.CL])
22. Forecasting Sensor Values in Waste-To-Fuel Plants: a Case Study. (arXiv:2209.13957v1 [cs.AI])
23. Big data analysis and distributed deep learning for next-generation intrusion detection system optimization. (arXiv:2209.13961v1 [cs.CR])
24. Machine Beats Machine: Machine Learning Models to Defend Against Adversarial Attacks. (arXiv:2209.13963v1 [cs.LG])
25. Graph Soft-Contrastive Learning via Neighborhood Ranking. (arXiv:2209.13964v1 [cs.LG])
26. Anomaly detection optimization using big data and deep learning to reduce false-positive. (arXiv:2209.13965v1 [cs.AI])
27. Toward Certification of Machine-Learning Systems for Low Criticality Airborne Applications. (arXiv:2209.13975v1 [cs.LG])
28. Medical Image Captioning via Generative Pretrained Transformers. (arXiv:2209.13983v1 [cs.CV])
29. Argumentative Reward Learning: Reasoning About Human Preferences. (arXiv:2209.14010v1 [cs.AI])
30. Leveraging machine learning for less developed languages: Progress on Urdu text detection. (arXiv:2209.14022v1 [cs.CV])
31. Advising Autonomous Cars about the Rules of the Road. (arXiv:2209.14035v1 [cs.AI])
32. Verifying Safety of Behaviour Trees in Event-B. (arXiv:2209.14045v1 [cs.RO])
33. Popularity Driven Data Integration. (arXiv:2209.14049v1 [cs.AI])
34. Review for AI-based Open-Circuit Faults Diagnosis Methods in Power Electronics Converters. (arXiv:2209.14058v1 [eess.SY])
35. Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure. (arXiv:2209.14107v1 [cs.LG])
36. Evaluation of Time-Series Forecasting Models for Chickenpox Cases Estimation in Hungary. (arXiv:2209.14129v1 [cs.AI])
37. Mobile Edge Computing, Metaverse, 6G Wireless Communications, Artificial Intelligence, and Blockchain: Survey and Their Convergence. (arXiv:2209.14147v1 [cs.DC])
38. Automatic Analysis of Available Source Code of Top Artificial Intelligence Conference Papers. (arXiv:2209.14155v1 [cs.SE])
39. TVLT: Textless Vision-Language Transformer. (arXiv:2209.14156v1 [cs.CV])
40. CALIP: Zero-Shot **Enhancement** of CLIP with Parameter-free Attention. (arXiv:2209.14169v1 [cs.CV])
41. Programmable and Customized Intelligence for Traffic Steering in 5G Networks Using Open RAN Architectures. (arXiv:2209.14171v1 [cs.NI])
42. DMAP: a Distributed Morphological Attention Policy for Learning to Locomote with a Changing Body. (arXiv:2209.14218v1 [cs.RO])
43. Online Subset Selection using $\alpha$-Core with no Augmented Regret. (arXiv:2209.14222v1 [cs.LG])
44. How to solve a classification problem using a cooperative tiling Multi-Agent System?. (arXiv:2209.14239v1 [cs.MA])
45. Physics-aware Differentiable Discrete Codesign for Diffractive Optical Neural Networks. (arXiv:2209.14252v1 [cs.AI])
46. Audio Retrieval with WavText5K and CLAP Training. (arXiv:2209.14275v1 [eess.AS])
47. Multilingual Search with Subword TF-IDF. (arXiv:2209.14281v1 [cs.CL])
48. Proceedings of the AI-HRI Symposium at AAAI-FSS 2022. (arXiv:2209.14292v1 [cs.AI])
49. Conformal Prediction is Robust to Label Noise. (arXiv:2209.14295v1 [cs.LG])
50. Modeling and solving the multimodal car- and ride-sharing problem. (arXiv:2001.05490v2 [cs.AI] UPDATED)
51. Modeling and solving a vehicle-sharing problem considering multiple alternative modes of transport. (arXiv:2003.08207v2 [cs.OH] UPDATED)
52. The bi-objective multimodal car-sharing problem. (arXiv:2010.10344v2 [cs.AI] UPDATED)
53. A Survey on Ensemble Learning under the Era of Deep Learning. (arXiv:2101.08387v6 [cs.LG] UPDATED)
54. Chromatic and spatial analysis of one-pixel attacks against an image classifier. (arXiv:2105.13771v4 [cs.CV] UPDATED)
55. A Neurorobotics Approach to Behaviour Selection based on Human Activity Recognition. (arXiv:2107.12540v2 [cs.RO] UPDATED)
56. Graph Condensation for Graph Neural Networks. (arXiv:2110.07580v4 [cs.LG] UPDATED)
57. One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v7 [cs.LG] UPDATED)
58. Periodic Residual Learning for Crowd Flow Forecasting. (arXiv:2112.06132v2 [cs.LG] UPDATED)
59. Towards Interactive Language Modeling. (arXiv:2112.11911v2 [cs.CL] UPDATED)
60. CausalSim: A Causal Inference Framework for Unbiased Trace-Driven Simulation. (arXiv:2201.01811v3 [cs.LG] UPDATED)
61. Constraint-Based Causal Structure Learning from Undersampled Graphs. (arXiv:2205.09235v3 [stat.ML] UPDATED)
62. Towards view-invariant vehicle speed detection from driving simulator images. (arXiv:2206.00343v2 [cs.CV] UPDATED)
63. Contrastive Learning for Unsupervised Domain Adaptation of Time Series. (arXiv:2206.06243v2 [cs.LG] UPDATED)
64. FLOWGEN: Fast and slow graph generation. (arXiv:2207.07656v4 [cs.LG] UPDATED)
65. Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v7 [cs.CL] UPDATED)
66. Is Your Model Sensitive? SPeDaC: A New Benchmark for Detecting and Classifying Sensitive Personal Data. (arXiv:2208.06216v2 [cs.CL] UPDATED)
67. Repair Is Nearly Generation: Multilingual Program Repair with LLMs. (arXiv:2208.11640v2 [cs.SE] UPDATED)
68. A Novel Nearest Neighbors Algorithm Based on Power Muirhead Mean. (arXiv:2209.01514v2 [cs.LG] UPDATED)
69. Semi-Counterfactual Risk Minimization Via Neural Networks. (arXiv:2209.07148v2 [cs.LG] UPDATED)
70. Importance Tempering: Group Robustness for Overparameterized Models. (arXiv:2209.08745v2 [cs.LG] UPDATED)
71. Neural Networks Base on Power Method and Inverse Power Method for Solving Linear Eigenvalue Problems. (arXiv:2209.11134v2 [math.NA] UPDATED)
72. Factual and Informative Review Generation for Explainable Recommendation. (arXiv:2209.12613v2 [cs.CL] UPDATED)
73. Environmental and Social Sustainability of Creative-Ai. (arXiv:2209.12879v2 [cs.HC] UPDATED)
74. The Ability of Self-Supervised Speech Models for Audio Representations. (arXiv:2209.12900v2 [cs.SD] UPDATED)
75. SmartFPS: Neural Network based Wireless-inertial fusion positioning system. (arXiv:2209.13261v2 [eess.SP] UPDATED)
76. End-to-End Intelligent Framework for Rockfall Detection. (arXiv:2102.06491v1 [cs.LG] CROSS LISTED)

