# Your interest papers
---
## cs.CV
---
### **Enhancement** by Your Aesthetic: An Intelligible Unsupervised Personalized Enhancer for **Low-Light** Images. (arXiv:2207.07317v1 [cs.CV])
- Authors : Naishan Zheng, Jie Huang, Qi Zhu, Man Zhou, Feng Zhao, Jun Zha
- Link : [http://arxiv.org/abs/2207.07317](http://arxiv.org/abs/2207.07317)
> ABSTRACT  :  **Low-light** image **enhancement** is an inherently subjective process whose targets vary with the user's aesthetic. Motivated by this, several personalized **enhancement** methods have been investigated. However, the **enhancement** process based on user preferences in these techniques is invisible, i.e., a "black box". In this work, we propose an intelligible unsupervised personalized enhancer (iUPEnhancer) for **low-light** images, which establishes the correlations between the **low-light** and the unpaired reference images with regard to three user-friendly attributions (brightness, chromaticity, and noise). The proposed iUP-Enhancer is trained with the guidance of these correlations and the corresponding unsupervised loss functions. Rather than a "black box" process, our iUP-Enhancer presents an intelligible **enhancement** process with the above attributions. Extensive experiments demonstrate that the proposed algorithm produces competitive qualitative and quantitative results while maintaining excellent flexibility and scalability. This can be validated by personalization with single/multiple references, cross-attribution references, or merely adjusting parameters.  
### Trainable Joint **Bilateral** Filters for Enhanced Prediction Stability in Low-dose CT. (arXiv:2207.07368v1 [eess.IV])
- Authors : Fabian Wagner, Mareike Thies, Felix Denzinger, Mingxuan Gu, Mayank Patwari, Stefan Ploner, Noah Maul, Laura Pfaff, Yixing Huang, Andreas Maier
- Link : [http://arxiv.org/abs/2207.07368](http://arxiv.org/abs/2207.07368)
> ABSTRACT  :  Low-dose computed tomography (CT) denoising algorithms aim to enable reduced patient dose in routine CT acquisitions while maintaining high image quality. Recently, deep learning~(DL)-based methods were introduced, outperforming conventional denoising algorithms on this task due to their high model capacity. However, for the transition of DL-based denoising to clinical practice, these data-driven approaches must generalize robustly beyond the seen training data. We, therefore, propose a hybrid denoising approach consisting of a set of trainable joint **bilateral** filters (JBFs) combined with a convolutional DL-based denoising network to predict the guidance image. Our proposed denoising pipeline combines the high model capacity enabled by DL-based feature extraction with the reliability of the conventional JBF. The pipeline's ability to generalize is demonstrated by training on abdomen CT scans without metal implants and testing on abdomen scans with metal implants as well as on head CT data. When embedding two well-established DL-based denoisers (RED-CNN/QAE) in our pipeline, the denoising performance is improved by $10\,\%$/$82\,\%$ (RMSE) and $3\,\%$/$81\,\%$ (PSNR) in regions containing metal and by $6\,\%$/$78\,\%$ (RMSE) and $2\,\%$/$4\,\%$ (PSNR) on head CT data, compared to the respective vanilla model. Concluding, the proposed trainable JBFs limit the error bound of deep neural networks to facilitate the applicability of DL-based denoisers in low-dose CT pipelines.  
### MegaPortraits: One-shot Megapixel Neural Head Avatars. (arXiv:2207.07621v1 [cs.CV])
- Authors : Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, Egor Zakharov
- Link : [http://arxiv.org/abs/2207.07621](http://arxiv.org/abs/2207.07621)
> ABSTRACT  :  In this work, we advance the neural head avatar technology to the megapixel resolution while focusing on the particularly challenging task of cross-driving synthesis, i.e., when the appearance of the driving image is substantially different from the animated source image. We propose a set of new neural architectures and training methods that can leverage both medium-resolution video data and high-resolution image data to achieve the desired levels of rendered image quality and generalization to novel views and motion. We demonstrate that suggested architectures and methods produce convincing high-resolution neural avatars, outperforming the competitors in the cross-driving scenario. Lastly, we show how a trained high-resolution neural avatar model can be distilled into a lightweight student model which runs in real-time and locks the identities of neural avatars to several dozens of pre-defined source images. **Real-time** operation and identity lock are essential for many practical applications head avatar systems.  
### UP**HDR**-GAN: Generative Adversarial Network for **High Dynamic Range** Imaging with Unpaired Data. (arXiv:2102.01850v2 [eess.IV] UPDATED)
- Authors : Ru Li, Chuan Wang, Jue Wang, Guanghui Liu, Yu Zhang, Bing Zeng, Shuaicheng Liu
- Link : [http://arxiv.org/abs/2102.01850](http://arxiv.org/abs/2102.01850)
> ABSTRACT  :  The paper proposes a method to effectively fuse multi-**exposure** inputs and generate high-quality **high dynamic range** (**HDR**) images with unpaired datasets. Deep learning-based **HDR** image generation methods rely heavily on paired datasets. The ground truth images play a leading role in generating reasonable **HDR** images. Datasets without ground truth are hard to be applied to train deep neural networks. Recently, Generative Adversarial Networks (GAN) have demonstrated their potentials of translating images from source domain X to target domain Y in the absence of paired examples. In this paper, we propose a GAN-based network for solving such problems while generating enjoyable **HDR** results, named UP**HDR**-GAN. The proposed method relaxes the constraint of the paired dataset and learns the mapping from the LDR domain to the **HDR** domain. Although the pair data are missing, UP**HDR**-GAN can properly handle the ghosting artifacts caused by moving objects or misalignments with the help of the modified GAN loss, the improved discriminator network and the useful initialization phase. The proposed method preserves the details of important regions and improves the total image perceptual quality. Qualitative and quantitative comparisons against the representative methods demonstrate the superiority of the proposed UP**HDR**-GAN.  
### CVFNet: **Real-time** 3D Object Detection by Learning Cross View Features. (arXiv:2203.06585v2 [cs.CV] UPDATED)
- Authors : Jiaqi Gu, Zhiyu Xiang, Pan Zhao, Tingming Bai, Lingxuan Wang, Xijun Zhao, Zhiyuan Zhang
- Link : [http://arxiv.org/abs/2203.06585](http://arxiv.org/abs/2203.06585)
> ABSTRACT  :  In recent years 3D object detection from LiDAR point clouds has made great progress thanks to the development of deep learning technologies. Although voxel or point based methods are popular in 3D object detection, they usually involve time-consuming operations such as 3D convolutions on voxels or ball query among points, making the resulting network inappropriate for time critical applications. On the other hand, 2D view-based methods feature high computing efficiency while usually obtaining inferior performance than the voxel or point based methods. In this work, we present a real-time view-based single stage 3D object detector, namely CVFNet to fulfill this task. To strengthen the cross-view feature learning under the condition of demanding efficiency, our framework extracts the features of different views and fuses them in an efficient progressive way. We first propose a novel Point-Range feature fusion module that deeply integrates point and range view features in multiple stages. Then, a special Slice Pillar is designed to well maintain the 3D geometry when transforming the obtained deep point-view features into bird's eye view. To better balance the ratio of samples, a sparse pillar detection head is presented to focus the detection on the nonempty grids. We conduct experiments on the popular KITTI and NuScenes benchmark, and state-of-the-art performances are achieved in terms of both accuracy and speed.  
### Implicit Neural Representations for Variable Length Human Motion Generation. (arXiv:2203.13694v2 [cs.CV] UPDATED)
- Authors : Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, Koichi Shinoda
- Link : [http://arxiv.org/abs/2203.13694](http://arxiv.org/abs/2203.13694)
> ABSTRACT  :  We propose an action-conditional human motion generation method using variational **implicit neural representation**s (INR). The variational formalism enables action-conditional distributions of INRs, from which one can easily sample representations to generate novel human motion sequences. Our method offers variable-length sequence generation by construction because a part of INR is optimized for a whole sequence of arbitrary length with temporal embeddings. In contrast, previous works reported difficulties with modeling variable-length sequences. We confirm that our method with a Transformer decoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC datasets in terms of realism and diversity of generated motions. Surprisingly, even our method with an MLP decoder consistently outperforms the state-of-the-art Transformer-based auto-encoder. In particular, we show that variable-length motions generated by our method are better than fixed-length motions generated by the state-of-the-art method in terms of realism and diversity. Code at https://github.com/PACerv/ImplicitMotion.  
### DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v3 [cs.CV] UPDATED)
- Authors : Shuai Chen, Xinghui Li, Zirui Wang, Victor Adrian
- Link : [http://arxiv.org/abs/2204.00559](http://arxiv.org/abs/2204.00559)
> ABSTRACT  :  We introduce a camera relocalization pipeline that combines absolute pose regression (APR) and direct feature matching. By incorporating **exposure**-adaptive novel view synthesis, our method successfully addresses photometric distortions in outdoor environments that existing photometric-based methods fail to handle. With domain-invariant feature matching, our solution improves pose regression accuracy using semi-supervised learning on unlabeled data. In particular, the pipeline consists of two components: Novel View Synthesizer and DFNet. The former synthesizes novel views compensating for changes in **exposure** and the latter regresses camera poses and extracts robust features that close the domain gap between real images and synthetic ones. Furthermore, we introduce an online synthetic data generation scheme. We show that these approaches effectively enhance camera pose estimation both in indoor and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by outperforming existing single-image APR methods by as much as 56%, comparable to 3D structure-based methods.  
### Attentive Fine-Grained Structured Sparsity for Image **Restoration**. (arXiv:2204.12266v2 [cs.CV] UPDATED)
- Authors : Junghun Oh, Heewon Kim, Seungjun Nah, Cheeun Hong, Jonghyun Choi, Kyoung Mu
- Link : [http://arxiv.org/abs/2204.12266](http://arxiv.org/abs/2204.12266)
> ABSTRACT  :  Image **restoration** tasks have witnessed great performance improvement in recent years by developing large deep models. Despite the outstanding performance, the heavy computation demanded by the deep models has restricted the application of image **restoration**. To lift the restriction, it is required to reduce the size of the networks while maintaining accuracy. Recently, N:M structured pruning has appeared as one of the effective and practical pruning approaches for making the model efficient with the accuracy constraint. However, it fails to account for different computational complexities and performance requirements for different layers of an image **restoration** network. To further optimize the trade-off between the efficiency and the **restoration** accuracy, we propose a novel pruning method that determines the pruning ratio for N:M structured sparsity at each layer. Extensive experimental results on super-resolution and deblurring tasks demonstrate the efficacy of our method which outperforms previous pruning methods significantly. PyTorch implementation for the proposed methods will be publicly available at https://github.com/JungHunOh/SLS_CVPR2022.  
### HyperRes: Efficient Hypernetwork-Based Continuous Image **Restoration**. (arXiv:2206.05970v2 [cs.CV] UPDATED)
- Authors : Shai Aharon, Gil Ben
- Link : [http://arxiv.org/abs/2206.05970](http://arxiv.org/abs/2206.05970)
> ABSTRACT  :  Continuous image **restoration** attempts to provide a model that can restore images with unseen degradation levels during training at inference time. Existing methods are limited in terms of either the accuracy of the **restoration**, the range of degradation levels they can support, or the size of the model they require. We introduce a novel approach that achieves the optimal accuracy of multiple dedicated models for a wide range of degradation levels with the same number of parameters as a single base model. We present a hypernetwork that can efficiently generate an image **restoration** network to best adapt to the required level of degradation. Experiments on popular datasets show that our approach outperforms the state-of-the-art for a variety of image **restoration** tasks, including denoising, DeJPEG, and super-resolution.  
## eess.IV
---
### Trainable Joint **Bilateral** Filters for Enhanced Prediction Stability in Low-dose CT. (arXiv:2207.07368v1 [eess.IV])
- Authors : Fabian Wagner, Mareike Thies, Felix Denzinger, Mingxuan Gu, Mayank Patwari, Stefan Ploner, Noah Maul, Laura Pfaff, Yixing Huang, Andreas Maier
- Link : [http://arxiv.org/abs/2207.07368](http://arxiv.org/abs/2207.07368)
> ABSTRACT  :  Low-dose computed tomography (CT) denoising algorithms aim to enable reduced patient dose in routine CT acquisitions while maintaining high image quality. Recently, deep learning~(DL)-based methods were introduced, outperforming conventional denoising algorithms on this task due to their high model capacity. However, for the transition of DL-based denoising to clinical practice, these data-driven approaches must generalize robustly beyond the seen training data. We, therefore, propose a hybrid denoising approach consisting of a set of trainable joint **bilateral** filters (JBFs) combined with a convolutional DL-based denoising network to predict the guidance image. Our proposed denoising pipeline combines the high model capacity enabled by DL-based feature extraction with the reliability of the conventional JBF. The pipeline's ability to generalize is demonstrated by training on abdomen CT scans without metal implants and testing on abdomen scans with metal implants as well as on head CT data. When embedding two well-established DL-based denoisers (RED-CNN/QAE) in our pipeline, the denoising performance is improved by $10\,\%$/$82\,\%$ (RMSE) and $3\,\%$/$81\,\%$ (PSNR) in regions containing metal and by $6\,\%$/$78\,\%$ (RMSE) and $2\,\%$/$4\,\%$ (PSNR) on head CT data, compared to the respective vanilla model. Concluding, the proposed trainable JBFs limit the error bound of deep neural networks to facilitate the applicability of DL-based denoisers in low-dose CT pipelines.  
### UP**HDR**-GAN: Generative Adversarial Network for **High Dynamic Range** Imaging with Unpaired Data. (arXiv:2102.01850v2 [eess.IV] UPDATED)
- Authors : Ru Li, Chuan Wang, Jue Wang, Guanghui Liu, Yu Zhang, Bing Zeng, Shuaicheng Liu
- Link : [http://arxiv.org/abs/2102.01850](http://arxiv.org/abs/2102.01850)
> ABSTRACT  :  The paper proposes a method to effectively fuse multi-**exposure** inputs and generate high-quality **high dynamic range** (**HDR**) images with unpaired datasets. Deep learning-based **HDR** image generation methods rely heavily on paired datasets. The ground truth images play a leading role in generating reasonable **HDR** images. Datasets without ground truth are hard to be applied to train deep neural networks. Recently, Generative Adversarial Networks (GAN) have demonstrated their potentials of translating images from source domain X to target domain Y in the absence of paired examples. In this paper, we propose a GAN-based network for solving such problems while generating enjoyable **HDR** results, named UP**HDR**-GAN. The proposed method relaxes the constraint of the paired dataset and learns the mapping from the LDR domain to the **HDR** domain. Although the pair data are missing, UP**HDR**-GAN can properly handle the ghosting artifacts caused by moving objects or misalignments with the help of the modified GAN loss, the improved discriminator network and the useful initialization phase. The proposed method preserves the details of important regions and improves the total image perceptual quality. Qualitative and quantitative comparisons against the representative methods demonstrate the superiority of the proposed UP**HDR**-GAN.  
### HyperRes: Efficient Hypernetwork-Based Continuous Image **Restoration**. (arXiv:2206.05970v2 [cs.CV] UPDATED)
- Authors : Shai Aharon, Gil Ben
- Link : [http://arxiv.org/abs/2206.05970](http://arxiv.org/abs/2206.05970)
> ABSTRACT  :  Continuous image **restoration** attempts to provide a model that can restore images with unseen degradation levels during training at inference time. Existing methods are limited in terms of either the accuracy of the **restoration**, the range of degradation levels they can support, or the size of the model they require. We introduce a novel approach that achieves the optimal accuracy of multiple dedicated models for a wide range of degradation levels with the same number of parameters as a single base model. We present a hypernetwork that can efficiently generate an image **restoration** network to best adapt to the required level of degradation. Experiments on popular datasets show that our approach outperforms the state-of-the-art for a variety of image **restoration** tasks, including denoising, DeJPEG, and super-resolution.  
## cs.LG
---
### Direction-Aware Joint Adaptation of Neural Speech **Enhancement** and Recognition in Real Multiparty Conversational Environments. (arXiv:2207.07273v1 [eess.AS])
- Authors : Yicheng Du, Aditya Arie, Kouhei Sekiguchi, Yoshiaki Bando, Mathieu Fontaine, Kazuyoshi Yoshii
- Link : [http://arxiv.org/abs/2207.07273](http://arxiv.org/abs/2207.07273)
> ABSTRACT  :  This paper describes noisy speech recognition for an augmented reality headset that helps verbal communication within real multiparty conversational environments. A major approach that has actively been studied in simulated environments is to sequentially perform speech **enhancement** and automatic speech recognition (ASR) based on deep neural networks (DNNs) trained in a supervised manner. In our task, however, such a pretrained system fails to work due to the mismatch between the training and test conditions and the head movements of the user. To enhance only the utterances of a target speaker, we use beamforming based on a DNN-based speech mask estimator that can adaptively extract the speech components corresponding to a head-relative particular direction. We propose a semi-supervised adaptation method that jointly updates the mask estimator and the ASR model at run-time using clean speech signals with ground-truth transcriptions and noisy speech signals with highly-confident estimated transcriptions. Comparative experiments using the state-of-the-art distant speech recognition system show that the proposed method significantly improves the ASR performance.  
### Direction-Aware Adaptive Online Neural Speech **Enhancement** with an Augmented Reality Headset in Real Noisy Conversational Environments. (arXiv:2207.07296v1 [eess.AS])
- Authors : Kouhei Sekiguchi, Aditya Arie, Yicheng Du, Yoshiaki Bando, Mathieu Fontaine, Kazuyoshi Yoshii
- Link : [http://arxiv.org/abs/2207.07296](http://arxiv.org/abs/2207.07296)
> ABSTRACT  :  This paper describes the practical response- and performance-aware development of online speech **enhancement** for an augmented reality (AR) headset that helps a user understand conversations made in real noisy echoic environments (e.g., cocktail party). One may use a state-of-the-art blind source separation method called fast multichannel nonnegative matrix factorization (FastMNMF) that works well in various environments thanks to its unsupervised nature. Its heavy computational cost, however, prevents its application to real-time processing. In contrast, a supervised beamforming method that uses a deep neural network (DNN) for estimating spatial information of speech and noise readily fits real-time processing, but suffers from drastic performance degradation in mismatched conditions. Given such complementary characteristics, we propose a dual-process robust online speech **enhancement** method based on DNN-based beamforming with FastMNMF-guided adaptation. FastMNMF (back end) is performed in a mini-batch style and the noisy and enhanced speech pairs are used together with the original parallel training data for updating the direction-aware DNN (front end) with backpropagation at a computationally-allowable interval. This method is used with a blind dereverberation method called weighted prediction error (WPE) for transcribing the noisy reverberant speech of a speaker, which can be detected from video or selected by a user's hand gesture or eye gaze, in a streaming manner and spatially showing the transcriptions with an AR technique. Our experiment showed that the word error rate was improved by more than 10 points with the run-time adaptation using only twelve minutes of observation.  
### MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources. (arXiv:2207.07307v1 [eess.AS])
- Authors : Haoran Yin, Meng Ge, Yanjie Fu, Gaoyan Zhang, Longbiao Wang, **Lei Zhang**, Lin Qiu, Jianwu Dang
- Link : [http://arxiv.org/abs/2207.07307](http://arxiv.org/abs/2207.07307)
> ABSTRACT  :  Recent neural network based Direction of Arrival (DoA) estimation algorithms have performed well on unknown number of sound sources scenarios. These algorithms are usually achieved by mapping the multi-channel audio input to the single output (i.e. overall spatial pseudo-spectrum (SPS) of all sources), that is called MISO. However, such MISO algorithms strongly depend on empirical threshold setting and the angle assumption that the angles between the sound sources are greater than a fixed angle. To address these limitations, we propose a novel multi-channel input and multiple outputs DoA network called MIMO-DoAnet. Unlike the general MISO algorithms, MIMO-DoAnet predicts the SPS coding of each sound source with the help of the informative spatial covariance matrix. By doing so, the threshold task of detecting the number of sound sources becomes an easier task of detecting whether there is a sound source in each output, and the serious interaction between sound sources disappears during inference stage. Experimental results show that MIMO-DoAnet achieves relative 18.6% and absolute 13.3%, relative 34.4% and absolute 20.2% F1 score improvement compared with the MISO baseline system in 3, 4 sources scenes. The results also demonstrate MIMO-DoAnet alleviates the threshold setting problem and solves the angle assumption problem effectively.  
### Online Continual Learning for Embedded Devices. (arXiv:2203.10681v3 [cs.LG] UPDATED)
- Authors : Christopher Kanan
- Link : [http://arxiv.org/abs/2203.10681](http://arxiv.org/abs/2203.10681)
> ABSTRACT  :  **Real-time** on-device continual learning is needed for new applications such as home robots, user personalization on smartphones, and augmented/virtual reality headsets. However, this setting poses unique challenges: embedded devices have limited memory and compute capacity and conventional machine learning models suffer from catastrophic forgetting when updated on non-stationary data streams. While several online continual learning models have been developed, their effectiveness for embedded applications has not been rigorously studied. In this paper, we first identify criteria that online continual learners must meet to effectively perform real-time, on-device learning. We then study the efficacy of several online continual learning methods when used with mobile neural networks. We measure their performance, memory usage, compute requirements, and ability to generalize to out-of-domain inputs.  
## cs.AI
---
### **Enhancement** by Your Aesthetic: An Intelligible Unsupervised Personalized Enhancer for **Low-Light** Images. (arXiv:2207.07317v1 [cs.CV])
- Authors : Naishan Zheng, Jie Huang, Qi Zhu, Man Zhou, Feng Zhao, Jun Zha
- Link : [http://arxiv.org/abs/2207.07317](http://arxiv.org/abs/2207.07317)
> ABSTRACT  :  **Low-light** image **enhancement** is an inherently subjective process whose targets vary with the user's aesthetic. Motivated by this, several personalized **enhancement** methods have been investigated. However, the **enhancement** process based on user preferences in these techniques is invisible, i.e., a "black box". In this work, we propose an intelligible unsupervised personalized enhancer (iUPEnhancer) for **low-light** images, which establishes the correlations between the **low-light** and the unpaired reference images with regard to three user-friendly attributions (brightness, chromaticity, and noise). The proposed iUP-Enhancer is trained with the guidance of these correlations and the corresponding unsupervised loss functions. Rather than a "black box" process, our iUP-Enhancer presents an intelligible **enhancement** process with the above attributions. Extensive experiments demonstrate that the proposed algorithm produces competitive qualitative and quantitative results while maintaining excellent flexibility and scalability. This can be validated by personalization with single/multiple references, cross-attribution references, or merely adjusting parameters.  
### Online Continual Learning for Embedded Devices. (arXiv:2203.10681v3 [cs.LG] UPDATED)
- Authors : Christopher Kanan
- Link : [http://arxiv.org/abs/2203.10681](http://arxiv.org/abs/2203.10681)
> ABSTRACT  :  **Real-time** on-device continual learning is needed for new applications such as home robots, user personalization on smartphones, and augmented/virtual reality headsets. However, this setting poses unique challenges: embedded devices have limited memory and compute capacity and conventional machine learning models suffer from catastrophic forgetting when updated on non-stationary data streams. While several online continual learning models have been developed, their effectiveness for embedded applications has not been rigorously studied. In this paper, we first identify criteria that online continual learners must meet to effectively perform real-time, on-device learning. We then study the efficacy of several online continual learning methods when used with mobile neural networks. We measure their performance, memory usage, compute requirements, and ability to generalize to out-of-domain inputs.  
# Paper List
---
## cs.CV
---
**96** new papers in cs.CV:-) 
1. Image Clustering with Contrastive Learning and Multi-scale Graph Convolutional Networks. (arXiv:2207.07173v1 [cs.CV])
2. Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification. (arXiv:2207.07189v1 [cs.CV])
3. Lipschitz Bound Analysis of Neural Networks. (arXiv:2207.07232v1 [cs.LG])
4. Single Model Uncertainty Estimation via Stochastic Data Centering. (arXiv:2207.07235v1 [cs.LG])
5. Classification of Bark Beetle-Induced Forest Tree Mortality using Deep Learning. (arXiv:2207.07241v1 [cs.CV])
6. LineCap: Line Charts for Data Visualization Captioning Models. (arXiv:2207.07243v1 [cs.CV])
7. Decoupling Recognition from Detection: Single Shot Self-Reliant Scene Text Spotter. (arXiv:2207.07253v1 [cs.CV])
8. ScaleNet: Searching for the Model to Scale. (arXiv:2207.07267v1 [cs.CV])
9. Lightweight Vision Transformer with Cross Feature Attention. (arXiv:2207.07268v1 [cs.CV])
10. Weakly Supervised Video Salient Object Detection via Point Supervision. (arXiv:2207.07269v1 [cs.CV])
11. Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization. (arXiv:2207.07278v1 [cs.CV])
12. Parameterization of Cross-Token Relations with Relative Positional Encoding for Vision MLP. (arXiv:2207.07284v1 [cs.CV])
13. X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval. (arXiv:2207.07285v1 [cs.CV])
14. WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation. (arXiv:2207.07288v1 [cs.CV])
15. Robust Deep Compressive Sensing with Recurrent-Residual Structural Constraints. (arXiv:2207.07301v1 [eess.IV])
16. Towards Better Dermoscopic Image Feature Representation Learning for Melanoma Classification. (arXiv:2207.07303v1 [eess.IV])
17. Towards Privacy-Preserving Person Re-identification via Person Identify Shift. (arXiv:2207.07311v1 [cs.CV])
18. Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain. (arXiv:2207.07316v1 [cs.CV])
19. **Enhancement** by Your Aesthetic: An Intelligible Unsupervised Personalized Enhancer for **Low-Light** Images. (arXiv:2207.07317v1 [cs.CV])
20. Stereo Co-capture System for Recording and Tracking Fish with Frame- and Event Cameras. (arXiv:2207.07332v1 [cs.CV])
21. Rain Rate Estimation with SAR using NEXRAD measurements with Convolutional Neural Networks. (arXiv:2207.07333v1 [cs.CV])
22. Learning Parallax Transformer Network for Stereo Image JPEG Artifacts Removal. (arXiv:2207.07335v1 [cs.CV])
23. DuetFace: Collaborative Privacy-Preserving Face Recognition via Channel Splitting in the Frequency Domain. (arXiv:2207.07340v1 [cs.CV])
24. Feasibility of Inconspicuous GAN-generated Adversarial Patches against Object Detection. (arXiv:2207.07347v1 [cs.CV])
25. Diverse Human Motion Prediction via Gumbel-Softmax Sampling from an Auxiliary Space. (arXiv:2207.07351v1 [cs.CV])
26. Registration based Few-Shot Anomaly Detection. (arXiv:2207.07361v1 [cs.CV])
27. Trainable Joint **Bilateral** Filters for Enhanced Prediction Stability in Low-dose CT. (arXiv:2207.07368v1 [eess.IV])
28. CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer with Modality-Correlated Cross-Attention for Brain Tumor Segmentation. (arXiv:2207.07370v1 [eess.IV])
29. 3D Instances as 1D Kernels. (arXiv:2207.07372v1 [cs.CV])
30. A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion. (arXiv:2207.07381v1 [cs.CV])
31. LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds Representing Laparoscopic Scenes. (arXiv:2207.07418v1 [cs.CV])
32. Multi-Object Tracking and Segmentation via Neural Message Passing. (arXiv:2207.07454v1 [cs.CV])
33. DeepSolar tracker: towards unsupervised assessment with open-source data of the accuracy of deep learning-based distributed PV mapping. (arXiv:2207.07466v1 [cs.CV])
34. USegScene: Unsupervised Learning of Depth, Optical Flow and Ego-Motion with Semantic Guidance and Coupled Networks. (arXiv:2207.07469v1 [cs.CV])
35. RESECT-SEG: Open access annotations of intra-operative brain tumor ultrasound images. (arXiv:2207.07494v1 [physics.med-ph])
36. Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v1 [cs.LG])
37. On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection. (arXiv:2207.07517v1 [cs.LG])
38. Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation. (arXiv:2207.07522v1 [cs.CV])
39. 3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models. (arXiv:2207.07539v1 [cs.CV])
40. CheXplaining in Style: Counterfactual Explanations for Chest X-rays using StyleGAN. (arXiv:2207.07553v1 [eess.IV])
41. Mobile Keystroke Biometrics Using Transformers. (arXiv:2207.07596v1 [cs.CR])
42. ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning. (arXiv:2207.07601v1 [cs.CV])
43. Image and Texture Independent Deep Learning Noise Estimation using Multiple Frames. (arXiv:2207.07604v1 [cs.CV])
44. DOLPHINS: Dataset for Collaborative Perception enabled Harmonious and Interconnected Self-driving. (arXiv:2207.07609v1 [cs.CV])
45. Position Prediction as an Effective Pretraining Strategy. (arXiv:2207.07611v1 [cs.LG])
46. A Non-Anatomical Graph Structure for isolated hand gesture separation in continuous gesture sequences. (arXiv:2207.07619v1 [cs.CV])
47. MegaPortraits: One-shot Megapixel Neural Head Avatars. (arXiv:2207.07621v1 [cs.CV])
48. Brain MRI study for glioma segmentation using convolutional neural networks and original post-processing techniques with low computational demand. (arXiv:2207.07622v1 [eess.IV])
49. GUSOT: Green and Unsupervised Single Object Tracking for Long Video Sequences. (arXiv:2207.07629v1 [cs.CV])
50. Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning. (arXiv:2207.07635v1 [cs.CV])
51. Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models. (arXiv:2207.07646v1 [cs.CV])
52. Distributionally Robust Deep Learning using Hardness Weighted Sampling. (arXiv:2001.02658v4 [cs.LG] UPDATED)
53. clDice -- A Novel Topology-Preserving Loss Function for Tubular Structure Segmentation. (arXiv:2003.07311v7 [cs.CV] UPDATED)
54. ODFNet: Using orientation distribution functions to characterize 3D point clouds. (arXiv:2012.04708v2 [cs.CV] UPDATED)
55. JigsawGAN: Auxiliary Learning for Solving Jigsaw Puzzles with Generative Adversarial Networks. (arXiv:2101.07555v3 [cs.CV] UPDATED)
56. UP**HDR**-GAN: Generative Adversarial Network for **High Dynamic Range** Imaging with Unpaired Data. (arXiv:2102.01850v2 [eess.IV] UPDATED)
57. AutoMix: Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v5 [cs.CV] UPDATED)
58. Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v4 [cs.CV] UPDATED)
59. ReFormer: The Relational Transformer for Image Captioning. (arXiv:2107.14178v2 [cs.CV] UPDATED)
60. EKTVQA: Generalized use of External Knowledge to empower Scene Text in Text-VQA. (arXiv:2108.09717v8 [cs.CV] UPDATED)
61. Combining Diverse Feature Priors. (arXiv:2110.08220v2 [cs.LG] UPDATED)
62. Revisiting Batch Norm Initialization. (arXiv:2110.13989v2 [cs.CV] UPDATED)
63. Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes. (arXiv:2111.15000v2 [cs.CV] UPDATED)
64. Prompting Visual-Language Models for Efficient Video Understanding. (arXiv:2112.04478v2 [cs.CV] UPDATED)
65. HODOR: High-level Object Descriptors for Object Re-segmentation in Video Learned from Static Images. (arXiv:2112.09131v2 [cs.CV] UPDATED)
66. Out-of-distribution Detection with Boundary Aware Learning. (arXiv:2112.11648v3 [cs.CV] UPDATED)
67. Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition. (arXiv:2112.12175v3 [cs.CV] UPDATED)
68. Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v3 [cs.SD] UPDATED)
69. Are We Ready for Robust and Resilient SLAM? A Framework For Quantitative Characterization of SLAM Datasets. (arXiv:2202.11312v2 [cs.RO] UPDATED)
70. PanoFlow: Learning 360{\deg} Optical Flow for Surrounding Temporal Understanding. (arXiv:2202.13388v2 [cs.CV] UPDATED)
71. Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v4 [cs.CV] UPDATED)
72. Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v3 [cs.MM] UPDATED)
73. CVFNet: **Real-time** 3D Object Detection by Learning Cross View Features. (arXiv:2203.06585v2 [cs.CV] UPDATED)
74. STDAN: Deformable Attention Network for Space-Time Video Super-Resolution. (arXiv:2203.06841v2 [cs.CV] UPDATED)
75. deepNIR: Datasets for generating synthetic NIR images and improved fruit detection system using deep learning techniques. (arXiv:2203.09091v2 [cs.CV] UPDATED)
76. Implicit Neural Representations for Variable Length Human Motion Generation. (arXiv:2203.13694v2 [cs.CV] UPDATED)
77. DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v3 [cs.CV] UPDATED)
78. Multimodal Token Fusion for Vision Transformers. (arXiv:2204.08721v2 [cs.CV] UPDATED)
79. Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v3 [cs.CV] UPDATED)
80. Attentive Fine-Grained Structured Sparsity for Image **Restoration**. (arXiv:2204.12266v2 [cs.CV] UPDATED)
81. Self-supervised learning in non-small cell lung cancer discovers novel morphological clusters linked to patient outcome and molecular phenotypes. (arXiv:2205.01931v2 [cs.CV] UPDATED)
82. Distilling Inter-Class Distance for Semantic Segmentation. (arXiv:2205.03650v2 [cs.CV] UPDATED)
83. Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT. (arXiv:2205.07180v2 [eess.AS] UPDATED)
84. blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v2 [cs.CV] UPDATED)
85. HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images. (arXiv:2205.08390v2 [eess.IV] UPDATED)
86. SALAD: Source-free Active Label-Agnostic Domain Adaptation for Classification, Segmentation and Detection. (arXiv:2205.12840v2 [cs.CV] UPDATED)
87. ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping. (arXiv:2206.05963v3 [cs.CV] UPDATED)
88. HyperRes: Efficient Hypernetwork-Based Continuous Image **Restoration**. (arXiv:2206.05970v2 [cs.CV] UPDATED)
89. Embodied Scene-aware Human Pose Estimation. (arXiv:2206.09106v2 [cs.CV] UPDATED)
90. Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v2 [cs.CR] UPDATED)
91. Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling. (arXiv:2207.05249v3 [cs.CV] UPDATED)
92. Controllable Shadow Generation Using Pixel Height Maps. (arXiv:2207.05385v2 [cs.CV] UPDATED)
93. Tracking Objects as Pixel-wise Distributions. (arXiv:2207.05518v2 [cs.CV] UPDATED)
94. Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting. (arXiv:2207.06694v2 [cs.CV] UPDATED)
95. iColoriT: Towards Propagating Local Hint to the Right Region in Interactive Colorization by Leveraging Vision Transformer. (arXiv:2207.06831v2 [cs.CV] UPDATED)
96. Benchmarking Omni-Vision Representation through the Lens of Visual Realms. (arXiv:2207.07106v2 [cs.CV] UPDATED)
## eess.IV
---
**18** new papers in eess.IV:-) 
1. WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation. (arXiv:2207.07288v1 [cs.CV])
2. Robust Deep Compressive Sensing with Recurrent-Residual Structural Constraints. (arXiv:2207.07301v1 [eess.IV])
3. Towards Better Dermoscopic Image Feature Representation Learning for Melanoma Classification. (arXiv:2207.07303v1 [eess.IV])
4. On the Construction of Averaged Deep Denoisers for Image Regularization. (arXiv:2207.07321v1 [eess.IV])
5. Computer Vision for Volunteer Cotton Detection in a Corn Field with UAS Remote Sensing Imagery and Spot Spray Applications. (arXiv:2207.07334v1 [eess.IV])
6. Trainable Joint **Bilateral** Filters for Enhanced Prediction Stability in Low-dose CT. (arXiv:2207.07368v1 [eess.IV])
7. CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer with Modality-Correlated Cross-Attention for Brain Tumor Segmentation. (arXiv:2207.07370v1 [eess.IV])
8. CheXplaining in Style: Counterfactual Explanations for Chest X-rays using StyleGAN. (arXiv:2207.07553v1 [eess.IV])
9. DOLPHINS: Dataset for Collaborative Perception enabled Harmonious and Interconnected Self-driving. (arXiv:2207.07609v1 [cs.CV])
10. Brain MRI study for glioma segmentation using convolutional neural networks and original post-processing techniques with low computational demand. (arXiv:2207.07622v1 [eess.IV])
11. Dual-space Compressed Sensing. (arXiv:2207.07627v1 [eess.IV])
12. clDice -- A Novel Topology-Preserving Loss Function for Tubular Structure Segmentation. (arXiv:2003.07311v7 [cs.CV] UPDATED)
13. UP**HDR**-GAN: Generative Adversarial Network for **High Dynamic Range** Imaging with Unpaired Data. (arXiv:2102.01850v2 [eess.IV] UPDATED)
14. PanoFlow: Learning 360{\deg} Optical Flow for Surrounding Temporal Understanding. (arXiv:2202.13388v2 [cs.CV] UPDATED)
15. Analysis of the Visually Detectable Wear Progress on Ball Screws. (arXiv:2205.01149v2 [eess.IV] UPDATED)
16. blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v2 [cs.CV] UPDATED)
17. HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images. (arXiv:2205.08390v2 [eess.IV] UPDATED)
18. HyperRes: Efficient Hypernetwork-Based Continuous Image **Restoration**. (arXiv:2206.05970v2 [cs.CV] UPDATED)
## cs.LG
---
**131** new papers in cs.LG:-) 
1. Making Linear MDPs Practical via Contrastive Representation Learning. (arXiv:2207.07150v1 [cs.LG])
2. Case study on quantum convolutional neural network scalability. (arXiv:2207.07160v1 [cs.ET])
3. Audio-guided Album Cover Art Generation with Genetic Algorithms. (arXiv:2207.07162v1 [cs.SD])
4. K-level Reasoning for Zero-Shot Coordination in Hanabi. (arXiv:2207.07166v1 [cs.AI])
5. Causal Graphs Underlying Generative Models: Path to Learning with Limited Data. (arXiv:2207.07174v1 [cs.LG])
6. Contrastive Adapters for Foundation Model Group Robustness. (arXiv:2207.07180v1 [cs.LG])
7. NASRec: Weight Sharing Neural Architecture Search for Recommender Systems. (arXiv:2207.07187v1 [cs.IR])
8. Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification. (arXiv:2207.07189v1 [cs.CV])
9. COOR-PLT: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning. (arXiv:2207.07195v1 [cs.LG])
10. Provably Adversarially Robust Nearest Prototype Classifiers. (arXiv:2207.07208v1 [cs.LG])
11. Sound Randomized Smoothing in Floating-Point Arithmetics. (arXiv:2207.07209v1 [cs.LG])
12. Attention, Filling in The Gaps for Generalization in Routing Problems. (arXiv:2207.07212v1 [cs.LG])
13. Assortment Optimization with Customer Choice Modeling in a Crowdfunding Setting. (arXiv:2207.07222v1 [q-fin.MF])
14. Accelerated Federated Learning with Decoupled Adaptive Optimization. (arXiv:2207.07223v1 [cs.LG])
15. Lipschitz Bound Analysis of Neural Networks. (arXiv:2207.07232v1 [cs.LG])
16. Single Model Uncertainty Estimation via Stochastic Data Centering. (arXiv:2207.07235v1 [cs.LG])
17. Emotion Recognition in Conversation using Probabilistic Soft Logic. (arXiv:2207.07238v1 [cs.LG])
18. LineCap: Line Charts for Data Visualization Captioning Models. (arXiv:2207.07243v1 [cs.CV])
19. On the Super-exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU($d$) Symmetry. (arXiv:2207.07250v1 [quant-ph])
20. Modeling Non-Cooperative Dialogue: Theoretical and Empirical Insights. (arXiv:2207.07255v1 [cs.CL])
21. Improving Task-free Continual Learning by Distributionally Robust Memory Evolution. (arXiv:2207.07256v1 [cs.LG])
22. Accelerated Probabilistic Marching Cubes by Deep Learning for Time-Varying Scalar Ensembles. (arXiv:2207.07260v1 [cs.LG])
23. ScaleNet: Searching for the Model to Scale. (arXiv:2207.07267v1 [cs.CV])
24. Set-based value operators for non-stationary Markovian environments. (arXiv:2207.07271v1 [cs.LG])
25. Direction-Aware Joint Adaptation of Neural Speech **Enhancement** and Recognition in Real Multiparty Conversational Environments. (arXiv:2207.07273v1 [eess.AS])
26. Riemannian Natural Gradient Methods. (arXiv:2207.07287v1 [math.OC])
27. Direction-Aware Adaptive Online Neural Speech **Enhancement** with an Augmented Reality Headset in Real Noisy Conversational Environments. (arXiv:2207.07296v1 [eess.AS])
28. Towards Better Dermoscopic Image Feature Representation Learning for Melanoma Classification. (arXiv:2207.07303v1 [eess.IV])
29. MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources. (arXiv:2207.07307v1 [eess.AS])
30. Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet Text. (arXiv:2207.07308v1 [cs.CL])
31. Pattern Analysis of Money Flow in the Bitcoin Blockchain. (arXiv:2207.07315v1 [cs.SI])
32. Context-sensitive neocortical neurons transform the effectiveness and efficiency of neural information processing. (arXiv:2207.07338v1 [cs.NE])
33. Feasibility of Inconspicuous GAN-generated Adversarial Patches against Object Detection. (arXiv:2207.07347v1 [cs.CV])
34. Error analysis for deep neural network approximations of parametric hyperbolic conservation laws. (arXiv:2207.07362v1 [math.NA])
35. An Approach for Link Prediction in Directed Complex Networks based on Asymmetric Similarity-Popularity. (arXiv:2207.07399v1 [cs.SI])
36. pathGCN: Learning General Graph Spatial Operators from Paths. (arXiv:2207.07408v1 [cs.LG])
37. Plex: Towards Reliability using Pretrained Large Model Extensions. (arXiv:2207.07411v1 [cs.LG])
38. Low Rank Approximation for General Tensor Networks. (arXiv:2207.07417v1 [cs.DS])
39. LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds Representing Laparoscopic Scenes. (arXiv:2207.07418v1 [cs.CV])
40. Joint Application of the Target Trial Causal Framework and Machine Learning Modeling to Optimize Antibiotic Therapy: Use Case on Acute Bacterial Skin and Skin Structure Infections due to Methicillin-resistant Staphylococcus aureus. (arXiv:2207.07458v1 [stat.ML])
41. Creating an Explainable Intrusion Detection System Using Self Organizing Maps. (arXiv:2207.07465v1 [cs.CR])
42. Stable Invariant Models via Koopman Spectra. (arXiv:2207.07475v1 [cs.LG])
43. The Mechanical Neural Network(MNN) -- A physical implementation of a multilayer perceptron for education and hands-on experimentation. (arXiv:2207.07482v1 [cs.LG])
44. A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. (arXiv:2207.07483v1 [cs.IR])
45. Communication-Efficient Diffusion Strategy for Performance Improvement of Federated Learning with Non-IID Data. (arXiv:2207.07493v1 [cs.DC])
46. Low-bit Shift Network for End-to-End Spoken Language Understanding. (arXiv:2207.07497v1 [cs.SD])
47. Explainable Sparse Knowledge Graph Completion via High-order Graph Reasoning Network. (arXiv:2207.07503v1 [cs.LG])
48. Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v1 [cs.LG])
49. Sparse Relational Reasoning with Object-Centric Representations. (arXiv:2207.07512v1 [cs.LG])
50. On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection. (arXiv:2207.07517v1 [cs.LG])
51. Short-Term Trajectory Prediction for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2207.07520v1 [cs.NI])
52. Heuristic-free Optimization of Force-Controlled Robot Search Strategies in Stochastic Environments. (arXiv:2207.07524v1 [cs.RO])
53. Modeling Quality and Machine Learning Pipelines through Extended Feature Models. (arXiv:2207.07528v1 [cs.SE])
54. Selection of the Most Probable Best. (arXiv:2207.07533v1 [stat.ME])
55. 3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models. (arXiv:2207.07539v1 [cs.CV])
56. Pick your Neighbor: Local Gauss-Southwell Rule for Fast Asynchronous Decentralized Optimization. (arXiv:2207.07543v1 [math.OC])
57. CheXplaining in Style: Counterfactual Explanations for Chest X-rays using StyleGAN. (arXiv:2207.07553v1 [eess.IV])
58. Skill-based Model-based Reinforcement Learning. (arXiv:2207.07560v1 [cs.LG])
59. QSAN: A Near-term Achievable Quantum Self-Attention Network. (arXiv:2207.07563v1 [quant-ph])
60. Rethinking Attention Mechanism in Time Series Classification. (arXiv:2207.07564v1 [cs.LG])
61. The Nature of Temporal Difference Errors in Multi-step Distributional Reinforcement Learning. (arXiv:2207.07570v1 [cs.LG])
62. Outlier detection of vital sign trajectories from COVID-19 patients. (arXiv:2207.07572v1 [cs.LG])
63. Quantitative Stock Investment by Routing Uncertainty-Aware Trading Experts: A Multi-Task Learning Approach. (arXiv:2207.07578v1 [q-fin.TR])
64. Analysis, Characterization, Prediction and Attribution of Extreme Atmospheric Events with Machine Learning: a Review. (arXiv:2207.07580v1 [cs.LG])
65. Does Twitter know your political views? POLiTweets dataset and semi-automatic method for political leaning discovery. (arXiv:2207.07586v1 [cs.CL])
66. A two-step machine learning approach to statistical post-processing of weather forecasts for power generation. (arXiv:2207.07589v1 [stat.ML])
67. OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text. (arXiv:2207.07597v1 [cs.CL])
68. Algorithms to estimate Shapley value feature attributions. (arXiv:2207.07605v1 [cs.LG])
69. Position Prediction as an Effective Pretraining Strategy. (arXiv:2207.07611v1 [cs.LG])
70. Blessing of Nonconvexity in Deep Linear Models: Depth Flattens the Optimization Landscape Around the True Solution. (arXiv:2207.07612v1 [cs.LG])
71. Feed-Forward Source-Free Latent Domain Adaptation via Cross-Attention. (arXiv:2207.07624v1 [cs.LG])
72. Computing-In-Memory Neural Network Accelerators for Safety-Critical Systems: Can Small Device Variations Be Disastrous?. (arXiv:2207.07626v1 [cs.AR])
73. Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning. (arXiv:2207.07635v1 [cs.CV])
74. A Probabilistic Autoencoder for Type Ia Supernovae Spectral Time Series. (arXiv:2207.07645v1 [astro-ph.CO])
75. Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models. (arXiv:2207.07646v1 [cs.CV])
76. Optimal Rates for Spectral Algorithms with Least-Squares Regression over Hilbert Spaces. (arXiv:1801.06720v4 [stat.ML] UPDATED)
77. Kernel Conjugate Gradient Methods with Random Projections. (arXiv:1811.01760v2 [stat.ML] UPDATED)
78. Distributionally Robust Deep Learning using Hardness Weighted Sampling. (arXiv:2001.02658v4 [cs.LG] UPDATED)
79. clDice -- A Novel Topology-Preserving Loss Function for Tubular Structure Segmentation. (arXiv:2003.07311v7 [cs.CV] UPDATED)
80. Optimal No-regret Learning in Repeated First-price Auctions. (arXiv:2003.09795v5 [cs.LG] UPDATED)
81. Permutationless Many-Jet Event Reconstruction with Symmetry Preserving Attention Networks. (arXiv:2010.09206v6 [hep-ex] UPDATED)
82. Feature Learning in Infinite-Width Neural Networks. (arXiv:2011.14522v3 [cs.LG] UPDATED)
83. ODFNet: Using orientation distribution functions to characterize 3D point clouds. (arXiv:2012.04708v2 [cs.CV] UPDATED)
84. Flexible Model Aggregation for Quantile Regression. (arXiv:2103.00083v4 [stat.ML] UPDATED)
85. Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond. (arXiv:2103.10689v3 [cs.LG] UPDATED)
86. Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error. (arXiv:2106.09613v2 [cs.LG] UPDATED)
87. Learning Sparse Fixed-Structure Gaussian Bayesian Networks. (arXiv:2107.10450v2 [cs.DS] UPDATED)
88. Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v4 [cs.CV] UPDATED)
89. Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v2 [stat.ML] UPDATED)
90. Assessments of epistemic uncertainty using Gaussian stochastic weight averaging for fluid-flow regression. (arXiv:2109.08248v2 [physics.flu-dyn] UPDATED)
91. Differentially Private Fine-tuning of Language Models. (arXiv:2110.06500v2 [cs.LG] UPDATED)
92. PER-ETD: A Polynomially Efficient Emphatic Temporal Difference Learning Method. (arXiv:2110.06906v2 [cs.LG] UPDATED)
93. Combining Diverse Feature Priors. (arXiv:2110.08220v2 [cs.LG] UPDATED)
94. Selective Regression Under Fairness Criteria. (arXiv:2110.15403v3 [cs.LG] UPDATED)
95. A Scalable AutoML Approach Based on Graph Neural Networks. (arXiv:2111.00083v4 [cs.LG] UPDATED)
96. FedFly: Towards Migration in Edge-based Distributed Federated Learning. (arXiv:2111.01516v2 [cs.DC] UPDATED)
97. Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes. (arXiv:2111.15000v2 [cs.CV] UPDATED)
98. An original model for multi-target learning of logical rules for knowledge graph reasoning. (arXiv:2112.06189v2 [cs.AI] UPDATED)
99. Acoustic scene classification using auditory datasets. (arXiv:2112.13450v2 [cs.SD] UPDATED)
100. Graph Signal Reconstruction Techniques for IoT Air Pollution Monitoring Platforms. (arXiv:2201.00378v3 [eess.SP] UPDATED)
101. Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v3 [cs.SD] UPDATED)
102. Causal Inference Through the Structural Causal Marginal Problem. (arXiv:2202.01300v3 [cs.AI] UPDATED)
103. Teaching Networks to Solve Optimization Problems. (arXiv:2202.04104v2 [cs.LG] UPDATED)
104. Fine-Grained Population Mobility Data-Based Community-Level COVID-19 Prediction Model. (arXiv:2202.06257v3 [cs.LG] UPDATED)
105. Zero-Shot Assistance in Novel Decision Problems. (arXiv:2202.07364v2 [cs.LG] UPDATED)
106. Online Continual Learning for Embedded Devices. (arXiv:2203.10681v3 [cs.LG] UPDATED)
107. Theory of Acceleration of Decision Making by Correlated Time Sequences. (arXiv:2203.16004v4 [cs.LG] UPDATED)
108. Estimating and Penalizing Induced Preference Shifts in Recommender Systems. (arXiv:2204.11966v2 [cs.LG] UPDATED)
109. Self-supervised learning in non-small cell lung cancer discovers novel morphological clusters linked to patient outcome and molecular phenotypes. (arXiv:2205.01931v2 [cs.CV] UPDATED)
110. Dynamic categories, dynamic operads: From deep learning to prediction markets. (arXiv:2205.03906v2 [math.CT] UPDATED)
111. RITA: a Study on Scaling Up Generative Protein Sequence Models. (arXiv:2205.05789v2 [q-bio.QM] UPDATED)
112. blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v2 [cs.CV] UPDATED)
113. Calibration of Natural Language Understanding Models with Venn--ABERS Predictors. (arXiv:2205.10586v2 [cs.CL] UPDATED)
114. Prompt Injection: Parameterization of Fixed Inputs. (arXiv:2206.11349v2 [cs.LG] UPDATED)
115. HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network. (arXiv:2206.12747v3 [q-bio.QM] UPDATED)
116. A note on large deviations for interacting particle dynamics for finding mixed equilibria in zero-sum games. (arXiv:2206.15177v2 [stat.ML] UPDATED)
117. FAIR principles for AI models, with a practical application for accelerated high energy diffraction microscopy. (arXiv:2207.00611v2 [cs.AI] UPDATED)
118. Breaking Feedback Loops in Recommender Systems with Causal Inference. (arXiv:2207.01616v2 [cs.IR] UPDATED)
119. Effective and Efficient Training for Sequential Recommendation using Recency Sampling. (arXiv:2207.02643v2 [cs.IR] UPDATED)
120. Improved conformalized quantile regression. (arXiv:2207.02808v2 [stat.ML] UPDATED)
121. Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v2 [cs.CR] UPDATED)
122. Learning to Separate Voices by Spatial Regions. (arXiv:2207.04203v2 [cs.SD] UPDATED)
123. Dynamic Budget Throttling in Repeated Second-Price Auctions. (arXiv:2207.04690v3 [cs.GT] UPDATED)
124. HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation. (arXiv:2207.04906v2 [cs.CL] UPDATED)
125. Efficient and Privacy Preserving Group Signature for Federated Learning. (arXiv:2207.05297v2 [cs.CR] UPDATED)
126. A Benchmark dataset for predictive maintenance. (arXiv:2207.05466v2 [cs.LG] UPDATED)
127. Tracking Objects as Pixel-wise Distributions. (arXiv:2207.05518v2 [cs.CV] UPDATED)
128. PAC Reinforcement Learning for Predictive State Representations. (arXiv:2207.05738v2 [cs.LG] UPDATED)
129. Modeling Long-term Dependencies and Short-term Correlations in Patient Journey Data with Temporal Attention Networks for Health Prediction. (arXiv:2207.06414v2 [cs.LG] UPDATED)
130. Estimating Classification Confidence Using Kernel Densities. (arXiv:2207.06529v2 [stat.ML] UPDATED)
131. Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey. (arXiv:2207.07068v2 [cs.LG] UPDATED)
## cs.AI
---
**52** new papers in cs.AI:-) 
1. K-level Reasoning for Zero-Shot Coordination in Hanabi. (arXiv:2207.07166v1 [cs.AI])
2. Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification. (arXiv:2207.07189v1 [cs.CV])
3. Lipschitz Bound Analysis of Neural Networks. (arXiv:2207.07232v1 [cs.LG])
4. On the Super-exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU($d$) Symmetry. (arXiv:2207.07250v1 [quant-ph])
5. A Flexible Schema-Guided Dialogue Management Framework: From Friendly Peer to Virtual Standardized Cancer Patient. (arXiv:2207.07276v1 [cs.AI])
6. Parameterization of Cross-Token Relations with Relative Positional Encoding for Vision MLP. (arXiv:2207.07284v1 [cs.CV])
7. Robust Deep Compressive Sensing with Recurrent-Residual Structural Constraints. (arXiv:2207.07301v1 [eess.IV])
8. **Enhancement** by Your Aesthetic: An Intelligible Unsupervised Personalized Enhancer for **Low-Light** Images. (arXiv:2207.07317v1 [cs.CV])
9. Modeling Multi-interest News Sequence for News Recommendation. (arXiv:2207.07331v1 [cs.IR])
10. Computer Vision for Volunteer Cotton Detection in a Corn Field with UAS Remote Sensing Imagery and Spot Spray Applications. (arXiv:2207.07334v1 [eess.IV])
11. Context-sensitive neocortical neurons transform the effectiveness and efficiency of neural information processing. (arXiv:2207.07338v1 [cs.NE])
12. Fuzzy Labeling Semantics for Quantitative Argumentation. (arXiv:2207.07339v1 [cs.AI])
13. DuetFace: Collaborative Privacy-Preserving Face Recognition via Channel Splitting in the Frequency Domain. (arXiv:2207.07340v1 [cs.CV])
14. Stochastic Market Games. (arXiv:2207.07388v1 [cs.MA])
15. The Federal Disaster Assistance Policy -- a declarative analysis. (arXiv:2207.07392v1 [cs.AI])
16. Space-based gravitational wave signal detection and extraction with deep neural network. (arXiv:2207.07414v1 [gr-qc])
17. LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds Representing Laparoscopic Scenes. (arXiv:2207.07418v1 [cs.CV])
18. Continual Learning For On-Device Environmental Sound Classification. (arXiv:2207.07429v1 [cs.SD])
19. Learning Flexible Translation between Robot Actions and Language Descriptions. (arXiv:2207.07437v1 [cs.RO])
20. Creating an Explainable Intrusion Detection System Using Self Organizing Maps. (arXiv:2207.07465v1 [cs.CR])
21. A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. (arXiv:2207.07483v1 [cs.IR])
22. Explainable Sparse Knowledge Graph Completion via High-order Graph Reasoning Network. (arXiv:2207.07503v1 [cs.LG])
23. Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v1 [cs.LG])
24. Sparse Relational Reasoning with Object-Centric Representations. (arXiv:2207.07512v1 [cs.LG])
25. On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection. (arXiv:2207.07517v1 [cs.LG])
26. Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation. (arXiv:2207.07522v1 [cs.CV])
27. Heuristic-free Optimization of Force-Controlled Robot Search Strategies in Stochastic Environments. (arXiv:2207.07524v1 [cs.RO])
28. Skill-based Model-based Reinforcement Learning. (arXiv:2207.07560v1 [cs.LG])
29. QSAN: A Near-term Achievable Quantum Self-Attention Network. (arXiv:2207.07563v1 [quant-ph])
30. Rethinking Attention Mechanism in Time Series Classification. (arXiv:2207.07564v1 [cs.LG])
31. Quantitative Stock Investment by Routing Uncertainty-Aware Trading Experts: A Multi-Task Learning Approach. (arXiv:2207.07578v1 [q-fin.TR])
32. OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text. (arXiv:2207.07597v1 [cs.CL])
33. AutoMix: Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v5 [cs.CV] UPDATED)
34. Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v2 [stat.ML] UPDATED)
35. Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes. (arXiv:2111.15000v2 [cs.CV] UPDATED)
36. An Analytical Update Rule for General Policy Optimization. (arXiv:2112.02045v4 [cs.AI] UPDATED)
37. An original model for multi-target learning of logical rules for knowledge graph reasoning. (arXiv:2112.06189v2 [cs.AI] UPDATED)
38. HODOR: High-level Object Descriptors for Object Re-segmentation in Video Learned from Static Images. (arXiv:2112.09131v2 [cs.CV] UPDATED)
39. Causal Inference Through the Structural Causal Marginal Problem. (arXiv:2202.01300v3 [cs.AI] UPDATED)
40. Zero-Shot Assistance in Novel Decision Problems. (arXiv:2202.07364v2 [cs.LG] UPDATED)
41. Online Continual Learning for Embedded Devices. (arXiv:2203.10681v3 [cs.LG] UPDATED)
42. BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis. (arXiv:2204.03117v2 [cs.CL] UPDATED)
43. Prompt Injection: Parameterization of Fixed Inputs. (arXiv:2206.11349v2 [cs.LG] UPDATED)
44. HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network. (arXiv:2206.12747v3 [q-bio.QM] UPDATED)
45. FAIR principles for AI models, with a practical application for accelerated high energy diffraction microscopy. (arXiv:2207.00611v2 [cs.AI] UPDATED)
46. Effective and Efficient Training for Sequential Recommendation using Recency Sampling. (arXiv:2207.02643v2 [cs.IR] UPDATED)
47. Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v2 [cs.CR] UPDATED)
48. Reinforced Lin-Kernighan-Helsgaun Algorithms for the Traveling Salesman Problems. (arXiv:2207.03876v2 [cs.AI] UPDATED)
49. HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation. (arXiv:2207.04906v2 [cs.CL] UPDATED)
50. A Benchmark dataset for predictive maintenance. (arXiv:2207.05466v2 [cs.LG] UPDATED)
51. Tracking Objects as Pixel-wise Distributions. (arXiv:2207.05518v2 [cs.CV] UPDATED)
52. Modeling Long-term Dependencies and Short-term Correlations in Patient Journey Data with Temporal Attention Networks for Health Prediction. (arXiv:2207.06414v2 [cs.LG] UPDATED)

