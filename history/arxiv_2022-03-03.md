# Your interest papers
---
## cs.CV
---
### Asynchronous Optimisation for Event-based Visual Odometry. (arXiv:2203.01037v1 [cs.CV])
- Authors : Daqi Liu, Alvaro Parra, Yasir Latif, Bo Chen, Jun Chin, Ian Reid
- Link : [http://arxiv.org/abs/2203.01037](http://arxiv.org/abs/2203.01037)
> ABSTRACT  :  Event cameras open up new possibilities for robotic perception due to their low latency and **high dynamic range**. On the other hand, developing effective event-based vision algorithms that fully exploit the beneficial properties of event cameras remains work in progress. In this paper, we focus on event-based visual odometry (VO). While existing event-driven VO pipelines have adopted continuous-time representations to asynchronously process event data, they either assume a known map, restrict the camera to planar trajectories, or integrate other sensors into the system. Towards map-free event-only monocular VO in SE(3), we propose an asynchronous structure-from-motion optimisation back-end. Our formulation is underpinned by a principled joint optimisation problem involving non-parametric Gaussian Process motion modelling and incremental maximum a posteriori inference. A high-performance incremental computation engine is employed to reason about the camera trajectory with every incoming event. We demonstrate the robustness of our asynchronous back-end in comparison to frame-based methods which depend on accurate temporal accumulation of measurements.  
### Half Wavelet Attention on M-Net+ for Low-Light Image **Enhancement**. (arXiv:2203.01296v1 [eess.IV])
- Authors : Mao Fan, Jung Liu, Hsien Liu
- Link : [http://arxiv.org/abs/2203.01296](http://arxiv.org/abs/2203.01296)
> ABSTRACT  :  Low-Light Image **Enhancement** is a computer vision task which intensifies the **dark** images to appropriate brightness. It can also be seen as an ill-posed problem in image **restoration** domain. With the success of deep neural networks, the convolutional neural networks surpass the traditional algorithm-based methods and become the mainstream in the computer vision area. To advance the performance of **enhancement** algorithms, we propose an image **enhancement** network (HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use a half wavelet attention block on M-Net+ to enrich the features from wavelet domain. Furthermore, our HWMNet has competitive performance results on two image **enhancement** datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at https://github.com/FanChiMao/HWMNet.  
### DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v1 [cs.CV])
- Authors : Feng Li, Hao Zhang, Shilong Liu, Jian Guo, **Lei Zhang**
- Link : [http://arxiv.org/abs/2203.01305](http://arxiv.org/abs/2203.01305)
> ABSTRACT  :  We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) training and offer a deepened understanding of the slow convergence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable improvement. As a result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and $50$ epochs of training respectively) among DETR-like methods with ResNet-$50$ backbone. Compared with the baseline under the same setting, DN-DETR achieves comparable performance with $50\%$ training epochs. Code is available at \url{https://github.com/FengLi-ust/DN-DETR}.  
### Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v2 [cs.CV] UPDATED)
- Authors : Wencan Zhang, Mariella Dimiccoli
- Link : [http://arxiv.org/abs/2012.05567](http://arxiv.org/abs/2012.05567)
> ABSTRACT  :  Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/**night**). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.  
### A ConvNet for the 2020s. (arXiv:2201.03545v2 [cs.CV] UPDATED)
- Authors : Zhuang Liu, Hanzi Mao, Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie
- Link : [http://arxiv.org/abs/2201.03545](http://arxiv.org/abs/2201.03545)
> ABSTRACT  :  The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., **Swin** Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming **Swin** Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.  
### Using Multi-scale **Swin**Transformer-HTC with Data augmentation in CoNIC Challenge. (arXiv:2202.13588v2 [eess.IV] UPDATED)
- Authors : Yen Lee, Chin Chien, Ping Wang, Hong Yen, Wen Zhen, Kun Lin
- Link : [http://arxiv.org/abs/2202.13588](http://arxiv.org/abs/2202.13588)
> ABSTRACT  :  Colorectal cancer is one of the most common cancers worldwide, so early pathological examination is very important. However, it is time-consuming and labor-intensive to identify the number and type of cells on H&amp;E images in clinical. Therefore, automatic segmentation and classification task and counting the cellular composition of H&amp;E images from pathological sections is proposed by CoNIC Challenge 2022. We proposed a multi-scale **Swin** transformer with HTC for this challenge, and also applied the known normalization methods to generate more augmentation data. Finally, our strategy showed that the multi-scale played a crucial role to identify different scale features and the augmentation arose the recognition of model.  
## eess.IV
---
### Machine learning based lens-free imaging technique for field-portable cytometry. (arXiv:2203.00899v1 [eess.IV])
- Authors : Rajkumar Vaghashiya, Sanghoon Shin, Varun Chauhan, Kaushal Kapadiya, Smit Sanghavi, Sungkyu Seo, Mohendra Roy
- Link : [http://arxiv.org/abs/2203.00899](http://arxiv.org/abs/2203.00899)
> ABSTRACT  :  Lens-free Shadow Imaging Technique (LSIT) is a well-established technique for the characterization of microparticles and biological cells. Due to its simplicity and cost-effectiveness, various low-cost solutions have been evolved, such as automatic analysis of complete blood count (CBC), cell viability, 2D cell morphology, 3D cell tomography, etc. The developed auto characterization algorithm so far for this custom-developed LSIT cytometer was based on the hand-crafted features of the cell diffraction patterns from the LSIT cytometer, that were determined from our empirical findings on thousands of samples of individual cell types, which limit the system in terms of induction of a new cell type for auto classification or characterization. Further, its performance is suffering from poor image (cell diffraction pattern) signatures due to its small signal or background noise. In this work, we address these issues by leveraging the artificial intelligence-powered auto signal enhancing scheme such as denoising autoencoder and adaptive cell characterization technique based on the transfer of learning in deep neural networks. The performance of our proposed method shows an increase in accuracy &gt;98% along with the signal **enhancement** of &gt;5 dB for most of the cell types, such as Red Blood Cell (RBC) and White Blood Cell (WBC). Furthermore, the model is adaptive to learn new type of samples within a few learning iterations and able to successfully classify the newly introduced sample along with the existing other sample types.  
### Half Wavelet Attention on M-Net+ for Low-Light Image **Enhancement**. (arXiv:2203.01296v1 [eess.IV])
- Authors : Mao Fan, Jung Liu, Hsien Liu
- Link : [http://arxiv.org/abs/2203.01296](http://arxiv.org/abs/2203.01296)
> ABSTRACT  :  Low-Light Image **Enhancement** is a computer vision task which intensifies the **dark** images to appropriate brightness. It can also be seen as an ill-posed problem in image **restoration** domain. With the success of deep neural networks, the convolutional neural networks surpass the traditional algorithm-based methods and become the mainstream in the computer vision area. To advance the performance of **enhancement** algorithms, we propose an image **enhancement** network (HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use a half wavelet attention block on M-Net+ to enrich the features from wavelet domain. Furthermore, our HWMNet has competitive performance results on two image **enhancement** datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at https://github.com/FanChiMao/HWMNet.  
### Using Multi-scale **Swin**Transformer-HTC with Data augmentation in CoNIC Challenge. (arXiv:2202.13588v2 [eess.IV] UPDATED)
- Authors : Yen Lee, Chin Chien, Ping Wang, Hong Yen, Wen Zhen, Kun Lin
- Link : [http://arxiv.org/abs/2202.13588](http://arxiv.org/abs/2202.13588)
> ABSTRACT  :  Colorectal cancer is one of the most common cancers worldwide, so early pathological examination is very important. However, it is time-consuming and labor-intensive to identify the number and type of cells on H&amp;E images in clinical. Therefore, automatic segmentation and classification task and counting the cellular composition of H&amp;E images from pathological sections is proposed by CoNIC Challenge 2022. We proposed a multi-scale **Swin** transformer with HTC for this challenge, and also applied the known normalization methods to generate more augmentation data. Finally, our strategy showed that the multi-scale played a crucial role to identify different scale features and the augmentation arose the recognition of model.  
## cs.LG
---
### Towards Efficient and Stable K-Asynchronous Federated Learning with Unbounded Stale Gradients on Non-IID Data. (arXiv:2203.01214v1 [cs.LG])
- Authors : Zihao Zhou, Yanan Li, Xuebin Ren, Shusen Yang
- Link : [http://arxiv.org/abs/2203.01214](http://arxiv.org/abs/2203.01214)
> ABSTRACT  :  Federated learning (FL) is an emerging privacy-preserving paradigm that enables multiple participants collaboratively to train a global model without uploading raw data. Considering heterogeneous computing and communication capabilities of different participants, asynchronous FL can avoid the stragglers effect in synchronous FL and adapts to scenarios with vast participants. Both staleness and non-IID data in asynchronous FL would reduce the model utility. However, there exists an inherent contradiction between the solutions to the two problems. That is, mitigating the staleness requires to select less but consistent gradients while coping with non-IID data demands more comprehensive gradients. To address the dilemma, this paper proposes a two-stage weighted $K$ asynchronous FL with adaptive learning rate (WKAFL). By selecting consistent gradients and adjusting learning rate adaptively, WKAFL utilizes stale gradients and mitigates the impact of non-IID data, which can achieve multifaceted **enhancement** in training speed, prediction accuracy and training stability. We also present the convergence analysis for WKAFL under the assumption of unbounded staleness to understand the impact of staleness and non-IID data. Experiments implemented on both benchmark and synthetic FL datasets show that WKAFL has better overall performance compared to existing algorithms.  
### Sensor-Based Estimation of Dim Light Melatonin Onset (DLMO) Using Features of Two Time Scales. (arXiv:1908.07483v5 [cs.LG] UPDATED)
- Authors : Cheng Wan, Elizabeth Klerman, Akane Sano
- Link : [http://arxiv.org/abs/1908.07483](http://arxiv.org/abs/1908.07483)
> ABSTRACT  :  Circadian rhythms influence multiple essential biological activities including sleep, performance, and mood. The dim light melatonin onset (DLMO) is the gold standard for measuring human circadian phase (i.e., timing). The collection of DLMO is expensive and time-consuming since multiple saliva or blood samples are required over**night** in special conditions, and the samples must then be assayed for melatonin. Recently, several computational approaches have been designed for estimating DLMO. These methods collect daily sampled data (e.g., sleep onset/offset times) or frequently sampled data (e.g., light **exposure**/skin temperature/physical activity collected every minute) to train learning models for estimating DLMO. One limitation of these studies is that they only leverage one time-scale data. We propose a two-step framework for estimating DLMO using data from both time scales. The first step summarizes data from before the current day, while the second step combines this summary with frequently sampled data of the current day. We evaluate three moving average models that input sleep timing data as the first step and use recurrent neural network models as the second step. The results using data from 207 undergraduates show that our two-step model with two time-scale features has statistically significantly lower root-mean-square errors than models that use either daily sampled data or frequently sampled data.  
### Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v2 [cs.CV] UPDATED)
- Authors : Wencan Zhang, Mariella Dimiccoli
- Link : [http://arxiv.org/abs/2012.05567](http://arxiv.org/abs/2012.05567)
> ABSTRACT  :  Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/**night**). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.  
### Predicting the Thermal Sunyaev-Zel'dovich Field using Modular and Equivariant Set-Based Neural Networks. (arXiv:2203.00026v1 [astro-ph.CO] CROSS LISTED)
- Authors : Leander Thiele, Miles Cranmer, William Coulton, Shirley Ho
- Link : [http://arxiv.org/abs/2203.00026](http://arxiv.org/abs/2203.00026)
> ABSTRACT  :  Theoretical uncertainty limits our ability to extract cosmological information from baryonic fields such as the thermal Sunyaev-Zel'dovich (tSZ) effect. Being sourced by the electron pressure field, the tSZ effect depends on baryonic physics that is usually modeled by expensive hydrodynamic simulations. We train neural networks on the IllustrisTNG-300 cosmological simulation to predict the continuous electron pressure field in galaxy clusters from gravity-only simulations. Modeling clusters is challenging for neural networks as most of the gas pressure is concentrated in a handful of voxels and even the largest hydrodynamical simulations contain only a few hundred clusters that can be used for training. Instead of conventional convolutional neural net (CNN) architectures, we choose to employ a rotationally equivariant DeepSets architecture to operate directly on the set of **dark** matter particles. We argue that set-based architectures provide distinct advantages over CNNs. For example, we can enforce exact rotational and permutation equivariance, incorporate existing knowledge on the tSZ field, and work with sparse fields as are standard in cosmology. We compose our architecture with separate, physically meaningful modules, making it amenable to interpretation. For example, we can separately study the influence of local and cluster-scale environment, determine that cluster triaxiality has negligible impact, and train a module that corrects for mis-centering. Our model improves by 70 % on analytic profiles fit to the same simulation data. We argue that the electron pressure field, viewed as a function of a gravity-only simulation, has inherent stochasticity, and model this property through a conditional-VAE extension to the network. This modification yields further improvement by 7 %, it is limited by our small training set however. (abridged)  
## cs.AI
---
### Machine learning based lens-free imaging technique for field-portable cytometry. (arXiv:2203.00899v1 [eess.IV])
- Authors : Rajkumar Vaghashiya, Sanghoon Shin, Varun Chauhan, Kaushal Kapadiya, Smit Sanghavi, Sungkyu Seo, Mohendra Roy
- Link : [http://arxiv.org/abs/2203.00899](http://arxiv.org/abs/2203.00899)
> ABSTRACT  :  Lens-free Shadow Imaging Technique (LSIT) is a well-established technique for the characterization of microparticles and biological cells. Due to its simplicity and cost-effectiveness, various low-cost solutions have been evolved, such as automatic analysis of complete blood count (CBC), cell viability, 2D cell morphology, 3D cell tomography, etc. The developed auto characterization algorithm so far for this custom-developed LSIT cytometer was based on the hand-crafted features of the cell diffraction patterns from the LSIT cytometer, that were determined from our empirical findings on thousands of samples of individual cell types, which limit the system in terms of induction of a new cell type for auto classification or characterization. Further, its performance is suffering from poor image (cell diffraction pattern) signatures due to its small signal or background noise. In this work, we address these issues by leveraging the artificial intelligence-powered auto signal enhancing scheme such as denoising autoencoder and adaptive cell characterization technique based on the transfer of learning in deep neural networks. The performance of our proposed method shows an increase in accuracy &gt;98% along with the signal **enhancement** of &gt;5 dB for most of the cell types, such as Red Blood Cell (RBC) and White Blood Cell (WBC). Furthermore, the model is adaptive to learn new type of samples within a few learning iterations and able to successfully classify the newly introduced sample along with the existing other sample types.  
### Half Wavelet Attention on M-Net+ for Low-Light Image **Enhancement**. (arXiv:2203.01296v1 [eess.IV])
- Authors : Mao Fan, Jung Liu, Hsien Liu
- Link : [http://arxiv.org/abs/2203.01296](http://arxiv.org/abs/2203.01296)
> ABSTRACT  :  Low-Light Image **Enhancement** is a computer vision task which intensifies the **dark** images to appropriate brightness. It can also be seen as an ill-posed problem in image **restoration** domain. With the success of deep neural networks, the convolutional neural networks surpass the traditional algorithm-based methods and become the mainstream in the computer vision area. To advance the performance of **enhancement** algorithms, we propose an image **enhancement** network (HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use a half wavelet attention block on M-Net+ to enrich the features from wavelet domain. Furthermore, our HWMNet has competitive performance results on two image **enhancement** datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at https://github.com/FanChiMao/HWMNet.  
### DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v1 [cs.CV])
- Authors : Feng Li, Hao Zhang, Shilong Liu, Jian Guo, **Lei Zhang**
- Link : [http://arxiv.org/abs/2203.01305](http://arxiv.org/abs/2203.01305)
> ABSTRACT  :  We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) training and offer a deepened understanding of the slow convergence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable improvement. As a result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and $50$ epochs of training respectively) among DETR-like methods with ResNet-$50$ backbone. Compared with the baseline under the same setting, DN-DETR achieves comparable performance with $50\%$ training epochs. Code is available at \url{https://github.com/FengLi-ust/DN-DETR}.  
# Paper List
---
## cs.CV
---
**109** new papers in cs.CV:-) 
1. Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data. (arXiv:2203.00734v1 [cs.CV])
2. 3D Skeleton-based Human Motion Prediction with Manifold-Aware GAN. (arXiv:2203.00736v1 [cs.CV])
3. Runtime Detection of Executional Errors in Robot-Assisted Surgery. (arXiv:2203.00737v1 [cs.CV])
4. There is a Time and Place for Reasoning Beyond the Image. (arXiv:2203.00758v1 [cs.CV])
5. Tricks and Plugins to GBM on Images and Sequences. (arXiv:2203.00761v1 [cs.LG])
6. Low-Cost On-device Partial Domain Adaptation (LoCO-PDA): Enabling efficient CNN retraining on edge devices. (arXiv:2203.00772v1 [cs.CV])
7. Image analysis for automatic measurement of crustose lichens. (arXiv:2203.00787v1 [cs.CV])
8. Unified Physical Threat Monitoring System Aided by Virtual Building Simulation. (arXiv:2203.00789v1 [cs.CV])
9. Stable, accurate and efficient deep neural networks for inverse problems with analysis-sparse models. (arXiv:2203.00804v1 [cs.LG])
10. InCloud: Incremental Learning for Point Cloud Place Recognition. (arXiv:2203.00807v1 [cs.CV])
11. Instance-aware multi-object self-supervision for monocular depth prediction. (arXiv:2203.00809v1 [cs.CV])
12. Robust Seatbelt Detection and Usage Recognition for Driver Monitoring Systems. (arXiv:2203.00810v1 [cs.CV])
13. 3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification. (arXiv:2203.00828v1 [cs.CV])
14. GSC Loss: A Gaussian Score Calibrating Loss for Deep Learning. (arXiv:2203.00833v1 [cs.LG])
15. OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion. (arXiv:2203.00838v1 [cs.CV])
16. X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. (arXiv:2203.00843v1 [cs.CV])
17. Can No-reference features help in Full-reference image quality estimation?. (arXiv:2203.00845v1 [eess.IV])
18. Clean-Annotation Backdoor Attack against Lane Detection Systems in the Wild. (arXiv:2203.00858v1 [cs.CV])
19. MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v1 [cs.CV])
20. D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale Attention. (arXiv:2203.00860v1 [cs.CV])
21. Styleverse: Towards Identity Stylization across Heterogeneous Domains. (arXiv:2203.00861v1 [cs.CV])
22. SEA: Bridging the Gap Between One- and Two-stage Detector Distillation via SEmantic-aware Alignment. (arXiv:2203.00862v1 [cs.CV])
23. Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding. (arXiv:2203.00867v1 [cs.CV])
24. Hybrid Optimized Deep Convolution Neural Network based Learning Model for Object Detection. (arXiv:2203.00869v1 [cs.CV])
25. Dense Voxel Fusion for 3D Object Detection. (arXiv:2203.00871v1 [cs.CV])
26. A Split Semantic Detection Algorithm for Psychological Sandplay Image. (arXiv:2203.00907v1 [cs.CV])
27. Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence. (arXiv:2203.00911v1 [eess.IV])
28. A Principled Design of Image Representation: Towards Forensic Tasks. (arXiv:2203.00913v1 [cs.CV])
29. PUFA-GAN: A Frequency-Aware Generative Adversarial Network for 3D Point Cloud Upsampling. (arXiv:2203.00914v1 [cs.CV])
30. Translation Invariant Global Estimation of Heading Angle Using Sinogram of LiDAR Point Cloud. (arXiv:2203.00924v1 [cs.RO])
31. Parameterized Image Quality Score Distribution Prediction. (arXiv:2203.00926v1 [eess.IV])
32. TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration. (arXiv:2203.00927v1 [cs.CV])
33. ParaPose: Parameter and Domain Randomization Optimization for Pose Estimation using Synthetic Data. (arXiv:2203.00945v1 [cs.CV])
34. CD-GAN: a robust fusion-based generative adversarial network for unsupervised change detection between heterogeneous images. (arXiv:2203.00948v1 [eess.IV])
35. Sketched RT3D: How to reconstruct billions of photons per second. (arXiv:2203.00952v1 [eess.IV])
36. GRASP EARTH: Intuitive Software for Discovering Changes on the Planet. (arXiv:2203.00955v1 [cs.CV])
37. Learning Moving-Object Tracking with FMCW LiDAR. (arXiv:2203.00959v1 [cs.CV])
38. Aggregated Pyramid Vision Transformer: Split-transform-merge Strategy for Image Recognition without Convolutions. (arXiv:2203.00960v1 [cs.CV])
39. Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation. (arXiv:2203.00962v1 [cs.CV])
40. Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training. (arXiv:2203.00972v1 [cs.CV])
41. TableFormer: Table Structure Understanding with Transformers. (arXiv:2203.01017v1 [cs.CV])
42. Asynchronous Optimisation for Event-based Visual Odometry. (arXiv:2203.01037v1 [cs.CV])
43. Image-based material analysis of ancient historical documents. (arXiv:2203.01042v1 [cs.CV])
44. 3D object reconstruction and 6D-pose estimation from 2D shape for robotic grasping of objects. (arXiv:2203.01051v1 [cs.CV])
45. Unsupervised Anomaly Detection from Time-of-Flight Depth Images. (arXiv:2203.01052v1 [cs.CV])
46. Colar: Effective and Efficient Online Action Detection by Consulting Exemplars. (arXiv:2203.01057v1 [cs.CV])
47. OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation. (arXiv:2203.01072v1 [cs.CV])
48. Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation. (arXiv:2203.01074v1 [cs.CV])
49. Vision-based Large-scale 3D Semantic Mapping for Autonomous Driving Applications. (arXiv:2203.01087v1 [cs.CV])
50. Shape constrained CNN for segmentation guided prediction of myocardial shape and pose parameters in cardiac MRI. (arXiv:2203.01089v1 [eess.IV])
51. A Generalized Approach for Cancellable Template and Its Realization for Minutia Cylinder-Code. (arXiv:2203.01095v1 [cs.CV])
52. Self-Supervised Scene Flow Estimation with 4D Automotive Radar. (arXiv:2203.01137v1 [cs.CV])
53. Improving Lidar-Based Semantic Segmentation of Top-View Grid Maps by Learning Features in Complementary Representations. (arXiv:2203.01151v1 [cs.CV])
54. DisARM: Displacement Aware Relation Module for 3D Detection. (arXiv:2203.01152v1 [cs.CV])
55. Applying multi-angled parallelism to Spanish topographical maps. (arXiv:2203.01169v1 [cs.CV])
56. Detecting Adversarial Perturbations in Multi-Task Perception. (arXiv:2203.01177v1 [cs.CV])
57. Fast and Robust Ground Surface Estimation from LIDAR Measurements using Uniform B-Splines. (arXiv:2203.01180v1 [cs.RO])
58. Visual Feature Encoding for GNNs on Road Networks. (arXiv:2203.01187v1 [cs.CV])
59. Improving Generalization of Deep Networks for Estimating Physical Properties of Containers and Fillings. (arXiv:2203.01192v1 [cs.CV])
60. VAE-iForest: Auto-encoding Reconstruction and Isolation-based Anomalies Detecting Fallen Objects on Road Surface. (arXiv:2203.01193v1 [cs.CV])
61. Container Localisation and Mass Estimation with an RGB-D Camera. (arXiv:2203.01207v1 [cs.CV])
62. A simple and universal rotation equivariant point-cloud network. (arXiv:2203.01216v1 [cs.LG])
63. Hybrid Tracker with Pixel and Instance for Video Panoptic Segmentation. (arXiv:2203.01217v1 [cs.CV])
64. Video Question Answering: Datasets, Algorithms and Challenges. (arXiv:2203.01225v1 [cs.CV])
65. Differentiable IFS Fractals. (arXiv:2203.01231v1 [cs.GR])
66. H4D: Human 4D Modeling by Learning Neural Compositional Representation. (arXiv:2203.01247v1 [cs.CV])
67. A Unified Query-based Paradigm for Point Cloud Understanding. (arXiv:2203.01252v1 [cs.CV])
68. Self-supervised Transformer for Deepfake Detection. (arXiv:2203.01265v1 [cs.CV])
69. Deep Temporal Interpolation of Radar-based Precipitation. (arXiv:2203.01277v1 [cs.CV])
70. ADVISE: ADaptive Feature Relevance and VISual Explanations for Convolutional Neural Networks. (arXiv:2203.01289v1 [cs.CV])
71. Half Wavelet Attention on M-Net+ for Low-Light Image **Enhancement**. (arXiv:2203.01296v1 [eess.IV])
72. DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v1 [cs.CV])
73. HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v1 [cs.LG])
74. Protecting Celebrities with Identity Consistency Transformer. (arXiv:2203.01318v1 [cs.CV])
75. Deep Learning Methods and Applications for Region of Interest Detection in Dermoscopic Images. (arXiv:1807.10711v3 [cs.CV] UPDATED)
76. Ricci Curvature Based Volumetric Segmentation of the Auditory Ossicles. (arXiv:2006.14788v3 [cs.CV] UPDATED)
77. Manipulation-Oriented Object Perception in Clutter through Affordance Coordinate Frames. (arXiv:2010.08202v3 [cs.RO] UPDATED)
78. Learning Efficient GANs for Image Translation via Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v4 [cs.CV] UPDATED)
79. Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v2 [cs.CV] UPDATED)
80. Learning Class-Agnostic Pseudo Mask Generation for Box-Supervised Semantic Segmentation. (arXiv:2103.05463v2 [cs.CV] UPDATED)
81. DeepChange: A Large Long-Term Person Re-Identification Benchmark with Clothes Change. (arXiv:2105.14685v4 [cs.CV] UPDATED)
82. Sparse PointPillars: Maintaining and Exploiting Input Sparsity to Improve Runtime on Embedded Systems. (arXiv:2106.06882v3 [cs.CV] UPDATED)
83. Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration. (arXiv:2107.02433v3 [cs.CV] UPDATED)
84. Learning Local-Global Contextual Adaptation for Multi-Person Pose Estimation. (arXiv:2109.03622v2 [cs.CV] UPDATED)
85. MotionHint: Self-Supervised Monocular Visual Odometry with Motion Constraints. (arXiv:2109.06768v3 [cs.CV] UPDATED)
86. PnP-DETR: Towards Efficient Visual Analysis with Transformers. (arXiv:2109.07036v4 [cs.CV] UPDATED)
87. S3LAM: Structured Scene SLAM. (arXiv:2109.07339v2 [cs.RO] UPDATED)
88. Generalizable Human Pose Triangulation. (arXiv:2110.00280v2 [cs.CV] UPDATED)
89. Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v2 [cs.CV] UPDATED)
90. Towards Optimal Correlational Object Search. (arXiv:2110.09991v2 [cs.RO] UPDATED)
91. An Image Patch is a Wave: Quantum Inspired Vision MLP. (arXiv:2111.12294v3 [cs.CV] UPDATED)
92. SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional, and Incremental Robot Learning. (arXiv:2111.14693v3 [cs.RO] UPDATED)
93. Self-Supervised Camera Self-Calibration from Video. (arXiv:2112.03325v2 [cs.CV] UPDATED)
94. Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v2 [cs.CV] UPDATED)
95. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. (arXiv:2112.05139v3 [cs.CV] UPDATED)
96. HairCLIP: Design Your Hair by Text and Reference Image. (arXiv:2112.05142v2 [cs.CV] UPDATED)
97. Learning to integrate vision data into road network data. (arXiv:2112.10624v2 [cs.CV] UPDATED)
98. Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond. (arXiv:2201.03176v2 [cs.CV] UPDATED)
99. A ConvNet for the 2020s. (arXiv:2201.03545v2 [cs.CV] UPDATED)
100. MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v4 [eess.IV] UPDATED)
101. Crafting Better Contrastive Views for Siamese Representation Learning. (arXiv:2202.03278v2 [cs.CV] UPDATED)
102. Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling. (arXiv:2202.03543v2 [eess.AS] UPDATED)
103. Incorporating Texture Information into Dimensionality Reduction for High-Dimensional Images. (arXiv:2202.09179v2 [cs.CV] UPDATED)
104. Vision-Language Pre-Training with Triple Contrastive Learning. (arXiv:2202.10401v2 [cs.CV] UPDATED)
105. Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?. (arXiv:2202.12426v2 [cs.CV] UPDATED)
106. Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v3 [cs.CV] UPDATED)
107. Using Multi-scale **Swin**Transformer-HTC with Data augmentation in CoNIC Challenge. (arXiv:2202.13588v2 [eess.IV] UPDATED)
108. Multi-Modal Recurrent Fusion for Indoor Localization. (arXiv:2203.00510v2 [eess.SP] UPDATED)
109. CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding. (arXiv:2203.00680v2 [cs.CV] UPDATED)
## eess.IV
---
**21** new papers in eess.IV:-) 
1. ONIX: an X-ray deep-learning tool for 3D reconstructions from sparse views. (arXiv:2203.00682v1 [eess.IV])
2. Unified Physical Threat Monitoring System Aided by Virtual Building Simulation. (arXiv:2203.00789v1 [cs.CV])
3. Stable, accurate and efficient deep neural networks for inverse problems with analysis-sparse models. (arXiv:2203.00804v1 [cs.LG])
4. Can No-reference features help in Full-reference image quality estimation?. (arXiv:2203.00845v1 [eess.IV])
5. Machine learning based lens-free imaging technique for field-portable cytometry. (arXiv:2203.00899v1 [eess.IV])
6. Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence. (arXiv:2203.00911v1 [eess.IV])
7. PUFA-GAN: A Frequency-Aware Generative Adversarial Network for 3D Point Cloud Upsampling. (arXiv:2203.00914v1 [cs.CV])
8. Parameterized Image Quality Score Distribution Prediction. (arXiv:2203.00926v1 [eess.IV])
9. TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration. (arXiv:2203.00927v1 [cs.CV])
10. CD-GAN: a robust fusion-based generative adversarial network for unsupervised change detection between heterogeneous images. (arXiv:2203.00948v1 [eess.IV])
11. Sketched RT3D: How to reconstruct billions of photons per second. (arXiv:2203.00952v1 [eess.IV])
12. Structural Gaussian Priors for Bayesian CT reconstruction of Subsea Pipes. (arXiv:2203.01030v1 [math.NA])
13. Shape constrained CNN for segmentation guided prediction of myocardial shape and pose parameters in cardiac MRI. (arXiv:2203.01089v1 [eess.IV])
14. Decoding-Energy-Rate-Distortion Optimization for Video Coding. (arXiv:2203.01099v1 [eess.IV])
15. Omnidirectional MediA Format (OMAF): Toolbox for Virtual Reality Services. (arXiv:2203.01183v1 [eess.IV])
16. Hybrid Model-based / Data-driven Graph Transform for Image Coding. (arXiv:2203.01186v1 [eess.IV])
17. Container Localisation and Mass Estimation with an RGB-D Camera. (arXiv:2203.01207v1 [cs.CV])
18. Half Wavelet Attention on M-Net+ for Low-Light Image **Enhancement**. (arXiv:2203.01296v1 [eess.IV])
19. Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration. (arXiv:2107.02433v3 [cs.CV] UPDATED)
20. MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v4 [eess.IV] UPDATED)
21. Using Multi-scale **Swin**Transformer-HTC with Data augmentation in CoNIC Challenge. (arXiv:2202.13588v2 [eess.IV] UPDATED)
## cs.LG
---
**160** new papers in cs.LG:-) 
1. Learning Robust Real-Time Cultural Transmission without Human Data. (arXiv:2203.00715v1 [cs.LG])
2. Runtime Detection of Executional Errors in Robot-Assisted Surgery. (arXiv:2203.00737v1 [cs.CV])
3. E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models. (arXiv:2203.00748v1 [cs.CL])
4. HyperPrompt: Prompt-based Task-Conditioning of Transformers. (arXiv:2203.00759v1 [cs.CL])
5. Tricks and Plugins to GBM on Images and Sequences. (arXiv:2203.00761v1 [cs.LG])
6. Topic Analysis for Text with Side Data. (arXiv:2203.00762v1 [cs.LG])
7. Multi-Layer Perceptron Neural Network for Improving Detection Performance of Malicious Phishing URLs Without Affecting Other Attack Types Classification. (arXiv:2203.00774v1 [cs.CR])
8. Enhanced Nearest Neighbor Classification for Crowdsourcing. (arXiv:2203.00781v1 [cs.HC])
9. TANDEM: Learning Joint Exploration and Decision Making with Tactile Sensors. (arXiv:2203.00798v1 [cs.RO])
10. Code Smells in Machine Learning Systems. (arXiv:2203.00803v1 [cs.SE])
11. Stable, accurate and efficient deep neural networks for inverse problems with analysis-sparse models. (arXiv:2203.00804v1 [cs.LG])
12. The quantum low-rank approximation problem. (arXiv:2203.00811v1 [quant-ph])
13. Partial Likelihood Thompson Sampling. (arXiv:2203.00820v1 [stat.ME])
14. Keeping Minimal Experience to Achieve Efficient Interpretable Policy Distillation. (arXiv:2203.00822v1 [cs.LG])
15. Personalized Federated Learning With Structure. (arXiv:2203.00829v1 [cs.LG])
16. GSC Loss: A Gaussian Score Calibrating Loss for Deep Learning. (arXiv:2203.00833v1 [cs.LG])
17. CandidateDrug4Cancer: An Open Molecular Graph Learning Benchmark on Drug Discovery for Cancer. (arXiv:2203.00836v1 [cs.LG])
18. PUMA: Performance Unchanged Model Augmentation for Training Data Removal. (arXiv:2203.00846v1 [stat.ML])
19. Adversarially Robust Learning with Tolerance. (arXiv:2203.00849v1 [stat.ML])
20. Transfer Learning of High-Fidelity Opacity Spectra in Autoencoders and Surrogate Models. (arXiv:2203.00853v1 [physics.plasm-ph])
21. FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours. (arXiv:2203.00854v1 [cs.LG])
22. Faith-Shap: The Faithful Shapley Shapley Interaction Index. (arXiv:2203.00870v1 [cs.LG])
23. Follow your Nose: Using General Value Functions for Directed Exploration in Reinforcement Learning. (arXiv:2203.00874v1 [cs.LG])
24. A Learning Based Framework for Handling Uncertain Lead Times in Multi-Product Inventory Management. (arXiv:2203.00885v1 [cs.LG])
25. Sampling Random Group Fair Rankings. (arXiv:2203.00887v1 [cs.LG])
26. Combining Reinforcement Learning and Optimal Transport for the Traveling Salesman Problem. (arXiv:2203.00903v1 [cs.LG])
27. Weakly Supervised Correspondence Learning. (arXiv:2203.00904v1 [cs.RO])
28. MIAShield: Defending Membership Inference Attacks via Preemptive Exclusion of Members. (arXiv:2203.00915v1 [cs.CR])
29. Canonical foliations of neural networks: application to robustness. (arXiv:2203.00922v1 [stat.ML])
30. Continual Learning of Multi-modal Dynamics with External Memory. (arXiv:2203.00936v1 [cs.LG])
31. ES-dRNN with Dynamic Attention for Short-Term Load Forecasting. (arXiv:2203.00937v1 [cs.LG])
32. Neuro-Symbolic Verification of Deep Neural Networks. (arXiv:2203.00938v1 [cs.AI])
33. CD-GAN: a robust fusion-based generative adversarial network for unsupervised change detection between heterogeneous images. (arXiv:2203.00948v1 [eess.IV])
34. GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation. (arXiv:2203.00949v1 [cs.LG])
35. Parallel Spatio-Temporal Attention-Based TCN for Multivariate Time Series Prediction. (arXiv:2203.00971v1 [cs.LG])
36. A density peaks clustering algorithm with sparse search and K-d tree. (arXiv:2203.00973v1 [stat.ML])
37. Predicting the temporal dynamics of turbulent channels through deep learning. (arXiv:2203.00974v1 [physics.flu-dyn])
38. L4KDE: Learning for KinoDynamic Tree Expansion. (arXiv:2203.00975v1 [cs.RO])
39. Chained Generalisation Bounds. (arXiv:2203.00977v1 [stat.ML])
40. Boosted Ensemble Learning based on Randomized NNs for Time Series Forecasting. (arXiv:2203.00980v1 [cs.LG])
41. Beyond GAP screening for Lasso by exploiting new dual cutting half-spaces with supplementary material. (arXiv:2203.00987v1 [cs.LG])
42. Learning Efficiently Function Approximation for Contextual MDP. (arXiv:2203.00995v1 [cs.LG])
43. Improving the Diversity of Bootstrapped DQN via Noisy Priors. (arXiv:2203.01004v1 [cs.LG])
44. UAV-Aided Decentralized Learning over Mesh Networks. (arXiv:2203.01008v1 [cs.IT])
45. Continual Feature Selection: Spurious Features in Continual Learning. (arXiv:2203.01012v1 [cs.LG])
46. The Theoretical Expressiveness of Maxpooling. (arXiv:2203.01016v1 [cs.LG])
47. TableFormer: Table Structure Understanding with Transformers. (arXiv:2203.01017v1 [cs.CV])
48. Learning in Sparse Rewards settings through Quality-Diversity algorithms. (arXiv:2203.01027v1 [cs.LG])
49. Discriminating Against Unrealistic Interpolations in Generative Adversarial Networks. (arXiv:2203.01035v1 [cs.LG])
50. SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs. (arXiv:2203.01044v1 [cs.LG])
51. Discontinuous Constituency and BERT: A Case Study of Dutch. (arXiv:2203.01063v1 [cs.CL])
52. Reliable validation of Reinforcement Learning Benchmarks. (arXiv:2203.01075v1 [cs.LG])
53. On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v1 [cs.LG])
54. Information Gain Propagation: a new way to Graph Active Learning with Soft Labels. (arXiv:2203.01093v1 [cs.LG])
55. Model-agnostic out-of-distribution detection using combined statistical tests. (arXiv:2203.01097v1 [stat.ML])
56. Practical Recommendations for the Design of Automatic Fault Detection Algorithms Based on Experiments with Field Monitoring Data. (arXiv:2203.01103v1 [eess.SY])
57. Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models. (arXiv:2203.01104v1 [cs.CL])
58. The Optimal Noise in Noise-Contrastive Learning Is Not What You Think. (arXiv:2203.01110v1 [stat.ML])
59. Hyperparameter optimization of data-driven AI models on HPC systems. (arXiv:2203.01112v1 [physics.data-an])
60. Pattern Recognition and Event Detection on IoT Data-streams. (arXiv:2203.01114v1 [cs.LG])
61. VaiPhy: a Variational Inference Based Algorithm for Phylogeny. (arXiv:2203.01121v1 [q-bio.PE])
62. A Constrained Optimization Approach to Bilevel Optimization with Multiple Inner Minima. (arXiv:2203.01123v1 [math.OC])
63. Defining a synthetic data generator for realistic electric vehicle charging sessions. (arXiv:2203.01129v1 [cs.OH])
64. Engineering the Neural Automatic Passenger Counter. (arXiv:2203.01156v1 [cs.LG])
65. Rethinking Pretraining as a Bridge from ANNs to SNNs. (arXiv:2203.01158v1 [cs.NE])
66. Discrete Optimal Transport with Independent Marginals is #P-Hard. (arXiv:2203.01161v1 [math.OC])
67. Speaker recognition improvement using blind inversion of distortions. (arXiv:2203.01164v1 [cs.SD])
68. Applying multi-angled parallelism to Spanish topographical maps. (arXiv:2203.01169v1 [cs.CV])
69. Efficient Online Linear Control with Stochastic Convex Costs and Unknown Dynamics. (arXiv:2203.01170v1 [math.OC])
70. DCT-Former: Efficient Self-Attention withDiscrete Cosine Transform. (arXiv:2203.01178v1 [cs.LG])
71. Hybrid Model-based / Data-driven Graph Transform for Image Coding. (arXiv:2203.01186v1 [eess.IV])
72. Model-free Neural Lyapunov Control for Safe Robot Navigation. (arXiv:2203.01190v1 [cs.RO])
73. Linear Stochastic Bandits over a Bit-Constrained Channel. (arXiv:2203.01198v1 [cs.LG])
74. A Quantitative Geometric Approach to Neural Network Smoothness. (arXiv:2203.01212v1 [cs.LG])
75. Towards Efficient and Stable K-Asynchronous Federated Learning with Unbounded Stale Gradients on Non-IID Data. (arXiv:2203.01214v1 [cs.LG])
76. A simple and universal rotation equivariant point-cloud network. (arXiv:2203.01216v1 [cs.LG])
77. Learning Conditional Variational Autoencoders with Missing Covariates. (arXiv:2203.01218v1 [stat.ML])
78. Are Latent Factor Regression and Sparse Regression Adequate?. (arXiv:2203.01219v1 [stat.ME])
79. Estimating average causal effects from patient trajectories. (arXiv:2203.01228v1 [stat.ML])
80. On the application of generative adversarial networks for nonlinear modal analysis. (arXiv:2203.01229v1 [cs.LG])
81. Convolutional neural networks as an alternative to Bayesian retrievals. (arXiv:2203.01236v1 [astro-ph.EP])
82. On the Optimization Landscape of Neural Collapse under MSE Loss: Global Optimality with Unconstrained Features. (arXiv:2203.01238v1 [cs.LG])
83. Flow-based density of states for complex actions. (arXiv:2203.01243v1 [hep-lat])
84. Low-Degree Multicalibration. (arXiv:2203.01255v1 [cs.LG])
85. TAE: A Semi-supervised Controllable Behavior-aware Trajectory Generator and Predictor. (arXiv:2203.01261v1 [cs.RO])
86. Interactive Visualization of Protein RINs using NetworKit in the Cloud. (arXiv:2203.01263v1 [cs.SI])
87. Machine learning models predict calculation outcomes with the transferability necessary for computational catalysis. (arXiv:2203.01276v1 [physics.chem-ph])
88. Deep Temporal Interpolation of Radar-based Precipitation. (arXiv:2203.01277v1 [cs.CV])
89. ADVISE: ADaptive Feature Relevance and VISual Explanations for Convolutional Neural Networks. (arXiv:2203.01289v1 [cs.CV])
90. Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering. (arXiv:2203.01294v1 [cs.LG])
91. STEADY: Simultaneous State Estimation and Dynamics Learning from Indirect Observations. (arXiv:2203.01299v1 [cs.RO])
92. Evolving Curricula with Regret-Based Environment Design. (arXiv:2203.01302v1 [cs.LG])
93. An Analysis of Ensemble Sampling. (arXiv:2203.01303v1 [cs.LG])
94. Supervised Hebbian learning: toward eXplainable AI. (arXiv:2203.01304v1 [cond-mat.dis-nn])
95. HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v1 [cs.LG])
96. Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits. (arXiv:1807.07623v6 [cs.LG] UPDATED)
97. Principled Deep Neural Network Training through Linear Programming. (arXiv:1810.03218v3 [cs.LG] UPDATED)
98. Incentivizing Exploration with Selective Data Disclosure. (arXiv:1811.06026v5 [cs.GT] UPDATED)
99. Sensor-Based Estimation of Dim Light Melatonin Onset (DLMO) Using Features of Two Time Scales. (arXiv:1908.07483v5 [cs.LG] UPDATED)
100. Adaptive Granularity in Tensors: A Quest for Interpretable Structure. (arXiv:1912.09009v2 [cs.LG] UPDATED)
101. LIMEADE: From AI Explanations to Advice Taking. (arXiv:2003.04315v3 [cs.IR] UPDATED)
102. The OARF Benchmark Suite: Characterization and Implications for Federated Learning Systems. (arXiv:2006.07856v4 [cs.LG] UPDATED)
103. Online Competitive Influence Maximization. (arXiv:2006.13411v4 [cs.LG] UPDATED)
104. FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance. (arXiv:2011.09607v2 [q-fin.TR] UPDATED)
105. Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v2 [cs.CV] UPDATED)
106. Beyond the Hype: A Real-World Evaluation of the Impact and Cost of Machine Learning-Based Malware Detection. (arXiv:2012.09214v3 [cs.CR] UPDATED)
107. Invariance, encodings, and generalization: learning identity effects with neural networks. (arXiv:2101.08386v5 [cs.LG] UPDATED)
108. Modeling Extremes with d-max-decreasing Neural Networks. (arXiv:2102.09042v2 [stat.ML] UPDATED)
109. Reinforcement learning for linear-convex models with jumps via stability analysis of feedback controls. (arXiv:2104.09311v2 [math.OC] UPDATED)
110. Continual Learning via Bit-Level Information Preserving. (arXiv:2105.04444v3 [cs.LG] UPDATED)
111. A new perspective on low-rank optimization. (arXiv:2105.05947v2 [math.OC] UPDATED)
112. Accelerating Neural ODEs Using Model Order Reduction. (arXiv:2105.14070v2 [cs.LG] UPDATED)
113. Sparse PointPillars: Maintaining and Exploiting Input Sparsity to Improve Runtime on Embedded Systems. (arXiv:2106.06882v3 [cs.CV] UPDATED)
114. Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v5 [cs.LG] UPDATED)
115. Expert Q-learning: Deep Reinforcement Learning with Coarse State Values from Offline Expert Examples. (arXiv:2106.14642v3 [cs.LG] UPDATED)
116. Improving Uncertainty Calibration of Deep Neural Networks via Truth Discovery and Geometric Optimization. (arXiv:2106.14662v3 [cs.LG] UPDATED)
117. Metalearning Linear Bandits by Prior Update. (arXiv:2107.05320v2 [stat.ML] UPDATED)
118. An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v3 [cs.CL] UPDATED)
119. Optimality and complexity of classification by random projection. (arXiv:2108.06339v2 [cs.LG] UPDATED)
120. Mitigating Statistical Bias within Differentially Private Synthetic Data. (arXiv:2108.10934v2 [stat.ML] UPDATED)
121. It's not Rocket Science : Interpreting Figurative Language in Narratives. (arXiv:2109.00087v3 [cs.CL] UPDATED)
122. How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data. (arXiv:2109.01300v2 [cs.LG] UPDATED)
123. Unrolling SGD: Understanding Factors Influencing Machine Unlearning. (arXiv:2109.13398v2 [cs.LG] UPDATED)
124. Leveraging power grid topology in machine learning assisted optimal power flow. (arXiv:2110.00306v2 [cs.LG] UPDATED)
125. Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition. (arXiv:2110.03327v2 [eess.AS] UPDATED)
126. Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v2 [cs.CV] UPDATED)
127. Human-robot collaboration and machine learning: a systematic review of recent research. (arXiv:2110.07448v2 [cs.RO] UPDATED)
128. Label-Descriptive Patterns and Their Application to Characterizing Classification Errors. (arXiv:2110.09599v2 [cs.LG] UPDATED)
129. Efficient Meta Subspace Optimization. (arXiv:2110.14920v2 [math.OC] UPDATED)
130. On Label Shift in Domain Adaptation via Wasserstein Distance. (arXiv:2110.15520v2 [cs.LG] UPDATED)
131. A Minimax Learning Approach to Off-Policy Evaluation in Confounded Partially Observable Markov Decision Processes. (arXiv:2111.06784v3 [cs.LG] UPDATED)
132. Maximum Mean Discrepancy for Generalization in the Presence of Distribution and Missingness Shift. (arXiv:2111.10344v2 [cs.LG] UPDATED)
133. SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional, and Incremental Robot Learning. (arXiv:2111.14693v3 [cs.RO] UPDATED)
134. Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v2 [cs.CV] UPDATED)
135. FinRL-Meta: A Universe of Near-Real Market Environments for Data-Driven Deep Reinforcement Learning in Quantitative Finance. (arXiv:2112.06753v2 [q-fin.TR] UPDATED)
136. Explanation of Machine Learning Models Using Shapley Additive Explanation and Application for Real Data in Hospital. (arXiv:2112.11071v2 [cs.LG] UPDATED)
137. Visual Attention Prediction Improves Performance of Autonomous Drone Racing Agents. (arXiv:2201.02569v3 [cs.RO] UPDATED)
138. Weisfeiler and Leman Go Infinite: Spectral and Combinatorial Pre-Colorings. (arXiv:2201.13410v2 [cs.LG] UPDATED)
139. Meta-Learning Hypothesis Spaces for Sequential Decision-making. (arXiv:2202.00602v2 [stat.ML] UPDATED)
140. JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v2 [cs.CL] UPDATED)
141. Learning from Imperfect Demonstrations via Adversarial Confidence Transfer. (arXiv:2202.02967v2 [cs.RO] UPDATED)
142. Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling. (arXiv:2202.03543v2 [eess.AS] UPDATED)
143. Topogivity: A Machine-Learned Chemical Rule for Discovering Topological Materials. (arXiv:2202.05255v2 [cond-mat.mtrl-sci] UPDATED)
144. Concurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion. (arXiv:2202.05481v2 [cs.RO] UPDATED)
145. Bounded nonlinear forecasts of partially observed geophysical systems with physics-constrained deep learning. (arXiv:2202.05750v2 [stat.ML] UPDATED)
146. Transformers in Time Series: A Survey. (arXiv:2202.07125v2 [cs.LG] UPDATED)
147. Incorporating Texture Information into Dimensionality Reduction for High-Dimensional Images. (arXiv:2202.09179v2 [cs.CV] UPDATED)
148. Signal Decomposition Using Masked Proximal Operators. (arXiv:2202.09338v2 [cs.LG] UPDATED)
149. Equivariant Graph Attention Networks for Molecular Property Prediction. (arXiv:2202.09891v2 [cs.LG] UPDATED)
150. Quantum Heterogeneous Distributed Deep Learning Architectures: Models, Discussions, and Applications. (arXiv:2202.11200v2 [quant-ph] UPDATED)
151. Deep Learning Reproducibility and Explainable AI (XAI). (arXiv:2202.11452v3 [cs.LG] UPDATED)
152. Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications. (arXiv:2202.12396v2 [math.OC] UPDATED)
153. An Evaluation of the EEG alpha-to-theta and theta-to-alpha band Ratios as Indexes of Mental Workload. (arXiv:2202.12937v2 [eess.SP] UPDATED)
154. Learning the Beauty in Songs: Neural Singing Voice Beautifier. (arXiv:2202.13277v2 [eess.AS] UPDATED)
155. Variational Interpretable Learning from Multi-view Data. (arXiv:2202.13503v2 [stat.ML] UPDATED)
156. Selection, Ignorability and Challenges With Causal Fairness. (arXiv:2202.13774v2 [stat.ML] UPDATED)
157. The Concordance Index decomposition: a measure for a deeper understanding of survival prediction models. (arXiv:2203.00144v2 [cs.LG] UPDATED)
158. GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for Memory-Efficient Graph Convolutional Neural Networks. (arXiv:2203.00158v2 [cs.AR] UPDATED)
159. Parameter estimation for WMTI-Watson model of white matter using encoder-decoder recurrent neural network. (arXiv:2203.00595v2 [physics.med-ph] UPDATED)
160. Predicting the Thermal Sunyaev-Zel'dovich Field using Modular and Equivariant Set-Based Neural Networks. (arXiv:2203.00026v1 [astro-ph.CO] CROSS LISTED)
## cs.AI
---
**76** new papers in cs.AI:-) 
1. Determining Research Priorities for Astronomy Using Machine Learning. (arXiv:2203.00713v1 [astro-ph.IM])
2. Learning Robust Real-Time Cultural Transmission without Human Data. (arXiv:2203.00715v1 [cs.LG])
3. A Conformer Based Acoustic Model for Robust Automatic Speech Recognition. (arXiv:2203.00725v1 [cs.SD])
4. Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data. (arXiv:2203.00734v1 [cs.CV])
5. There is a Time and Place for Reasoning Beyond the Image. (arXiv:2203.00758v1 [cs.CV])
6. Low-Cost On-device Partial Domain Adaptation (LoCO-PDA): Enabling efficient CNN retraining on edge devices. (arXiv:2203.00772v1 [cs.CV])
7. TANDEM: Learning Joint Exploration and Decision Making with Tactile Sensors. (arXiv:2203.00798v1 [cs.RO])
8. InCloud: Incremental Learning for Point Cloud Place Recognition. (arXiv:2203.00807v1 [cs.CV])
9. Efficient Dynamic Clustering: Capturing Patterns fromHistorical Cluster Evolution. (arXiv:2203.00812v1 [cs.DB])
10. Computerization of Clinical Pathways: A Literature Review and Directions for Future Research. (arXiv:2203.00815v1 [cs.AI])
11. TSAM: A Two-Stream Attention Model for Causal Emotion Entailment. (arXiv:2203.00819v1 [cs.CL])
12. Keeping Minimal Experience to Achieve Efficient Interpretable Policy Distillation. (arXiv:2203.00822v1 [cs.LG])
13. Can No-reference features help in Full-reference image quality estimation?. (arXiv:2203.00845v1 [eess.IV])
14. FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours. (arXiv:2203.00854v1 [cs.LG])
15. Centralized Fairness for Redistricting. (arXiv:2203.00872v1 [cs.GT])
16. Follow your Nose: Using General Value Functions for Directed Exploration in Reinforcement Learning. (arXiv:2203.00874v1 [cs.LG])
17. A Learning Based Framework for Handling Uncertain Lead Times in Multi-Product Inventory Management. (arXiv:2203.00885v1 [cs.LG])
18. Machine learning based lens-free imaging technique for field-portable cytometry. (arXiv:2203.00899v1 [eess.IV])
19. Combining Reinforcement Learning and Optimal Transport for the Traveling Salesman Problem. (arXiv:2203.00903v1 [cs.LG])
20. Weakly Supervised Correspondence Learning. (arXiv:2203.00904v1 [cs.RO])
21. Responsible-AI-by-Design: a Pattern Collection for Designing Responsible AI Systems. (arXiv:2203.00905v1 [cs.AI])
22. A Split Semantic Detection Algorithm for Psychological Sandplay Image. (arXiv:2203.00907v1 [cs.CV])
23. Neuro-Symbolic Verification of Deep Neural Networks. (arXiv:2203.00938v1 [cs.AI])
24. Speaker Adaption with Intuitive Prosodic Features for Statistical Parametric Speech Synthesis. (arXiv:2203.00951v1 [cs.SD])
25. PKGM: A Pre-trained Knowledge Graph Model for E-commerce Application. (arXiv:2203.00964v1 [cs.AI])
26. Parallel Spatio-Temporal Attention-Based TCN for Multivariate Time Series Prediction. (arXiv:2203.00971v1 [cs.LG])
27. Characterizing the organizational diversity of protein interaction networks across three domains of life. (arXiv:2203.00999v1 [q-bio.MN])
28. Improving the Diversity of Bootstrapped DQN via Noisy Priors. (arXiv:2203.01004v1 [cs.LG])
29. Continual Feature Selection: Spurious Features in Continual Learning. (arXiv:2203.01012v1 [cs.LG])
30. On the Configuration of More and Less Expressive Logic Programs. (arXiv:2203.01024v1 [cs.AI])
31. Learning in Sparse Rewards settings through Quality-Diversity algorithms. (arXiv:2203.01027v1 [cs.LG])
32. Satellite Image and Machine Learning based Knowledge Extraction in the Poverty and Welfare Domain. (arXiv:2203.01068v1 [cs.CY])
33. On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v1 [cs.LG])
34. Vision-based Large-scale 3D Semantic Mapping for Autonomous Driving Applications. (arXiv:2203.01087v1 [cs.CV])
35. Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models. (arXiv:2203.01104v1 [cs.CL])
36. Controlling the Focus of Pretrained Language Generation Models. (arXiv:2203.01146v1 [cs.AI])
37. InsertionNet 2.0: Minimal Contact Multi-Step Insertion Using Multimodal Multiview Sensory Input. (arXiv:2203.01153v1 [cs.RO])
38. Engineering the Neural Automatic Passenger Counter. (arXiv:2203.01156v1 [cs.LG])
39. Avant-Satie! Using ERIK to encode task-relevant expressivity into the animation of autonomous social robots. (arXiv:2203.01176v1 [cs.RO])
40. Analytical Solutions for the Inverse Problem within Gradual Semantics. (arXiv:2203.01201v1 [cs.AI])
41. Audio Self-supervised Learning: A Survey. (arXiv:2203.01205v1 [cs.SD])
42. On the Optimization Landscape of Neural Collapse under MSE Loss: Global Optimality with Unconstrained Features. (arXiv:2203.01238v1 [cs.LG])
43. WaveY-Net: Physics-augmented deep learning for high-speed electromagnetic simulation and optimization. (arXiv:2203.01248v1 [physics.app-ph])
44. Deep Temporal Interpolation of Radar-based Precipitation. (arXiv:2203.01277v1 [cs.CV])
45. The role of haptic communication in dyadic collaborative object manipulation tasks. (arXiv:2203.01287v1 [cs.RO])
46. Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering. (arXiv:2203.01294v1 [cs.LG])
47. Half Wavelet Attention on M-Net+ for Low-Light Image **Enhancement**. (arXiv:2203.01296v1 [eess.IV])
48. Pareto Frontier Approximation Network (PA-Net) to Solve Bi-objective TSP. (arXiv:2203.01298v1 [cs.RO])
49. DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v1 [cs.CV])
50. Counterfactually Evaluating Explanations in Recommender Systems. (arXiv:2203.01310v1 [cs.AI])
51. HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v1 [cs.LG])
52. Institutional Grammar 2.0 Codebook. (arXiv:2008.08937v4 [cs.MA] UPDATED)
53. REAL-X -- Robot open-Ended Autonomous Learning Architectures: Achieving Truly End-to-End Sensorimotor Autonomous Learning Systems. (arXiv:2011.13880v2 [cs.RO] UPDATED)
54. Interpreting intermediate convolutional layers of CNNs trained on raw speech. (arXiv:2104.09489v3 [cs.SD] UPDATED)
55. Expert Q-learning: Deep Reinforcement Learning with Coarse State Values from Offline Expert Examples. (arXiv:2106.14642v3 [cs.LG] UPDATED)
56. Improving Uncertainty Calibration of Deep Neural Networks via Truth Discovery and Geometric Optimization. (arXiv:2106.14662v3 [cs.LG] UPDATED)
57. Metalearning Linear Bandits by Prior Update. (arXiv:2107.05320v2 [stat.ML] UPDATED)
58. Automatic Speech Recognition And Limited Vocabulary: A Survey. (arXiv:2108.10254v2 [cs.AI] UPDATED)
59. Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideation. (arXiv:2109.10149v3 [cs.HC] UPDATED)
60. Benchmarking Augmentation Methods for Learning Robust Navigation Agents: the Winning Entry of the 2021 iGibson Challenge. (arXiv:2109.10493v2 [cs.RO] UPDATED)
61. Building Chinese Biomedical Language Models via Multi-Level Text Discrimination. (arXiv:2110.07244v2 [cs.CL] UPDATED)
62. Towards Optimal Correlational Object Search. (arXiv:2110.09991v2 [cs.RO] UPDATED)
63. flip-hoisting: Exploiting Repeated Parameters in Discrete Probabilistic Programs. (arXiv:2110.10284v2 [cs.AI] UPDATED)
64. Efficient Meta Subspace Optimization. (arXiv:2110.14920v2 [math.OC] UPDATED)
65. Real-World Semantic Grasp Detection Based on Attention Mechanism. (arXiv:2111.10522v2 [cs.RO] UPDATED)
66. SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional, and Incremental Robot Learning. (arXiv:2111.14693v3 [cs.RO] UPDATED)
67. Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval. (arXiv:2201.05409v3 [cs.IR] UPDATED)
68. Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection. (arXiv:2201.12806v2 [cs.CL] UPDATED)
69. Meta-Learning Hypothesis Spaces for Sequential Decision-making. (arXiv:2202.00602v2 [stat.ML] UPDATED)
70. JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v2 [cs.CL] UPDATED)
71. Transformers in Time Series: A Survey. (arXiv:2202.07125v2 [cs.LG] UPDATED)
72. Deep Learning Reproducibility and Explainable AI (XAI). (arXiv:2202.11452v3 [cs.LG] UPDATED)
73. Matching Papers and Reviewers at Large Conferences. (arXiv:2202.12273v3 [cs.AI] UPDATED)
74. An Evaluation of the EEG alpha-to-theta and theta-to-alpha band Ratios as Indexes of Mental Workload. (arXiv:2202.12937v2 [eess.SP] UPDATED)
75. GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for Memory-Efficient Graph Convolutional Neural Networks. (arXiv:2203.00158v2 [cs.AR] UPDATED)
76. Exploring and Adapting Chinese GPT to Pinyin Input Method. (arXiv:2203.00249v2 [cs.CL] UPDATED)

