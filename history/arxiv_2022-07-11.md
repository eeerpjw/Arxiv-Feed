# Your interest papers
---
## cs.CV
---
### Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers. (arXiv:2207.03489v1 [cs.CV])
- Authors : Hyuntai Kim
- Link : [http://arxiv.org/abs/2207.03489](http://arxiv.org/abs/2207.03489)
> ABSTRACT  :  In this paper, a mode decomposition (MD) method for degenerated modes has been studied. Convolution neural network (CNN) has been applied for image training and predicting the mode coefficients. Four-fold degenerated $LP_{11}$ series has been the target to be decomposed. Multiple images are regarded as an input to decompose the degenerate modes. Total of seven different images, including the full original near-field image, and images after linear polarizers of four directions (0$^\circ$, 45$^\circ$, 90$^\circ$, and 135$^\circ$), and images after two circular polarizers (right-handed and left-handed) has been considered for training, validation, and test. The output label of the model has been chosen as the real and imaginary components of the mode coefficient, and the loss function has been selected to be the root-mean-square (RMS) of the labels. The RMS and mean-absolute-error (MAE) of the label, intensity, phase, and field correlation between the actual and predicted values have been selected to be the metrics to evaluate the CNN model. The CNN model has been trained with 100,000 three-dimensional images with depths of three, four, and seven. The performance of the trained model was evaluated via 10,000 test samples with four sets of images - images after three linear polarizers (0$^\circ$, 45$^\circ$, 90$^\circ$) and image after right-handed circular polarizer - showed 0.0634 of label RMS, 0.0292 of intensity RMS, 0.1867 rad of phase MAE, and 0.9978 of average field correlation. The performance of 4 image sets showed at least 50.68\% of performance **enhancement** compared to models considering only images after linear polarizers.  
### Mirror Complementary Transformer Network for RGB-thermal Salient Object Detection. (arXiv:2207.03558v1 [cs.CV])
- Authors : Xiurong Jiang, Lin Zhu, Yifan Hou, Hui Tian
- Link : [http://arxiv.org/abs/2207.03558](http://arxiv.org/abs/2207.03558)
> ABSTRACT  :  RGB-thermal salient object detection (RGB-T SOD) aims to locate the common prominent objects of an aligned visible and thermal infrared image pair and accurately segment all the pixels belonging to those objects. It is promising in challenging scenes such as **night**time and complex backgrounds due to the insensitivity to lighting conditions of thermal images. Thus, the key problem of RGB-T SOD is to make the features from the two modalities complement and adjust each other flexibly, since it is inevitable that any modalities of RGB-T image pairs failure due to challenging scenes such as extreme light conditions and thermal crossover. In this paper, we propose a novel mirror complementary Transformer network (MCNet) for RGB-T SOD. Specifically, we introduce a Transformer-based feature extraction module to effective extract hierarchical features of RGB and thermal images. Then, through the attention-based feature interaction and serial multiscale dilated convolution (SDC) based feature fusion modules, the proposed model achieves the complementary interaction of low-level features and the semantic fusion of deep features. Finally, based on the mirror complementary structure, the salient regions of the two modalities can be accurately extracted even one modality is invalid. To demonstrate the robustness of the proposed model under challenging scenes in real world, we build a novel RGB-T SOD dataset VT723 based on a large public semantic segmentation RGB-T dataset used in the autonomous driving domain. Expensive experiments on benchmark and VT723 datasets show that the proposed method outperforms state-of-the-art approaches, including CNN-based and Transformer-based methods. The code and dataset will be released later at https://github.com/jxr326/**Swin**MCNet.  
### The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning. (arXiv:2207.03568v1 [eess.IV])
- Authors : Alireza Borjali, Soheil Ashkani, Rohan Bhimani, Daniel Guss, Kartik Mangudi, Bart Lubberts
- Link : [http://arxiv.org/abs/2207.03568](http://arxiv.org/abs/2207.03568)
> ABSTRACT  :  Delayed diagnosis of syndesmosis instability can lead to significant morbidity and accelerated arthritic change in the ankle joint. Weight-bearing computed tomography (WBCT) has shown promising potential for early and reliable detection of isolated syndesmotic instability using 3D volumetric measurements. While these measurements have been reported to be highly accurate, they are also experience-dependent, time-consuming, and need a particular 3D measurement software tool that leads the clinicians to still show more interest in the conventional diagnostic methods for syndesmotic instability. The purpose of this study was to increase accuracy, accelerate analysis time, and reduce inter-observer bias by automating 3D volume assessment of syndesmosis anatomy using WBCT scans. We conducted a retrospective study using previously collected WBCT scans of patients with unilateral syndesmotic instability. 144 **bilateral** ankle WBCT scans were evaluated (48 unstable, 96 control). We developed three deep learning (DL) models for analyzing WBCT scans to recognize syndesmosis instability. These three models included two state-of-the-art models (Model 1 - 3D convolutional neural network [CNN], and Model 2 - CNN with long short-term memory [LSTM]), and a new model (Model 3 - differential CNN LSTM) that we introduced in this study. Model 1 failed to analyze the WBCT scans (F1-score = 0). Model 2 only misclassified two cases (F1-score = 0.80). Model 3 outperformed Model 2 and achieved a nearly perfect performance, misclassifying only one case (F1-score = 0.91) in the control group as unstable while being faster than Model 2.  
### More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity. (arXiv:2207.03620v1 [cs.CV])
- Authors : Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu, Zhangyang Wang
- Link : [http://arxiv.org/abs/2207.03620](http://arxiv.org/abs/2207.03620)
> ABSTRACT  :  Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local but large attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as **Swin** Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as typical downstream tasks. Our code is available here https://github.com/VITA-Group/SLaK.  
### Learning High-quality Proposals for Acne Detection. (arXiv:2207.03674v1 [cs.CV])
- Authors : Jianwei Zhang, **Lei Zhang**, Junyou Wang, Xin Wei, Jiaqi Li, Xian Jiang, Dan Du
- Link : [http://arxiv.org/abs/2207.03674](http://arxiv.org/abs/2207.03674)
> ABSTRACT  :  Acne detection is crucial for interpretative diagnosis and precise treatment of skin disease. The arbitrary boundary and small size of acne lesions lead to a significant number of poor-quality proposals in two-stage detection. In this paper, we propose a novel head structure for Region Proposal Network to improve the proposals' quality in two ways. At first, a Spatial Aware Double Head(SADH) structure is proposed to disentangle the representation learning for classification and localization from two different spatial perspectives. The proposed SADH ensures a steeper classification confidence gradient and suppresses the proposals having low intersection-over-union(IoU) with the matched ground truth. Then, we propose a Normalized Wasserstein Distance prediction branch to improve the correlation between the proposals' classification scores and IoUs. In addition, to facilitate further research on acne detection, we construct a new dataset named AcneSCU, with high-resolution imageries, precise annotations, and fine-grained lesion categories. Extensive experiments are conducted on both AcneSCU and the public dataset ACNE04, and the results demonstrate the proposed method could improve the proposals' quality, consistently outperforming state-of-the-art approaches. Code and the collected dataset are available in https://github.com/pingguokiller/acnedetection.  
### A Mask Attention Interaction and Scale **Enhancement** Network for SAR Ship Instance Segmentation. (arXiv:2207.03912v1 [cs.CV])
- Authors : Tianwen Zhang, Xiaoling Zhang
- Link : [http://arxiv.org/abs/2207.03912](http://arxiv.org/abs/2207.03912)
> ABSTRACT  :  Most of existing synthetic aperture radar (SAR) ship in-stance segmentation models do not achieve mask interac-tion or offer limited interaction performance. Besides, their multi-scale ship instance segmentation performance is moderate especially for small ships. To solve these problems, we propose a mask attention interaction and scale **enhancement** network (MAI-SE-Net) for SAR ship instance segmentation. MAI uses an atrous spatial pyra-mid pooling (ASPP) to gain multi-resolution feature re-sponses, a non-local block (NLB) to model long-range spa-tial dependencies, and a concatenation shuffle attention block (CSAB) to improve interaction benefits. SE uses a content-aware reassembly of features block (CARAFEB) to generate an extra pyramid bottom-level to boost small ship performance, a feature balance operation (FBO) to improve scale feature description, and a global context block (GCB) to refine features. Experimental results on two public SSDD and HRSID datasets reveal that MAI-SE-Net outperforms the other nine competitive models, better than the suboptimal model by 4.7% detec-tion AP and 3.4% segmentation AP on SSDD and by 3.0% detection AP and 2.4% segmentation AP on HRSID.  
### Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v3 [cs.CV] UPDATED)
- Authors : Christoph Angermann, la Moravov, Markus Haltmeier, Christian Laubichler
- Link : [http://arxiv.org/abs/2103.16938](http://arxiv.org/abs/2103.16938)
> ABSTRACT  :  **Real-time** estimation of actual environment depth is an essential module for various autonomous system tasks such as localization, obstacle detection and pose estimation. During the last decade of machine learning, extensive deployment of deep learning methods to computer vision tasks yielded successful approaches for realistic depth synthesis out of a simple RGB modality. While most of these models rest on paired depth data or availability of video sequences and stereo images, there is a lack of methods facing single-image depth synthesis in an unsupervised manner. Therefore, in this study, latest advancements in the field of generative neural networks are leveraged to fully unsupervised single-image depth synthesis. To be more exact, two cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously optimized using the Wasserstein-1 distance. To ensure plausibility of the proposed method, we apply the models to a self acquised industrial data set as well as to the renown NYU Depth v2 data set, which allows comparison with existing approaches. The observed success in this study suggests high potential for unpaired single-image depth estimation in real world applications.  
### Enhancing **Low-Light** Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v2 [eess.IV] UPDATED)
- Authors : Lanqing Guo, Renjie Wan, **Wenhan Yang**, Alex Kot, Bihan Wen
- Link : [http://arxiv.org/abs/2201.03145](http://arxiv.org/abs/2201.03145)
> ABSTRACT  :  Images captured in the **low-light** condition suffer from low visibility and various imaging artifacts, e.g., real noise. Existing supervised enlightening algorithms require a large set of pixel-aligned training image pairs, which are hard to prepare in practice. Though weakly-supervised or unsupervised methods can alleviate such challenges without using paired training images, some real-world artifacts inevitably get falsely amplified because of the lack of corresponded supervision. In this paper, instead of using perfectly aligned images for training, we creatively employ the misaligned real-world images as the guidance, which are considerably easier to collect. Specifically, we propose a Cross-Image Disentanglement Network (CIDN) to separately extract cross-image brightness and image-specific content features from low/normal-light images. Based on that, CIDN can simultaneously correct the brightness and suppress image artifacts in the feature domain, which largely increases the robustness to the pixel shifts. Furthermore, we collect a new **low-light** image **enhancement** dataset consisting of misaligned training images with real-world corruptions. Experimental results show that our model achieves state-of-the-art performances on both the newly proposed dataset and other popular **low-light** datasets.  
### Visual Attention Network. (arXiv:2202.09741v4 [cs.CV] UPDATED)
- Authors : Hao Guo, Ze Lu, Ning Liu, Ming Cheng, Min Hu
- Link : [http://arxiv.org/abs/2202.09741](http://arxiv.org/abs/2202.09741)
> ABSTRACT  :  While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN surpasses similar size vision transformers(ViTs) and convolutional neural networks(CNNs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses **Swin**-T 4% mIoU (50.1 vs. 46.1) for semantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community. Code is available at https://github.com/Visual-Attention-Network.  
### Semi-supervised classification of medical ultrasound images based on generative adversarial network. (arXiv:2203.06184v2 [eess.IV] UPDATED)
- Authors : Zhaoshan Liu, Chau Hung, Lei Shen
- Link : [http://arxiv.org/abs/2203.06184](http://arxiv.org/abs/2203.06184)
> ABSTRACT  :  Medical ultrasound (US) is one of the most widely used imaging modalities in clinical practice. However, its use presents unique challenges such as variable imaging quality. Deep learning (DL) can be used as an advanced medical US image analysis tool, while the performance of the DL model is greatly limited by the scarcity of big datasets. Here, we develop semi-supervised classification **enhancement** (SSCE) structures by combining convolutional neural network (CNN) and generative adversarial network (GAN) to address the data shortage. A breast cancer dataset with 780 images is used as our base dataset. The results show that our SSCE structures obtain an accuracy of up to 97.9%, showing a maximum 21.6% improvement compared with utilizing CNN models alone and outperforming the previous methods using the same dataset by up to 23.9%. We believe our proposed state-of-the-art method can be regarded as a potential auxiliary tool for the diagnoses of medical US images.  
### How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v2 [cs.CV] UPDATED)
- Authors : Tobias Fischer, Michael Milford
- Link : [http://arxiv.org/abs/2206.13673](http://arxiv.org/abs/2206.13673)
> ABSTRACT  :  Event cameras continue to attract interest due to desirable characteristics such as **high dynamic range**, low latency, virtually no motion blur, and high energy efficiency. One of the potential applications that would benefit from these characteristics lies in visual place recognition for robot localization, i.e. matching a query observation to the corresponding reference place in the database. In this letter, we explore the distinctiveness of event streams from a small subset of pixels (in the tens or hundreds). We demonstrate that the absolute difference in the number of events at those pixel locations accumulated into event frames can be sufficient for the place recognition task, when pixels that display large variations in the reference set are used. Using such sparse (over image coordinates) but varying (variance over the number of events per pixel location) pixels enables frequent and computationally cheap updates of the location estimates. Furthermore, when event frames contain a constant number of events, our method takes full advantage of the event-driven nature of the sensory stream and displays promising robustness to changes in velocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset in an outdoor driving scenario, as well as the newly contributed indoor QCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a mobile robotic platform. Our results show that our approach achieves competitive performance when compared to several baseline methods on those datasets, and is particularly well suited for compute- and energy-constrained platforms such as interplanetary rovers.  
### Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans. (arXiv:2207.01579v2 [eess.IV] UPDATED)
- Authors : Chung Hsu, Han Tsai, Lin Chen, Di Ma, Chieh Tai
- Link : [http://arxiv.org/abs/2207.01579](http://arxiv.org/abs/2207.01579)
> ABSTRACT  :  Computed tomography (CT) imaging could be very practical for diagnosing various diseases. However, the nature of the CT images is even more diverse since the resolution and number of the slices of a CT scan are determined by the machine and its settings. Conventional deep learning models are hard to tickle such diverse data since the essential requirement of the deep neural network is the consistent shape of the input data. In this paper, we propose a novel, effective, two-step-wise approach to tickle this issue for COVID-19 symptom classification thoroughly. First, the semantic feature embedding of each slice for a CT scan is extracted by conventional backbone networks. Then, we proposed a long short-term memory (LSTM) and Transformer-based sub-network to deal with temporal feature learning, leading to spatiotemporal feature representation learning. In this fashion, the proposed two-step LSTM model could prevent overfitting, as well as increase performance. Comprehensive experiments reveal that the proposed two-step method not only shows excellent performance but also could be compensated for each other. More specifically, the two-step LSTM model has a lower false-negative rate, while the 2-step **Swin** model has a lower false-positive rate. In summary, it is suggested that the model ensemble could be adopted for more stable and promising performance in real-world applications.  
## eess.IV
---
### Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers. (arXiv:2207.03489v1 [cs.CV])
- Authors : Hyuntai Kim
- Link : [http://arxiv.org/abs/2207.03489](http://arxiv.org/abs/2207.03489)
> ABSTRACT  :  In this paper, a mode decomposition (MD) method for degenerated modes has been studied. Convolution neural network (CNN) has been applied for image training and predicting the mode coefficients. Four-fold degenerated $LP_{11}$ series has been the target to be decomposed. Multiple images are regarded as an input to decompose the degenerate modes. Total of seven different images, including the full original near-field image, and images after linear polarizers of four directions (0$^\circ$, 45$^\circ$, 90$^\circ$, and 135$^\circ$), and images after two circular polarizers (right-handed and left-handed) has been considered for training, validation, and test. The output label of the model has been chosen as the real and imaginary components of the mode coefficient, and the loss function has been selected to be the root-mean-square (RMS) of the labels. The RMS and mean-absolute-error (MAE) of the label, intensity, phase, and field correlation between the actual and predicted values have been selected to be the metrics to evaluate the CNN model. The CNN model has been trained with 100,000 three-dimensional images with depths of three, four, and seven. The performance of the trained model was evaluated via 10,000 test samples with four sets of images - images after three linear polarizers (0$^\circ$, 45$^\circ$, 90$^\circ$) and image after right-handed circular polarizer - showed 0.0634 of label RMS, 0.0292 of intensity RMS, 0.1867 rad of phase MAE, and 0.9978 of average field correlation. The performance of 4 image sets showed at least 50.68\% of performance **enhancement** compared to models considering only images after linear polarizers.  
### The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning. (arXiv:2207.03568v1 [eess.IV])
- Authors : Alireza Borjali, Soheil Ashkani, Rohan Bhimani, Daniel Guss, Kartik Mangudi, Bart Lubberts
- Link : [http://arxiv.org/abs/2207.03568](http://arxiv.org/abs/2207.03568)
> ABSTRACT  :  Delayed diagnosis of syndesmosis instability can lead to significant morbidity and accelerated arthritic change in the ankle joint. Weight-bearing computed tomography (WBCT) has shown promising potential for early and reliable detection of isolated syndesmotic instability using 3D volumetric measurements. While these measurements have been reported to be highly accurate, they are also experience-dependent, time-consuming, and need a particular 3D measurement software tool that leads the clinicians to still show more interest in the conventional diagnostic methods for syndesmotic instability. The purpose of this study was to increase accuracy, accelerate analysis time, and reduce inter-observer bias by automating 3D volume assessment of syndesmosis anatomy using WBCT scans. We conducted a retrospective study using previously collected WBCT scans of patients with unilateral syndesmotic instability. 144 **bilateral** ankle WBCT scans were evaluated (48 unstable, 96 control). We developed three deep learning (DL) models for analyzing WBCT scans to recognize syndesmosis instability. These three models included two state-of-the-art models (Model 1 - 3D convolutional neural network [CNN], and Model 2 - CNN with long short-term memory [LSTM]), and a new model (Model 3 - differential CNN LSTM) that we introduced in this study. Model 1 failed to analyze the WBCT scans (F1-score = 0). Model 2 only misclassified two cases (F1-score = 0.80). Model 3 outperformed Model 2 and achieved a nearly perfect performance, misclassifying only one case (F1-score = 0.91) in the control group as unstable while being faster than Model 2.  
### Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v3 [cs.CV] UPDATED)
- Authors : Christoph Angermann, la Moravov, Markus Haltmeier, Christian Laubichler
- Link : [http://arxiv.org/abs/2103.16938](http://arxiv.org/abs/2103.16938)
> ABSTRACT  :  **Real-time** estimation of actual environment depth is an essential module for various autonomous system tasks such as localization, obstacle detection and pose estimation. During the last decade of machine learning, extensive deployment of deep learning methods to computer vision tasks yielded successful approaches for realistic depth synthesis out of a simple RGB modality. While most of these models rest on paired depth data or availability of video sequences and stereo images, there is a lack of methods facing single-image depth synthesis in an unsupervised manner. Therefore, in this study, latest advancements in the field of generative neural networks are leveraged to fully unsupervised single-image depth synthesis. To be more exact, two cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously optimized using the Wasserstein-1 distance. To ensure plausibility of the proposed method, we apply the models to a self acquised industrial data set as well as to the renown NYU Depth v2 data set, which allows comparison with existing approaches. The observed success in this study suggests high potential for unpaired single-image depth estimation in real world applications.  
### Enhancing **Low-Light** Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v2 [eess.IV] UPDATED)
- Authors : Lanqing Guo, Renjie Wan, **Wenhan Yang**, Alex Kot, Bihan Wen
- Link : [http://arxiv.org/abs/2201.03145](http://arxiv.org/abs/2201.03145)
> ABSTRACT  :  Images captured in the **low-light** condition suffer from low visibility and various imaging artifacts, e.g., real noise. Existing supervised enlightening algorithms require a large set of pixel-aligned training image pairs, which are hard to prepare in practice. Though weakly-supervised or unsupervised methods can alleviate such challenges without using paired training images, some real-world artifacts inevitably get falsely amplified because of the lack of corresponded supervision. In this paper, instead of using perfectly aligned images for training, we creatively employ the misaligned real-world images as the guidance, which are considerably easier to collect. Specifically, we propose a Cross-Image Disentanglement Network (CIDN) to separately extract cross-image brightness and image-specific content features from low/normal-light images. Based on that, CIDN can simultaneously correct the brightness and suppress image artifacts in the feature domain, which largely increases the robustness to the pixel shifts. Furthermore, we collect a new **low-light** image **enhancement** dataset consisting of misaligned training images with real-world corruptions. Experimental results show that our model achieves state-of-the-art performances on both the newly proposed dataset and other popular **low-light** datasets.  
### Semi-supervised classification of medical ultrasound images based on generative adversarial network. (arXiv:2203.06184v2 [eess.IV] UPDATED)
- Authors : Zhaoshan Liu, Chau Hung, Lei Shen
- Link : [http://arxiv.org/abs/2203.06184](http://arxiv.org/abs/2203.06184)
> ABSTRACT  :  Medical ultrasound (US) is one of the most widely used imaging modalities in clinical practice. However, its use presents unique challenges such as variable imaging quality. Deep learning (DL) can be used as an advanced medical US image analysis tool, while the performance of the DL model is greatly limited by the scarcity of big datasets. Here, we develop semi-supervised classification **enhancement** (SSCE) structures by combining convolutional neural network (CNN) and generative adversarial network (GAN) to address the data shortage. A breast cancer dataset with 780 images is used as our base dataset. The results show that our SSCE structures obtain an accuracy of up to 97.9%, showing a maximum 21.6% improvement compared with utilizing CNN models alone and outperforming the previous methods using the same dataset by up to 23.9%. We believe our proposed state-of-the-art method can be regarded as a potential auxiliary tool for the diagnoses of medical US images.  
### Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans. (arXiv:2207.01579v2 [eess.IV] UPDATED)
- Authors : Chung Hsu, Han Tsai, Lin Chen, Di Ma, Chieh Tai
- Link : [http://arxiv.org/abs/2207.01579](http://arxiv.org/abs/2207.01579)
> ABSTRACT  :  Computed tomography (CT) imaging could be very practical for diagnosing various diseases. However, the nature of the CT images is even more diverse since the resolution and number of the slices of a CT scan are determined by the machine and its settings. Conventional deep learning models are hard to tickle such diverse data since the essential requirement of the deep neural network is the consistent shape of the input data. In this paper, we propose a novel, effective, two-step-wise approach to tickle this issue for COVID-19 symptom classification thoroughly. First, the semantic feature embedding of each slice for a CT scan is extracted by conventional backbone networks. Then, we proposed a long short-term memory (LSTM) and Transformer-based sub-network to deal with temporal feature learning, leading to spatiotemporal feature representation learning. In this fashion, the proposed two-step LSTM model could prevent overfitting, as well as increase performance. Comprehensive experiments reveal that the proposed two-step method not only shows excellent performance but also could be compensated for each other. More specifically, the two-step LSTM model has a lower false-negative rate, while the 2-step **Swin** model has a lower false-positive rate. In summary, it is suggested that the model ensemble could be adopted for more stable and promising performance in real-world applications.  
## cs.LG
---
### Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers. (arXiv:2207.03489v1 [cs.CV])
- Authors : Hyuntai Kim
- Link : [http://arxiv.org/abs/2207.03489](http://arxiv.org/abs/2207.03489)
> ABSTRACT  :  In this paper, a mode decomposition (MD) method for degenerated modes has been studied. Convolution neural network (CNN) has been applied for image training and predicting the mode coefficients. Four-fold degenerated $LP_{11}$ series has been the target to be decomposed. Multiple images are regarded as an input to decompose the degenerate modes. Total of seven different images, including the full original near-field image, and images after linear polarizers of four directions (0$^\circ$, 45$^\circ$, 90$^\circ$, and 135$^\circ$), and images after two circular polarizers (right-handed and left-handed) has been considered for training, validation, and test. The output label of the model has been chosen as the real and imaginary components of the mode coefficient, and the loss function has been selected to be the root-mean-square (RMS) of the labels. The RMS and mean-absolute-error (MAE) of the label, intensity, phase, and field correlation between the actual and predicted values have been selected to be the metrics to evaluate the CNN model. The CNN model has been trained with 100,000 three-dimensional images with depths of three, four, and seven. The performance of the trained model was evaluated via 10,000 test samples with four sets of images - images after three linear polarizers (0$^\circ$, 45$^\circ$, 90$^\circ$) and image after right-handed circular polarizer - showed 0.0634 of label RMS, 0.0292 of intensity RMS, 0.1867 rad of phase MAE, and 0.9978 of average field correlation. The performance of 4 image sets showed at least 50.68\% of performance **enhancement** compared to models considering only images after linear polarizers.  
### The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning. (arXiv:2207.03568v1 [eess.IV])
- Authors : Alireza Borjali, Soheil Ashkani, Rohan Bhimani, Daniel Guss, Kartik Mangudi, Bart Lubberts
- Link : [http://arxiv.org/abs/2207.03568](http://arxiv.org/abs/2207.03568)
> ABSTRACT  :  Delayed diagnosis of syndesmosis instability can lead to significant morbidity and accelerated arthritic change in the ankle joint. Weight-bearing computed tomography (WBCT) has shown promising potential for early and reliable detection of isolated syndesmotic instability using 3D volumetric measurements. While these measurements have been reported to be highly accurate, they are also experience-dependent, time-consuming, and need a particular 3D measurement software tool that leads the clinicians to still show more interest in the conventional diagnostic methods for syndesmotic instability. The purpose of this study was to increase accuracy, accelerate analysis time, and reduce inter-observer bias by automating 3D volume assessment of syndesmosis anatomy using WBCT scans. We conducted a retrospective study using previously collected WBCT scans of patients with unilateral syndesmotic instability. 144 **bilateral** ankle WBCT scans were evaluated (48 unstable, 96 control). We developed three deep learning (DL) models for analyzing WBCT scans to recognize syndesmosis instability. These three models included two state-of-the-art models (Model 1 - 3D convolutional neural network [CNN], and Model 2 - CNN with long short-term memory [LSTM]), and a new model (Model 3 - differential CNN LSTM) that we introduced in this study. Model 1 failed to analyze the WBCT scans (F1-score = 0). Model 2 only misclassified two cases (F1-score = 0.80). Model 3 outperformed Model 2 and achieved a nearly perfect performance, misclassifying only one case (F1-score = 0.91) in the control group as unstable while being faster than Model 2.  
### Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v3 [cs.CV] UPDATED)
- Authors : Christoph Angermann, la Moravov, Markus Haltmeier, Christian Laubichler
- Link : [http://arxiv.org/abs/2103.16938](http://arxiv.org/abs/2103.16938)
> ABSTRACT  :  **Real-time** estimation of actual environment depth is an essential module for various autonomous system tasks such as localization, obstacle detection and pose estimation. During the last decade of machine learning, extensive deployment of deep learning methods to computer vision tasks yielded successful approaches for realistic depth synthesis out of a simple RGB modality. While most of these models rest on paired depth data or availability of video sequences and stereo images, there is a lack of methods facing single-image depth synthesis in an unsupervised manner. Therefore, in this study, latest advancements in the field of generative neural networks are leveraged to fully unsupervised single-image depth synthesis. To be more exact, two cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously optimized using the Wasserstein-1 distance. To ensure plausibility of the proposed method, we apply the models to a self acquised industrial data set as well as to the renown NYU Depth v2 data set, which allows comparison with existing approaches. The observed success in this study suggests high potential for unpaired single-image depth estimation in real world applications.  
### Combining Machine Learning and Effective Feature Selection for **Real-time** Stock Trading in Variable Time-frames. (arXiv:2107.13148v2 [q-fin.TR] UPDATED)
- Authors : Amanat Ullah, Fahim Imtiaz, Miftah Uddin, Md Ihsan, Golam Rabiul, Mahbub Majumdar
- Link : [http://arxiv.org/abs/2107.13148](http://arxiv.org/abs/2107.13148)
> ABSTRACT  :  The unpredictability and volatility of the stock market render it challenging to make a substantial profit using any generalised scheme. Many previous studies tried different techniques to build a machine learning model, which can make a significant profit in the US stock market by performing live trading. However, very few studies have focused on the importance of finding the best features for a particular trading period. Our top approach used the performance to narrow down the features from a total of 148 to about 30. Furthermore, the top 25 features were dynamically selected before each time training our machine learning model. It uses ensemble learning with four classifiers: Gaussian Naive Bayes, Decision Tree, Logistic Regression with L1 regularization, and Stochastic Gradient Descent, to decide whether to go long or short on a particular stock. Our best model performed daily trade between July 2011 and January 2019, generating 54.35% profit. Finally, our work showcased that mixtures of weighted classifiers perform better than any individual predictor of making trading decisions in the stock market.  
### Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans. (arXiv:2207.01579v2 [eess.IV] UPDATED)
- Authors : Chung Hsu, Han Tsai, Lin Chen, Di Ma, Chieh Tai
- Link : [http://arxiv.org/abs/2207.01579](http://arxiv.org/abs/2207.01579)
> ABSTRACT  :  Computed tomography (CT) imaging could be very practical for diagnosing various diseases. However, the nature of the CT images is even more diverse since the resolution and number of the slices of a CT scan are determined by the machine and its settings. Conventional deep learning models are hard to tickle such diverse data since the essential requirement of the deep neural network is the consistent shape of the input data. In this paper, we propose a novel, effective, two-step-wise approach to tickle this issue for COVID-19 symptom classification thoroughly. First, the semantic feature embedding of each slice for a CT scan is extracted by conventional backbone networks. Then, we proposed a long short-term memory (LSTM) and Transformer-based sub-network to deal with temporal feature learning, leading to spatiotemporal feature representation learning. In this fashion, the proposed two-step LSTM model could prevent overfitting, as well as increase performance. Comprehensive experiments reveal that the proposed two-step method not only shows excellent performance but also could be compensated for each other. More specifically, the two-step LSTM model has a lower false-negative rate, while the 2-step **Swin** model has a lower false-positive rate. In summary, it is suggested that the model ensemble could be adopted for more stable and promising performance in real-world applications.  
### Semi-unsupervised Learning for Time Series Classification. (arXiv:2207.03119v2 [cs.LG] UPDATED)
- Authors : Padraig Davidson, Michael Steininger, Anna Krause, Andreas Hotho
- Link : [http://arxiv.org/abs/2207.03119](http://arxiv.org/abs/2207.03119)
> ABSTRACT  :  Time series are ubiquitous and therefore inherently hard to analyze and ultimately to label or cluster. With the rise of the Internet of Things (IoT) and its smart devices, data is collected in large amounts any given second. The collected data is rich in information, as one can detect accidents (e.g. cars) in **real time**, or assess injury/sickness over a given time span (e.g. health devices). Due to its chaotic nature and massive amounts of datapoints, timeseries are hard to label manually. Furthermore new classes within the data could emerge over time (contrary to e.g. handwritten digits), which would require relabeling the data. In this paper we present SuSL4TS, a deep generative Gaussian mixture model for semi-unsupervised learning, to classify time series data. With our approach we can alleviate manual labeling steps, since we can detect sparsely labeled classes (semi-supervised) and identify emerging classes hidden in the data (unsupervised). We demonstrate the efficacy of our approach with established time series classification datasets from different domains.  
## cs.AI
---
### How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v2 [cs.CV] UPDATED)
- Authors : Tobias Fischer, Michael Milford
- Link : [http://arxiv.org/abs/2206.13673](http://arxiv.org/abs/2206.13673)
> ABSTRACT  :  Event cameras continue to attract interest due to desirable characteristics such as **high dynamic range**, low latency, virtually no motion blur, and high energy efficiency. One of the potential applications that would benefit from these characteristics lies in visual place recognition for robot localization, i.e. matching a query observation to the corresponding reference place in the database. In this letter, we explore the distinctiveness of event streams from a small subset of pixels (in the tens or hundreds). We demonstrate that the absolute difference in the number of events at those pixel locations accumulated into event frames can be sufficient for the place recognition task, when pixels that display large variations in the reference set are used. Using such sparse (over image coordinates) but varying (variance over the number of events per pixel location) pixels enables frequent and computationally cheap updates of the location estimates. Furthermore, when event frames contain a constant number of events, our method takes full advantage of the event-driven nature of the sensory stream and displays promising robustness to changes in velocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset in an outdoor driving scenario, as well as the newly contributed indoor QCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a mobile robotic platform. Our results show that our approach achieves competitive performance when compared to several baseline methods on those datasets, and is particularly well suited for compute- and energy-constrained platforms such as interplanetary rovers.  
# Paper List
---
## cs.CV
---
**85** new papers in cs.CV:-) 
1. Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers. (arXiv:2207.03489v1 [cs.CV])
2. False Negative Reduction in Semantic Segmentation under Domain Shift using Depth Estimation. (arXiv:2207.03513v1 [cs.CV])
3. Should All Proposals be Treated Equally in Object Detection?. (arXiv:2207.03520v1 [cs.CV])
4. RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments. (arXiv:2207.03539v1 [cs.CV])
5. Highlight Specular Reflection Separation based on Tensor Low-rank and Sparse Decomposition Using Polarimetric Cues. (arXiv:2207.03543v1 [cs.CV])
6. An Embedding-Dynamic Approach to Self-supervised Learning. (arXiv:2207.03552v1 [cs.CV])
7. Mirror Complementary Transformer Network for RGB-thermal Salient Object Detection. (arXiv:2207.03558v1 [cs.CV])
8. The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning. (arXiv:2207.03568v1 [eess.IV])
9. Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v1 [cs.CR])
10. GaitTAKE: Gait Recognition by Temporal Attention \\and Keypoint-guided Embedding. (arXiv:2207.03608v1 [cs.CV])
11. PoseGU: 3D Human Pose Estimation with Novel Human Pose Generator and Unbiased Learning. (arXiv:2207.03618v1 [cs.CV])
12. More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity. (arXiv:2207.03620v1 [cs.CV])
13. A Support Vector Model of Pruning Trees Evaluation Based on OTSU Algorithm. (arXiv:2207.03638v1 [cs.CV])
14. Pruning Early Exit Networks. (arXiv:2207.03644v1 [cs.LG])
15. Abs-CAM: A Gradient Optimization Interpretable Approach for Explanation of Convolutional Neural Networks. (arXiv:2207.03648v1 [cs.CV])
16. Video Dialog as Conversation about Objects Living in Space-Time. (arXiv:2207.03656v1 [cs.CV])
17. Deepfake Face Traceability with Disentangling Reversing Network. (arXiv:2207.03666v1 [cs.CV])
18. Learning High-quality Proposals for Acne Detection. (arXiv:2207.03674v1 [cs.CV])
19. SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning. (arXiv:2207.03677v1 [cs.CV])
20. Music-driven Dance Regeneration with Controllable Key Pose Constraints. (arXiv:2207.03682v1 [cs.CV])
21. Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization. (arXiv:2207.03684v1 [cs.CV])
22. Neural Implicit Dictionary via Mixture-of-Expert Training. (arXiv:2207.03691v1 [cs.CV])
23. Mining Discriminative Food Regions for Accurate Food Recognition. (arXiv:2207.03692v1 [cs.CV])
24. SST-Calib: Simultaneous Spatial-Temporal Parameter Calibration between LIDAR and Camera. (arXiv:2207.03704v1 [cs.CV])
25. Video-based Smoky Vehicle Detection with A Coarse-to-Fine Framework. (arXiv:2207.03708v1 [cs.CV])
26. Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation. (arXiv:2207.03714v1 [cs.CV])
27. Bounding Box Disparity: 3D Metrics for Object Detection With Full Degree of Freedom. (arXiv:2207.03720v1 [cs.CV])
28. Exploring the Effectiveness of Video Perceptual Representation in Blind Video Quality Assessment. (arXiv:2207.03723v1 [cs.CV])
29. TGRMPT: A Head-Shoulder Aided Multi-Person Tracker and a New Large-Scale Dataset for Tour-Guide Robot. (arXiv:2207.03726v1 [cs.CV])
30. GEMS: Scene Expansion using Generative Models of Graphs. (arXiv:2207.03729v1 [cs.CV])
31. Combining Deep Learning with Good Old-Fashioned Machine Learning. (arXiv:2207.03757v1 [cs.LG])
32. Virtual Axle Detector based on Analysis of Bridge Acceleration Measurements by Fully Convolutional Network. (arXiv:2207.03758v1 [cs.CV])
33. Towards Intrinsic Common Discriminative Features Learning for Face Forgery Detection using Adversarial Learning. (arXiv:2207.03776v1 [cs.CV])
34. VidConv: A modernized 2D ConvNet for Efficient Video Recognition. (arXiv:2207.03782v1 [cs.CV])
35. Continuous Target-free Extrinsic Calibration of a Multi-Sensor System from a Sequence of Static Viewpoints. (arXiv:2207.03785v1 [cs.RO])
36. Complementing Brightness Constancy with Deep Networks for Optical Flow Prediction. (arXiv:2207.03790v1 [cs.CV])
37. FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v1 [cs.SD])
38. Beyond Transfer Learning: Co-finetuning for Action Localisation. (arXiv:2207.03807v1 [cs.CV])
39. Boosting Zero-shot Learning via Contrastive Optimization of Attribute Representations. (arXiv:2207.03824v1 [cs.CV])
40. Continuous Methods : Hamiltonian Domain Translation. (arXiv:2207.03843v1 [cs.CV])
41. Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain. (arXiv:2207.03860v1 [cs.CV])
42. Pixel-level Correspondence for Self-Supervised Learning from Video. (arXiv:2207.03866v1 [cs.CV])
43. Learning Sequential Descriptors for Sequence-based Visual Place Recognition. (arXiv:2207.03868v1 [cs.CV])
44. BlindSpotNet: Seeing Where We Cannot See. (arXiv:2207.03870v1 [cs.CV])
45. The Power of Transfer Learning in Agricultural Applications: AgriNet. (arXiv:2207.03881v1 [cs.CV])
46. Generative Adversarial Networks and Other Generative Models. (arXiv:2207.03887v1 [cs.CV])
47. Defense Against Multi-target Trojan Attacks. (arXiv:2207.03895v1 [cs.CV])
48. Big Learning: A Universal Machine Learning Paradigm?. (arXiv:2207.03899v1 [cs.LG])
49. Reproducing sensory induced hallucinations via neural fields. (arXiv:2207.03901v1 [q-bio.NC])
50. A Mask Attention Interaction and Scale **Enhancement** Network for SAR Ship Instance Segmentation. (arXiv:2207.03912v1 [cs.CV])
51. RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection. (arXiv:2207.03917v1 [cs.CV])
52. Detection of Furigana Text in Images. (arXiv:2207.03960v1 [cs.CV])
53. CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination. (arXiv:2207.03961v1 [cs.CL])
54. Event Collapse in Contrast Maximization Frameworks. (arXiv:2207.04007v1 [cs.CV])
55. CoCAtt: A Cognitive-Conditioned Driver Attention Dataset (Supplementary Material). (arXiv:2207.04028v1 [cs.CV])
56. k-means Mask Transformer. (arXiv:2207.04044v1 [cs.CV])
57. Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification. (arXiv:2101.10667v2 [eess.IV] UPDATED)
58. Surface Topography Characterization Using a Simple Optical Device and Artificial Neural Networks. (arXiv:2103.08482v4 [cs.CV] UPDATED)
59. Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v3 [cs.CV] UPDATED)
60. Greedy Bayesian Posterior Approximation with Deep Ensembles. (arXiv:2105.14275v4 [cs.LG] UPDATED)
61. LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction. (arXiv:2106.12102v2 [cs.CV] UPDATED)
62. Image scaling by de la Vall\'ee-Poussin filtered interpolation. (arXiv:2109.13897v2 [cs.CV] UPDATED)
63. Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v3 [cs.LG] UPDATED)
64. Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions. (arXiv:2111.10337v2 [cs.CV] UPDATED)
65. diffConv: Analyzing Irregular Point Clouds with an Irregular View. (arXiv:2111.14658v2 [cs.CV] UPDATED)
66. CloudWalker: Random walks for 3D point cloud shape analysis. (arXiv:2112.01050v3 [cs.CV] UPDATED)
67. Fully Attentional Network for Semantic Segmentation. (arXiv:2112.04108v3 [cs.CV] UPDATED)
68. Enhancing **Low-Light** Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v2 [eess.IV] UPDATED)
69. Visual Attention Network. (arXiv:2202.09741v4 [cs.CV] UPDATED)
70. Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v4 [cs.CR] UPDATED)
71. Semi-supervised classification of medical ultrasound images based on generative adversarial network. (arXiv:2203.06184v2 [eess.IV] UPDATED)
72. Sparsity and Heterogeneous Dropout for Continual Learning in the Null Space of Neural Activations. (arXiv:2203.06514v2 [cs.LG] UPDATED)
73. Unbiased Directed Object Attention Graph for Object Navigation. (arXiv:2204.04421v2 [cs.CV] UPDATED)
74. Satellite-based high-resolution maps of cocoa for C\^ote d'Ivoire and Ghana. (arXiv:2206.06119v2 [cs.CV] UPDATED)
75. How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v2 [cs.CV] UPDATED)
76. Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis. (arXiv:2206.15328v2 [cs.CV] UPDATED)
77. COVID-19 Detection Using Transfer Learning Approach from Computed Tomography Images. (arXiv:2207.00259v3 [eess.IV] UPDATED)
78. Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval. (arXiv:2207.00733v2 [cs.CV] UPDATED)
79. Game State Learning via Game Scene Augmentation. (arXiv:2207.01289v2 [cs.CV] UPDATED)
80. Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans. (arXiv:2207.01579v2 [eess.IV] UPDATED)
81. GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning. (arXiv:2207.01798v2 [cs.CV] UPDATED)
82. OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers. (arXiv:2207.02255v2 [cs.CV] UPDATED)
83. TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v2 [eess.IV] UPDATED)
84. Uncertainty of Atmospheric Motion Vectors by Sampling Tempered Posterior Distributions. (arXiv:2207.03182v2 [stat.ME] UPDATED)
85. NeuralGrasps: Learning Implicit Representations for Grasps of Multiple Robotic Hands. (arXiv:2207.02959v1 [cs.RO] CROSS LISTED)
## eess.IV
---
**15** new papers in eess.IV:-) 
1. Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers. (arXiv:2207.03489v1 [cs.CV])
2. The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning. (arXiv:2207.03568v1 [eess.IV])
3. Exploring the Effectiveness of Video Perceptual Representation in Blind Video Quality Assessment. (arXiv:2207.03723v1 [cs.CV])
4. Spatio-temporal prediction in video coding by spatially refined motion compensation. (arXiv:2207.03766v1 [eess.IV])
5. Content-Adaptive Motion Compensated Frequency Selective Extrapolation for Error Concealment in Video Communication. (arXiv:2207.03770v1 [eess.IV])
6. Spatio-temporal error concealment in video by denoised temporal extrapolation refinement. (arXiv:2207.03774v1 [eess.IV])
7. Generative Adversarial Networks and Other Generative Models. (arXiv:2207.03887v1 [cs.CV])
8. Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification. (arXiv:2101.10667v2 [eess.IV] UPDATED)
9. Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v3 [cs.CV] UPDATED)
10. Enhancing **Low-Light** Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v2 [eess.IV] UPDATED)
11. Semi-supervised classification of medical ultrasound images based on generative adversarial network. (arXiv:2203.06184v2 [eess.IV] UPDATED)
12. Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis. (arXiv:2206.15328v2 [cs.CV] UPDATED)
13. COVID-19 Detection Using Transfer Learning Approach from Computed Tomography Images. (arXiv:2207.00259v3 [eess.IV] UPDATED)
14. Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans. (arXiv:2207.01579v2 [eess.IV] UPDATED)
15. TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v2 [eess.IV] UPDATED)
## cs.LG
---
**140** new papers in cs.LG:-) 
1. AVDDPG: Federated reinforcement learning applied to autonomous platoon control. (arXiv:2207.03484v1 [cs.LG])
2. On Non-Linear operators for Geometric Deep Learning. (arXiv:2207.03485v1 [cs.LG])
3. Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers. (arXiv:2207.03489v1 [cs.CV])
4. Recent Results of Energy Disaggregation with Behind-the-Meter Solar Generation. (arXiv:2207.03490v1 [cs.LG])
5. HierarchicalForecast: A Python Benchmarking Framework for Hierarchical Forecasting. (arXiv:2207.03517v1 [stat.ML])
6. TF-GNN: Graph Neural Networks in TensorFlow. (arXiv:2207.03522v1 [cs.LG])
7. A Novel IoT-based Framework for Non-Invasive Human Hygiene Monitoring using Machine Learning Techniques. (arXiv:2207.03529v1 [cs.LG])
8. VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning. (arXiv:2207.03530v1 [cs.RO])
9. Deep Learning to Jointly Schema Match, Impute, and Transform Databases. (arXiv:2207.03536v1 [cs.DB])
10. An Embedding-Dynamic Approach to Self-supervised Learning. (arXiv:2207.03552v1 [cs.CV])
11. G2L: A Geometric Approach for Generating Pseudo-labels that Improve Transfer Learning. (arXiv:2207.03554v1 [cs.LG])
12. The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning. (arXiv:2207.03568v1 [eess.IV])
13. A Study on the Predictability of Sample Learning Consistency. (arXiv:2207.03571v1 [cs.LG])
14. Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v1 [cs.CR])
15. Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems. (arXiv:2207.03576v1 [cs.CR])
16. Automatic Synthesis of Neurons for Recurrent Neural Nets. (arXiv:2207.03577v1 [cs.NE])
17. Code Translation with Compiler Representations. (arXiv:2207.03578v1 [cs.PL])
18. Dynamic Community Detection via Adversarial Temporal Graph Representation Learning. (arXiv:2207.03580v1 [cs.SI])
19. Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling. (arXiv:2207.03584v1 [cs.LG])
20. CausalAgents: A Robustness Benchmark for Motion Forecasting using Causal Relationships. (arXiv:2207.03586v1 [cs.LG])
21. Hyper-Universal Policy Approximation: Learning to Generate Actions from a Single Image using Hypernets. (arXiv:2207.03593v1 [cs.LG])
22. Individual Preference Stability for Clustering. (arXiv:2207.03600v1 [cs.LG])
23. Learning-based Autonomous Channel Access in the Presence of Hidden Terminals. (arXiv:2207.03605v1 [cs.LG])
24. One for All: Simultaneous Metric and Preference Learning over Multiple Users. (arXiv:2207.03609v1 [stat.ML])
25. Learning and generalization of one-hidden-layer neural networks, going beyond standard Gaussian data. (arXiv:2207.03615v1 [cs.LG])
26. PoseGU: 3D Human Pose Estimation with Novel Human Pose Generator and Unbiased Learning. (arXiv:2207.03618v1 [cs.CV])
27. Information-Gathering in Latent Bandits. (arXiv:2207.03635v1 [cs.LG])
28. A Support Vector Model of Pruning Trees Evaluation Based on OTSU Algorithm. (arXiv:2207.03638v1 [cs.CV])
29. Nonparametric Embeddings of Sparse High-Order Interaction Events. (arXiv:2207.03639v1 [cs.LG])
30. Pruning Early Exit Networks. (arXiv:2207.03644v1 [cs.LG])
31. Abs-CAM: A Gradient Optimization Interpretable Approach for Explanation of Convolutional Neural Networks. (arXiv:2207.03648v1 [cs.CV])
32. Balanced Self-Paced Learning for AUC Maximization. (arXiv:2207.03650v1 [cs.LG])
33. Private independence testing across two parties. (arXiv:2207.03652v1 [math.ST])
34. Video Dialog as Conversation about Objects Living in Space-Time. (arXiv:2207.03656v1 [cs.CV])
35. SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning. (arXiv:2207.03677v1 [cs.CV])
36. Stability of Aggregation Graph Neural Networks. (arXiv:2207.03678v1 [cs.LG])
37. Getting BART to Ride the Idiomatic Train: Learning to Represent Idiomatic Expressions. (arXiv:2207.03679v1 [cs.CL])
38. A Survey on Participant Selection for Federated Learning in Mobile Networks. (arXiv:2207.03681v1 [cs.DC])
39. Predicting Li-ion Battery Cycle Life with LSTM RNN. (arXiv:2207.03687v1 [cs.LG])
40. GCN-based Multi-task Representation Learning for Anomaly Detection in Attributed Networks. (arXiv:2207.03688v1 [cs.LG])
41. Guiding the retraining of convolutional neural networks against adversarial inputs. (arXiv:2207.03689v1 [cs.SE])
42. End-to-End Binaural Speech Synthesis. (arXiv:2207.03697v1 [cs.SD])
43. Tightening Discretization-based MILP Models for the Pooling Problem using Upper Bounds on Bilinear Terms. (arXiv:2207.03699v1 [math.OC])
44. Convolutional Neural Networks for Time-dependent Classification of Variable-length Time Series. (arXiv:2207.03718v1 [cs.LG])
45. Tackling Data Heterogeneity: A New Unified Framework for Decentralized SGD with Sample-induced Topology. (arXiv:2207.03730v1 [math.OC])
46. Combining Deep Learning with Good Old-Fashioned Machine Learning. (arXiv:2207.03757v1 [cs.LG])
47. A Non-isotropic Probabilistic Take on Proxy-based Deep Metric Learning. (arXiv:2207.03784v1 [cs.LG])
48. Product Segmentation Newsvendor Problems: A Robust Learning Approach. (arXiv:2207.03801v1 [cs.LG])
49. On the Subspace Structure of Gradient-Based Meta-Learning. (arXiv:2207.03804v1 [cs.LG])
50. UDRN: Unified Dimensional Reduction Neural Network for Feature Selection and Feature Projection. (arXiv:2207.03809v1 [cs.LG])
51. Deep Learning for Anomaly Detection in Log Data: A Survey. (arXiv:2207.03820v1 [cs.LG])
52. Safe reinforcement learning for multi-energy management systems with known constraint functions. (arXiv:2207.03830v1 [eess.SY])
53. Storehouse: a Reinforcement Learning Environment for Optimizing Warehouse Management. (arXiv:2207.03851v1 [cs.LG])
54. Variational Inference of overparameterized Bayesian Neural Networks: a theoretical and empirical study. (arXiv:2207.03859v1 [stat.ML])
55. Constrained Training of Neural Networks via Theorem Proving. (arXiv:2207.03880v1 [cs.AI])
56. The Power of Transfer Learning in Agricultural Applications: AgriNet. (arXiv:2207.03881v1 [cs.CV])
57. NExG: Provable and Guided State Space Exploration of Neural Network Control Systems using Sensitivity Approximation. (arXiv:2207.03884v1 [eess.SY])
58. Generative Adversarial Networks and Other Generative Models. (arXiv:2207.03887v1 [cs.CV])
59. Encoding NetFlows for State-Machine Learning. (arXiv:2207.03890v1 [cs.LG])
60. Big Learning: A Universal Machine Learning Paradigm?. (arXiv:2207.03899v1 [cs.LG])
61. Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning. (arXiv:2207.03902v1 [cs.LG])
62. Ensemble random forest filter: An alternative to the ensemble Kalman filter for inverse modeling. (arXiv:2207.03909v1 [cs.LG])
63. Towards Semantic Communication Protocols: A Probabilistic Logic Perspective. (arXiv:2207.03920v1 [cs.IT])
64. BAST: Binaural Audio Spectrogram Transformer for Binaural Sound Localization. (arXiv:2207.03927v1 [cs.SD])
65. GT4SD: Generative Toolkit for Scientific Discovery. (arXiv:2207.03928v1 [cs.LG])
66. Memory-free Online Change-point Detection: A Novel Neural Network Approach. (arXiv:2207.03932v1 [cs.LG])
67. A law of adversarial risk, interpolation, and label noise. (arXiv:2207.03933v1 [stat.ML])
68. Active Learning-based Isolation Forest (ALIF): Enhancing Anomaly Detection in Decision Support Systems. (arXiv:2207.03934v1 [cs.LG])
69. ControlBurn: Nonlinear Feature Selection with Sparse Tree Ensembles. (arXiv:2207.03935v1 [stat.ML])
70. High Performance Simulation for Scalable Multi-Agent Reinforcement Learning. (arXiv:2207.03945v1 [cs.MA])
71. Learning with Muscles: Benefits for Data-Efficiency and Robustness in Anthropomorphic Tasks. (arXiv:2207.03952v1 [cs.RO])
72. Black and Gray Box Learning of Amplitude Equations: Application to Phase Field Systems. (arXiv:2207.03954v1 [stat.ML])
73. Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with Inexact Prox. (arXiv:2207.03957v1 [cs.LG])
74. Generalization-Memorization Machines. (arXiv:2207.03976v1 [cs.LG])
75. Predicting Opinion Dynamics via Sociologically-Informed Neural Networks. (arXiv:2207.03990v1 [cs.SI])
76. On Improving the Performance of Glitch Classification for Gravitational Wave Detection by using Generative Adversarial Networks. (arXiv:2207.04001v1 [astro-ph.HE])
77. MACFE: A Meta-learning and Causality Based Feature Engineering Framework. (arXiv:2207.04010v1 [cs.LG])
78. Implicit Bias of Gradient Descent on Reparametrized Models: On Equivalence to Mirror Descent. (arXiv:2207.04036v1 [cs.LG])
79. The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications. (arXiv:2207.04043v1 [cs.CL])
80. ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features. (arXiv:1908.08381v3 [cs.HC] UPDATED)
81. Bayesian Quantile and Expectile Optimisation. (arXiv:2001.04833v2 [stat.ML] UPDATED)
82. Feature Selection Methods for Uplift Modeling and Heterogeneous Treatment Effect. (arXiv:2005.03447v2 [cs.LG] UPDATED)
83. On the representation and learning of monotone triangular transport maps. (arXiv:2009.10303v2 [stat.ML] UPDATED)
84. Interlocking Backpropagation: Improving depthwise model-parallelism. (arXiv:2010.04116v3 [cs.LG] UPDATED)
85. Distributed Saddle-Point Problems: Lower Bounds, Near-Optimal and Robust Algorithms. (arXiv:2010.13112v8 [cs.LG] UPDATED)
86. Neighbors From Hell: Voltage Attacks Against Deep Learning Accelerators on Multi-Tenant FPGAs. (arXiv:2012.07242v2 [cs.CR] UPDATED)
87. BF++: a language for general-purpose program synthesis. (arXiv:2101.09571v6 [cs.AI] UPDATED)
88. Approximately Solving Mean Field Games via Entropy-Regularized Deep Reinforcement Learning. (arXiv:2102.01585v2 [cs.MA] UPDATED)
89. Online Learning in Budget-Constrained Dynamic Colonel Blotto Games. (arXiv:2103.12833v3 [cs.LG] UPDATED)
90. Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v3 [cs.CV] UPDATED)
91. Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations. (arXiv:2104.13907v3 [cs.RO] UPDATED)
92. Greedy Bayesian Posterior Approximation with Deep Ensembles. (arXiv:2105.14275v4 [cs.LG] UPDATED)
93. Fair Exploration via Axiomatic Bargaining. (arXiv:2106.02553v2 [cs.LG] UPDATED)
94. Test Sample Accuracy Scales with Training Sample Density in Neural Networks. (arXiv:2106.08365v6 [cs.LG] UPDATED)
95. DeepSplit: Scalable Verification of Deep Neural Networks via Operator Splitting. (arXiv:2106.09117v3 [cs.LG] UPDATED)
96. Combining Machine Learning and Effective Feature Selection for **Real-time** Stock Trading in Variable Time-frames. (arXiv:2107.13148v2 [q-fin.TR] UPDATED)
97. Layer Adaptive Node Selection in Bayesian Neural Networks: Statistical Guarantees and Implementation Details. (arXiv:2108.11000v2 [stat.ML] UPDATED)
98. Supervising the Decoder of Variational Autoencoders to Improve Scientific Utility. (arXiv:2109.04561v3 [stat.ML] UPDATED)
99. An AO-ADMM approach to constraining PARAFAC2 on all modes. (arXiv:2110.01278v3 [cs.LG] UPDATED)
100. A methodology for training homomorphicencryption friendly neural networks. (arXiv:2111.03362v3 [cs.CR] UPDATED)
101. TEA: A Sequential Recommendation Framework via Temporally Evolving Aggregations. (arXiv:2111.07378v2 [cs.IR] UPDATED)
102. Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v3 [cs.LG] UPDATED)
103. Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning. (arXiv:2112.08932v2 [cs.LG] UPDATED)
104. High Throughput Multi-Channel Parallelized Diffraction Convolutional Neural Network Accelerator. (arXiv:2112.12297v2 [cs.LG] UPDATED)
105. Standard Vs Uniform Binary Search and Their Variants in Learned Static Indexing: The Case of the Searching on Sorted Data Benchmarking Software Platform. (arXiv:2201.01554v2 [cs.DS] UPDATED)
106. Invariant Ancestry Search. (arXiv:2202.00913v2 [stat.ME] UPDATED)
107. Multi-Task Learning as a Bargaining Game. (arXiv:2202.01017v2 [cs.LG] UPDATED)
108. How to Leverage Unlabeled Data in Offline Reinforcement Learning. (arXiv:2202.01741v4 [cs.LG] UPDATED)
109. The Importance of Non-Markovianity in Maximum State Entropy Exploration. (arXiv:2202.03060v2 [cs.LG] UPDATED)
110. Evaluating Causal Inference Methods. (arXiv:2202.04208v3 [stat.ME] UPDATED)
111. Optimal sizing of a holdout set for safe predictive model updating. (arXiv:2202.06374v3 [stat.ML] UPDATED)
112. Architecture Agnostic Federated Learning for Neural Networks. (arXiv:2202.07757v3 [cs.LG] UPDATED)
113. Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations. (arXiv:2202.10638v2 [stat.ML] UPDATED)
114. Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v4 [cs.CR] UPDATED)
115. Neuro-Inspired Deep Neural Networks with Sparse, Strong Activations. (arXiv:2202.13074v3 [cs.NE] UPDATED)
116. GraphWorld: Fake Graphs Bring Real Insights for GNNs. (arXiv:2203.00112v2 [cs.LG] UPDATED)
117. Sparsity and Heterogeneous Dropout for Continual Learning in the Null Space of Neural Activations. (arXiv:2203.06514v2 [cs.LG] UPDATED)
118. Your Policy Regularizer is Secretly an Adversary. (arXiv:2203.12592v4 [cs.LG] UPDATED)
119. Rich Feature Construction for the Optimization-Generalization Dilemma. (arXiv:2203.15516v2 [cs.LG] UPDATED)
120. Understanding Gradual Domain Adaptation: Improved Analysis, Optimal Path and Beyond. (arXiv:2204.08200v2 [cs.LG] UPDATED)
121. Investigating Generalization by Controlling Normalized Margin. (arXiv:2205.03940v2 [cs.LG] UPDATED)
122. A Structured Span Selector. (arXiv:2205.03977v2 [cs.CL] UPDATED)
123. Risk aversion in learning algorithms and recommendation systems. (arXiv:2205.04619v2 [cs.LG] UPDATED)
124. Data-driven Numerical Invariant Synthesis with Automatic Generation of Attributes. (arXiv:2205.14943v3 [cs.PL] UPDATED)
125. Flexible Group Fairness Metrics for Survival Analysis. (arXiv:2206.03256v2 [cs.CY] UPDATED)
126. Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v8 [cs.RO] UPDATED)
127. Automated Cancer Subtyping via Vector Quantization Mutual Information Maximization. (arXiv:2206.10801v2 [cs.LG] UPDATED)
128. Play It Cool: Dynamic Shifting Prevents Thermal Throttling. (arXiv:2206.10849v2 [cs.LG] UPDATED)
129. Bayesian Optimization Over Iterative Learners with Structured Responses: A Budget-aware Planning Approach. (arXiv:2206.12708v2 [cs.LG] UPDATED)
130. ECG Heartbeat classification using deep transfer learning with Convolutional Neural Network and STFT technique. (arXiv:2206.14200v3 [cs.LG] UPDATED)
131. Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis. (arXiv:2206.15328v2 [cs.CV] UPDATED)
132. Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans. (arXiv:2207.01579v2 [eess.IV] UPDATED)
133. GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning. (arXiv:2207.01798v2 [cs.CV] UPDATED)
134. Data-driven synchronization-avoiding algorithms in the explicit distributed structural analysis of soft tissue. (arXiv:2207.02194v2 [cs.DC] UPDATED)
135. TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v2 [eess.IV] UPDATED)
136. Many-body localized hidden Born machine. (arXiv:2207.02346v2 [quant-ph] UPDATED)
137. A domain-specific language for describing machine learning datasets. (arXiv:2207.02848v2 [cs.LG] UPDATED)
138. Multi-Label Learning to Rank through Multi-Objective Optimization. (arXiv:2207.03060v2 [cs.IR] UPDATED)
139. Semi-unsupervised Learning for Time Series Classification. (arXiv:2207.03119v2 [cs.LG] UPDATED)
140. NeuralGrasps: Learning Implicit Representations for Grasps of Multiple Robotic Hands. (arXiv:2207.02959v1 [cs.RO] CROSS LISTED)
## cs.AI
---
**61** new papers in cs.AI:-) 
1. AVDDPG: Federated reinforcement learning applied to autonomous platoon control. (arXiv:2207.03484v1 [cs.LG])
2. On Non-Linear operators for Geometric Deep Learning. (arXiv:2207.03485v1 [cs.LG])
3. HierarchicalForecast: A Python Benchmarking Framework for Hierarchical Forecasting. (arXiv:2207.03517v1 [stat.ML])
4. G2L: A Geometric Approach for Generating Pseudo-labels that Improve Transfer Learning. (arXiv:2207.03554v1 [cs.LG])
5. Finite-rate sparse quantum codes aplenty. (arXiv:2207.03562v1 [quant-ph])
6. A Study on the Predictability of Sample Learning Consistency. (arXiv:2207.03571v1 [cs.LG])
7. The ACII 2022 Affective Vocal Bursts Workshop & Competition: Understanding a critically understudied modality of emotional expression. (arXiv:2207.03572v1 [eess.AS])
8. Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v1 [cs.CR])
9. Online Trajectory Prediction for Metropolitan Scale Mobility Digital Twin. (arXiv:2207.03575v1 [cs.SI])
10. Automatic Synthesis of Neurons for Recurrent Neural Nets. (arXiv:2207.03577v1 [cs.NE])
11. DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection. (arXiv:2207.03579v1 [cs.SI])
12. Dynamic Community Detection via Adversarial Temporal Graph Representation Learning. (arXiv:2207.03580v1 [cs.SI])
13. CausalAgents: A Robustness Benchmark for Motion Forecasting using Causal Relationships. (arXiv:2207.03586v1 [cs.LG])
14. One for All: Simultaneous Metric and Preference Learning over Multiple Users. (arXiv:2207.03609v1 [stat.ML])
15. SETSum: Summarization and Visualization of Student Evaluations of Teaching. (arXiv:2207.03640v1 [cs.CL])
16. Abs-CAM: A Gradient Optimization Interpretable Approach for Explanation of Convolutional Neural Networks. (arXiv:2207.03648v1 [cs.CV])
17. Balanced Self-Paced Learning for AUC Maximization. (arXiv:2207.03650v1 [cs.LG])
18. Partition refinement for emulation. (arXiv:2207.03669v1 [cs.AI])
19. Guiding the retraining of convolutional neural networks against adversarial inputs. (arXiv:2207.03689v1 [cs.SE])
20. End-to-End Binaural Speech Synthesis. (arXiv:2207.03697v1 [cs.SD])
21. Healthcare Knowledge Graph Construction: State-of-the-art, open issues, and opportunities. (arXiv:2207.03771v1 [cs.AI])
22. Hidden Schema Networks. (arXiv:2207.03777v1 [cs.CL])
23. Safe reinforcement learning for multi-energy management systems with known constraint functions. (arXiv:2207.03830v1 [eess.SY])
24. Storehouse: a Reinforcement Learning Environment for Optimizing Warehouse Management. (arXiv:2207.03851v1 [cs.LG])
25. Reinforced Lin-Kernighan-Helsgaun Algorithms for the Traveling Salesman Problems. (arXiv:2207.03876v1 [cs.AI])
26. Constrained Training of Neural Networks via Theorem Proving. (arXiv:2207.03880v1 [cs.AI])
27. Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning. (arXiv:2207.03902v1 [cs.LG])
28. Towards Semantic Communication Protocols: A Probabilistic Logic Perspective. (arXiv:2207.03920v1 [cs.IT])
29. GT4SD: Generative Toolkit for Scientific Discovery. (arXiv:2207.03928v1 [cs.LG])
30. Memory-free Online Change-point Detection: A Novel Neural Network Approach. (arXiv:2207.03932v1 [cs.LG])
31. Active Learning-based Isolation Forest (ALIF): Enhancing Anomaly Detection in Decision Support Systems. (arXiv:2207.03934v1 [cs.LG])
32. CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination. (arXiv:2207.03961v1 [cs.CL])
33. Dreamento: An open-source dream engineering toolbox utilizing sleep wearable. (arXiv:2207.03977v1 [cs.HC])
34. Optimization of Temperature and Relative Humidity in an Automatic Egg Incubator Using Mamdani Interference System. (arXiv:2207.03996v1 [eess.SY])
35. MACFE: A Meta-learning and Causality Based Feature Engineering Framework. (arXiv:2207.04010v1 [cs.LG])
36. A Multi-tasking Model of Speaker-Keyword Classification for Keeping Human in the Loop of Drone-assisted Inspection. (arXiv:2207.04027v1 [cs.SD])
37. CoCAtt: A Cognitive-Conditioned Driver Attention Dataset (Supplementary Material). (arXiv:2207.04028v1 [cs.CV])
38. Lessons from Deep Learning applied to Scholarly Information Extraction: What Works, What Doesn't, and Future Directions. (arXiv:2207.04029v1 [cs.IR])
39. Interlocking Backpropagation: Improving depthwise model-parallelism. (arXiv:2010.04116v3 [cs.LG] UPDATED)
40. BF++: a language for general-purpose program synthesis. (arXiv:2101.09571v6 [cs.AI] UPDATED)
41. Approximately Solving Mean Field Games via Entropy-Regularized Deep Reinforcement Learning. (arXiv:2102.01585v2 [cs.MA] UPDATED)
42. Test Sample Accuracy Scales with Training Sample Density in Neural Networks. (arXiv:2106.08365v6 [cs.LG] UPDATED)
43. Reinforced Hybrid Genetic Algorithm for the Traveling Salesman Problem. (arXiv:2107.06870v3 [cs.NE] UPDATED)
44. Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework. (arXiv:2110.08423v2 [cs.AI] UPDATED)
45. Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning. (arXiv:2112.08932v2 [cs.LG] UPDATED)
46. How to Leverage Unlabeled Data in Offline Reinforcement Learning. (arXiv:2202.01741v4 [cs.LG] UPDATED)
47. Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v4 [cs.CR] UPDATED)
48. Sparsity and Heterogeneous Dropout for Continual Learning in the Null Space of Neural Activations. (arXiv:2203.06514v2 [cs.LG] UPDATED)
49. Rich Feature Construction for the Optimization-Generalization Dilemma. (arXiv:2203.15516v2 [cs.LG] UPDATED)
50. Unbiased Directed Object Attention Graph for Object Navigation. (arXiv:2204.04421v2 [cs.CV] UPDATED)
51. Risk aversion in learning algorithms and recommendation systems. (arXiv:2205.04619v2 [cs.LG] UPDATED)
52. Uncertainty-aware Personal Assistant for Making Personalized Privacy Decisions. (arXiv:2205.06544v3 [cs.AI] UPDATED)
53. Satellite-based high-resolution maps of cocoa for C\^ote d'Ivoire and Ghana. (arXiv:2206.06119v2 [cs.CV] UPDATED)
54. Automated Cancer Subtyping via Vector Quantization Mutual Information Maximization. (arXiv:2206.10801v2 [cs.LG] UPDATED)
55. Play It Cool: Dynamic Shifting Prevents Thermal Throttling. (arXiv:2206.10849v2 [cs.LG] UPDATED)
56. Bayesian Optimization Over Iterative Learners with Structured Responses: A Budget-aware Planning Approach. (arXiv:2206.12708v2 [cs.LG] UPDATED)
57. How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v2 [cs.CV] UPDATED)
58. Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis. (arXiv:2206.15328v2 [cs.CV] UPDATED)
59. Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval. (arXiv:2207.00733v2 [cs.CV] UPDATED)
60. A domain-specific language for describing machine learning datasets. (arXiv:2207.02848v2 [cs.LG] UPDATED)
61. NeuralGrasps: Learning Implicit Representations for Grasps of Multiple Robotic Hands. (arXiv:2207.02959v1 [cs.RO] CROSS LISTED)

