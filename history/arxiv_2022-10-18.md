# Your interest papers
---
## cs.CV
---
### Optimizing Vision Transformers for Medical Image Segmentation and Few-Shot Domain Adaptation. (arXiv:2210.08066v1 [cs.CV])
- Authors : Qianying Liu, Chaitanya Kaul, Christos Anagnostopoulos, Roderick Murray, Fani Deligianni
- Link : [http://arxiv.org/abs/2210.08066](http://arxiv.org/abs/2210.08066)
> ABSTRACT  :  The adaptation of transformers to computer vision is not straightforward because the modelling of image contextual information results in quadratic computational complexity with relation to the input features. Most of existing methods require extensive pre-training on massive datasets such as ImageNet and therefore their application to fields such as healthcare is less effective. CNNs are the dominant architecture in computer vision tasks because convolutional filters can effectively model local dependencies and reduce drastically the parameters required. However, convolutional filters cannot handle more complex interactions, which are beyond a small neighbour of pixels. Furthermore, their weights are fixed after training and thus they do not take into consideration changes in the visual input. Inspired by recent work on hybrid visual transformers with convolutions and hierarchical transformers, we propose Convolutional **Swin**-Unet (CS-Unet) transformer blocks and optimise their settings with relation to patch embedding, projection, the feed-forward network, up sampling and skip connections. CS-Unet can be trained from scratch and inherits the superiority of convolutions in each feature process phase. It helps to encode precise spatial information and produce hierarchical representations that contribute to object concepts at various scales. Experiments show that CS-Unet without pre-training surpasses other state-of-the-art counterparts by large margins on two medical CT and MRI datasets with fewer parameters. In addition, two domain-adaptation experiments on optic disc and polyp image segmentation further prove that our method is highly generalizable and effectively bridges the domain gap between images from different sources.  
### Learning Dual Memory Dictionaries for Blind Face **Restoration**. (arXiv:2210.08160v1 [cs.CV])
- Authors : Xiaoming Li, Shiguang Zhang, Shangchen Zhou, **Lei Zhang**, Wangmeng Zuo
- Link : [http://arxiv.org/abs/2210.08160](http://arxiv.org/abs/2210.08160)
> ABSTRACT  :  To improve the performance of blind face **restoration**, recent works mainly treat the two aspects, i.e., generic and specific **restoration**, separately. In particular, generic **restoration** attempts to restore the results through general facial structure prior, while on the one hand, cannot generalize to real-world degraded observations due to the limited capability of direct CNNs' mappings in learning blind **restoration**, and on the other hand, fails to exploit the identity-specific details. On the contrary, specific **restoration** aims to incorporate the identity features from the reference of the same identity, in which the requirement of proper reference severely limits the application scenarios. Generally, it is a challenging and intractable task to improve the photo-realistic performance of blind **restoration** and adaptively handle the generic and specific **restoration** scenarios with a single unified model. Instead of implicitly learning the mapping from a low-quality image to its high-quality counterpart, this paper suggests a DMDNet by explicitly memorizing the generic and specific features through dual dictionaries. First, the generic dictionary learns the general facial priors from high-quality images of any identity, while the specific dictionary stores the identity-belonging features for each person individually. Second, to handle the degraded input with or without specific reference, dictionary transform module is suggested to read the relevant details from the dual dictionaries which are subsequently fused into the input features. Finally, multi-scale dictionaries are leveraged to benefit the coarse-to-fine **restoration**. Moreover, a new high-quality dataset, termed CelebRef-HQ, is constructed to promote the exploration of specific face **restoration** in the high-resolution space.  
### Panchromatic and Multispectral Image Fusion via Alternating Reverse Filtering Network. (arXiv:2210.08181v1 [cs.CV])
- Authors : Keyu Yan, Man Zhou, Jie Huang, Feng Zhao, Chengjun Xie, **Chongyi Li**, Danfeng Hong
- Link : [http://arxiv.org/abs/2210.08181](http://arxiv.org/abs/2210.08181)
> ABSTRACT  :  Panchromatic (PAN) and multi-spectral (MS) image fusion, named Pan-sharpening, refers to super-resolve the low-resolution (LR) multi-spectral (MS) images in the spatial domain to generate the expected high-resolution (HR) MS images, conditioning on the corresponding high-resolution PAN images. In this paper, we present a simple yet effective \textit{alternating reverse filtering network} for pan-sharpening. Inspired by the classical reverse filtering that reverses images to the status before filtering, we formulate pan-sharpening as an alternately iterative reverse filtering process, which fuses LR MS and HR MS in an interpretable manner. Different from existing model-driven methods that require well-designed priors and degradation assumptions, the reverse filtering process avoids the dependency on pre-defined exact priors. To guarantee the stability and convergence of the iterative process via contraction mapping on a metric space, we develop the learnable multi-scale Gaussian kernel module, instead of using specific filters. We demonstrate the theoretical feasibility of such formulations. Extensive experiments on diverse scenes to thoroughly verify the performance of our method, significantly outperforming the state of the arts.  
### Hand Gestures Recognition in Videos Taken with Lensless Camera. (arXiv:2210.08233v1 [cs.CV])
- Authors : Yinger Zhang, Zhouyi Wu, Peiying Lin, Yang Pan, Yuting Wu, Liufang Zhang, Jiangtao Huangfu
- Link : [http://arxiv.org/abs/2210.08233](http://arxiv.org/abs/2210.08233)
> ABSTRACT  :  A lensless camera is an imaging system that uses a mask in place of a lens, making it thinner, lighter, and less expensive than a lensed camera. However, additional complex computation and time are required for image reconstruction. This work proposes a deep learning model named Raw3dNet that recognizes hand gestures directly on raw videos captured by a lensless camera without the need for image **restoration**. In addition to conserving computational resources, the reconstruction-free method provides privacy protection. Raw3dNet is a novel end-to-end deep neural network model for the recognition of hand gestures in lensless imaging systems. It is created specifically for raw video captured by a lensless camera and has the ability to properly extract and combine temporal and spatial features. The network is composed of two stages: 1. spatial feature extractor (SFE), which enhances the spatial features of each frame prior to temporal convolution; 2. 3D-ResNet, which implements spatial and temporal convolution of video streams. The proposed model achieves 98.59% accuracy on the Cambridge Hand Gesture dataset in the lensless optical experiment, which is comparable to the lensed-camera result. Additionally, the feasibility of physical object recognition is assessed. Furtherly, we show that the recognition can be achieved with respectable accuracy using only a tiny portion of the original raw data, indicating the potential for reducing data traffic in cloud computing scenarios.  
### Effects of Image Size on Deep Learning. (arXiv:2101.11508v6 [cs.CV] UPDATED)
- Authors : Olivier Rukundo
- Link : [http://arxiv.org/abs/2101.11508](http://arxiv.org/abs/2101.11508)
> ABSTRACT  :  In this paper the main objective is to determine the best size of late gadolinium **enhancement** (LGE)-magnetic resonance imaging (MRI) images for the training dataset to achieve optimal deep learning training outcomes. To determine the new size of LGE-MRI images from the reference training dataset, non-extra pixel and extra pixel interpolation algorithms are used. A novel strategy based on thresholding, median filtering, and subtraction operations is introduced and applied to remove extra class labels in interpolated ground truth (GT) segmentation masks. Fully automated quantification is achieved using the expectation maximization, weighted intensity, a priori information (EWA) algorithm, and the outcome of automatic semantic segmentation of LGE-MRI images with the convolutional neural network (CNN). In the experiments, common class metrics are used to evaluate the quality of semantic segmentation with a CNN architecture of interest (U-net) against the GT segmentation. Arbitrary threshold, comparison of the sums, and sums of differences are criteria or options used to estimate the relationship between semi-automatic and fully automated quantification of MI results. A close relationship between semi-automatic or manual and fully automated quantification of MI results was more identified in the case involving the dataset of bigger LGE MRI images than in that of the dataset of smaller LGE-MRI images where the best quantification results based on the dataset of bigger LGE MRI images were 55.5% closer the manual or semiautomatic results while the best quantification results based on the dataset of smaller LGE MRI images were 22.2% closer the manual results.  
### Real-world Video Deblurring: A Benchmark Dataset and An Efficient Recurrent Neural Network. (arXiv:2106.16028v2 [cs.CV] UPDATED)
- Authors : Zhihang Zhong, Ye Gao, Yinqiang Zheng, Bo Zheng, Imari Sato
- Link : [http://arxiv.org/abs/2106.16028](http://arxiv.org/abs/2106.16028)
> ABSTRACT  :  Real-world video deblurring in **real time** still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. Another issue that needs to be addressed urgently is the lack of a real-world benchmark dataset. Thus, we contribute a novel dataset (BSD) to the community, by collecting paired blurry/sharp video clips using a co-axis beam splitter acquisition system. Experimental results show that the proposed method (ESTRNN) can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods. In addition, cross-validation experiments between datasets illustrate the high generality of BSD over the synthetic datasets. The code and dataset are released at https://github.com/zzh-tech/ESTRNN.  
### Strong Lensing Source Reconstruction Using Continuous Neural Fields. (arXiv:2206.14820v2 [astro-ph.CO] UPDATED)
- Authors : Siddharth Mishra, Ge Yang
- Link : [http://arxiv.org/abs/2206.14820](http://arxiv.org/abs/2206.14820)
> ABSTRACT  :  From the nature of **dark** matter to the rate of expansion of our Universe, observations of distant galaxies distorted through strong gravitational lensing have the potential to answer some of the major open questions in astrophysics. Modeling galaxy-galaxy strong lensing observations presents a number of challenges as the exact configuration of both the background source and foreground lens galaxy is unknown. A timely call, prompted by a number of upcoming surveys anticipating high-resolution lensing images, demands methods that can efficiently model lenses at their full complexity. In this work, we introduce a method that uses continuous neural fields to non-parametrically reconstruct the complex morphology of a source galaxy while simultaneously inferring a distribution over foreground lens galaxy configurations. We demonstrate the efficacy of our method through experiments on simulated data targeting high-resolution lensing images similar to those anticipated in near-future astrophysical surveys.  
### Wide Range MRI Artifact Removal with Transformers. (arXiv:2210.07976v2 [eess.IV] UPDATED)
- Authors : Lennart Alexander, Van der, Kevin Smith
- Link : [http://arxiv.org/abs/2210.07976](http://arxiv.org/abs/2210.07976)
> ABSTRACT  :  Artifacts on magnetic resonance scans are a serious challenge for both radiologists and computer-aided diagnosis systems. Most commonly, artifacts are caused by motion of the patients, but can also arise from device-specific abnormalities such as noise patterns. Irrespective of the source, artifacts can not only render a scan useless, but can potentially induce misdiagnoses if left unnoticed. For instance, an artifact may masquerade as a tumor or other abnormality. Retrospective artifact correction (RAC) is concerned with removing artifacts after the scan has already been taken. In this work, we propose a method capable of retrospectively removing eight common artifacts found in native-resolution MR imagery. Knowledge of the presence or location of a specific artifact is not assumed and the system is, by design, capable of undoing interactions of multiple artifacts. Our method is realized through the design of a novel volumetric transformer-based neural network that generalizes a \emph{window-centered} approach popularized by the **Swin** transformer. Unlike **Swin**, our method is (i) natively volumetric, (ii) geared towards dense prediction tasks instead of classification, and (iii), uses a novel and more global mechanism to enable information exchange between windows. Our experiments show that our reconstructions are considerably better than those attained by ResNet, V-Net, MobileNet-v2, DenseNet, CycleGAN and BicycleGAN. Moreover, we show that the reconstructed images from our model improves the accuracy of FSL BET, a standard skull-stripping method typically applied in diagnostic workflows.  
### BOAT: **Bilateral** Local Attention Vision Transformer. (arXiv:2201.13027v1 [cs.CV] CROSS LISTED)
- Authors : Tan Yu, Gangming Zhao, Ping Li, Yizhou Yu
- Link : [http://arxiv.org/abs/2201.13027](http://arxiv.org/abs/2201.13027)
> ABSTRACT  :  Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a **Bilateral** lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both **Swin** and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.  
## eess.IV
---
### Reversing Image Signal Processors by Reverse Style Transferring. (arXiv:2210.09074v1 [cs.CV])
- Authors : 
- Link : [http://arxiv.org/abs/2210.09074](http://arxiv.org/abs/2210.09074)
> ABSTRACT  :  RAW image datasets are more suitable than the standard RGB image datasets for the ill-posed inverse problems in low-level vision, but not common in the literature. There are also a few studies to focus on mapping sRGB images to RAW format. Mapping from sRGB to RAW format could be a relevant domain for reverse style transferring since the task is an ill-posed reversing problem. In this study, we seek an answer to the question: Can the **ISP** operations be modeled as the style factor in an end-to-end learning pipeline? To investigate this idea, we propose a novel architecture, namely RST-**ISP**-Net, for learning to reverse the **ISP** operations with the help of adaptive feature normalization. We formulate this problem as a reverse style transferring and mostly follow the practice used in the prior work. We have participated in the AIM Reversed **ISP** challenge with our proposed architecture. Results indicate that the idea of modeling disruptive or modifying factors as style is still valid, but further improvements are required to be competitive in such a challenge.  
### Effects of Image Size on Deep Learning. (arXiv:2101.11508v6 [cs.CV] UPDATED)
- Authors : Olivier Rukundo
- Link : [http://arxiv.org/abs/2101.11508](http://arxiv.org/abs/2101.11508)
> ABSTRACT  :  In this paper the main objective is to determine the best size of late gadolinium **enhancement** (LGE)-magnetic resonance imaging (MRI) images for the training dataset to achieve optimal deep learning training outcomes. To determine the new size of LGE-MRI images from the reference training dataset, non-extra pixel and extra pixel interpolation algorithms are used. A novel strategy based on thresholding, median filtering, and subtraction operations is introduced and applied to remove extra class labels in interpolated ground truth (GT) segmentation masks. Fully automated quantification is achieved using the expectation maximization, weighted intensity, a priori information (EWA) algorithm, and the outcome of automatic semantic segmentation of LGE-MRI images with the convolutional neural network (CNN). In the experiments, common class metrics are used to evaluate the quality of semantic segmentation with a CNN architecture of interest (U-net) against the GT segmentation. Arbitrary threshold, comparison of the sums, and sums of differences are criteria or options used to estimate the relationship between semi-automatic and fully automated quantification of MI results. A close relationship between semi-automatic or manual and fully automated quantification of MI results was more identified in the case involving the dataset of bigger LGE MRI images than in that of the dataset of smaller LGE-MRI images where the best quantification results based on the dataset of bigger LGE MRI images were 55.5% closer the manual or semiautomatic results while the best quantification results based on the dataset of smaller LGE MRI images were 22.2% closer the manual results.  
### Wide Range MRI Artifact Removal with Transformers. (arXiv:2210.07976v2 [eess.IV] UPDATED)
- Authors : Lennart Alexander, Van der, Kevin Smith
- Link : [http://arxiv.org/abs/2210.07976](http://arxiv.org/abs/2210.07976)
> ABSTRACT  :  Artifacts on magnetic resonance scans are a serious challenge for both radiologists and computer-aided diagnosis systems. Most commonly, artifacts are caused by motion of the patients, but can also arise from device-specific abnormalities such as noise patterns. Irrespective of the source, artifacts can not only render a scan useless, but can potentially induce misdiagnoses if left unnoticed. For instance, an artifact may masquerade as a tumor or other abnormality. Retrospective artifact correction (RAC) is concerned with removing artifacts after the scan has already been taken. In this work, we propose a method capable of retrospectively removing eight common artifacts found in native-resolution MR imagery. Knowledge of the presence or location of a specific artifact is not assumed and the system is, by design, capable of undoing interactions of multiple artifacts. Our method is realized through the design of a novel volumetric transformer-based neural network that generalizes a \emph{window-centered} approach popularized by the **Swin** transformer. Unlike **Swin**, our method is (i) natively volumetric, (ii) geared towards dense prediction tasks instead of classification, and (iii), uses a novel and more global mechanism to enable information exchange between windows. Our experiments show that our reconstructions are considerably better than those attained by ResNet, V-Net, MobileNet-v2, DenseNet, CycleGAN and BicycleGAN. Moreover, we show that the reconstructed images from our model improves the accuracy of FSL BET, a standard skull-stripping method typically applied in diagnostic workflows.  
## cs.LG
---
### Parameter-free Dynamic Graph Embedding for Link Prediction. (arXiv:2210.08189v1 [cs.LG])
- Authors : Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Ning Gu
- Link : [http://arxiv.org/abs/2210.08189](http://arxiv.org/abs/2210.08189)
> ABSTRACT  :  Dynamic interaction graphs have been widely adopted to model the evolution of user-item interactions over time. There are two crucial factors when modelling user preferences for link prediction in dynamic interaction graphs: 1) collaborative relationship among users and 2) user personalized interaction patterns. Existing methods often implicitly consider these two factors together, which may lead to noisy user modelling when the two factors diverge. In addition, they usually require time-consuming parameter learning with back-propagation, which is prohibitive for real-time user preference modelling. To this end, this paper proposes FreeGEM, a parameter-free dynamic graph embedding method for link prediction. Firstly, to take advantage of the collaborative relationships, we propose an incremental graph embedding engine to obtain user/item embeddings, which is an Online-Monitor-Offline architecture consisting of an Online module to approximately embed users/items over time, a Monitor module to estimate the approximation error in **real time** and an Offline module to calibrate the user/item embeddings when the online approximation errors exceed a threshold. Meanwhile, we integrate attribute information into the model, which enables FreeGEM to better model users belonging to some under represented groups. Secondly, we design a personalized dynamic interaction pattern modeller, which combines dynamic time decay with attention mechanism to model user short-term interests. Experimental results on two link prediction tasks show that FreeGEM can outperform the state-of-the-art methods in accuracy while achieving over 36X improvement in efficiency. All code and datasets can be found in https://github.com/FudanCISL/FreeGEM.  
### Improving Your Graph Neural Networks: A High-Frequency Booster. (arXiv:2210.08251v1 [cs.LG])
- Authors : Jiaqi Sun, Lin Zhang, Shenglin Zhao, Yujiu Yang
- Link : [http://arxiv.org/abs/2210.08251](http://arxiv.org/abs/2210.08251)
> ABSTRACT  :  Graph neural networks (GNNs) hold the promise of learning efficient representations of graph-structured data, and one of its most important applications is semi-supervised node classification. However, in this application, GNN frameworks tend to fail due to the following issues: over-smoothing and heterophily. The most popular GNNs are known to be focused on the message-passing framework, and recent research shows that these GNNs are often bounded by low-pass filters from a signal processing perspective. We thus incorporate high-frequency information into GNNs to alleviate this genetic problem. In this paper, we argue that the complement of the original graph incorporates a high-pass filter and propose Complement Laplacian Regularization (CLAR) for an efficient **enhancement** of high-frequency components. The experimental results demonstrate that CLAR helps GNNs tackle over-smoothing, improving the expressiveness of heterophilic graphs, which adds up to 3.6% improvement over popular baselines and ensures topological robustness.  
### Classification of Web Phishing Kits for early detection by platform providers. (arXiv:2210.08273v1 [cs.CR])
- Authors : Andrea Venturi, Michele Colajanni, Marco Ramilli, Giorgio Valenziano
- Link : [http://arxiv.org/abs/2210.08273](http://arxiv.org/abs/2210.08273)
> ABSTRACT  :  Phishing kits are tools that **dark** side experts provide to the community of criminal phishers to facilitate the construction of malicious Web sites. As these kits evolve in sophistication, providers of Web-based services need to keep pace with continuous complexity. We present an original classification of a corpus of over 2000 recent phishing kits according to their adopted evasion and obfuscation functions. We carry out an initial deterministic analysis of the source code of the kits to extract the most discriminant features and information about their principal authors. We then integrate this initial classification through supervised machine learning models. Thanks to the ground-truth achieved in the first step, we can demonstrate whether and which machine learning models are able to suitably classify even the kits adopting novel evasion and obfuscation techniques that were unseen during the training phase. We compare different algorithms and evaluate their robustness in the realistic case in which only a small number of phishing kits are available for training. This paper represents an initial but important step to support Web service providers and analysts in improving early detection mechanisms and intelligence operations for the phishing kits that might be installed on their platforms.  
### Effects of Image Size on Deep Learning. (arXiv:2101.11508v6 [cs.CV] UPDATED)
- Authors : Olivier Rukundo
- Link : [http://arxiv.org/abs/2101.11508](http://arxiv.org/abs/2101.11508)
> ABSTRACT  :  In this paper the main objective is to determine the best size of late gadolinium **enhancement** (LGE)-magnetic resonance imaging (MRI) images for the training dataset to achieve optimal deep learning training outcomes. To determine the new size of LGE-MRI images from the reference training dataset, non-extra pixel and extra pixel interpolation algorithms are used. A novel strategy based on thresholding, median filtering, and subtraction operations is introduced and applied to remove extra class labels in interpolated ground truth (GT) segmentation masks. Fully automated quantification is achieved using the expectation maximization, weighted intensity, a priori information (EWA) algorithm, and the outcome of automatic semantic segmentation of LGE-MRI images with the convolutional neural network (CNN). In the experiments, common class metrics are used to evaluate the quality of semantic segmentation with a CNN architecture of interest (U-net) against the GT segmentation. Arbitrary threshold, comparison of the sums, and sums of differences are criteria or options used to estimate the relationship between semi-automatic and fully automated quantification of MI results. A close relationship between semi-automatic or manual and fully automated quantification of MI results was more identified in the case involving the dataset of bigger LGE MRI images than in that of the dataset of smaller LGE-MRI images where the best quantification results based on the dataset of bigger LGE MRI images were 55.5% closer the manual or semiautomatic results while the best quantification results based on the dataset of smaller LGE MRI images were 22.2% closer the manual results.  
### Telehealthcare and Telepathology in Pandemic: A Noninvasive, Low-Cost Micro-Invasive and Multimodal Real-Time Online Application for Early Diagnosis of COVID-19 Infection. (arXiv:2109.07846v2 [cs.LG] UPDATED)
- Authors : Abdullah Bin, Mohsin Sarker, Mohi Uddin, Ocean Monjur, Rahat Bin
- Link : [http://arxiv.org/abs/2109.07846](http://arxiv.org/abs/2109.07846)
> ABSTRACT  :  To contain the spread of the virus and stop the overcrowding of hospitalized patients, the coronavirus pandemic crippled healthcare facilities, mandating lockdowns and promoting remote work. As a result, telehealth has become increasingly popular for offering low-risk care to patients. However, the difficulty of preventing the next potential waves of infection has increased by constant virus mutation into new forms and a general lack of test kits, particularly in developing nations. In this research, a unique cloud-based application for the early identification of individuals who may have COVID-19 infection is proposed. The application provides five modes of diagnosis from possible symptoms (f1), cough sound (f2), specific blood biomarkers (f3), Raman spectral data of blood specimens (f4), and ECG signal paper-based image (f5). When a user selects an option and enters the information, the data is sent to the cloud server. The deployed machine learning (ML) and deep learning (DL) models classify the data in **real time** and inform the user of the likelihood of COVID-19 infection. Our deployed models can classify with an accuracy of 100%, 99.80%, 99.55%, 95.65%, and 77.59% from f3, f4, f5, f2, and f1 respectively. Moreover, the sensitivity for f2, f3, and f4 is 100%, which indicates the correct identification of COVID positive patients. This is significant in limiting the spread of the virus. Additionally, another ML model, as seen to offer 92% accuracy serves to identify patients who, out of a large group of patients admitted to the hospital cohort, need immediate critical care support by estimating the mortality risk of patients from blood parameters. The instantaneous multimodal nature of our technique offers multiplex and accurate diagnostic methods, highlighting the effectiveness of telehealth as a simple, widely available, and low-cost diagnostic solution, even for future pandemics.  
### Strong Lensing Source Reconstruction Using Continuous Neural Fields. (arXiv:2206.14820v2 [astro-ph.CO] UPDATED)
- Authors : Siddharth Mishra, Ge Yang
- Link : [http://arxiv.org/abs/2206.14820](http://arxiv.org/abs/2206.14820)
> ABSTRACT  :  From the nature of **dark** matter to the rate of expansion of our Universe, observations of distant galaxies distorted through strong gravitational lensing have the potential to answer some of the major open questions in astrophysics. Modeling galaxy-galaxy strong lensing observations presents a number of challenges as the exact configuration of both the background source and foreground lens galaxy is unknown. A timely call, prompted by a number of upcoming surveys anticipating high-resolution lensing images, demands methods that can efficiently model lenses at their full complexity. In this work, we introduce a method that uses continuous neural fields to non-parametrically reconstruct the complex morphology of a source galaxy while simultaneously inferring a distribution over foreground lens galaxy configurations. We demonstrate the efficacy of our method through experiments on simulated data targeting high-resolution lensing images similar to those anticipated in near-future astrophysical surveys.  
### Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit. (arXiv:2207.08799v2 [cs.LG] UPDATED)
- Authors : Boaz Barak, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang
- Link : [http://arxiv.org/abs/2207.08799](http://arxiv.org/abs/2207.08799)
> ABSTRACT  :  There is mounting empirical evidence of emergent phenomena in the capabilities of deep learning methods as we scale up datasets, model sizes, and training times. While there are some accounts of how these resources modulate statistical capacity, far less is known about their effect on the computational problem of model training. This work conducts such an exploration through the lens of learning $k$-sparse parities of $n$ bits, a canonical family of problems which pose theoretical computational barriers. In this setting, we find that neural networks exhibit surprising phase transitions when scaling up dataset size and running time. In particular, we demonstrate empirically that with standard training, a variety of architectures learn sparse parities with $n^{O(k)}$ examples, with loss (and error) curves abruptly dropping after $n^{O(k)}$ iterations. These positive results nearly match known SQ lower bounds, even without an explicit sparsity-promoting prior. We elucidate the mechanisms of these phenomena with a theoretical analysis: we find that the phase transition in performance is not due to SGD "stumbling in the **dark**" until it finds the hidden set of features (a natural algorithm which also runs in $n^{O(k)}$ time); instead, we show that SGD gradually amplifies a Fourier gap in the population gradient.  
### BOAT: **Bilateral** Local Attention Vision Transformer. (arXiv:2201.13027v1 [cs.CV] CROSS LISTED)
- Authors : Tan Yu, Gangming Zhao, Ping Li, Yizhou Yu
- Link : [http://arxiv.org/abs/2201.13027](http://arxiv.org/abs/2201.13027)
> ABSTRACT  :  Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a **Bilateral** lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both **Swin** and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.  
## cs.AI
---
### Parameter-free Dynamic Graph Embedding for Link Prediction. (arXiv:2210.08189v1 [cs.LG])
- Authors : Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Ning Gu
- Link : [http://arxiv.org/abs/2210.08189](http://arxiv.org/abs/2210.08189)
> ABSTRACT  :  Dynamic interaction graphs have been widely adopted to model the evolution of user-item interactions over time. There are two crucial factors when modelling user preferences for link prediction in dynamic interaction graphs: 1) collaborative relationship among users and 2) user personalized interaction patterns. Existing methods often implicitly consider these two factors together, which may lead to noisy user modelling when the two factors diverge. In addition, they usually require time-consuming parameter learning with back-propagation, which is prohibitive for real-time user preference modelling. To this end, this paper proposes FreeGEM, a parameter-free dynamic graph embedding method for link prediction. Firstly, to take advantage of the collaborative relationships, we propose an incremental graph embedding engine to obtain user/item embeddings, which is an Online-Monitor-Offline architecture consisting of an Online module to approximately embed users/items over time, a Monitor module to estimate the approximation error in **real time** and an Offline module to calibrate the user/item embeddings when the online approximation errors exceed a threshold. Meanwhile, we integrate attribute information into the model, which enables FreeGEM to better model users belonging to some under represented groups. Secondly, we design a personalized dynamic interaction pattern modeller, which combines dynamic time decay with attention mechanism to model user short-term interests. Experimental results on two link prediction tasks show that FreeGEM can outperform the state-of-the-art methods in accuracy while achieving over 36X improvement in efficiency. All code and datasets can be found in https://github.com/FudanCISL/FreeGEM.  
### Shielding Federated Learning Systems against Inference Attacks with ARM TrustZone. (arXiv:2208.05895v4 [cs.CR] UPDATED)
- Authors : Aghiles Ait, Sonia Ben, Vlad Nitu, Valerio Schiavoni
- Link : [http://arxiv.org/abs/2208.05895](http://arxiv.org/abs/2208.05895)
> ABSTRACT  :  Federated Learning (FL) opens new perspectives for training machine learning models while keeping personal data on the users premises. Specifically, in FL, models are trained on the users devices and only model updates (i.e., gradients) are sent to a central server for aggregation purposes. However, the long list of inference attacks that leak private data from gradients, published in the recent years, have emphasized the need of devising effective protection mechanisms to incentivize the adoption of FL at scale. While there exist solutions to mitigate these attacks on the server side, little has been done to protect users from attacks performed on the client side. In this context, the use of Trusted Execution Environments (TEEs) on the client side are among the most proposing solutions. However, existing frameworks (e.g., **Dark**neTZ) require statically putting a large portion of the machine learning model into the TEE to effectively protect against complex attacks or a combination of attacks. We present GradSec, a solution that allows protecting in a TEE only sensitive layers of a machine learning model, either statically or dynamically, hence reducing both the TCB size and the overall training time by up to 30% and 56%, respectively compared to state-of-the-art competitors.  
# Paper List
---
## cs.CV
---
**142** new papers in cs.CV:-) 
1. Misaligned orientations of 4f optical neural network for image classification accuracy on various datasets. (arXiv:2210.08004v1 [physics.optics])
2. On the Relationship Between Variational Inference and Auto-Associative Memory. (arXiv:2210.08013v1 [cs.LG])
3. Neural Attentive Circuits. (arXiv:2210.08031v1 [cs.LG])
4. Meta Transferring for Deblurring. (arXiv:2210.08036v1 [cs.CV])
5. Semi-supervised Body Parsing and Pose Estimation for Enhancing Infant General Movement Assessment. (arXiv:2210.08054v1 [cs.CV])
6. Pishgu: Universal Path Prediction Architecture through Graph Isomorphism and Attentive Convolution. (arXiv:2210.08057v1 [cs.CV])
7. Motion Inspired Unsupervised Perception and Prediction in Autonomous Driving. (arXiv:2210.08061v1 [cs.CV])
8. LESS: Label-Efficient Semantic Segmentation for LiDAR Point Clouds. (arXiv:2210.08064v1 [cs.CV])
9. Optimizing Vision Transformers for Medical Image Segmentation and Few-Shot Domain Adaptation. (arXiv:2210.08066v1 [cs.CV])
10. Whole-body tumor segmentation of 18F -FDG PET/CT using a cascaded and ensembled convolutional neural networks. (arXiv:2210.08068v1 [eess.IV])
11. Deep Learning based Super-Resolution for Medical Volume Visualization with Direct Volume Rendering. (arXiv:2210.08080v1 [cs.GR])
12. Reference Based Color Transfer for Medical Volume Rendering. (arXiv:2210.08083v1 [cs.CV])
13. Knowledge Distillation approach towards Melanoma Detection. (arXiv:2210.08086v1 [cs.CV])
14. Parameter Sharing in Budget-Aware Adapters for Multi-Domain Learning. (arXiv:2210.08101v1 [cs.CV])
15. Instance Segmentation with Cross-Modal Consistency. (arXiv:2210.08113v1 [cs.CV])
16. QuAnt: Quantum Annealing with Learnt Couplings. (arXiv:2210.08114v1 [quant-ph])
17. An Improved Multi-State Constraint Kalman Filter for Visual-Inertial Odometry. (arXiv:2210.08117v1 [cs.RO])
18. Towards a Fully Autonomous UAV Controller for Moving Platform Detection and Landing. (arXiv:2210.08120v1 [cs.RO])
19. Keypoint Cascade Voting for Point Cloud Based 6DoF Pose Estimation. (arXiv:2210.08123v1 [cs.CV])
20. Dynamics-aware Adversarial Attack of Adaptive Neural Networks. (arXiv:2210.08159v1 [cs.CV])
21. Learning Dual Memory Dictionaries for Blind Face **Restoration**. (arXiv:2210.08160v1 [cs.CV])
22. Geometric Representation Learning for Document Image Rectification. (arXiv:2210.08161v1 [cs.CV])
23. Linear Video Transformer with Feature Fixation. (arXiv:2210.08164v1 [cs.CV])
24. MKIS-Net: A Light-Weight Multi-Kernel Network for Medical Image Segmentation. (arXiv:2210.08168v1 [eess.IV])
25. Attention Regularized Laplace Graph for Domain Adaptation. (arXiv:2210.08170v1 [cs.CV])
26. Is Face Recognition Safe from Realizable Attacks?. (arXiv:2210.08178v1 [cs.CV])
27. Panchromatic and Multispectral Image Fusion via Alternating Reverse Filtering Network. (arXiv:2210.08181v1 [cs.CV])
28. Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers. (arXiv:2210.08198v1 [cs.CV])
29. IBL-NeRF: Image-Based Lighting Formulation of Neural Radiance Fields. (arXiv:2210.08202v1 [cs.CV])
30. Providing Error Detection for Deep Learning Image Classifiers Using Self-Explainability. (arXiv:2210.08210v1 [cs.CV])
31. UDoc-GAN: Unpaired Document Illumination Correction with Background Light Prior. (arXiv:2210.08216v1 [cs.CV])
32. Learned Video Compression for YUV 4:2:0 Content Using Flow-based Conditional Inter-frame Coding. (arXiv:2210.08225v1 [eess.IV])
33. Self-Distillation for Unsupervised 3D Domain Adaptation. (arXiv:2210.08226v1 [cs.CV])
34. A Codec Information Assisted Framework for Efficient Compressed Video Super-Resolution. (arXiv:2210.08229v1 [cs.CV])
35. Hand Gestures Recognition in Videos Taken with Lensless Camera. (arXiv:2210.08233v1 [cs.CV])
36. Motion estimation and filtered prediction for dynamic point cloud attribute compression. (arXiv:2210.08262v1 [cs.CV])
37. LAD: A Hybrid Deep Learning System for Benign Paroxysmal Positional Vertigo Disorders Diagnostic. (arXiv:2210.08282v1 [cs.CV])
38. Transformer-based dimensionality reduction. (arXiv:2210.08288v1 [cs.CV])
39. Prediction Calibration for Generalized Few-shot Semantic Segmentation. (arXiv:2210.08290v1 [cs.CV])
40. Bidirectional Semi-supervised Dual-branch CNN for Robust 3D Reconstruction of Stereo Endoscopic Images via Adaptive Cross and Parallel Supervisions. (arXiv:2210.08291v1 [cs.CV])
41. Improving Radiology Summarization with Radiograph and Anatomy Prompts. (arXiv:2210.08303v1 [cs.CV])
42. PointNeuron: 3D Neuron Reconstruction via Geometry and Topology Learning of Point Clouds. (arXiv:2210.08305v1 [cs.CV])
43. CoRe: An Automated Pipeline for The Prediction of Liver Resection Complexity from Preoperative CT Scans. (arXiv:2210.08318v1 [eess.IV])
44. Aplicaci\'on de redes neuronales convolucionales profundas al diagn\'ostico asistido de la enfermedad de Alzheimer. (arXiv:2210.08330v1 [eess.IV])
45. MiniMax Entropy Network: Learning Category-Invariant Features for Domain Adaptation. (arXiv:1904.09601v3 [cs.CV] UPDATED)
46. Exploration of Interpretability Techniques for Deep COVID-19 Classification using Chest X-ray Images. (arXiv:2006.02570v3 [eess.IV] UPDATED)
47. DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis. (arXiv:2008.05865v4 [cs.CV] UPDATED)
48. Learning to Segment Dynamic Objects using SLAM Outliers. (arXiv:2011.06259v2 [cs.CV] UPDATED)
49. Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v5 [cs.CV] UPDATED)
50. RainNet: A Large-Scale Imagery Dataset and Benchmark for Spatial Precipitation Downscaling. (arXiv:2012.09700v3 [cs.CV] UPDATED)
51. Effects of Image Size on Deep Learning. (arXiv:2101.11508v6 [cs.CV] UPDATED)
52. Towards Generalising Neural Implicit Representations. (arXiv:2101.12690v3 [cs.CV] UPDATED)
53. ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning. (arXiv:2103.11395v3 [cs.CV] UPDATED)
54. 1xN Pattern for Pruning Convolutional Neural Networks. (arXiv:2105.14713v6 [cs.CV] UPDATED)
55. Real-world Video Deblurring: A Benchmark Dataset and An Efficient Recurrent Neural Network. (arXiv:2106.16028v2 [cs.CV] UPDATED)
56. Know Thyself: Transferable Visual Control Policies Through Robot-Awareness. (arXiv:2107.09047v3 [cs.LG] UPDATED)
57. SPG-VTON: Semantic Prediction Guidance for Multi-pose Virtual Try-on. (arXiv:2108.01578v2 [cs.CV] UPDATED)
58. PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v3 [eess.IV] UPDATED)
59. Ground material classification for UAV-based photogrammetric 3D data A 2D-3D Hybrid Approach. (arXiv:2109.12221v2 [cs.CV] UPDATED)
60. Weak-shot Semantic Segmentation by Transferring Semantic Affinity and Boundary. (arXiv:2110.01519v2 [cs.CV] UPDATED)
61. Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v5 [cs.LG] UPDATED)
62. Deep Fusion Prior for Plenoptic Super-Resolution All-in-Focus Imaging. (arXiv:2110.05706v5 [cs.CV] UPDATED)
63. GenURL: A General Framework for Unsupervised Representation Learning. (arXiv:2110.14553v3 [cs.LG] UPDATED)
64. Towards Axiomatic, Hierarchical, and Symbolic Explanation for Deep Models. (arXiv:2111.06206v4 [cs.LG] UPDATED)
65. Deep Semantic Manipulation of Facial Videos. (arXiv:2111.07902v2 [cs.CV] UPDATED)
66. TransMorph: Transformer for unsupervised medical image registration. (arXiv:2111.10480v6 [eess.IV] UPDATED)
67. MASTAF: A Model-Agnostic Spatio-Temporal Attention Fusion Network for Few-shot Video Classification. (arXiv:2112.04585v3 [cs.CV] UPDATED)
68. Constrained Mean Shift Using Distant Yet Related Neighbors for Representation Learning. (arXiv:2112.04607v2 [cs.CV] UPDATED)
69. Deep ViT Features as Dense Visual Descriptors. (arXiv:2112.05814v3 [cs.CV] UPDATED)
70. Distilled Dual-Encoder Model for Vision-Language Understanding. (arXiv:2112.08723v2 [cs.CL] UPDATED)
71. Anomaly Clustering: Grouping Images into Coherent Clusters of Anomaly Types. (arXiv:2112.11573v2 [cs.CV] UPDATED)
72. Mobile Robot Manipulation using Pure Object Detection. (arXiv:2201.12437v2 [cs.CV] UPDATED)
73. I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning. (arXiv:2202.06574v2 [cs.CL] UPDATED)
74. A Transformer-based Network for Deformable Medical Image Registration. (arXiv:2202.12104v3 [eess.IV] UPDATED)
75. DICS-Net: Dictionary-guided Implicit-Component-Supervision Network for Few-Shot Classification. (arXiv:2203.07738v2 [cs.CV] UPDATED)
76. A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v3 [cs.LG] UPDATED)
77. RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection. (arXiv:2204.02553v3 [cs.CV] UPDATED)
78. Future Object Detection with Spatiotemporal Transformers. (arXiv:2204.10321v2 [cs.CV] UPDATED)
79. Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging. (arXiv:2205.10102v3 [eess.IV] UPDATED)
80. Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v4 [cs.CV] UPDATED)
81. Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v3 [cs.CV] UPDATED)
82. AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition. (arXiv:2205.13535v3 [cs.CV] UPDATED)
83. Decomposing NeRF for Editing via Feature Field Distillation. (arXiv:2205.15585v2 [cs.CV] UPDATED)
84. SNAKE: Shape-aware Neural 3D Keypoint Field. (arXiv:2206.01724v2 [cs.CV] UPDATED)
85. Additive MIL: Intrinsically Interpretable Multiple Instance Learning for Pathology. (arXiv:2206.01794v2 [cs.CV] UPDATED)
86. An Empirical Study on Disentanglement of Negative-free Contrastive Learning. (arXiv:2206.04756v2 [cs.LG] UPDATED)
87. On Enforcing Better Conditioned Meta-Learning for Rapid Few-Shot Adaptation. (arXiv:2206.07260v2 [cs.LG] UPDATED)
88. Rethinking Generalization in Few-Shot Classification. (arXiv:2206.07267v3 [cs.CV] UPDATED)
89. A Unified Sequence Interface for Vision Tasks. (arXiv:2206.07669v2 [cs.CV] UPDATED)
90. Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline. (arXiv:2206.08129v2 [cs.CV] UPDATED)
91. SpA-Former: Transformer image shadow detection and removal via spatial attention. (arXiv:2206.10910v3 [cs.CV] UPDATED)
92. FastBVP-Net: a lightweight pulse simulation network for measuring heart rhythm via facial videos. (arXiv:2206.12558v2 [cs.CV] UPDATED)
93. NeuRIS: Neural Reconstruction of Indoor Scenes Using Normal Priors. (arXiv:2206.13597v2 [cs.CV] UPDATED)
94. Strong Lensing Source Reconstruction Using Continuous Neural Fields. (arXiv:2206.14820v2 [astro-ph.CO] UPDATED)
95. ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal Self-Ensemble for Active Learning. (arXiv:2207.02182v2 [cs.CV] UPDATED)
96. Semi-supervised Object Detection via Virtual Category Learning. (arXiv:2207.03433v2 [cs.CV] UPDATED)
97. Learning to Register Unbalanced Point Pairs. (arXiv:2207.04221v2 [cs.CV] UPDATED)
98. EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations. (arXiv:2207.06635v4 [cs.CV] UPDATED)
99. BusyBot: Learning to Interact, Reason, and Plan in a BusyBoard Environment. (arXiv:2207.08192v2 [cs.RO] UPDATED)
100. LR-Net: A Block-based Convolutional Neural Network for Low-Resolution Image Classification. (arXiv:2207.09531v4 [cs.CV] UPDATED)
101. DC-BENCH: Dataset Condensation Benchmark. (arXiv:2207.09639v2 [cs.LG] UPDATED)
102. Focused Decoding Enables 3D Anatomical Detection by Transformers. (arXiv:2207.10774v3 [cs.CV] UPDATED)
103. Bessel Equivariant Networks for Inversion of Transmission Effects in Multi-Mode Optical Fibres. (arXiv:2207.12849v2 [physics.optics] UPDATED)
104. Towards Sequence-Level Training for Visual Tracking. (arXiv:2208.05810v3 [cs.CV] UPDATED)
105. RFLA: Gaussian Receptive Field based Label Assignment for Tiny Object Detection. (arXiv:2208.08738v2 [cs.CV] UPDATED)
106. A diverse large-scale building dataset and a novel plug-and-play domain generalization method for building extraction. (arXiv:2208.10004v2 [cs.CV] UPDATED)
107. Q-Net: Query-Informed Few-Shot Medical Image Segmentation. (arXiv:2208.11451v2 [cs.CV] UPDATED)
108. Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v8 [cs.LG] UPDATED)
109. First Hitting Diffusion Models for Generating Manifold, Graph and Categorical Data. (arXiv:2209.01170v2 [cs.CV] UPDATED)
110. Wavelength-aware 2D Convolutions for Hyperspectral Imaging. (arXiv:2209.03136v2 [cs.CV] UPDATED)
111. Treating Motion as Option to Reduce Motion Dependency in Unsupervised Video Object Segmentation. (arXiv:2209.03138v2 [cs.CV] UPDATED)
112. Multi-Granularity Prediction for Scene Text Recognition. (arXiv:2209.03592v2 [cs.CV] UPDATED)
113. 3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation. (arXiv:2209.05434v3 [cs.CV] UPDATED)
114. Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models. (arXiv:2209.06970v2 [cs.CV] UPDATED)
115. DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection. (arXiv:2209.09407v2 [cs.CV] UPDATED)
116. Data-Centric AI Paradigm Based on Application-Driven Fine-Grained Dataset Design. (arXiv:2209.09449v3 [cs.CV] UPDATED)
117. GAMA: Generative Adversarial Multi-Object Scene Attacks. (arXiv:2209.09502v2 [cs.CV] UPDATED)
118. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. (arXiv:2209.09513v2 [cs.CL] UPDATED)
119. Open-vocabulary Queryable Scene Representations for Real World Planning. (arXiv:2209.09874v2 [cs.RO] UPDATED)
120. Multimodal Exponentially Modified Gaussian Oscillators. (arXiv:2209.12202v3 [cs.SD] UPDATED)
121. Deep Learning Based Detection of Enlarged Perivascular Spaces on Brain MRI. (arXiv:2209.13727v2 [eess.IV] UPDATED)
122. SinGRAV: Learning a Generative Radiance Volume from a Single Natural Scene. (arXiv:2210.01202v3 [cs.CV] UPDATED)
123. Probabilistic Volumetric Fusion for Dense Monocular SLAM. (arXiv:2210.01276v2 [cs.CV] UPDATED)
124. Complementary consistency semi-supervised learning for 3D left atrial image segmentation. (arXiv:2210.01438v3 [eess.IV] UPDATED)
125. A2: Efficient Automated Attacker for Boosting Adversarial Training. (arXiv:2210.03543v2 [cs.CV] UPDATED)
126. Is your noise correction noisy? PLS: Robustness to label noise with two stage detection. (arXiv:2210.04578v2 [cs.CV] UPDATED)
127. FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings. (arXiv:2210.04620v2 [cs.LG] UPDATED)
128. CASAPose: Class-Adaptive and Semantic-Aware Multi-Object Pose Estimation. (arXiv:2210.05318v2 [cs.CV] UPDATED)
129. Visual Language Maps for Robot Navigation. (arXiv:2210.05714v3 [cs.RO] UPDATED)
130. Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v3 [cs.CL] UPDATED)
131. What's in a Decade? Transforming Faces Through Time. (arXiv:2210.06642v2 [cs.CV] UPDATED)
132. Brain Network Transformer. (arXiv:2210.06681v2 [cs.LG] UPDATED)
133. HoechstGAN: Virtual Lymphocyte Staining Using Generative Adversarial Networks. (arXiv:2210.06909v2 [cs.CV] UPDATED)
134. PDEBENCH: An Extensive Benchmark for Scientific Machine Learning. (arXiv:2210.07182v2 [cs.LG] UPDATED)
135. 3D GAN Inversion with Pose Optimization. (arXiv:2210.07301v2 [cs.CV] UPDATED)
136. Controllable Style Transfer via Test-time Training of Implicit Neural Representation. (arXiv:2210.07762v2 [cs.CV] UPDATED)
137. SAILOR: Scaling Anchors via Insights into Latent Object Representation. (arXiv:2210.07811v2 [cs.CV] UPDATED)
138. Contrastive Audio-Visual Masked Autoencoder. (arXiv:2210.07839v2 [cs.MM] UPDATED)
139. One Model to Edit Them All: Free-Form Text-Driven Image Manipulation with Semantic Modulations. (arXiv:2210.07883v2 [cs.CV] UPDATED)
140. Wide Range MRI Artifact Removal with Transformers. (arXiv:2210.07976v2 [eess.IV] UPDATED)
141. BOAT: **Bilateral** Local Attention Vision Transformer. (arXiv:2201.13027v1 [cs.CV] CROSS LISTED)
142. DCANet: Differential Convolution Attention Network for RGB-D Semantic Segmentation. (arXiv:2210.06747v1 [eess.IV] CROSS LISTED)
## eess.IV
---
**42** new papers in eess.IV:-) 
1. Misaligned orientations of 4f optical neural network for image classification accuracy on various datasets. (arXiv:2210.08004v1 [physics.optics])
2. Pishgu: Universal Path Prediction Architecture through Graph Isomorphism and Attentive Convolution. (arXiv:2210.08057v1 [cs.CV])
3. Combining band-frequency separation and deep neural networks for optoacoustic imaging. (arXiv:2210.08099v1 [eess.IV])
4. An Improved Multi-State Constraint Kalman Filter for Visual-Inertial Odometry. (arXiv:2210.08117v1 [cs.RO])
5. Learned Video Compression for YUV 4:2:0 Content Using Flow-based Conditional Inter-frame Coding. (arXiv:2210.08225v1 [eess.IV])
6. Motion estimation and filtered prediction for dynamic point cloud attribute compression. (arXiv:2210.08262v1 [cs.CV])
7. Aplicaci\'on de redes neuronales convolucionales profundas al diagn\'ostico asistido de la enfermedad de Alzheimer. (arXiv:2210.08330v1 [eess.IV])
8. Metasurface Smart Glass for Object Recognition. (arXiv:2210.08369v1 [physics.optics])
9. ResAttUNet: Detecting Marine Debris using an Attention activated Residual UNet. (arXiv:2210.08506v1 [cs.CV])
10. Multisubject Task-Related fMRI Data Processing via a Two-Stage Generalized Canonical Correlation Analysis. (arXiv:2210.08531v1 [eess.SP])
11. 3D-GMIC: an efficient deep neural network to find small objects in large 3D images. (arXiv:2210.08645v1 [cs.CV])
12. An Ontology-based Method to Identify Triggering Conditions for Perception Insufficiency of Autonomous Vehicles. (arXiv:2210.08724v1 [eess.IV])
13. How many radiographs are needed to re-train a deep learning system for object detection?. (arXiv:2210.08734v1 [eess.IV])
14. Use of a smartphone camera to determine the focal length of a thin lens by finding the transverse magnification of the virtual image of an object. (arXiv:2210.08751v1 [cs.CV])
15. Breathing Pattern Monitoring using Remote Sensors. (arXiv:2210.08799v1 [eess.SP])
16. Cerebrovascular Segmentation via Vessel Oriented Filtering Network. (arXiv:2210.08868v1 [eess.IV])
17. An Interactive Interpretability System for Breast Cancer Screening with Deep Learning. (arXiv:2210.08979v1 [eess.IV])
18. Heterogeneous Feature Distillation Network for SAR Image Semantic Segmentation. (arXiv:2210.08988v1 [cs.CV])
19. AIM 2022 Challenge on Instagram Filter Removal: Methods and Results. (arXiv:2210.08997v1 [cs.CV])
20. Periodic Artifact Reduction in Fourier transforms of Full Field Atomic Resolution Images. (arXiv:2210.09024v1 [eess.IV])
21. Reversing Image Signal Processors by Reverse Style Transferring. (arXiv:2210.09074v1 [cs.CV])
22. Nish: A Novel Negative Stimulated Hybrid Activation Function. (arXiv:2210.09083v1 [cs.LG])
23. Gated Recurrent Unit for Video Denoising. (arXiv:2210.09135v1 [cs.CV])
24. A Fault Detection Scheme Utilizing Convolutional Neural Network for PV Solar Panels with High Accuracy. (arXiv:2210.09226v1 [eess.IV])
25. Learning Texture Transformer Network for Light Field Super-Resolution. (arXiv:2210.09293v1 [cs.CV])
26. Virtual-Reality based Vestibular Ocular Motor Screening for Concussion Detection using Machine-Learning. (arXiv:2210.09295v1 [eess.IV])
27. Exploration of Interpretability Techniques for Deep COVID-19 Classification using Chest X-ray Images. (arXiv:2006.02570v3 [eess.IV] UPDATED)
28. Automated Detection and Forecasting of COVID-19 using Deep Learning Techniques: A Review. (arXiv:2007.10785v5 [cs.LG] UPDATED)
29. Effects of Image Size on Deep Learning. (arXiv:2101.11508v6 [cs.CV] UPDATED)
30. PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v3 [eess.IV] UPDATED)
31. Deep Fusion Prior for Plenoptic Super-Resolution All-in-Focus Imaging. (arXiv:2110.05706v5 [cs.CV] UPDATED)
32. TransMorph: Transformer for unsupervised medical image registration. (arXiv:2111.10480v6 [eess.IV] UPDATED)
33. A Transformer-based Network for Deformable Medical Image Registration. (arXiv:2202.12104v3 [eess.IV] UPDATED)
34. Opening the Black Box of Learned Image Coders. (arXiv:2202.13209v2 [eess.IV] UPDATED)
35. SINR: Deconvolving Circular SAS Images Using Implicit Neural Representations. (arXiv:2204.10428v2 [eess.IV] UPDATED)
36. Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging. (arXiv:2205.10102v3 [eess.IV] UPDATED)
37. Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v4 [cs.CV] UPDATED)
38. Bessel Equivariant Networks for Inversion of Transmission Effects in Multi-Mode Optical Fibres. (arXiv:2207.12849v2 [physics.optics] UPDATED)
39. StreamNet: A WAE for White Matter Streamline Analysis. (arXiv:2209.01498v2 [q-bio.QM] UPDATED)
40. Deep Learning Based Detection of Enlarged Perivascular Spaces on Brain MRI. (arXiv:2209.13727v2 [eess.IV] UPDATED)
41. Complementary consistency semi-supervised learning for 3D left atrial image segmentation. (arXiv:2210.01438v3 [eess.IV] UPDATED)
42. Wide Range MRI Artifact Removal with Transformers. (arXiv:2210.07976v2 [eess.IV] UPDATED)
## cs.LG
---
**296** new papers in cs.LG:-) 
1. Hierarchical Decentralized Deep Reinforcement Learning Architecture for a Simulated Four-Legged Agent. (arXiv:2210.08003v1 [cs.AI])
2. Inductive Logical Query Answering in Knowledge Graphs. (arXiv:2210.08008v1 [cs.AI])
3. Autoencoder based Anomaly Detection and Explained Fault Localization in Industrial Cooling Systems. (arXiv:2210.08011v1 [cs.LG])
4. On the Relationship Between Variational Inference and Auto-Associative Memory. (arXiv:2210.08013v1 [cs.LG])
5. Prediction of drug effectiveness in rheumatoid arthritis patients based on machine learning algorithms. (arXiv:2210.08016v1 [q-bio.QM])
6. Neural Attentive Circuits. (arXiv:2210.08031v1 [cs.LG])
7. Differentiable Hybrid Traffic Simulation. (arXiv:2210.08046v1 [cs.GR])
8. Injecting Domain Knowledge from Empirical Interatomic Potentials to Neural Networks for Predicting Material Properties. (arXiv:2210.08047v1 [cs.LG])
9. Multi-trainer Interactive Reinforcement Learning System. (arXiv:2210.08050v1 [cs.LG])
10. Pishgu: Universal Path Prediction Architecture through Graph Isomorphism and Attentive Convolution. (arXiv:2210.08057v1 [cs.CV])
11. Motion Inspired Unsupervised Perception and Prediction in Autonomous Driving. (arXiv:2210.08061v1 [cs.CV])
12. Just Round: Quantized Observation Spaces Enable Memory Efficient Learning of Dynamic Locomotion. (arXiv:2210.08065v1 [cs.RO])
13. Whole-body tumor segmentation of 18F -FDG PET/CT using a cascaded and ensembled convolutional neural networks. (arXiv:2210.08068v1 [eess.IV])
14. Zonotope Domains for Lagrangian Neural Network Verification. (arXiv:2210.08069v1 [cs.LG])
15. Deep Learning based Super-Resolution for Medical Volume Visualization with Direct Volume Rendering. (arXiv:2210.08080v1 [cs.GR])
16. Reference Based Color Transfer for Medical Volume Rendering. (arXiv:2210.08083v1 [cs.CV])
17. Knowledge Distillation approach towards Melanoma Detection. (arXiv:2210.08086v1 [cs.CV])
18. Movement Penalized Bayesian Optimization with Application to Wind Energy Systems. (arXiv:2210.08087v1 [stat.ML])
19. Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning. (arXiv:2210.08090v1 [cs.LG])
20. Bayesian Spline Learning for Equation Discovery of Nonlinear Dynamics with Quantified Uncertainty. (arXiv:2210.08095v1 [cs.LG])
21. TestAug: A Framework for Augmenting Capability-based NLP Tests. (arXiv:2210.08097v1 [cs.SE])
22. A Primal-Dual Algorithm for Hybrid Federated Learning. (arXiv:2210.08106v1 [cs.LG])
23. A Multistep Frank-Wolfe Method. (arXiv:2210.08110v1 [math.OC])
24. QuAnt: Quantum Annealing with Learnt Couplings. (arXiv:2210.08114v1 [quant-ph])
25. A Low-cost Humanoid Prototype Intended to assist people with disability using Raspberry Pi. (arXiv:2210.08116v1 [cs.RO])
26. A Survey on Knowledge Graph-based Methods for Automated Driving. (arXiv:2210.08119v1 [cs.RO])
27. Towards a Fully Autonomous UAV Controller for Moving Platform Detection and Landing. (arXiv:2210.08120v1 [cs.RO])
28. Inferring Versatile Behavior from Demonstrations by Matching Geometric Descriptors. (arXiv:2210.08121v1 [cs.RO])
29. Old can be Gold: Better Gradient Flow can Make Vanilla-GCNs Great Again. (arXiv:2210.08122v1 [cs.LG])
30. TweetNERD -- End to End Entity Linking Benchmark for Tweets. (arXiv:2210.08129v1 [cs.CL])
31. Model-Free Characterizations of the Hamilton-Jacobi-Bellman Equation and Convex Q-Learning in Continuous Time. (arXiv:2210.08131v1 [math.OC])
32. Partial Identification of Treatment Effects with Implicit Generative Models. (arXiv:2210.08139v1 [cs.LG])
33. A Kernel Approach for PDE Discovery and Operator Learning. (arXiv:2210.08140v1 [stat.ML])
34. Pseudo AI Bias. (arXiv:2210.08141v1 [cs.AI])
35. ProtoVAE: A Trustworthy Self-Explainable Prototypical Variational Model. (arXiv:2210.08151v1 [cs.LG])
36. AMD-DBSCAN: An Adaptive Multi-density DBSCAN for datasets of extremely variable density. (arXiv:2210.08162v1 [cs.DB])
37. MKIS-Net: A Light-Weight Multi-Kernel Network for Medical Image Segmentation. (arXiv:2210.08168v1 [eess.IV])
38. Self-supervised Graph Learning for Long-tailed Cognitive Diagnosis. (arXiv:2210.08169v1 [cs.CY])
39. Attention Regularized Laplace Graph for Domain Adaptation. (arXiv:2210.08170v1 [cs.CV])
40. Invertible Monotone Operators for Normalizing Flows. (arXiv:2210.08176v1 [cs.LG])
41. Is Face Recognition Safe from Realizable Attacks?. (arXiv:2210.08178v1 [cs.CV])
42. Label distribution learning via label correlation grid. (arXiv:2210.08184v1 [cs.LG])
43. GFlowCausal: Generative Flow Networks for Causal Discovery. (arXiv:2210.08185v1 [cs.LG])
44. Machine Learning Approach for Predicting Students Academic Performance and Study Strategies based on their Motivation. (arXiv:2210.08186v1 [cs.LG])
45. How Does Pseudo-Labeling Affect the Generalization Error of the Semi-Supervised Gibbs Algorithm?. (arXiv:2210.08188v1 [cs.IT])
46. Parameter-free Dynamic Graph Embedding for Link Prediction. (arXiv:2210.08189v1 [cs.LG])
47. HP-GMN: Graph Memory Networks for Heterophilous Graphs. (arXiv:2210.08195v1 [cs.LG])
48. Deep Regression Unlearning. (arXiv:2210.08196v1 [cs.LG])
49. DyFEn: Agent-Based Fee Setting in Payment Channel Networks. (arXiv:2210.08197v1 [cs.LG])
50. Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers. (arXiv:2210.08198v1 [cs.CV])
51. Active Learning from the Web. (arXiv:2210.08205v1 [cs.LG])
52. Providing Error Detection for Deep Learning Image Classifiers Using Self-Explainability. (arXiv:2210.08210v1 [cs.CV])
53. D.MCA: Outlier Detection with Explicit Micro-Cluster Assignments. (arXiv:2210.08212v1 [cs.LG])
54. PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale. (arXiv:2210.08217v1 [cs.RO])
55. Unveiling the Sampling Density in Non-Uniform Geometric Graphs. (arXiv:2210.08219v1 [cs.LG])
56. Learned Video Compression for YUV 4:2:0 Content Using Flow-based Conditional Inter-frame Coding. (arXiv:2210.08225v1 [eess.IV])
57. Near-Optimal Regret Bounds for Multi-batch Reinforcement Learning. (arXiv:2210.08238v1 [cs.LG])
58. Substructure-Atom Cross Attention for Molecular Representation Learning. (arXiv:2210.08243v1 [cs.LG])
59. Extreme-Long-short Term Memory for Time-series Prediction. (arXiv:2210.08244v1 [cs.LG])
60. A Closer Look at the Calibration of Differentially Private Learners. (arXiv:2210.08248v1 [cs.LG])
61. Improving Your Graph Neural Networks: A High-Frequency Booster. (arXiv:2210.08251v1 [cs.LG])
62. DI-NIDS: Domain Invariant Network Intrusion Detection System. (arXiv:2210.08252v1 [cs.CR])
63. Handling missing values in healthcare data: A systematic review of deep learning-based imputation techniques. (arXiv:2210.08258v1 [cs.LG])
64. MenuAI: Restaurant Food Recommendation System via a Transformer-based Deep Learning Model. (arXiv:2210.08266v1 [cs.IR])
65. Product Ranking for Revenue Maximization with Multiple Purchases. (arXiv:2210.08268v1 [cs.LG])
66. Classification of Web Phishing Kits for early detection by platform providers. (arXiv:2210.08273v1 [cs.CR])
67. Deep Differentiable Logic Gate Networks. (arXiv:2210.08277v1 [cs.LG])
68. Robot Navigation Anticipative Strategies in Deep Reinforcement Motion Planning. (arXiv:2210.08280v1 [cs.RO])
69. FedCross: Towards Accurate Federated Learning via Multi-Model Cross Aggregation. (arXiv:2210.08285v1 [cs.LG])
70. Linear Scalarization for Byzantine-robust learning on non-IID data. (arXiv:2210.08287v1 [cs.LG])
71. MoRSE: Deep Learning-based Arm Gesture Recognition for Search and Rescue Operations. (arXiv:2210.08307v1 [cs.LG])
72. CoRe: An Automated Pipeline for The Prediction of Liver Resection Complexity from Preoperative CT Scans. (arXiv:2210.08318v1 [eess.IV])
73. A Scalable Reinforcement Learning Approach for Attack Allocation in Swarm to Swarm Engagement Problems. (arXiv:2210.08319v1 [cs.RO])
74. A Policy-Guided Imitation Approach for Offline Reinforcement Learning. (arXiv:2210.08323v1 [cs.LG])
75. Distributionally Robust Causal Inference with Observational Data. (arXiv:2210.08326v1 [stat.ME])
76. Aplicaci\'on de redes neuronales convolucionales profundas al diagn\'ostico asistido de la enfermedad de Alzheimer. (arXiv:2210.08330v1 [eess.IV])
77. Code Recommendation for Open Source Software Developers. (arXiv:2210.08332v1 [cs.SE])
78. Gradient Descent: The Ultimate Optimizer. (arXiv:1909.13371v2 [cs.LG] UPDATED)
79. A Simple Convergence Proof of Adam and Adagrad. (arXiv:2003.02395v3 [stat.ML] UPDATED)
80. Reliability and Robustness analysis of Machine Learning based Phishing URL Detectors. (arXiv:2005.08454v2 [cs.CR] UPDATED)
81. Exploration of Interpretability Techniques for Deep COVID-19 Classification using Chest X-ray Images. (arXiv:2006.02570v3 [eess.IV] UPDATED)
82. On Mixup Regularization. (arXiv:2006.06049v3 [cs.LG] UPDATED)
83. Automated Detection and Forecasting of COVID-19 using Deep Learning Techniques: A Review. (arXiv:2007.10785v5 [cs.LG] UPDATED)
84. First-Order Optimization Inspired from Finite-Time Convergent Flows. (arXiv:2010.02990v4 [cs.LG] UPDATED)
85. Robust Low-tubal-rank Tensor Completion based on Tensor Factorization and Maximum Correntopy Criterion. (arXiv:2010.11740v2 [cs.LG] UPDATED)
86. Multi-Armed Bandits with Censored Consumption of Resources. (arXiv:2011.00813v2 [cs.LG] UPDATED)
87. Learning Visual Robotic Control Efficiently with Contrastive Pre-training and Data Augmentation. (arXiv:2012.07975v3 [cs.RO] UPDATED)
88. Crossover-SGD: A gossip-based communication in distributed deep learning for alleviating large mini-batch problem and enhancing scalability. (arXiv:2012.15198v2 [cs.LG] UPDATED)
89. Effects of Image Size on Deep Learning. (arXiv:2101.11508v6 [cs.CV] UPDATED)
90. Biologically Plausible Learning using GAIT-prop Scales to ImageNet. (arXiv:2102.11598v2 [cs.LG] UPDATED)
91. ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning. (arXiv:2103.11395v3 [cs.CV] UPDATED)
92. Pay attention to your loss: understanding misconceptions about 1-Lipschitz neural networks. (arXiv:2104.05097v6 [cs.LG] UPDATED)
93. Differentiable Model Compression via Pseudo Quantization Noise. (arXiv:2104.09987v3 [stat.ML] UPDATED)
94. EXoN: EXplainable encoder Network. (arXiv:2105.10867v3 [stat.ML] UPDATED)
95. SGD with Coordinate Sampling: Theory and Practice. (arXiv:2105.11818v2 [stat.ML] UPDATED)
96. ParK: Sound and Efficient Kernel Ridge Regression by Feature Space Partitions. (arXiv:2106.12231v2 [stat.ML] UPDATED)
97. Multi-Domain Active Learning: Literature Review and Comparative Study. (arXiv:2106.13516v6 [cs.LG] UPDATED)
98. Know Thyself: Transferable Visual Control Policies Through Robot-Awareness. (arXiv:2107.09047v3 [cs.LG] UPDATED)
99. Model Preserving Compression for Neural Networks. (arXiv:2108.00065v2 [cs.LG] UPDATED)
100. The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. (arXiv:2108.06084v4 [cs.LG] UPDATED)
101. X-GOAL: Multiplex Heterogeneous Graph Prototypical Contrastive Learning. (arXiv:2109.03560v4 [cs.LG] UPDATED)
102. PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v3 [eess.IV] UPDATED)
103. Telehealthcare and Telepathology in Pandemic: A Noninvasive, Low-Cost Micro-Invasive and Multimodal Real-Time Online Application for Early Diagnosis of COVID-19 Infection. (arXiv:2109.07846v2 [cs.LG] UPDATED)
104. Learning from Few Samples: Transformation-Invariant SVMs with Composition and Locality at Multiple Scales. (arXiv:2109.12784v5 [cs.LG] UPDATED)
105. Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v5 [cs.LG] UPDATED)
106. Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free Reinforcement Learning. (arXiv:2110.04645v2 [cs.LG] UPDATED)
107. Deep Fusion Prior for Plenoptic Super-Resolution All-in-Focus Imaging. (arXiv:2110.05706v5 [cs.CV] UPDATED)
108. Temporal Abstraction in Reinforcement Learning with the Successor Representation. (arXiv:2110.05740v2 [cs.LG] UPDATED)
109. BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications. (arXiv:2110.05781v3 [eess.AS] UPDATED)
110. On the Double Descent of Random Features Models Trained with SGD. (arXiv:2110.06910v6 [stat.ML] UPDATED)
111. Practical Benefits of Feature Feedback Under Distribution Shift. (arXiv:2110.07566v2 [cs.CL] UPDATED)
112. Almost Optimal Batch-Regret Tradeoff for Batch Linear Contextual Bandits. (arXiv:2110.08057v3 [cs.LG] UPDATED)
113. On Model Selection Consistency of Lasso for High-Dimensional Ising Models. (arXiv:2110.08500v3 [stat.ML] UPDATED)
114. Domain Adaptation via Maximizing Surrogate Mutual Information. (arXiv:2110.12184v3 [cs.LG] UPDATED)
115. Operator Shifting for Model-based Policy Evaluation. (arXiv:2110.12658v2 [cs.LG] UPDATED)
116. GenURL: A General Framework for Unsupervised Representation Learning. (arXiv:2110.14553v3 [cs.LG] UPDATED)
117. Free Probability for predicting the performance of feed-forward fully connected neural networks. (arXiv:2111.00841v3 [stat.ML] UPDATED)
118. Towards Green Automated Machine Learning: Status Quo and Future Directions. (arXiv:2111.05850v3 [cs.LG] UPDATED)
119. Towards Axiomatic, Hierarchical, and Symbolic Explanation for Deep Models. (arXiv:2111.06206v4 [cs.LG] UPDATED)
120. Subgraph Permutation Equivariant Networks. (arXiv:2111.11840v4 [cs.LG] UPDATED)
121. Classification of animal sounds in a hyperdiverse rainforest using Convolutional Neural Networks. (arXiv:2111.14971v2 [cs.LG] UPDATED)
122. Deep Policy Iteration with Integer Programming for Inventory Management. (arXiv:2112.02215v2 [cs.LG] UPDATED)
123. A graph representation based on fluid diffusion model for data analysis: theoretical aspects and enhanced community detection. (arXiv:2112.04388v2 [cs.SI] UPDATED)
124. MASTAF: A Model-Agnostic Spatio-Temporal Attention Fusion Network for Few-shot Video Classification. (arXiv:2112.04585v3 [cs.CV] UPDATED)
125. Training Recurrent Neural Networks by Sequential Least Squares and the Alternating Direction Method of Multipliers. (arXiv:2112.15348v3 [cs.LG] UPDATED)
126. Leveraging Unlabeled Data to Predict Out-of-Distribution Performance. (arXiv:2201.04234v3 [cs.LG] UPDATED)
127. Constraint Learning to Define Trust Regions in Predictive-Model Embedded Optimization. (arXiv:2201.04429v2 [cs.LG] UPDATED)
128. On generalization bounds for deep networks based on loss surface implicit regularization. (arXiv:2201.04545v3 [stat.ML] UPDATED)
129. Invariant Representation Driven Neural Classifier for Anti-QCD Jet Tagging. (arXiv:2201.07199v5 [hep-ph] UPDATED)
130. CsFEVER and CTKFacts: Acquiring Czech data for fact verification. (arXiv:2201.11115v3 [cs.CL] UPDATED)
131. AntBO: Towards Real-World Automated Antibody Design with Combinatorial Bayesian Optimisation. (arXiv:2201.12570v4 [q-bio.BM] UPDATED)
132. POTATO: exPlainable infOrmation exTrAcTion framewOrk. (arXiv:2201.13230v2 [cs.CL] UPDATED)
133. Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation. (arXiv:2202.01336v5 [cs.LG] UPDATED)
134. Robust Binary Models by Pruning Randomly-initialized Networks. (arXiv:2202.01341v2 [cs.LG] UPDATED)
135. Sampling with Riemannian Hamiltonian Monte Carlo in a Constrained Space. (arXiv:2202.01908v2 [cs.LG] UPDATED)
136. Exact Solutions of a Deep Linear Network. (arXiv:2202.04777v4 [stat.ML] UPDATED)
137. End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking. (arXiv:2202.05826v3 [cs.LG] UPDATED)
138. CAREER: Transfer Learning for Economic Prediction of Labor Sequence Data. (arXiv:2202.08370v3 [cs.LG] UPDATED)
139. Learning Predictions for Algorithms with Predictions. (arXiv:2202.09312v2 [cs.LG] UPDATED)
140. Equivariant Graph Hierarchy-Based Neural Networks. (arXiv:2202.10643v2 [cs.LG] UPDATED)
141. Learning Fast and Slow for Online Time Series Forecasting. (arXiv:2202.11672v2 [cs.LG] UPDATED)
142. A Transformer-based Network for Deformable Medical Image Registration. (arXiv:2202.12104v3 [eess.IV] UPDATED)
143. Influencing Long-Term Behavior in Multiagent Reinforcement Learning. (arXiv:2203.03535v4 [cs.LG] UPDATED)
144. LEMON: LanguagE ModeL for Negative Sampling of Knowledge Graph Embeddings. (arXiv:2203.04703v3 [cs.AI] UPDATED)
145. Differentially Private Learning Needs Hidden State (Or Much Faster Convergence). (arXiv:2203.05363v2 [stat.ML] UPDATED)
146. Distraction is All You Need for Fairness. (arXiv:2203.07593v2 [cs.LG] UPDATED)
147. Feature Distribution Matching for Federated Domain Generalization. (arXiv:2203.11635v3 [cs.LG] UPDATED)
148. A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v3 [cs.LG] UPDATED)
149. Zero-shot meta-learning for small-scale data from human subjects. (arXiv:2203.16309v3 [cs.LG] UPDATED)
150. PerfectDou: Dominating DouDizhu with Perfect Information Distillation. (arXiv:2203.16406v6 [cs.AI] UPDATED)
151. How Does Pre-trained Wav2Vec 2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications. (arXiv:2203.16822v2 [eess.AS] UPDATED)
152. Efficient Active Learning with Abstention. (arXiv:2204.00043v2 [stat.ML] UPDATED)
153. Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v2 [cs.IR] UPDATED)
154. Experimental Standards for Deep Learning in Natural Language Processing Research. (arXiv:2204.06251v2 [cs.LG] UPDATED)
155. BrainGB: A Benchmark for Brain Network Analysis with Graph Neural Networks. (arXiv:2204.07054v2 [q-bio.NC] UPDATED)
156. TabNAS: Rejection Sampling for Neural Architecture Search on Tabular Datasets. (arXiv:2204.07615v3 [cs.LG] UPDATED)
157. Accelerating Inhibitor Discovery With A Deep Generative Foundation Model: Validation for SARS-CoV-2 Drug Targets. (arXiv:2204.09042v3 [q-bio.QM] UPDATED)
158. A Fast Post-Training Pruning Framework for Transformers. (arXiv:2204.09656v2 [cs.CL] UPDATED)
159. Future Object Detection with Spatiotemporal Transformers. (arXiv:2204.10321v2 [cs.CV] UPDATED)
160. Domain Adaptation meets Individual Fairness. And they get along. (arXiv:2205.00504v2 [stat.ML] UPDATED)
161. Generalized Variational Inference in Function Spaces: Gaussian Measures meet Bayesian Deep Learning. (arXiv:2205.06342v2 [stat.ML] UPDATED)
162. Supervised Learning for Coverage-Directed Test Selection in Simulation-Based Verification. (arXiv:2205.08524v3 [cs.AR] UPDATED)
163. Neural Network Architecture Beyond Width and Depth. (arXiv:2205.09459v3 [cs.LG] UPDATED)
164. Hybrid Intelligent Testing in Simulation-Based Verification. (arXiv:2205.09552v3 [cs.AR] UPDATED)
165. Posterior Refinement Improves Sample Efficiency in Bayesian Neural Networks. (arXiv:2205.10041v2 [cs.LG] UPDATED)
166. Symmetry Teleportation for Accelerated Optimization. (arXiv:2205.10637v2 [cs.LG] UPDATED)
167. Robust Flow-based Conformal Inference (FCI) with Statistical Guarantee. (arXiv:2205.10732v2 [stat.ML] UPDATED)
168. GraB: Finding Provably Better Data Permutations than Random Reshuffling. (arXiv:2205.10733v2 [cs.LG] UPDATED)
169. How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts. (arXiv:2205.10762v2 [cs.CL] UPDATED)
170. Generalization Gap in Amortized Inference. (arXiv:2205.11640v2 [stat.ML] UPDATED)
171. Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v4 [cs.CV] UPDATED)
172. Certified Robustness Against Natural Language Attacks by Causal Intervention. (arXiv:2205.12331v3 [cs.LG] UPDATED)
173. Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v3 [cs.CV] UPDATED)
174. How Powerful are K-hop Message Passing Graph Neural Networks. (arXiv:2205.13328v3 [cs.LG] UPDATED)
175. BagFlip: A Certified Defense against Data Poisoning. (arXiv:2205.13634v2 [cs.LG] UPDATED)
176. Learning Dynamical Systems via Koopman Operator Regression in Reproducing Kernel Hilbert Spaces. (arXiv:2205.14027v2 [cs.LG] UPDATED)
177. Spartan: Differentiable Sparsity via Regularized Transportation. (arXiv:2205.14107v2 [cs.LG] UPDATED)
178. A Character-Level Length-Control Algorithm for Non-Autoregressive Sentence Summarization. (arXiv:2205.14522v2 [cs.CL] UPDATED)
179. Multi-Game Decision Transformers. (arXiv:2205.15241v2 [cs.AI] UPDATED)
180. Variational inference via Wasserstein gradient flows. (arXiv:2205.15902v2 [stat.ML] UPDATED)
181. FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation. (arXiv:2206.00050v2 [cs.LG] UPDATED)
182. Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning. (arXiv:2206.00518v3 [cs.LG] UPDATED)
183. Score-Based Generative Models Detect Manifolds. (arXiv:2206.01018v3 [stat.ML] UPDATED)
184. Sample-Efficient Reinforcement Learning of Partially Observable Markov Games. (arXiv:2206.01315v2 [cs.LG] UPDATED)
185. Rethinking and Scaling Up Graph Contrastive Learning: An Extremely Efficient Approach with Group Discrimination. (arXiv:2206.01535v2 [cs.LG] UPDATED)
186. Additive MIL: Intrinsically Interpretable Multiple Instance Learning for Pathology. (arXiv:2206.01794v2 [cs.CV] UPDATED)
187. Neural Lyapunov Control of Unknown Nonlinear Systems with Stability Guarantees. (arXiv:2206.01913v2 [eess.SY] UPDATED)
188. Causal Discovery in Heterogeneous Environments Under the Sparse Mechanism Shift Hypothesis. (arXiv:2206.02013v2 [cs.LG] UPDATED)
189. Active Bayesian Causal Inference. (arXiv:2206.02063v2 [cs.LG] UPDATED)
190. Spectral Bias Outside the Training Set for Deep Networks in the Kernel Regime. (arXiv:2206.02927v2 [stat.ML] UPDATED)
191. Receding Horizon Inverse Reinforcement Learning. (arXiv:2206.04477v2 [cs.LG] UPDATED)
192. An Empirical Study on Disentanglement of Negative-free Contrastive Learning. (arXiv:2206.04756v2 [cs.LG] UPDATED)
193. On Enforcing Better Conditioned Meta-Learning for Rapid Few-Shot Adaptation. (arXiv:2206.07260v2 [cs.LG] UPDATED)
194. CARD: Classification and Regression Diffusion Models. (arXiv:2206.07275v3 [stat.ML] UPDATED)
195. Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement Learning. (arXiv:2206.07376v2 [cs.LG] UPDATED)
196. Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity. (arXiv:2206.07659v2 [cs.LG] UPDATED)
197. A Unified Sequence Interface for Vision Tasks. (arXiv:2206.07669v2 [cs.CV] UPDATED)
198. On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v2 [cs.LG] UPDATED)
199. On Privacy and Personalization in Cross-Silo Federated Learning. (arXiv:2206.07902v2 [cs.LG] UPDATED)
200. Self-Supervised Contrastive Pre-Training For Time Series via Time-Frequency Consistency. (arXiv:2206.08496v3 [cs.LG] UPDATED)
201. Near-Optimal No-Regret Learning Dynamics for General Convex Games. (arXiv:2206.08742v3 [cs.GT] UPDATED)
202. TrafficFlowGAN: Physics-informed Flow based Generative Adversarial Network for Uncertainty Quantification. (arXiv:2206.09319v2 [cs.LG] UPDATED)
203. BOND: Benchmarking Unsupervised Outlier Node Detection on Static Attributed Graphs. (arXiv:2206.10071v2 [cs.LG] UPDATED)
204. SpA-Former: Transformer image shadow detection and removal via spatial attention. (arXiv:2206.10910v3 [cs.CV] UPDATED)
205. Ordered Subgraph Aggregation Networks. (arXiv:2206.11168v3 [cs.LG] UPDATED)
206. Topology-aware Generalization of Decentralized SGD. (arXiv:2206.12680v3 [cs.LG] UPDATED)
207. Distinguishing Learning Rules with Brain Machine Interfaces. (arXiv:2206.13448v2 [cs.NE] UPDATED)
208. When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning. (arXiv:2206.13464v2 [cs.LG] UPDATED)
209. No imputation without representation. (arXiv:2206.14254v2 [cs.LG] UPDATED)
210. Strong Lensing Source Reconstruction Using Continuous Neural Fields. (arXiv:2206.14820v2 [astro-ph.CO] UPDATED)
211. AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection. (arXiv:2206.15476v3 [cs.LG] UPDATED)
212. Adversarial Robustness is at Odds with Lazy Training. (arXiv:2207.00411v2 [cs.CR] UPDATED)
213. Multi-class Classification from Multiple Unlabeled Datasets with Partial Risk Regularization. (arXiv:2207.01555v2 [cs.LG] UPDATED)
214. Adapting to Online Label Shift with Provable Guarantees. (arXiv:2207.02121v2 [cs.LG] UPDATED)
215. ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal Self-Ensemble for Active Learning. (arXiv:2207.02182v2 [cs.CV] UPDATED)
216. The alignment property of SGD noise and how it helps select flat minima: A stability analysis. (arXiv:2207.02628v3 [stat.ML] UPDATED)
217. Improved conformalized quantile regression. (arXiv:2207.02808v5 [stat.ML] UPDATED)
218. Online Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods. (arXiv:2207.02829v3 [math.OC] UPDATED)
219. The Lepto-Variance of Stock Returns. (arXiv:2207.04867v2 [q-fin.ST] UPDATED)
220. URANUS: Radio Frequency Tracking, Classification and Identification of Unmanned Aircraft Vehicles. (arXiv:2207.06025v2 [cs.LG] UPDATED)
221. Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model. (arXiv:2207.06652v3 [cs.IR] UPDATED)
222. Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit. (arXiv:2207.08799v2 [cs.LG] UPDATED)
223. Gauge-equivariant flow models for sampling in lattice field theories with pseudofermions. (arXiv:2207.08945v3 [hep-lat] UPDATED)
224. Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v4 [cs.LG] UPDATED)
225. DC-BENCH: Dataset Condensation Benchmark. (arXiv:2207.09639v2 [cs.LG] UPDATED)
226. FOCUS: Fairness via Agent-Awareness for Federated Learning on Heterogeneous Data. (arXiv:2207.10265v2 [cs.LG] UPDATED)
227. PirouNet: Creating Dance through Artist-Centric Deep Learning. (arXiv:2207.12126v2 [cs.LG] UPDATED)
228. Domain Adaptation under Open Set Label Shift. (arXiv:2207.13048v2 [cs.LG] UPDATED)
229. A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v2 [stat.ML] UPDATED)
230. Disentangled Representation Learning for RF Fingerprint Extraction under Unknown Channel Statistics. (arXiv:2208.02724v2 [eess.SP] UPDATED)
231. Towards Sequence-Level Training for Visual Tracking. (arXiv:2208.05810v3 [cs.CV] UPDATED)
232. Markov Observation Models. (arXiv:2208.06368v2 [stat.ML] UPDATED)
233. Self-supervision is not magic: Understanding Data Augmentation in Image Anomaly Detection. (arXiv:2208.07734v4 [cs.LG] UPDATED)
234. Machine learning algorithms for three-dimensional mean-curvature computation in the level-set method. (arXiv:2208.09047v2 [cs.LG] UPDATED)
235. Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (arXiv:2208.10224v2 [cs.CR] UPDATED)
236. Multi-Modal Representation Learning with Self-Adaptive Threshold for Commodity Verification. (arXiv:2208.11064v4 [cs.LG] UPDATED)
237. Overparameterization from Computational Constraints. (arXiv:2208.12926v2 [cs.LG] UPDATED)
238. Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v2 [cs.LG] UPDATED)
239. Autoinverse: Uncertainty Aware Inversion of Neural Networks. (arXiv:2208.13780v2 [cs.LG] UPDATED)
240. Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v8 [cs.LG] UPDATED)
241. First Hitting Diffusion Models for Generating Manifold, Graph and Categorical Data. (arXiv:2209.01170v2 [cs.CV] UPDATED)
242. StreamNet: A WAE for White Matter Streamline Analysis. (arXiv:2209.01498v2 [q-bio.QM] UPDATED)
243. Rethinking Symmetric Matrix Factorization: A More General and Better Clustering Perspective. (arXiv:2209.02528v2 [cs.LG] UPDATED)
244. Improved Robust Algorithms for Learning with Discriminative Feature Feedback. (arXiv:2209.03753v2 [cs.LG] UPDATED)
245. IDIAPers @ Causal News Corpus 2022: Efficient Causal Relation Identification Through a Prompt-based Few-shot Approach. (arXiv:2209.03895v2 [cs.CL] UPDATED)
246. Gradient-Free Methods for Deterministic and Stochastic Nonsmooth Nonconvex Optimization. (arXiv:2209.05045v3 [math.OC] UPDATED)
247. Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models. (arXiv:2209.06970v2 [cs.CV] UPDATED)
248. Optimistic Curiosity Exploration and Conservative Exploitation with Linear Reward Shaping. (arXiv:2209.07288v2 [cs.LG] UPDATED)
249. Understanding Deep Neural Function Approximation in Reinforcement Learning via $\epsilon$-Greedy Exploration. (arXiv:2209.07376v2 [cs.LG] UPDATED)
250. Towards Healing the Blindness of Score Matching. (arXiv:2209.07396v2 [stat.ML] UPDATED)
251. Federated Coordinate Descent for Privacy-Preserving Multiparty Linear Regression. (arXiv:2209.07702v3 [cs.LG] UPDATED)
252. Extrapolation and Spectral Bias of Neural Nets with Hadamard Product: a Polynomial Net Study. (arXiv:2209.07736v2 [cs.LG] UPDATED)
253. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. (arXiv:2209.09513v2 [cs.CL] UPDATED)
254. Relational Reasoning via Set Transformers: Provable Efficiency and Applications to MARL. (arXiv:2209.09845v3 [cs.LG] UPDATED)
255. Off-Policy Evaluation for Episodic Partially Observable Markov Decision Processes under Non-Parametric Models. (arXiv:2209.10064v2 [stat.ML] UPDATED)
256. Evaluating Latent Space Robustness and Uncertainty of EEG-ML Models under Realistic Distribution Shifts. (arXiv:2209.11233v2 [eess.SP] UPDATED)
257. LEADER: Learning Attention over Driving Behaviors for Planning under Uncertainty. (arXiv:2209.11422v2 [cs.LG] UPDATED)
258. One-Shot Learning of Stochastic Differential Equations with Data Adapted Kernels. (arXiv:2209.12086v2 [stat.ML] UPDATED)
259. Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v2 [cs.SD] UPDATED)
260. PearNet: A Pearson Correlation-based Graph Attention Network for Sleep Stage Recognition. (arXiv:2209.13645v2 [eess.SP] UPDATED)
261. Deep Learning Based Detection of Enlarged Perivascular Spaces on Brain MRI. (arXiv:2209.13727v2 [eess.IV] UPDATED)
262. Graph Soft-Contrastive Learning via Neighborhood Ranking. (arXiv:2209.13964v2 [cs.LG] UPDATED)
263. Parameterized Quantum Circuits with Quantum Kernels for Machine Learning: A Hybrid Quantum-Classical Approach. (arXiv:2209.14449v2 [quant-ph] UPDATED)
264. E-Branchformer: Branchformer with Enhanced merging for speech recognition. (arXiv:2210.00077v2 [eess.AS] UPDATED)
265. Improved Algorithms for Neural Active Learning. (arXiv:2210.00423v2 [cs.LG] UPDATED)
266. Multi-objective Deep Data Generation with Correlated Property Control. (arXiv:2210.01796v3 [cs.LG] UPDATED)
267. Federated Learning with Server Learning: Enhancing Performance for Non-IID Data. (arXiv:2210.02614v2 [cs.LG] UPDATED)
268. Detecting Irregular Network Activity with Adversarial Learning and Expert Feedback. (arXiv:2210.02841v2 [cs.CR] UPDATED)
269. A Better Way to Decay: Proximal Gradient Training Algorithms for Neural Nets. (arXiv:2210.03069v2 [cs.LG] UPDATED)
270. A Unified Hard-Constraint Framework for Solving Geometrically Complex PDEs. (arXiv:2210.03526v3 [cs.LG] UPDATED)
271. Spectrally-Corrected and Regularized Linear Discriminant Analysis for Spiked Covariance Model. (arXiv:2210.03859v2 [stat.ML] UPDATED)
272. Sparse Teachers Can Be Dense with Knowledge. (arXiv:2210.03923v2 [cs.CL] UPDATED)
273. SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success. (arXiv:2210.04081v2 [cs.LG] UPDATED)
274. Is your noise correction noisy? PLS: Robustness to label noise with two stage detection. (arXiv:2210.04578v2 [cs.CV] UPDATED)
275. FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings. (arXiv:2210.04620v2 [cs.LG] UPDATED)
276. Neural Networks are Decision Trees. (arXiv:2210.05189v2 [cs.LG] UPDATED)
277. Planning Assembly Sequence with Graph Transformer. (arXiv:2210.05236v3 [cs.AI] UPDATED)
278. Visual Language Maps for Robot Navigation. (arXiv:2210.05714v3 [cs.RO] UPDATED)
279. Building Heterogeneous Cloud System for Machine Learning Inference. (arXiv:2210.05889v2 [cs.DC] UPDATED)
280. Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v3 [cs.CL] UPDATED)
281. Generalised Mutual Information for Discriminative Clustering. (arXiv:2210.06300v3 [stat.ML] UPDATED)
282. Betting the system: Using lineups to predict football scores. (arXiv:2210.06327v2 [cs.LG] UPDATED)
283. Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network. (arXiv:2210.06346v2 [cs.CL] UPDATED)
284. On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning. (arXiv:2210.06443v2 [cs.LG] UPDATED)
285. Find Your Friends: Personalized Federated Learning with the Right Collaborators. (arXiv:2210.06597v2 [cs.LG] UPDATED)
286. Brain Network Transformer. (arXiv:2210.06681v2 [cs.LG] UPDATED)
287. HoechstGAN: Virtual Lymphocyte Staining Using Generative Adversarial Networks. (arXiv:2210.06909v2 [cs.CV] UPDATED)
288. Self-explaining deep models with logic rule reasoning. (arXiv:2210.07024v2 [cs.AI] UPDATED)
289. PDEBENCH: An Extensive Benchmark for Scientific Machine Learning. (arXiv:2210.07182v2 [cs.LG] UPDATED)
290. Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v2 [cs.LG] UPDATED)
291. Federated Best Arm Identification with Heterogeneous Clients. (arXiv:2210.07780v2 [cs.LG] UPDATED)
292. Optimal AdaBoost Converges. (arXiv:2210.07808v2 [stat.ML] UPDATED)
293. SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of "Subjectivity" and "Identity Terms". (arXiv:2109.02691v1 [cs.CL] CROSS LISTED)
294. BOAT: **Bilateral** Local Attention Vision Transformer. (arXiv:2201.13027v1 [cs.CV] CROSS LISTED)
295. STaSy: Score-based Tabular data Synthesis. (arXiv:2210.04018v1 [cs.LG] CROSS LISTED)
296. ToupleGDD: A Fine-Designed Solution of Influence Maximization by Deep Reinforcement Learning. (arXiv:2210.07500v1 [cs.SI] CROSS LISTED)
## cs.AI
---
**130** new papers in cs.AI:-) 
1. Hierarchical Decentralized Deep Reinforcement Learning Architecture for a Simulated Four-Legged Agent. (arXiv:2210.08003v1 [cs.AI])
2. Knowledge acquisition via interactive Distributed Cognitive skill Modules. (arXiv:2210.08007v1 [cs.AI])
3. Inductive Logical Query Answering in Knowledge Graphs. (arXiv:2210.08008v1 [cs.AI])
4. Trajectory Prediction for Vehicle Conflict Identification at Intersections Using Sequence-to-Sequence Recurrent Neural Networks. (arXiv:2210.08009v1 [cs.AI])
5. Autoencoder based Anomaly Detection and Explained Fault Localization in Industrial Cooling Systems. (arXiv:2210.08011v1 [cs.LG])
6. On the Relationship Between Variational Inference and Auto-Associative Memory. (arXiv:2210.08013v1 [cs.LG])
7. Neural Attentive Circuits. (arXiv:2210.08031v1 [cs.LG])
8. Region2Vec: Community Detection on Spatial Networks Using Graph Embedding with Node Attributes and Spatial Interactions. (arXiv:2210.08041v1 [cs.SI])
9. Measuring Network Resilience via Geospatial Knowledge Graph: a Case Study of the US Multi-Commodity Flow Network. (arXiv:2210.08042v1 [cs.SI])
10. Eliciting Compatible Demonstrations for Multi-Human Imitation Learning. (arXiv:2210.08073v1 [cs.RO])
11. Adaptive patch foraging in deep reinforcement learning agents. (arXiv:2210.08085v1 [cs.AI])
12. Knowledge Distillation approach towards Melanoma Detection. (arXiv:2210.08086v1 [cs.CV])
13. Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning. (arXiv:2210.08090v1 [cs.LG])
14. Bayesian Spline Learning for Equation Discovery of Nonlinear Dynamics with Quantified Uncertainty. (arXiv:2210.08095v1 [cs.LG])
15. TestAug: A Framework for Augmenting Capability-based NLP Tests. (arXiv:2210.08097v1 [cs.SE])
16. High-resolution synthetic residential energy use profiles for the United States. (arXiv:2210.08103v1 [eess.SP])
17. Censored Deep Reinforcement Patrolling with Information Criterion for Monitoring Large Water Resources using Autonomous Surface Vehicles. (arXiv:2210.08115v1 [cs.RO])
18. A Low-cost Humanoid Prototype Intended to assist people with disability using Raspberry Pi. (arXiv:2210.08116v1 [cs.RO])
19. A Survey on Knowledge Graph-based Methods for Automated Driving. (arXiv:2210.08119v1 [cs.RO])
20. Towards a Fully Autonomous UAV Controller for Moving Platform Detection and Landing. (arXiv:2210.08120v1 [cs.RO])
21. TweetNERD -- End to End Entity Linking Benchmark for Tweets. (arXiv:2210.08129v1 [cs.CL])
22. VHetNets for AI and AI for VHetNets: An Anomaly Detection Case Study for Ubiquitous IoT. (arXiv:2210.08132v1 [cs.NI])
23. Pseudo AI Bias. (arXiv:2210.08141v1 [cs.AI])
24. CUP: Critic-Guided Policy Reuse. (arXiv:2210.08153v1 [cs.AI])
25. Self-supervised Graph Learning for Long-tailed Cognitive Diagnosis. (arXiv:2210.08169v1 [cs.CY])
26. Generating Synthetic Speech from SpokenVocab for Speech Translation. (arXiv:2210.08174v1 [cs.CL])
27. Learning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation. (arXiv:2210.08182v1 [cs.SD])
28. GFlowCausal: Generative Flow Networks for Causal Discovery. (arXiv:2210.08185v1 [cs.LG])
29. Machine Learning Approach for Predicting Students Academic Performance and Study Strategies based on their Motivation. (arXiv:2210.08186v1 [cs.LG])
30. Parameter-free Dynamic Graph Embedding for Link Prediction. (arXiv:2210.08189v1 [cs.LG])
31. Unit Selection: Learning Benefit Function from Finite Population Data. (arXiv:2210.08203v1 [cs.AI])
32. D.MCA: Outlier Detection with Explicit Micro-Cluster Assignments. (arXiv:2210.08212v1 [cs.LG])
33. PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale. (arXiv:2210.08217v1 [cs.RO])
34. Reinforcement Learning for ConnectX. (arXiv:2210.08263v1 [cs.AI])
35. AI-powered tiebreak mechanisms: An application to chess. (arXiv:2210.08289v1 [econ.TH])
36. A Secure Federated Data-Driven Evolutionary Multi-objective Optimization Algorithm. (arXiv:2210.08295v1 [cs.AI])
37. Improving Radiology Summarization with Radiograph and Anatomy Prompts. (arXiv:2210.08303v1 [cs.CV])
38. A Scalable Reinforcement Learning Approach for Attack Allocation in Swarm to Swarm Engagement Problems. (arXiv:2210.08319v1 [cs.RO])
39. A Policy-Guided Imitation Approach for Offline Reinforcement Learning. (arXiv:2210.08323v1 [cs.LG])
40. Learning Visual Robotic Control Efficiently with Contrastive Pre-training and Data Augmentation. (arXiv:2012.07975v3 [cs.RO] UPDATED)
41. Biologically Plausible Learning using GAIT-prop Scales to ImageNet. (arXiv:2102.11598v2 [cs.LG] UPDATED)
42. Pay attention to your loss: understanding misconceptions about 1-Lipschitz neural networks. (arXiv:2104.05097v6 [cs.LG] UPDATED)
43. Differentiable Model Compression via Pseudo Quantization Noise. (arXiv:2104.09987v3 [stat.ML] UPDATED)
44. 1xN Pattern for Pruning Convolutional Neural Networks. (arXiv:2105.14713v6 [cs.CV] UPDATED)
45. Multi-Domain Active Learning: Literature Review and Comparative Study. (arXiv:2106.13516v6 [cs.LG] UPDATED)
46. Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v5 [cs.LG] UPDATED)
47. Temporal Abstraction in Reinforcement Learning with the Successor Representation. (arXiv:2110.05740v2 [cs.LG] UPDATED)
48. Practical Benefits of Feature Feedback Under Distribution Shift. (arXiv:2110.07566v2 [cs.CL] UPDATED)
49. GenURL: A General Framework for Unsupervised Representation Learning. (arXiv:2110.14553v3 [cs.LG] UPDATED)
50. Free Probability for predicting the performance of feed-forward fully connected neural networks. (arXiv:2111.00841v3 [stat.ML] UPDATED)
51. Towards Axiomatic, Hierarchical, and Symbolic Explanation for Deep Models. (arXiv:2111.06206v4 [cs.LG] UPDATED)
52. Deep Semantic Manipulation of Facial Videos. (arXiv:2111.07902v2 [cs.CV] UPDATED)
53. TransMorph: Transformer for unsupervised medical image registration. (arXiv:2111.10480v6 [eess.IV] UPDATED)
54. Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples. (arXiv:2111.10962v3 [cs.CL] UPDATED)
55. Deep Policy Iteration with Integer Programming for Inventory Management. (arXiv:2112.02215v2 [cs.LG] UPDATED)
56. Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v2 [cs.CL] UPDATED)
57. AntBO: Towards Real-World Automated Antibody Design with Combinatorial Bayesian Optimisation. (arXiv:2201.12570v4 [q-bio.BM] UPDATED)
58. Locally Typical Sampling. (arXiv:2202.00666v4 [cs.CL] UPDATED)
59. ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization. (arXiv:2202.05599v2 [cs.CL] UPDATED)
60. End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking. (arXiv:2202.05826v3 [cs.LG] UPDATED)
61. Learning Predictions for Algorithms with Predictions. (arXiv:2202.09312v2 [cs.LG] UPDATED)
62. Influencing Long-Term Behavior in Multiagent Reinforcement Learning. (arXiv:2203.03535v4 [cs.LG] UPDATED)
63. LEMON: LanguagE ModeL for Negative Sampling of Knowledge Graph Embeddings. (arXiv:2203.04703v3 [cs.AI] UPDATED)
64. Distraction is All You Need for Fairness. (arXiv:2203.07593v2 [cs.LG] UPDATED)
65. Zero-shot meta-learning for small-scale data from human subjects. (arXiv:2203.16309v3 [cs.LG] UPDATED)
66. PerfectDou: Dominating DouDizhu with Perfect Information Distillation. (arXiv:2203.16406v6 [cs.AI] UPDATED)
67. Recent Advances and New Frontiers in Spiking Neural Networks. (arXiv:2204.07050v7 [cs.NE] UPDATED)
68. Domain Adaptation meets Individual Fairness. And they get along. (arXiv:2205.00504v2 [stat.ML] UPDATED)
69. Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v5 [cs.CL] UPDATED)
70. CSI-fingerprinting Indoor Localization via Attention-Augmented Residual Convolutional Neural Network. (arXiv:2205.05775v2 [eess.SP] UPDATED)
71. Supervised Learning for Coverage-Directed Test Selection in Simulation-Based Verification. (arXiv:2205.08524v3 [cs.AR] UPDATED)
72. Hybrid Intelligent Testing in Simulation-Based Verification. (arXiv:2205.09552v3 [cs.AR] UPDATED)
73. Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v3 [cs.CV] UPDATED)
74. How Powerful are K-hop Message Passing Graph Neural Networks. (arXiv:2205.13328v3 [cs.LG] UPDATED)
75. Experimental Design for Linear Functionals in Reproducing Kernel Hilbert Spaces. (arXiv:2205.13627v2 [cs.AI] UPDATED)
76. Learning to Find Proofs and Theorems by Learning to Refine Search Strategies: The Case of Loop Invariant Synthesis. (arXiv:2205.14229v3 [cs.AI] UPDATED)
77. Multi-Game Decision Transformers. (arXiv:2205.15241v2 [cs.AI] UPDATED)
78. Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning. (arXiv:2206.00518v3 [cs.LG] UPDATED)
79. Sample-Efficient Reinforcement Learning of Partially Observable Markov Games. (arXiv:2206.01315v2 [cs.LG] UPDATED)
80. Rethinking and Scaling Up Graph Contrastive Learning: An Extremely Efficient Approach with Group Discrimination. (arXiv:2206.01535v2 [cs.LG] UPDATED)
81. SNAKE: Shape-aware Neural 3D Keypoint Field. (arXiv:2206.01724v2 [cs.CV] UPDATED)
82. Causal Discovery in Heterogeneous Environments Under the Sparse Mechanism Shift Hypothesis. (arXiv:2206.02013v2 [cs.LG] UPDATED)
83. Active Bayesian Causal Inference. (arXiv:2206.02063v2 [cs.LG] UPDATED)
84. Receding Horizon Inverse Reinforcement Learning. (arXiv:2206.04477v2 [cs.LG] UPDATED)
85. An Empirical Study on Disentanglement of Negative-free Contrastive Learning. (arXiv:2206.04756v2 [cs.LG] UPDATED)
86. Exploring evolution-aware & -free protein language models as protein function predictors. (arXiv:2206.06583v2 [q-bio.QM] UPDATED)
87. Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement Learning. (arXiv:2206.07376v2 [cs.LG] UPDATED)
88. Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity. (arXiv:2206.07659v2 [cs.LG] UPDATED)
89. On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v2 [cs.LG] UPDATED)
90. Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline. (arXiv:2206.08129v2 [cs.CV] UPDATED)
91. Self-Supervised Contrastive Pre-Training For Time Series via Time-Frequency Consistency. (arXiv:2206.08496v3 [cs.LG] UPDATED)
92. Ordered Subgraph Aggregation Networks. (arXiv:2206.11168v3 [cs.LG] UPDATED)
93. Distinguishing Learning Rules with Brain Machine Interfaces. (arXiv:2206.13448v2 [cs.NE] UPDATED)
94. When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning. (arXiv:2206.13464v2 [cs.LG] UPDATED)
95. Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model. (arXiv:2207.06652v3 [cs.IR] UPDATED)
96. Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v4 [cs.LG] UPDATED)
97. DC-BENCH: Dataset Condensation Benchmark. (arXiv:2207.09639v2 [cs.LG] UPDATED)
98. Focused Decoding Enables 3D Anatomical Detection by Transformers. (arXiv:2207.10774v3 [cs.CV] UPDATED)
99. Shielding Federated Learning Systems against Inference Attacks with ARM TrustZone. (arXiv:2208.05895v4 [cs.CR] UPDATED)
100. What is it like to program with artificial intelligence?. (arXiv:2208.06213v2 [cs.HC] UPDATED)
101. Self-supervision is not magic: Understanding Data Augmentation in Image Anomaly Detection. (arXiv:2208.07734v4 [cs.LG] UPDATED)
102. Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v2 [cs.LG] UPDATED)
103. Autoinverse: Uncertainty Aware Inversion of Neural Networks. (arXiv:2208.13780v2 [cs.LG] UPDATED)
104. Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v8 [cs.LG] UPDATED)
105. Rethinking Symmetric Matrix Factorization: A More General and Better Clustering Perspective. (arXiv:2209.02528v2 [cs.LG] UPDATED)
106. IDIAPers @ Causal News Corpus 2022: Efficient Causal Relation Identification Through a Prompt-based Few-shot Approach. (arXiv:2209.03895v2 [cs.CL] UPDATED)
107. Optimistic Curiosity Exploration and Conservative Exploitation with Linear Reward Shaping. (arXiv:2209.07288v2 [cs.LG] UPDATED)
108. Multiscale Adaptive Scheduling and Path-Planning for Power-Constrained UAV-Relays via SMDPs. (arXiv:2209.07655v2 [eess.SY] UPDATED)
109. Extrapolation and Spectral Bias of Neural Nets with Hadamard Product: a Polynomial Net Study. (arXiv:2209.07736v2 [cs.LG] UPDATED)
110. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. (arXiv:2209.09513v2 [cs.CL] UPDATED)
111. Open-vocabulary Queryable Scene Representations for Real World Planning. (arXiv:2209.09874v2 [cs.RO] UPDATED)
112. Neural Networks Base on Power Method and Inverse Power Method for Solving Linear Eigenvalue Problems. (arXiv:2209.11134v3 [math.NA] UPDATED)
113. Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v2 [cs.SD] UPDATED)
114. Graph Soft-Contrastive Learning via Neighborhood Ranking. (arXiv:2209.13964v2 [cs.LG] UPDATED)
115. Programmable and Customized Intelligence for Traffic Steering in 5G Networks Using Open RAN Architectures. (arXiv:2209.14171v3 [cs.NI] UPDATED)
116. Simple Pooling Front-ends For Efficient Audio Classification. (arXiv:2210.00943v3 [eess.AS] UPDATED)
117. Multi-objective Deep Data Generation with Correlated Property Control. (arXiv:2210.01796v3 [cs.LG] UPDATED)
118. Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection. (arXiv:2210.04267v2 [cs.CL] UPDATED)
119. Planning Assembly Sequence with Graph Transformer. (arXiv:2210.05236v3 [cs.AI] UPDATED)
120. Visual Language Maps for Robot Navigation. (arXiv:2210.05714v3 [cs.RO] UPDATED)
121. Generalised Mutual Information for Discriminative Clustering. (arXiv:2210.06300v3 [stat.ML] UPDATED)
122. On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning. (arXiv:2210.06443v2 [cs.LG] UPDATED)
123. SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller. (arXiv:2210.06694v2 [cs.CL] UPDATED)
124. Re3: Generating Longer Stories With Recursive Reprompting and Revision. (arXiv:2210.06774v2 [cs.CL] UPDATED)
125. Self-explaining deep models with logic rule reasoning. (arXiv:2210.07024v2 [cs.AI] UPDATED)
126. A Relational Macrostate Theory Guides Artificial Intelligence to Learn Macro and Design Micro. (arXiv:2210.07374v2 [cs.AI] UPDATED)
127. Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v2 [cs.LG] UPDATED)
128. Learning to Autonomously Reach Objects with NICO and Grow-When-Required Networks. (arXiv:2210.07851v2 [cs.RO] UPDATED)
129. SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of "Subjectivity" and "Identity Terms". (arXiv:2109.02691v1 [cs.CL] CROSS LISTED)
130. STaSy: Score-based Tabular data Synthesis. (arXiv:2210.04018v1 [cs.LG] CROSS LISTED)

